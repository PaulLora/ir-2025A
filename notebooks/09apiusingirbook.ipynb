{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb15e9dd",
   "metadata": {},
   "source": [
    "Ejercicio 9: Usando API de Chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194b2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56518e83",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m client = OpenAI(api_key=\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrite a one-sentence bedtime story about a unicorn.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:735\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    704\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    734\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=\"key\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Users/paullora/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages (1.26.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ac2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(\"../data/irbookonlinereading.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecef500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '19',\n",
       " '49',\n",
       " '67',\n",
       " '85',\n",
       " '109',\n",
       " '135',\n",
       " '151',\n",
       " '177',\n",
       " '195',\n",
       " '219',\n",
       " '237',\n",
       " '253',\n",
       " '289',\n",
       " '319',\n",
       " '349',\n",
       " '377',\n",
       " '403',\n",
       " '421',\n",
       " '443',\n",
       " '461']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dict = doc[4]\n",
    "\n",
    "lines = book_dict.get_text(\"text\").split('\\n')\n",
    "\n",
    "lines = lines[7:]\n",
    "\n",
    "pages = [lines[i] for i in range(0, len(lines), 3)]\n",
    "pages\n",
    "# Iterate over each line\n",
    "# for line in lines:\n",
    "#     print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(\"../data/irbookonlinereading.pdf\")\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    if text:\n",
    "        corpus.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff72e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Online edition (c)\\n2009 Cambridge UP\\nAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nDraft of April 1, 2009\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAn\\nIntroduction\\nto\\nInformation\\nRetrieval\\nChristopher D. Manning\\nPrabhakar Raghavan\\nHinrich Schütze\\nCambridge University Press\\nCambridge, England\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT!\\nDO NOT DISTRIBUTE WITHOUT PRIOR PERMISSION\\n© 2009 Cambridge University Press\\nBy Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze\\nPrinted on April 1, 2009\\nWebsite: http://www.informationretrieval.org/\\nComments, corrections, and other feedback most welcome at:\\ninformationretrieval@yahoogroups.com\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nv\\nBrief Contents\\n1\\nBoolean retrieval\\n1\\n2\\nThe term vocabulary and postings lists\\n19\\n3\\nDictionaries and tolerant retrieval\\n49\\n4\\nIndex construction\\n67\\n5\\nIndex compression\\n85\\n6\\nScoring, term weighting and the vector space model\\n109\\n7\\nComputing scores in a complete search system\\n135\\n8\\nEvaluation in information retrieval\\n151\\n9\\nRelevance feedback and query expansion\\n177\\n10\\nXML retrieval\\n195\\n11\\nProbabilistic information retrieval\\n219\\n12\\nLanguage models for information retrieval\\n237\\n13\\nText classiﬁcation and Naive Bayes\\n253\\n14\\nVector space classiﬁcation\\n289\\n15\\nSupport vector machines and machine learning on documents\\n319\\n16\\nFlat clustering\\n349\\n17\\nHierarchical clustering\\n377\\n18\\nMatrix decompositions and latent semantic indexing\\n403\\n19\\nWeb search basics\\n421\\n20\\nWeb crawling and indexes\\n443\\n21\\nLink analysis\\n461\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nvii\\nContents\\nList of Tables\\nxv\\nList of Figures\\nxix\\nTable of Notation\\nxxvii\\nPreface\\nxxxi\\n1\\nBoolean retrieval\\n1\\n1.1\\nAn example information retrieval problem\\n3\\n1.2\\nA ﬁrst take at building an inverted index\\n6\\n1.3\\nProcessing Boolean queries\\n10\\n1.4\\nThe extended Boolean model versus ranked retrieval\\n14\\n1.5\\nReferences and further reading\\n17\\n2\\nThe term vocabulary and postings lists\\n19\\n2.1\\nDocument delineation and character sequence decoding\\n19\\n2.1.1\\nObtaining the character sequence in a document\\n19\\n2.1.2\\nChoosing a document unit\\n20\\n2.2\\nDetermining the vocabulary of terms\\n22\\n2.2.1\\nTokenization\\n22\\n2.2.2\\nDropping common terms: stop words\\n27\\n2.2.3\\nNormalization (equivalence classing of terms)\\n28\\n2.2.4\\nStemming and lemmatization\\n32\\n2.3\\nFaster postings list intersection via skip pointers\\n36\\n2.4\\nPositional postings and phrase queries\\n39\\n2.4.1\\nBiword indexes\\n39\\n2.4.2\\nPositional indexes\\n41\\n2.4.3\\nCombination schemes\\n43\\n2.5\\nReferences and further reading\\n45\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nviii\\nContents\\n3\\nDictionaries and tolerant retrieval\\n49\\n3.1\\nSearch structures for dictionaries\\n49\\n3.2\\nWildcard queries\\n51\\n3.2.1\\nGeneral wildcard queries\\n53\\n3.2.2\\nk-gram indexes for wildcard queries\\n54\\n3.3\\nSpelling correction\\n56\\n3.3.1\\nImplementing spelling correction\\n57\\n3.3.2\\nForms of spelling correction\\n57\\n3.3.3\\nEdit distance\\n58\\n3.3.4\\nk-gram indexes for spelling correction\\n60\\n3.3.5\\nContext sensitive spelling correction\\n62\\n3.4\\nPhonetic correction\\n63\\n3.5\\nReferences and further reading\\n65\\n4\\nIndex construction\\n67\\n4.1\\nHardware basics\\n68\\n4.2\\nBlocked sort-based indexing\\n69\\n4.3\\nSingle-pass in-memory indexing\\n73\\n4.4\\nDistributed indexing\\n74\\n4.5\\nDynamic indexing\\n78\\n4.6\\nOther types of indexes\\n80\\n4.7\\nReferences and further reading\\n83\\n5\\nIndex compression\\n85\\n5.1\\nStatistical properties of terms in information retrieval\\n86\\n5.1.1\\nHeaps’ law: Estimating the number of terms\\n88\\n5.1.2\\nZipf’s law: Modeling the distribution of terms\\n89\\n5.2\\nDictionary compression\\n90\\n5.2.1\\nDictionary as a string\\n91\\n5.2.2\\nBlocked storage\\n92\\n5.3\\nPostings ﬁle compression\\n95\\n5.3.1\\nVariable byte codes\\n96\\n5.3.2\\nγ codes\\n98\\n5.4\\nReferences and further reading\\n105\\n6\\nScoring, term weighting and the vector space model\\n109\\n6.1\\nParametric and zone indexes\\n110\\n6.1.1\\nWeighted zone scoring\\n112\\n6.1.2\\nLearning weights\\n113\\n6.1.3\\nThe optimal weight g\\n115\\n6.2\\nTerm frequency and weighting\\n117\\n6.2.1\\nInverse document frequency\\n117\\n6.2.2\\nTf-idf weighting\\n118\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nContents\\nix\\n6.3\\nThe vector space model for scoring\\n120\\n6.3.1\\nDot products\\n120\\n6.3.2\\nQueries as vectors\\n123\\n6.3.3\\nComputing vector scores\\n124\\n6.4\\nVariant tf-idf functions\\n126\\n6.4.1\\nSublinear tf scaling\\n126\\n6.4.2\\nMaximum tf normalization\\n127\\n6.4.3\\nDocument and query weighting schemes\\n128\\n6.4.4\\nPivoted normalized document length\\n129\\n6.5\\nReferences and further reading\\n133\\n7\\nComputing scores in a complete search system\\n135\\n7.1\\nEfﬁcient scoring and ranking\\n135\\n7.1.1\\nInexact top K document retrieval\\n137\\n7.1.2\\nIndex elimination\\n137\\n7.1.3\\nChampion lists\\n138\\n7.1.4\\nStatic quality scores and ordering\\n138\\n7.1.5\\nImpact ordering\\n140\\n7.1.6\\nCluster pruning\\n141\\n7.2\\nComponents of an information retrieval system\\n143\\n7.2.1\\nTiered indexes\\n143\\n7.2.2\\nQuery-term proximity\\n144\\n7.2.3\\nDesigning parsing and scoring functions\\n145\\n7.2.4\\nPutting it all together\\n146\\n7.3\\nVector space scoring and query operator interaction\\n147\\n7.4\\nReferences and further reading\\n149\\n8\\nEvaluation in information retrieval\\n151\\n8.1\\nInformation retrieval system evaluation\\n152\\n8.2\\nStandard test collections\\n153\\n8.3\\nEvaluation of unranked retrieval sets\\n154\\n8.4\\nEvaluation of ranked retrieval results\\n158\\n8.5\\nAssessing relevance\\n164\\n8.5.1\\nCritiques and justiﬁcations of the concept of\\nrelevance\\n166\\n8.6\\nA broader perspective: System quality and user utility\\n168\\n8.6.1\\nSystem issues\\n168\\n8.6.2\\nUser utility\\n169\\n8.6.3\\nReﬁning a deployed system\\n170\\n8.7\\nResults snippets\\n170\\n8.8\\nReferences and further reading\\n173\\n9\\nRelevance feedback and query expansion\\n177\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nx\\nContents\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n178\\n9.1.1\\nThe Rocchio algorithm for relevance feedback\\n178\\n9.1.2\\nProbabilistic relevance feedback\\n183\\n9.1.3\\nWhen does relevance feedback work?\\n183\\n9.1.4\\nRelevance feedback on the web\\n185\\n9.1.5\\nEvaluation of relevance feedback strategies\\n186\\n9.1.6\\nPseudo relevance feedback\\n187\\n9.1.7\\nIndirect relevance feedback\\n187\\n9.1.8\\nSummary\\n188\\n9.2\\nGlobal methods for query reformulation\\n189\\n9.2.1\\nVocabulary tools for query reformulation\\n189\\n9.2.2\\nQuery expansion\\n189\\n9.2.3\\nAutomatic thesaurus generation\\n192\\n9.3\\nReferences and further reading\\n193\\n10 XML retrieval\\n195\\n10.1\\nBasic XML concepts\\n197\\n10.2\\nChallenges in XML retrieval\\n201\\n10.3\\nA vector space model for XML retrieval\\n206\\n10.4\\nEvaluation of XML retrieval\\n210\\n10.5\\nText-centric vs. data-centric XML retrieval\\n214\\n10.6\\nReferences and further reading\\n216\\n10.7\\nExercises\\n217\\n11 Probabilistic information retrieval\\n219\\n11.1\\nReview of basic probability theory\\n220\\n11.2\\nThe Probability Ranking Principle\\n221\\n11.2.1\\nThe 1/0 loss case\\n221\\n11.2.2\\nThe PRP with retrieval costs\\n222\\n11.3\\nThe Binary Independence Model\\n222\\n11.3.1\\nDeriving a ranking function for query terms\\n224\\n11.3.2\\nProbability estimates in theory\\n226\\n11.3.3\\nProbability estimates in practice\\n227\\n11.3.4\\nProbabilistic approaches to relevance feedback\\n228\\n11.4\\nAn appraisal and some extensions\\n230\\n11.4.1\\nAn appraisal of probabilistic models\\n230\\n11.4.2\\nTree-structured dependencies between terms\\n231\\n11.4.3\\nOkapi BM25: a non-binary model\\n232\\n11.4.4\\nBayesian network approaches to IR\\n234\\n11.5\\nReferences and further reading\\n235\\n12 Language models for information retrieval\\n237\\n12.1\\nLanguage models\\n237\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nContents\\nxi\\n12.1.1\\nFinite automata and language models\\n237\\n12.1.2\\nTypes of language models\\n240\\n12.1.3\\nMultinomial distributions over words\\n241\\n12.2\\nThe query likelihood model\\n242\\n12.2.1\\nUsing query likelihood language models in IR\\n242\\n12.2.2\\nEstimating the query generation probability\\n243\\n12.2.3\\nPonte and Croft’s Experiments\\n246\\n12.3\\nLanguage modeling versus other approaches in IR\\n248\\n12.4\\nExtended language modeling approaches\\n250\\n12.5\\nReferences and further reading\\n252\\n13 Text classiﬁcation and Naive Bayes\\n253\\n13.1\\nThe text classiﬁcation problem\\n256\\n13.2\\nNaive Bayes text classiﬁcation\\n258\\n13.2.1\\nRelation to multinomial unigram language model\\n262\\n13.3\\nThe Bernoulli model\\n263\\n13.4\\nProperties of Naive Bayes\\n265\\n13.4.1\\nA variant of the multinomial model\\n270\\n13.5\\nFeature selection\\n271\\n13.5.1\\nMutual information\\n272\\n13.5.2\\nχ2 Feature selection\\n275\\n13.5.3\\nFrequency-based feature selection\\n277\\n13.5.4\\nFeature selection for multiple classiﬁers\\n278\\n13.5.5\\nComparison of feature selection methods\\n278\\n13.6\\nEvaluation of text classiﬁcation\\n279\\n13.7\\nReferences and further reading\\n286\\n14 Vector space classiﬁcation\\n289\\n14.1\\nDocument representations and measures of relatedness in\\nvector spaces\\n291\\n14.2\\nRocchio classiﬁcation\\n292\\n14.3\\nk nearest neighbor\\n297\\n14.3.1\\nTime complexity and optimality of kNN\\n299\\n14.4\\nLinear versus nonlinear classiﬁers\\n301\\n14.5\\nClassiﬁcation with more than two classes\\n306\\n14.6\\nThe bias-variance tradeoff\\n308\\n14.7\\nReferences and further reading\\n314\\n14.8\\nExercises\\n315\\n15 Support vector machines and machine learning on documents\\n319\\n15.1\\nSupport vector machines: The linearly separable case\\n320\\n15.2\\nExtensions to the SVM model\\n327\\n15.2.1\\nSoft margin classiﬁcation\\n327\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxii\\nContents\\n15.2.2\\nMulticlass SVMs\\n330\\n15.2.3\\nNonlinear SVMs\\n330\\n15.2.4\\nExperimental results\\n333\\n15.3\\nIssues in the classiﬁcation of text documents\\n334\\n15.3.1\\nChoosing what kind of classiﬁer to use\\n335\\n15.3.2\\nImproving classiﬁer performance\\n337\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n341\\n15.4.1\\nA simple example of machine-learned scoring\\n341\\n15.4.2\\nResult ranking by machine learning\\n344\\n15.5\\nReferences and further reading\\n346\\n16 Flat clustering\\n349\\n16.1\\nClustering in information retrieval\\n350\\n16.2\\nProblem statement\\n354\\n16.2.1\\nCardinality – the number of clusters\\n355\\n16.3\\nEvaluation of clustering\\n356\\n16.4\\nK-means\\n360\\n16.4.1\\nCluster cardinality in K-means\\n365\\n16.5\\nModel-based clustering\\n368\\n16.6\\nReferences and further reading\\n372\\n16.7\\nExercises\\n374\\n17 Hierarchical clustering\\n377\\n17.1\\nHierarchical agglomerative clustering\\n378\\n17.2\\nSingle-link and complete-link clustering\\n382\\n17.2.1\\nTime complexity of HAC\\n385\\n17.3\\nGroup-average agglomerative clustering\\n388\\n17.4\\nCentroid clustering\\n391\\n17.5\\nOptimality of HAC\\n393\\n17.6\\nDivisive clustering\\n395\\n17.7\\nCluster labeling\\n396\\n17.8\\nImplementation notes\\n398\\n17.9\\nReferences and further reading\\n399\\n17.10 Exercises\\n401\\n18 Matrix decompositions and latent semantic indexing\\n403\\n18.1\\nLinear algebra review\\n403\\n18.1.1\\nMatrix decompositions\\n406\\n18.2\\nTerm-document matrices and singular value\\ndecompositions\\n407\\n18.3\\nLow-rank approximations\\n410\\n18.4\\nLatent semantic indexing\\n412\\n18.5\\nReferences and further reading\\n417\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nContents\\nxiii\\n19 Web search basics\\n421\\n19.1\\nBackground and history\\n421\\n19.2\\nWeb characteristics\\n423\\n19.2.1\\nThe web graph\\n425\\n19.2.2\\nSpam\\n427\\n19.3\\nAdvertising as the economic model\\n429\\n19.4\\nThe search user experience\\n432\\n19.4.1\\nUser query needs\\n432\\n19.5\\nIndex size and estimation\\n433\\n19.6\\nNear-duplicates and shingling\\n437\\n19.7\\nReferences and further reading\\n441\\n20 Web crawling and indexes\\n443\\n20.1\\nOverview\\n443\\n20.1.1\\nFeatures a crawler must provide\\n443\\n20.1.2\\nFeatures a crawler should provide\\n444\\n20.2\\nCrawling\\n444\\n20.2.1\\nCrawler architecture\\n445\\n20.2.2\\nDNS resolution\\n449\\n20.2.3\\nThe URL frontier\\n451\\n20.3\\nDistributing indexes\\n454\\n20.4\\nConnectivity servers\\n455\\n20.5\\nReferences and further reading\\n458\\n21 Link analysis\\n461\\n21.1\\nThe Web as a graph\\n462\\n21.1.1\\nAnchor text and the web graph\\n462\\n21.2\\nPageRank\\n464\\n21.2.1\\nMarkov chains\\n465\\n21.2.2\\nThe PageRank computation\\n468\\n21.2.3\\nTopic-speciﬁc PageRank\\n471\\n21.3\\nHubs and Authorities\\n474\\n21.3.1\\nChoosing the subset of the Web\\n477\\n21.4\\nReferences and further reading\\n480\\nBibliography\\n483\\nAuthor Index\\n519\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nxv\\nList of Tables\\n4.1\\nTypical system parameters in 2007. The seek time is the time\\nneeded to position the disk head in a new position. The\\ntransfer time per byte is the rate of transfer from disk to\\nmemory when the head is in the right position.\\n68\\n4.2\\nCollection statistics for Reuters-RCV1. Values are rounded for\\nthe computations in this book. The unrounded values are:\\n806,791 documents, 222 tokens per document, 391,523\\n(distinct) terms, 6.04 bytes per token with spaces and\\npunctuation, 4.5 bytes per token without spaces and\\npunctuation, 7.5 bytes per term, and 96,969,056 tokens. The\\nnumbers in this table correspond to the third line (“case\\nfolding”) in Table 5.1 (page 87).\\n70\\n4.3\\nThe ﬁve steps in constructing an index for Reuters-RCV1 in\\nblocked sort-based indexing. Line numbers refer to Figure 4.2.\\n82\\n4.4\\nCollection statistics for a large collection.\\n82\\n5.1\\nThe effect of preprocessing on the number of terms,\\nnonpositional postings, and tokens for Reuters-RCV1. “∆%”\\nindicates the reduction in size from the previous line, except\\nthat “30 stop words” and “150 stop words” both use “case\\nfolding” as their reference line. “T%” is the cumulative\\n(“total”) reduction from unﬁltered. We performed stemming\\nwith the Porter stemmer (Chapter 2, page 33).\\n87\\n5.2\\nDictionary compression for Reuters-RCV1.\\n95\\n5.3\\nEncoding gaps instead of document IDs. For example, we\\nstore gaps 107, 5, 43, ..., instead of docIDs 283154, 283159,\\n283202, ... for computer. The ﬁrst docID is left unchanged\\n(only shown for arachnocentric).\\n96\\n5.4\\nVB encoding.\\n97\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxvi\\nList of Tables\\n5.5\\nSome examples of unary and γ codes. Unary codes are only\\nshown for the smaller numbers. Commas in γ codes are for\\nreadability only and are not part of the actual codes.\\n98\\n5.6\\nIndex and dictionary compression for Reuters-RCV1. The\\ncompression ratio depends on the proportion of actual text in\\nthe collection. Reuters-RCV1 contains a large amount of XML\\nmarkup. Using the two best compression schemes, γ\\nencoding and blocking with front coding, the ratio\\ncompressed index to collection size is therefore especially\\nsmall for Reuters-RCV1: (101 + 5.9)/3600 ≈0.03.\\n103\\n5.7\\nTwo gap sequences to be merged in blocked sort-based\\nindexing\\n105\\n6.1\\nCosine computation for Exercise 6.19.\\n132\\n8.1\\nCalculation of 11-point Interpolated Average Precision.\\n159\\n8.2\\nCalculating the kappa statistic.\\n165\\n10.1\\nRDB (relational database) search, unstructured information\\nretrieval and structured information retrieval.\\n196\\n10.2\\nINEX 2002 collection statistics.\\n211\\n10.3\\nINEX 2002 results of the vector space model in Section 10.3 for\\ncontent-and-structure (CAS) queries and the quantization\\nfunction Q.\\n213\\n10.4\\nA comparison of content-only and full-structure search in\\nINEX 2003/2004.\\n214\\n13.1\\nData for parameter estimation examples.\\n261\\n13.2\\nTraining and test times for NB.\\n261\\n13.3\\nMultinomial versus Bernoulli model.\\n268\\n13.4\\nCorrect estimation implies accurate prediction, but accurate\\nprediction does not imply correct estimation.\\n269\\n13.5\\nA set of documents for which the NB independence\\nassumptions are problematic.\\n270\\n13.6\\nCritical values of the χ2 distribution with one degree of\\nfreedom. For example, if the two events are independent,\\nthen P(X2 > 6.63) < 0.01. So for X2 > 6.63 the assumption of\\nindependence can be rejected with 99% conﬁdence.\\n277\\n13.7\\nThe ten largest classes in the Reuters-21578 collection with\\nnumber of documents in training and test sets.\\n280\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nList of Tables\\nxvii\\n13.8\\nMacro- and microaveraging. “Truth” is the true class and\\n“call” the decision of the classiﬁer. In this example,\\nmacroaveraged precision is\\n[10/(10 + 10) + 90/(10 + 90)]/2 = (0.5 + 0.9)/2 = 0.7.\\nMicroaveraged precision is 100/(100 + 20) ≈0.83.\\n282\\n13.9\\nText classiﬁcation effectiveness numbers on Reuters-21578 for\\nF1 (in percent). Results from Li and Yang (2003) (a), Joachims\\n(1998) (b: kNN) and Dumais et al. (1998) (b: NB, Rocchio,\\ntrees, SVM).\\n282\\n13.10\\nData for parameter estimation exercise.\\n284\\n14.1\\nVectors and class centroids for the data in Table 13.1.\\n294\\n14.2\\nTraining and test times for Rocchio classiﬁcation.\\n296\\n14.3\\nTraining and test times for kNN classiﬁcation.\\n299\\n14.4\\nA linear classiﬁer.\\n303\\n14.5\\nA confusion matrix for Reuters-21578.\\n308\\n15.1\\nTraining and testing complexity of various classiﬁers\\nincluding SVMs.\\n329\\n15.2\\nSVM classiﬁer break-even F1 from (Joachims 2002a, p. 114).\\n334\\n15.3\\nTraining examples for machine-learned scoring.\\n342\\n16.1\\nSome applications of clustering in information retrieval.\\n351\\n16.2\\nThe four external evaluation measures applied to the\\nclustering in Figure 16.4.\\n357\\n16.3\\nThe EM clustering algorithm.\\n371\\n17.1\\nComparison of HAC algorithms.\\n395\\n17.2\\nAutomatically computed cluster labels.\\n397\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nxix\\nList of Figures\\n1.1\\nA term-document incidence matrix.\\n4\\n1.2\\nResults from Shakespeare for the query Brutus AND Caesar\\nAND NOT Calpurnia.\\n5\\n1.3\\nThe two parts of an inverted index.\\n7\\n1.4\\nBuilding an index by sorting and grouping.\\n8\\n1.5\\nIntersecting the postings lists for Brutus and Calpurnia from\\nFigure 1.3.\\n10\\n1.6\\nAlgorithm for the intersection of two postings lists p1 and p2.\\n11\\n1.7\\nAlgorithm for conjunctive queries that returns the set of\\ndocuments containing each term in the input list of terms.\\n12\\n2.1\\nAn example of a vocalized Modern Standard Arabic word.\\n21\\n2.2\\nThe conceptual linear order of characters is not necessarily the\\norder that you see on the page.\\n21\\n2.3\\nThe standard unsegmented form of Chinese text using the\\nsimpliﬁed characters of mainland China.\\n26\\n2.4\\nAmbiguities in Chinese word segmentation.\\n26\\n2.5\\nA stop list of 25 semantically non-selective words which are\\ncommon in Reuters-RCV1.\\n26\\n2.6\\nAn example of how asymmetric expansion of query terms can\\nusefully model users’ expectations.\\n28\\n2.7\\nJapanese makes use of multiple intermingled writing systems\\nand, like Chinese, does not segment words.\\n31\\n2.8\\nA comparison of three stemming algorithms on a sample text.\\n34\\n2.9\\nPostings lists with skip pointers.\\n36\\n2.10\\nPostings lists intersection with skip pointers.\\n37\\n2.11\\nPositional index example.\\n41\\n2.12\\nAn algorithm for proximity intersection of postings lists p1\\nand p2.\\n42\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxx\\nList of Figures\\n3.1\\nA binary search tree.\\n51\\n3.2\\nA B-tree.\\n52\\n3.3\\nA portion of a permuterm index.\\n54\\n3.4\\nExample of a postings list in a 3-gram index.\\n55\\n3.5\\nDynamic programming algorithm for computing the edit\\ndistance between strings s1 and s2.\\n59\\n3.6\\nExample Levenshtein distance computation.\\n59\\n3.7\\nMatching at least two of the three 2-grams in the query bord.\\n61\\n4.1\\nDocument from the Reuters newswire.\\n70\\n4.2\\nBlocked sort-based indexing.\\n71\\n4.3\\nMerging in blocked sort-based indexing.\\n72\\n4.4\\nInversion of a block in single-pass in-memory indexing\\n73\\n4.5\\nAn example of distributed indexing with MapReduce.\\nAdapted from Dean and Ghemawat (2004).\\n76\\n4.6\\nMap and reduce functions in MapReduce.\\n77\\n4.7\\nLogarithmic merging. Each token (termID,docID) is initially\\nadded to in-memory index Z0 by LMERGEADDTOKEN.\\nLOGARITHMICMERGE initializes Z0 and indexes.\\n79\\n4.8\\nA user-document matrix for access control lists. Element (i, j)\\nis 1 if user i has access to document j and 0 otherwise. During\\nquery processing, a user’s access postings list is intersected\\nwith the results list returned by the text part of the index.\\n81\\n5.1\\nHeaps’ law.\\n88\\n5.2\\nZipf’s law for Reuters-RCV1.\\n90\\n5.3\\nStoring the dictionary as an array of ﬁxed-width entries.\\n91\\n5.4\\nDictionary-as-a-string storage.\\n92\\n5.5\\nBlocked storage with four terms per block.\\n93\\n5.6\\nSearch of the uncompressed dictionary (a) and a dictionary\\ncompressed by blocking with k = 4 (b).\\n94\\n5.7\\nFront coding.\\n94\\n5.8\\nVB encoding and decoding.\\n97\\n5.9\\nEntropy H(P) as a function of P(x1) for a sample space with\\ntwo outcomes x1 and x2.\\n100\\n5.10\\nStratiﬁcation of terms for estimating the size of a γ encoded\\ninverted index.\\n102\\n6.1\\nParametric search.\\n111\\n6.2\\nBasic zone index\\n111\\n6.3\\nZone index in which the zone is encoded in the postings\\nrather than the dictionary.\\n111\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nList of Figures\\nxxi\\n6.4\\nAlgorithm for computing the weighted zone score from two\\npostings lists.\\n113\\n6.5\\nAn illustration of training examples.\\n115\\n6.6\\nThe four possible combinations of sT and sB.\\n115\\n6.7\\nCollection frequency (cf) and document frequency (df) behave\\ndifferently, as in this example from the Reuters collection.\\n118\\n6.8\\nExample of idf values.\\n119\\n6.9\\nTable of tf values for Exercise 6.10.\\n120\\n6.10\\nCosine similarity illustrated.\\n121\\n6.11\\nEuclidean normalized tf values for documents in Figure 6.9.\\n122\\n6.12\\nTerm frequencies in three novels.\\n122\\n6.13\\nTerm vectors for the three novels of Figure 6.12.\\n123\\n6.14\\nThe basic algorithm for computing vector space scores.\\n125\\n6.15\\nSMART notation for tf-idf variants.\\n128\\n6.16\\nPivoted document length normalization.\\n130\\n6.17\\nImplementing pivoted document length normalization by\\nlinear scaling.\\n131\\n7.1\\nA faster algorithm for vector space scores.\\n136\\n7.2\\nA static quality-ordered index.\\n139\\n7.3\\nCluster pruning.\\n142\\n7.4\\nTiered indexes.\\n144\\n7.5\\nA complete search system.\\n147\\n8.1\\nGraph comparing the harmonic mean to other means.\\n157\\n8.2\\nPrecision/recall graph.\\n158\\n8.3\\nAveraged 11-point precision/recall graph across 50 queries\\nfor a representative TREC system.\\n160\\n8.4\\nThe ROC curve corresponding to the precision-recall curve in\\nFigure 8.2.\\n162\\n8.5\\nAn example of selecting text for a dynamic snippet.\\n172\\n9.1\\nRelevance feedback searching over images.\\n179\\n9.2\\nExample of relevance feedback on a text collection.\\n180\\n9.3\\nThe Rocchio optimal query for separating relevant and\\nnonrelevant documents.\\n181\\n9.4\\nAn application of Rocchio’s algorithm.\\n182\\n9.5\\nResults showing pseudo relevance feedback greatly\\nimproving performance.\\n187\\n9.6\\nAn example of query expansion in the interface of the Yahoo!\\nweb search engine in 2006.\\n190\\n9.7\\nExamples of query expansion via the PubMed thesaurus.\\n191\\n9.8\\nAn example of an automatically generated thesaurus.\\n192\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxii\\nList of Figures\\n10.1\\nAn XML document.\\n198\\n10.2\\nThe XML document in Figure 10.1 as a simpliﬁed DOM object.\\n198\\n10.3\\nAn XML query in NEXI format and its partial representation\\nas a tree.\\n199\\n10.4\\nTree representation of XML documents and queries.\\n200\\n10.5\\nPartitioning an XML document into non-overlapping\\nindexing units.\\n202\\n10.6\\nSchema heterogeneity: intervening nodes and mismatched\\nnames.\\n204\\n10.7\\nA structural mismatch between two queries and a document.\\n206\\n10.8\\nA mapping of an XML document (left) to a set of lexicalized\\nsubtrees (right).\\n207\\n10.9\\nThe algorithm for scoring documents with SIMNOMERGE.\\n209\\n10.10\\nScoring of a query with one structural term in SIMNOMERGE.\\n209\\n10.11\\nSimpliﬁed schema of the documents in the INEX collection.\\n211\\n11.1\\nA tree of dependencies between terms.\\n232\\n12.1\\nA simple ﬁnite automaton and some of the strings in the\\nlanguage it generates.\\n238\\n12.2\\nA one-state ﬁnite automaton that acts as a unigram language\\nmodel.\\n238\\n12.3\\nPartial speciﬁcation of two unigram language models.\\n239\\n12.4\\nResults of a comparison of tf-idf with language modeling\\n(LM) term weighting by Ponte and Croft (1998).\\n247\\n12.5\\nThree ways of developing the language modeling approach:\\n(a) query likelihood, (b) document likelihood, and (c) model\\ncomparison.\\n250\\n13.1\\nClasses, training set, and test set in text classiﬁcation .\\n257\\n13.2\\nNaive Bayes algorithm (multinomial model): Training and\\ntesting.\\n260\\n13.3\\nNB algorithm (Bernoulli model): Training and testing.\\n263\\n13.4\\nThe multinomial NB model.\\n266\\n13.5\\nThe Bernoulli NB model.\\n267\\n13.6\\nBasic feature selection algorithm for selecting the k best features. 271\\n13.7\\nFeatures with high mutual information scores for six\\nReuters-RCV1 classes.\\n274\\n13.8\\nEffect of feature set size on accuracy for multinomial and\\nBernoulli models.\\n275\\n13.9\\nA sample document from the Reuters-21578 collection.\\n281\\n14.1\\nVector space classiﬁcation into three classes.\\n290\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nList of Figures\\nxxiii\\n14.2\\nProjections of small areas of the unit sphere preserve distances.\\n291\\n14.3\\nRocchio classiﬁcation.\\n293\\n14.4\\nRocchio classiﬁcation: Training and testing.\\n295\\n14.5\\nThe multimodal class “a” consists of two different clusters\\n(small upper circles centered on X’s).\\n295\\n14.6\\nVoronoi tessellation and decision boundaries (double lines) in\\n1NN classiﬁcation.\\n297\\n14.7\\nkNN training (with preprocessing) and testing.\\n298\\n14.8\\nThere are an inﬁnite number of hyperplanes that separate two\\nlinearly separable classes.\\n301\\n14.9\\nLinear classiﬁcation algorithm.\\n302\\n14.10\\nA linear problem with noise.\\n304\\n14.11\\nA nonlinear problem.\\n305\\n14.12\\nJ hyperplanes do not divide space into J disjoint regions.\\n307\\n14.13\\nArithmetic transformations for the bias-variance decomposition. 310\\n14.14\\nExample for differences between Euclidean distance, dot\\nproduct similarity and cosine similarity.\\n316\\n14.15\\nA simple non-separable set of points.\\n317\\n15.1\\nThe support vectors are the 5 points right up against the\\nmargin of the classiﬁer.\\n320\\n15.2\\nAn intuition for large-margin classiﬁcation.\\n321\\n15.3\\nThe geometric margin of a point (r) and a decision boundary (ρ). 323\\n15.4\\nA tiny 3 data point training set for an SVM.\\n325\\n15.5\\nLarge margin classiﬁcation with slack variables.\\n327\\n15.6\\nProjecting data that is not linearly separable into a higher\\ndimensional space can make it linearly separable.\\n331\\n15.7\\nA collection of training examples.\\n343\\n16.1\\nAn example of a data set with a clear cluster structure.\\n349\\n16.2\\nClustering of search results to improve recall.\\n352\\n16.3\\nAn example of a user session in Scatter-Gather.\\n353\\n16.4\\nPurity as an external evaluation criterion for cluster quality.\\n357\\n16.5\\nThe K-means algorithm.\\n361\\n16.6\\nA K-means example for K = 2 in R2.\\n362\\n16.7\\nThe outcome of clustering in K-means depends on the initial\\nseeds.\\n364\\n16.8\\nEstimated minimal residual sum of squares as a function of\\nthe number of clusters in K-means.\\n366\\n17.1\\nA dendrogram of a single-link clustering of 30 documents\\nfrom Reuters-RCV1.\\n379\\n17.2\\nA simple, but inefﬁcient HAC algorithm.\\n381\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxiv\\nList of Figures\\n17.3\\nThe different notions of cluster similarity used by the four\\nHAC algorithms.\\n381\\n17.4\\nA single-link (left) and complete-link (right) clustering of\\neight documents.\\n382\\n17.5\\nA dendrogram of a complete-link clustering.\\n383\\n17.6\\nChaining in single-link clustering.\\n384\\n17.7\\nOutliers in complete-link clustering.\\n385\\n17.8\\nThe priority-queue algorithm for HAC.\\n386\\n17.9\\nSingle-link clustering algorithm using an NBM array.\\n387\\n17.10\\nComplete-link clustering is not best-merge persistent.\\n388\\n17.11\\nThree iterations of centroid clustering.\\n391\\n17.12\\nCentroid clustering is not monotonic.\\n392\\n18.1\\nIllustration of the singular-value decomposition.\\n409\\n18.2\\nIllustration of low rank approximation using the\\nsingular-value decomposition.\\n411\\n18.3\\nThe documents of Example 18.4 reduced to two dimensions\\nin (V′)T.\\n416\\n18.4\\nDocuments for Exercise 18.11.\\n418\\n18.5\\nGlossary for Exercise 18.11.\\n418\\n19.1\\nA dynamically generated web page.\\n425\\n19.2\\nTwo nodes of the web graph joined by a link.\\n425\\n19.3\\nA sample small web graph.\\n426\\n19.4\\nThe bowtie structure of the Web.\\n427\\n19.5\\nCloaking as used by spammers.\\n428\\n19.6\\nSearch advertising triggered by query keywords.\\n431\\n19.7\\nThe various components of a web search engine.\\n434\\n19.8\\nIllustration of shingle sketches.\\n439\\n19.9\\nTwo sets Sj1 and Sj2; their Jaccard coefﬁcient is 2/5.\\n440\\n20.1\\nThe basic crawler architecture.\\n446\\n20.2\\nDistributing the basic crawl architecture.\\n449\\n20.3\\nThe URL frontier.\\n452\\n20.4\\nExample of an auxiliary hosts-to-back queues table.\\n453\\n20.5\\nA lexicographically ordered set of URLs.\\n456\\n20.6\\nA four-row segment of the table of links.\\n457\\n21.1\\nThe random surfer at node A proceeds with probability 1/3 to\\neach of B, C and D.\\n464\\n21.2\\nA simple Markov chain with three states; the numbers on the\\nlinks indicate the transition probabilities.\\n466\\n21.3\\nThe sequence of probability vectors.\\n469\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nList of Figures\\nxxv\\n21.4\\nA small web graph.\\n470\\n21.5\\nTopic-speciﬁc PageRank.\\n472\\n21.6\\nA sample run of HITS on the query japan elementary schools.\\n479\\n21.7\\nWeb graph for Exercise 21.22.\\n480\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nxxvii\\nTable of Notation\\nSymbol\\nPage\\nMeaning\\nγ\\np. 98\\nγ code\\nγ\\np. 256\\nClassiﬁcation or clustering function: γ(d) is d’s class\\nor cluster\\nΓ\\np. 256\\nSupervised learning method in Chapters 13 and 14:\\nΓ(D) is the classiﬁcation function γ learned from\\ntraining set D\\nλ\\np. 404\\nEigenvalue\\n⃗µ(.)\\np. 292\\nCentroid of a class (in Rocchio classiﬁcation) or a\\ncluster (in K-means and centroid clustering)\\nΦ\\np. 114\\nTraining example\\nσ\\np. 408\\nSingular value\\nΘ(·)\\np. 11\\nA tight bound on the complexity of an algorithm\\nω, ωk\\np. 357\\nCluster in clustering\\nΩ\\np. 357\\nClustering or set of clusters {ω1, . . . , ωK}\\narg maxx f(x) p. 181\\nThe value of x for which f reaches its maximum\\narg minx f(x) p. 181\\nThe value of x for which f reaches its minimum\\nc, cj\\np. 256\\nClass or category in classiﬁcation\\ncft\\np. 89\\nThe collection frequency of term t (the total number\\nof times the term appears in the document collec-\\ntion)\\nC\\np. 256\\nSet {c1, . . . , cJ} of all classes\\nC\\np. 268\\nA random variable that takes as values members of\\nC\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxviii\\nTable of Notation\\nC\\np. 403\\nTerm-document matrix\\nd\\np. 4\\nIndex of the dth document in the collection D\\nd\\np. 71\\nA document\\n⃗d,⃗q\\np. 181\\nDocument vector, query vector\\nD\\np. 354\\nSet {d1, . . . , dN} of all documents\\nDc\\np. 292\\nSet of documents that is in class c\\nD\\np. 256\\nSet {⟨d1, c1⟩, . . . , ⟨dN, cN⟩} of all labeled documents\\nin Chapters 13–15\\ndft\\np. 118\\nThe document frequency of term t (the total number\\nof documents in the collection the term appears in)\\nH\\np. 99\\nEntropy\\nHM\\np. 101\\nMth harmonic number\\nI(X; Y)\\np. 272\\nMutual information of random variables X and Y\\nidft\\np. 118\\nInverse document frequency of term t\\nJ\\np. 256\\nNumber of classes\\nk\\np. 290\\nTop k items from a set, e.g., k nearest neighbors in\\nkNN, top k retrieved documents, top k selected fea-\\ntures from the vocabulary V\\nk\\np. 54\\nSequence of k characters\\nK\\np. 354\\nNumber of clusters\\nLd\\np. 233\\nLength of document d (in tokens)\\nLa\\np. 262\\nLength of the test document (or application docu-\\nment) in tokens\\nLave\\np. 70\\nAverage length of a document (in tokens)\\nM\\np. 5\\nSize of the vocabulary (|V|)\\nMa\\np. 262\\nSize of the vocabulary of the test document (or ap-\\nplication document)\\nMave\\np. 78\\nAverage size of the vocabulary in a document in the\\ncollection\\nMd\\np. 237\\nLanguage model for document d\\nN\\np. 4\\nNumber of documents in the retrieval or training\\ncollection\\nNc\\np. 259\\nNumber of documents in class c\\nN(ω)\\np. 298\\nNumber of times the event ω occurred\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nTable of Notation\\nxxix\\nO(·)\\np. 11\\nA bound on the complexity of an algorithm\\nO(·)\\np. 221\\nThe odds of an event\\nP\\np. 155\\nPrecision\\nP(·)\\np. 220\\nProbability\\nP\\np. 465\\nTransition probability matrix\\nq\\np. 59\\nA query\\nR\\np. 155\\nRecall\\nsi\\np. 58\\nA string\\nsi\\np. 112\\nBoolean values for zone scoring\\nsim(d1, d2)\\np. 121\\nSimilarity score for documents d1, d2\\nT\\np. 43\\nTotal number of tokens in the document collection\\nTct\\np. 259\\nNumber of occurrences of word t in documents of\\nclass c\\nt\\np. 4\\nIndex of the tth term in the vocabulary V\\nt\\np. 61\\nA term in the vocabulary\\ntft,d\\np. 117\\nThe term frequency of term t in document d (the to-\\ntal number of occurrences of t in d)\\nUt\\np. 266\\nRandom variable taking values 0 (term t is present)\\nand 1 (t is not present)\\nV\\np. 208\\nVocabulary of terms {t1, . . . , tM} in a collection (a.k.a.\\nthe lexicon)\\n⃗v(d)\\np. 122\\nLength-normalized document vector\\n⃗V(d)\\np. 120\\nVector of document d, not length-normalized\\nwft,d\\np. 125\\nWeight of term t in document d\\nw\\np. 112\\nA weight, for example for zones or terms\\n⃗wT⃗x = b\\np. 293\\nHyperplane; ⃗w is the normal vector of the hyper-\\nplane and wi component i of ⃗w\\n⃗x\\np. 222\\nTerm incidence vector ⃗x = (x1, . . . , xM); more gen-\\nerally: document feature representation\\nX\\np. 266\\nRandom variable taking values in V, the vocabulary\\n(e.g., at a given position k in a document)\\nX\\np. 256\\nDocument space in text classiﬁcation\\n|A|\\np. 61\\nSet cardinality: the number of members of set A\\n|S|\\np. 404\\nDeterminant of the square matrix S\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxx\\nTable of Notation\\n|si|\\np. 58\\nLength in characters of string si\\n|⃗x|\\np. 139\\nLength of vector ⃗x\\n|⃗x −⃗y|\\np. 131\\nEuclidean distance of ⃗x and ⃗y (which is the length of\\n(⃗x −⃗y))\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\nxxxi\\nPreface\\nAs recently as the 1990s, studies showed that most people preferred getting\\ninformation from other people rather than from information retrieval sys-\\ntems. Of course, in that time period, most people also used human travel\\nagents to book their travel. However, during the last decade, relentless opti-\\nmization of information retrieval effectiveness has driven web search engines\\nto new quality levels where most people are satisﬁed most of the time, and\\nweb search has become a standard and often preferred source of information\\nﬁnding. For example, the 2004 Pew Internet Survey (Fallows 2004) found\\nthat “92% of Internet users say the Internet is a good place to go for getting\\neveryday information.” To the surprise of many, the ﬁeld of information re-\\ntrieval has moved from being a primarily academic discipline to being the\\nbasis underlying most people’s preferred means of information access. This\\nbook presents the scientiﬁc underpinnings of this ﬁeld, at a level accessible\\nto graduate students as well as advanced undergraduates.\\nInformation retrieval did not begin with the Web. In response to various\\nchallenges of providing information access, the ﬁeld of information retrieval\\nevolved to give principled approaches to searching various forms of con-\\ntent. The ﬁeld began with scientiﬁc publications and library records, but\\nsoon spread to other forms of content, particularly those of information pro-\\nfessionals, such as journalists, lawyers, and doctors. Much of the scientiﬁc\\nresearch on information retrieval has occurred in these contexts, and much of\\nthe continued practice of information retrieval deals with providing access to\\nunstructured information in various corporate and governmental domains,\\nand this work forms much of the foundation of our book.\\nNevertheless, in recent years, a principal driver of innovation has been the\\nWorld Wide Web, unleashing publication at the scale of tens of millions of\\ncontent creators. This explosion of published information would be moot\\nif the information could not be found, annotated and analyzed so that each\\nuser can quickly ﬁnd information that is both relevant and comprehensive\\nfor their needs. By the late 1990s, many people felt that continuing to index\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxxii\\nPreface\\nthe whole Web would rapidly become impossible, due to the Web’s expo-\\nnential growth in size. But major scientiﬁc innovations, superb engineering,\\nthe rapidly declining price of computer hardware, and the rise of a commer-\\ncial underpinning for web search have all conspired to power today’s major\\nsearch engines, which are able to provide high-quality results within subsec-\\nond response times for hundreds of millions of searches a day over billions\\nof web pages.\\nBook organization and course development\\nThis book is the result of a series of courses we have taught at Stanford Uni-\\nversity and at the University of Stuttgart, in a range of durations including\\na single quarter, one semester and two quarters. These courses were aimed\\nat early-stage graduate students in computer science, but we have also had\\nenrollment from upper-class computer science undergraduates, as well as\\nstudents from law, medical informatics, statistics, linguistics and various en-\\ngineering disciplines. The key design principle for this book, therefore, was\\nto cover what we believe to be important in a one-term graduate course on\\ninformation retrieval. An additional principle is to build each chapter around\\nmaterial that we believe can be covered in a single lecture of 75 to 90 minutes.\\nThe ﬁrst eight chapters of the book are devoted to the basics of informa-\\ntion retrieval, and in particular the heart of search engines; we consider this\\nmaterial to be core to any course on information retrieval. Chapter 1 in-\\ntroduces inverted indexes, and shows how simple Boolean queries can be\\nprocessed using such indexes. Chapter 2 builds on this introduction by de-\\ntailing the manner in which documents are preprocessed before indexing\\nand by discussing how inverted indexes are augmented in various ways for\\nfunctionality and speed. Chapter 3 discusses search structures for dictionar-\\nies and how to process queries that have spelling errors and other imprecise\\nmatches to the vocabulary in the document collection being searched. Chap-\\nter 4 describes a number of algorithms for constructing the inverted index\\nfrom a text collection with particular attention to highly scalable and dis-\\ntributed algorithms that can be applied to very large collections. Chapter 5\\ncovers techniques for compressing dictionaries and inverted indexes. These\\ntechniques are critical for achieving subsecond response times to user queries\\nin large search engines. The indexes and queries considered in Chapters 1–5\\nonly deal with Boolean retrieval, in which a document either matches a query,\\nor does not. A desire to measure the extent to which a document matches a\\nquery, or the score of a document for a query, motivates the development of\\nterm weighting and the computation of scores in Chapters 6 and 7, leading\\nto the idea of a list of documents that are rank-ordered for a query. Chapter 8\\nfocuses on the evaluation of an information retrieval system based on the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nPreface\\nxxxiii\\nrelevance of the documents it retrieves, allowing us to compare the relative\\nperformances of different systems on benchmark document collections and\\nqueries.\\nChapters 9–21 build on the foundation of the ﬁrst eight chapters to cover\\na variety of more advanced topics. Chapter 9 discusses methods by which\\nretrieval can be enhanced through the use of techniques like relevance feed-\\nback and query expansion, which aim at increasing the likelihood of retriev-\\ning relevant documents. Chapter 10 considers information retrieval from\\ndocuments that are structured with markup languages like XML and HTML.\\nWe treat structured retrieval by reducing it to the vector space scoring meth-\\nods developed in Chapter 6. Chapters 11 and 12 invoke probability theory to\\ncompute scores for documents on queries. Chapter 11 develops traditional\\nprobabilistic information retrieval, which provides a framework for comput-\\ning the probability of relevance of a document, given a set of query terms.\\nThis probability may then be used as a score in ranking. Chapter 12 illus-\\ntrates an alternative, wherein for each document in a collection, we build a\\nlanguage model from which one can estimate a probability that the language\\nmodel generates a given query.\\nThis probability is another quantity with\\nwhich we can rank-order documents.\\nChapters 13–17 give a treatment of various forms of machine learning and\\nnumerical methods in information retrieval. Chapters 13–15 treat the prob-\\nlem of classifying documents into a set of known categories, given a set of\\ndocuments along with the classes they belong to. Chapter 13 motivates sta-\\ntistical classiﬁcation as one of the key technologies needed for a successful\\nsearch engine, introduces Naive Bayes, a conceptually simple and efﬁcient\\ntext classiﬁcation method, and outlines the standard methodology for evalu-\\nating text classiﬁers. Chapter 14 employs the vector space model from Chap-\\nter 6 and introduces two classiﬁcation methods, Rocchio and kNN, that op-\\nerate on document vectors. It also presents the bias-variance tradeoff as an\\nimportant characterization of learning problems that provides criteria for se-\\nlecting an appropriate method for a text classiﬁcation problem. Chapter 15\\nintroduces support vector machines, which many researchers currently view\\nas the most effective text classiﬁcation method. We also develop connections\\nin this chapter between the problem of classiﬁcation and seemingly disparate\\ntopics such as the induction of scoring functions from a set of training exam-\\nples.\\nChapters 16–18 consider the problem of inducing clusters of related doc-\\numents from a collection.\\nIn Chapter 16, we ﬁrst give an overview of a\\nnumber of important applications of clustering in information retrieval. We\\nthen describe two ﬂat clustering algorithms: the K-means algorithm, an ef-\\nﬁcient and widely used document clustering method; and the Expectation-\\nMaximization algorithm, which is computationally more expensive, but also\\nmore ﬂexible. Chapter 17 motivates the need for hierarchically structured\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxxiv\\nPreface\\nclusterings (instead of ﬂat clusterings) in many applications in information\\nretrieval and introduces a number of clustering algorithms that produce a\\nhierarchy of clusters. The chapter also addresses the difﬁcult problem of\\nautomatically computing labels for clusters. Chapter 18 develops methods\\nfrom linear algebra that constitute an extension of clustering, and also offer\\nintriguing prospects for algebraic methods in information retrieval, which\\nhave been pursued in the approach of latent semantic indexing.\\nChapters 19–21 treat the problem of web search. We give in Chapter 19 a\\nsummary of the basic challenges in web search, together with a set of tech-\\nniques that are pervasive in web information retrieval. Next, Chapter 20\\ndescribes the architecture and requirements of a basic web crawler. Finally,\\nChapter 21 considers the power of link analysis in web search, using in the\\nprocess several methods from linear algebra and advanced probability the-\\nory.\\nThis book is not comprehensive in covering all topics related to informa-\\ntion retrieval. We have put aside a number of topics, which we deemed\\noutside the scope of what we wished to cover in an introduction to infor-\\nmation retrieval class. Nevertheless, for people interested in these topics, we\\nprovide a few pointers to mainly textbook coverage here.\\nCross-language IR (Grossman and Frieder 2004, ch. 4) and (Oard and Dorr\\n1996).\\nImage and Multimedia IR (Grossman and Frieder 2004, ch. 4), (Baeza-Yates\\nand Ribeiro-Neto 1999, ch. 6), (Baeza-Yates and Ribeiro-Neto 1999, ch. 11),\\n(Baeza-Yates and Ribeiro-Neto 1999, ch. 12), (del Bimbo 1999), (Lew 2001),\\nand (Smeulders et al. 2000).\\nSpeech retrieval (Coden et al. 2002).\\nMusic Retrieval (Downie 2006) and http://www.ismir.net/.\\nUser interfaces for IR (Baeza-Yates and Ribeiro-Neto 1999, ch. 10).\\nParallel and Peer-to-Peer IR (Grossman and Frieder 2004, ch. 7), (Baeza-Yates\\nand Ribeiro-Neto 1999, ch. 9), and (Aberer 2001).\\nDigital libraries (Baeza-Yates and Ribeiro-Neto 1999, ch. 15) and (Lesk 2004).\\nInformation science perspective (Korfhage 1997), (Meadow et al. 1999), and\\n(Ingwersen and Järvelin 2005).\\nLogic-based approaches to IR (van Rijsbergen 1989).\\nNatural Language Processing techniques (Manning and Schütze 1999), (Ju-\\nrafsky and Martin 2008), and (Lewis and Jones 1996).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nPreface\\nxxxv\\nPrerequisites\\nIntroductory courses in data structures and algorithms, in linear algebra and\\nin probability theory sufﬁce as prerequisites for all 21 chapters. We now give\\nmore detail for the beneﬁt of readers and instructors who wish to tailor their\\nreading to some of the chapters.\\nChapters 1–5 assume as prerequisite a basic course in algorithms and data\\nstructures. Chapters 6 and 7 require, in addition, a knowledge of basic lin-\\near algebra including vectors and dot products. No additional prerequisites\\nare assumed until Chapter 11, where a basic course in probability theory is\\nrequired; Section 11.1 gives a quick review of the concepts necessary in Chap-\\nters 11–13. Chapter 15 assumes that the reader is familiar with the notion of\\nnonlinear optimization, although the chapter may be read without detailed\\nknowledge of algorithms for nonlinear optimization. Chapter 18 demands a\\nﬁrst course in linear algebra including familiarity with the notions of matrix\\nrank and eigenvectors; a brief review is given in Section 18.1. The knowledge\\nof eigenvalues and eigenvectors is also necessary in Chapter 21.\\nBook layout\\n\\x0f\\nWorked examples in the text appear with a pencil sign next to them in the left\\nmargin. Advanced or difﬁcult material appears in sections or subsections\\nindicated with scissors in the margin. Exercises are marked in the margin\\n$\\nwith a question mark. The level of difﬁculty of exercises is indicated as easy\\n(⋆), medium (⋆⋆), or difﬁcult (⋆⋆⋆).\\n?\\nAcknowledgments\\nWe would like to thank Cambridge University Press for allowing us to make\\nthe draft book available online, which facilitated much of the feedback we\\nhave received while writing the book. We also thank Lauren Cowles, who\\nhas been an outstanding editor, providing several rounds of comments on\\neach chapter, on matters of style, organization, and coverage, as well as de-\\ntailed comments on the subject matter of the book. To the extent that we\\nhave achieved our goals in writing this book, she deserves an important part\\nof the credit.\\nWe are very grateful to the many people who have given us comments,\\nsuggestions, and corrections based on draft versions of this book. We thank\\nfor providing various corrections and comments: Cheryl Aasheim, Josh At-\\ntenberg, Daniel Beck, Luc Bélanger, Georg Buscher, Tom Breuel, Daniel Bur-\\nckhardt, Fazli Can, Dinquan Chen, Stephen Clark, Ernest Davis, Pedro Domin-\\ngos, Rodrigo Panchiniak Fernandes, Paolo Ferragina, Alex Fraser, Norbert\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nxxxvi\\nPreface\\nFuhr, Vignesh Ganapathy, Elmer Garduno, Xiubo Geng, David Gondek, Ser-\\ngio Govoni, Corinna Habets, Ben Handy, Donna Harman, Benjamin Haskell,\\nThomas Hühn, Deepak Jain, Ralf Jankowitsch, Dinakar Jayarajan, Vinay Kakade,\\nMei Kobayashi, Wessel Kraaij, Rick Laﬂeur, Florian Laws, Hang Li, David\\nLosada, David Mann, Ennio Masi, Sven Meyer zu Eissen, Alexander Murzaku,\\nGonzalo Navarro, Frank McCown, Paul McNamee, Christoph Müller, Scott\\nOlsson, Tao Qin, Megha Raghavan, Michal Rosen-Zvi, Klaus Rothenhäusler,\\nKenyu L. Runner, Alexander Salamanca, Grigory Sapunov, Evgeny Shad-\\nchnev, Tobias Scheffer, Nico Schlaefer, Ian Soboroff, Benno Stein, Marcin\\nSydow, Andrew Turner, Jason Utt, Huey Vo, Travis Wade, Mike Walsh, Changliang\\nWang, Renjing Wang, and Thomas Zeume.\\nMany people gave us detailed feedback on individual chapters, either at\\nour request or through their own initiative. For this, we’re particularly grate-\\nful to: James Allan, Omar Alonso, Ismail Sengor Altingovde, Vo Ngoc Anh,\\nRoi Blanco, Eric Breck, Eric Brown, Mark Carman, Carlos Castillo, Junghoo\\nCho, Aron Culotta, Doug Cutting, Meghana Deodhar, Susan Dumais, Jo-\\nhannes Fürnkranz, Andreas Heß, Djoerd Hiemstra, David Hull, Thorsten\\nJoachims, Siddharth Jonathan J. B., Jaap Kamps, Mounia Lalmas, Amy Langville,\\nNicholas Lester, Dave Lewis, Daniel Lowd, Yosi Mass, Jeff Michels, Alessan-\\ndro Moschitti, Amir Najmi, Marc Najork, Giorgio Maria Di Nunzio, Paul\\nOgilvie, Priyank Patel, Jan Pedersen, Kathryn Pedings, Vassilis Plachouras,\\nDaniel Ramage, Ghulam Raza, Stefan Riezler, Michael Schiehlen, Helmut\\nSchmid, Falk Nicolas Scholer, Sabine Schulte im Walde, Fabrizio Sebastiani,\\nSarabjeet Singh, Valentin Spitkovsky, Alexander Strehl, John Tait, Shivaku-\\nmar Vaithyanathan, Ellen Voorhees, Gerhard Weikum, Dawid Weiss, Yiming\\nYang, Yisong Yue, Jian Zhang, and Justin Zobel.\\nAnd ﬁnally there were a few reviewers who absolutely stood out in terms\\nof the quality and quantity of comments that they provided. We thank them\\nfor their signiﬁcant impact on the content and structure of the book. We\\nexpress our gratitude to Pavel Berkhin, Stefan Büttcher, Jamie Callan, Byron\\nDom, Torsten Suel, and Andrew Trotman.\\nParts of the initial drafts of Chapters 13–15 were based on slides that were\\ngenerously provided by Ray Mooney. While the material has gone through\\nextensive revisions, we gratefully acknowledge Ray’s contribution to the\\nthree chapters in general and to the description of the time complexities of\\ntext classiﬁcation algorithms in particular.\\nThe above is unfortunately an incomplete list: we are still in the process of\\nincorporating feedback we have received. And, like all opinionated authors,\\nwe did not always heed the advice that was so freely given. The published\\nversions of the chapters remain solely the responsibility of the authors.\\nThe authors thank Stanford University and the University of Stuttgart for\\nproviding a stimulating academic environment for discussing ideas and the\\nopportunity to teach courses from which this book arose and in which its\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nPreface\\nxxxvii\\ncontents were reﬁned. CM thanks his family for the many hours they’ve let\\nhim spend working on this book, and hopes he’ll have a bit more free time on\\nweekends next year. PR thanks his family for their patient support through\\nthe writing of this book and is also grateful to Yahoo! Inc. for providing a\\nfertile environment in which to work on this book. HS would like to thank\\nhis parents, family, and friends for their support while writing this book.\\nWeb and contact information\\nThis book has a companion website at http://informationretrieval.org. As well as\\nlinks to some more general resources, it is our intent to maintain on this web-\\nsite a set of slides for each chapter which may be used for the corresponding\\nlecture. We gladly welcome further feedback, corrections, and suggestions\\non the book, which may be sent to all the authors at informationretrieval (at) yahoogroups (dot) com.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n1\\n1\\nBoolean retrieval\\nThe meaning of the term information retrieval can be very broad. Just getting\\na credit card out of your wallet so that you can type in the card number\\nis a form of information retrieval. However, as an academic ﬁeld of study,\\ninformation retrieval might be deﬁned thus:\\nINFORMATION\\nRETRIEVAL\\nInformation retrieval (IR) is ﬁnding material (usually documents) of\\nan unstructured nature (usually text) that satisﬁes an information need\\nfrom within large collections (usually stored on computers).\\nAs deﬁned in this way, information retrieval used to be an activity that only\\na few people engaged in: reference librarians, paralegals, and similar pro-\\nfessional searchers. Now the world has changed, and hundreds of millions\\nof people engage in information retrieval every day when they use a web\\nsearch engine or search their email.1 Information retrieval is fast becoming\\nthe dominant form of information access, overtaking traditional database-\\nstyle searching (the sort that is going on when a clerk says to you: “I’m sorry,\\nI can only look up your order if you can give me your Order ID”).\\nIR can also cover other kinds of data and information problems beyond\\nthat speciﬁed in the core deﬁnition above. The term “unstructured data”\\nrefers to data which does not have clear, semantically overt, easy-for-a-computer\\nstructure. It is the opposite of structured data, the canonical example of\\nwhich is a relational database, of the sort companies usually use to main-\\ntain product inventories and personnel records. In reality, almost no data\\nare truly “unstructured”. This is deﬁnitely true of all text data if you count\\nthe latent linguistic structure of human languages. But even accepting that\\nthe intended notion of structure is overt structure, most text has structure,\\nsuch as headings and paragraphs and footnotes, which is commonly repre-\\nsented in documents by explicit markup (such as the coding underlying web\\n1. In modern parlance, the word “search” has tended to replace “(information) retrieval”; the\\nterm “search” is quite ambiguous, but in context we use the two synonymously.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2\\n1\\nBoolean retrieval\\npages). IR is also used to facilitate “semistructured” search such as ﬁnding a\\ndocument where the title contains Java and the body contains threading.\\nThe ﬁeld of information retrieval also covers supporting users in browsing\\nor ﬁltering document collections or further processing a set of retrieved doc-\\numents. Given a set of documents, clustering is the task of coming up with a\\ngood grouping of the documents based on their contents. It is similar to ar-\\nranging books on a bookshelf according to their topic. Given a set of topics,\\nstanding information needs, or other categories (such as suitability of texts\\nfor different age groups), classiﬁcation is the task of deciding which class(es),\\nif any, each of a set of documents belongs to. It is often approached by ﬁrst\\nmanually classifying some documents and then hoping to be able to classify\\nnew documents automatically.\\nInformation retrieval systems can also be distinguished by the scale at\\nwhich they operate, and it is useful to distinguish three prominent scales.\\nIn web search, the system has to provide search over billions of documents\\nstored on millions of computers. Distinctive issues are needing to gather\\ndocuments for indexing, being able to build systems that work efﬁciently\\nat this enormous scale, and handling particular aspects of the web, such as\\nthe exploitation of hypertext and not being fooled by site providers manip-\\nulating page content in an attempt to boost their search engine rankings,\\ngiven the commercial importance of the web. We focus on all these issues\\nin Chapters 19–21. At the other extreme is personal information retrieval. In\\nthe last few years, consumer operating systems have integrated information\\nretrieval (such as Apple’s Mac OS X Spotlight or Windows Vista’s Instant\\nSearch). Email programs usually not only provide search but also text clas-\\nsiﬁcation: they at least provide a spam (junk mail) ﬁlter, and commonly also\\nprovide either manual or automatic means for classifying mail so that it can\\nbe placed directly into particular folders. Distinctive issues here include han-\\ndling the broad range of document types on a typical personal computer,\\nand making the search system maintenance free and sufﬁciently lightweight\\nin terms of startup, processing, and disk space usage that it can run on one\\nmachine without annoying its owner. In between is the space of enterprise,\\ninstitutional, and domain-speciﬁc search, where retrieval might be provided for\\ncollections such as a corporation’s internal documents, a database of patents,\\nor research articles on biochemistry. In this case, the documents will typi-\\ncally be stored on centralized ﬁle systems and one or a handful of dedicated\\nmachines will provide search over the collection. This book contains tech-\\nniques of value over this whole spectrum, but our coverage of some aspects\\nof parallel and distributed search in web-scale search systems is compara-\\ntively light owing to the relatively small published literature on the details\\nof such systems. However, outside of a handful of web search companies, a\\nsoftware developer is most likely to encounter the personal search and en-\\nterprise scenarios.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.1\\nAn example information retrieval problem\\n3\\nIn this chapter we begin with a very simple example of an information\\nretrieval problem, and introduce the idea of a term-document matrix (Sec-\\ntion 1.1) and the central inverted index data structure (Section 1.2). We will\\nthen examine the Boolean retrieval model and how Boolean queries are pro-\\ncessed (Sections 1.3 and 1.4).\\n1.1\\nAn example information retrieval problem\\nA fat book which many people own is Shakespeare’s Collected Works. Sup-\\npose you wanted to determine which plays of Shakespeare contain the words\\nBrutus AND Caesar AND NOT Calpurnia. One way to do that is to start at the\\nbeginning and to read through all the text, noting for each play whether\\nit contains Brutus and Caesar and excluding it from consideration if it con-\\ntains Calpurnia. The simplest form of document retrieval is for a computer\\nto do this sort of linear scan through documents. This process is commonly\\nreferred to as grepping through text, after the Unix command grep, which\\nGREP\\nperforms this process. Grepping through text can be a very effective process,\\nespecially given the speed of modern computers, and often allows useful\\npossibilities for wildcard pattern matching through the use of regular expres-\\nsions. With modern computers, for simple querying of modest collections\\n(the size of Shakespeare’s Collected Works is a bit under one million words\\nof text in total), you really need nothing more.\\nBut for many purposes, you do need more:\\n1. To process large document collections quickly. The amount of online data\\nhas grown at least as quickly as the speed of computers, and we would\\nnow like to be able to search collections that total in the order of billions\\nto trillions of words.\\n2. To allow more ﬂexible matching operations. For example, it is impractical\\nto perform the query Romans NEAR countrymen with grep, where NEAR\\nmight be deﬁned as “within 5 words” or “within the same sentence”.\\n3. To allow ranked retrieval: in many cases you want the best answer to an\\ninformation need among many documents that contain certain words.\\nThe way to avoid linearly scanning the texts for each query is to index the\\nINDEX\\ndocuments in advance. Let us stick with Shakespeare’s Collected Works,\\nand use it to introduce the basics of the Boolean retrieval model. Suppose\\nwe record for each document – here a play of Shakespeare’s – whether it\\ncontains each word out of all the words Shakespeare used (Shakespeare used\\nabout 32,000 different words). The result is a binary term-document incidence\\nINCIDENCE MATRIX\\nmatrix, as in Figure 1.1. Terms are the indexed units (further discussed in\\nTERM\\nSection 2.2); they are usually words, and for the moment you can think of\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4\\n1\\nBoolean retrieval\\nAntony\\nJulius\\nThe\\nHamlet\\nOthello\\nMacbeth\\n...\\nand\\nCaesar\\nTempest\\nCleopatra\\nAntony\\n1\\n1\\n0\\n0\\n0\\n1\\nBrutus\\n1\\n1\\n0\\n1\\n0\\n0\\nCaesar\\n1\\n1\\n0\\n1\\n1\\n1\\nCalpurnia\\n0\\n1\\n0\\n0\\n0\\n0\\nCleopatra\\n1\\n0\\n0\\n0\\n0\\n0\\nmercy\\n1\\n0\\n1\\n1\\n1\\n1\\nworser\\n1\\n0\\n1\\n1\\n1\\n0\\n...\\n◮Figure 1.1\\nA term-document incidence matrix. Matrix element (t, d) is 1 if the\\nplay in column d contains the word in row t, and is 0 otherwise.\\nthem as words, but the information retrieval literature normally speaks of\\nterms because some of them, such as perhaps I-9 or Hong Kong are not usually\\nthought of as words. Now, depending on whether we look at the matrix rows\\nor columns, we can have a vector for each term, which shows the documents\\nit appears in, or a vector for each document, showing the terms that occur in\\nit.2\\nTo answer the query Brutus AND Caesar AND NOT Calpurnia, we take the\\nvectors for Brutus, Caesar and Calpurnia, complement the last, and then do a\\nbitwise AND:\\n110100 AND 110111 AND 101111 = 100100\\nThe answers for this query are thus Antony and Cleopatra and Hamlet (Fig-\\nure 1.2).\\nThe Boolean retrieval model is a model for information retrieval in which we\\nBOOLEAN RETRIEVAL\\nMODEL\\ncan pose any query which is in the form of a Boolean expression of terms,\\nthat is, in which terms are combined with the operators AND, OR, and NOT.\\nThe model views each document as just a set of words.\\nLet us now consider a more realistic scenario, simultaneously using the\\nopportunity to introduce some terminology and notation. Suppose we have\\nN = 1 million documents. By documents we mean whatever units we have\\nDOCUMENT\\ndecided to build a retrieval system over. They might be individual memos\\nor chapters of a book (see Section 2.1.2 (page 20) for further discussion). We\\nwill refer to the group of documents over which we perform retrieval as the\\n(document) collection. It is sometimes also referred to as a corpus (a body of\\nCOLLECTION\\nCORPUS\\ntexts). Suppose each document is about 1000 words long (2–3 book pages). If\\n2. Formally, we take the transpose of the matrix to be able to get the terms as column vectors.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.1\\nAn example information retrieval problem\\n5\\nAntony and Cleopatra, Act III, Scene ii\\nAgrippa [Aside to Domitius Enobarbus]:\\nWhy, Enobarbus,\\nWhen Antony found Julius Caesar dead,\\nHe cried almost to roaring; and he wept\\nWhen at Philippi he found Brutus slain.\\nHamlet, Act III, Scene ii\\nLord Polonius:\\nI did enact Julius Caesar: I was killed i’ the\\nCapitol; Brutus killed me.\\n◮Figure 1.2\\nResults from Shakespeare for the query Brutus AND Caesar AND NOT\\nCalpurnia.\\nwe assume an average of 6 bytes per word including spaces and punctuation,\\nthen this is a document collection about 6 GB in size. Typically, there might\\nbe about M = 500,000 distinct terms in these documents. There is nothing\\nspecial about the numbers we have chosen, and they might vary by an order\\nof magnitude or more, but they give us some idea of the dimensions of the\\nkinds of problems we need to handle. We will discuss and model these size\\nassumptions in Section 5.1 (page 86).\\nOur goal is to develop a system to address the ad hoc retrieval task. This is\\nAD HOC RETRIEVAL\\nthe most standard IR task. In it, a system aims to provide documents from\\nwithin the collection that are relevant to an arbitrary user information need,\\ncommunicated to the system by means of a one-off, user-initiated query. An\\ninformation need is the topic about which the user desires to know more, and\\nINFORMATION NEED\\nis differentiated from a query, which is what the user conveys to the com-\\nQUERY\\nputer in an attempt to communicate the information need. A document is\\nrelevant if it is one that the user perceives as containing information of value\\nRELEVANCE\\nwith respect to their personal information need. Our example above was\\nrather artiﬁcial in that the information need was deﬁned in terms of par-\\nticular words, whereas usually a user is interested in a topic like “pipeline\\nleaks” and would like to ﬁnd relevant documents regardless of whether they\\nprecisely use those words or express the concept with other words such as\\npipeline rupture. To assess the effectiveness of an IR system (i.e., the quality of\\nEFFECTIVENESS\\nits search results), a user will usually want to know two key statistics about\\nthe system’s returned results for a query:\\nPrecision: What fraction of the returned results are relevant to the informa-\\nPRECISION\\ntion need?\\nRecall: What fraction of the relevant documents in the collection were re-\\nRECALL\\nturned by the system?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6\\n1\\nBoolean retrieval\\nDetailed discussion of relevance and evaluation measures including preci-\\nsion and recall is found in Chapter 8.\\nWe now cannot build a term-document matrix in a naive way. A 500K ×\\n1M matrix has half-a-trillion 0’s and 1’s – too many to ﬁt in a computer’s\\nmemory. But the crucial observation is that the matrix is extremely sparse,\\nthat is, it has few non-zero entries. Because each document is 1000 words\\nlong, the matrix has no more than one billion 1’s, so a minimum of 99.8% of\\nthe cells are zero. A much better representation is to record only the things\\nthat do occur, that is, the 1 positions.\\nThis idea is central to the ﬁrst major concept in information retrieval, the\\ninverted index. The name is actually redundant: an index always maps back\\nINVERTED INDEX\\nfrom terms to the parts of a document where they occur. Nevertheless, in-\\nverted index, or sometimes inverted ﬁle, has become the standard term in infor-\\nmation retrieval.3 The basic idea of an inverted index is shown in Figure 1.3.\\nWe keep a dictionary of terms (sometimes also referred to as a vocabulary or\\nDICTIONARY\\nVOCABULARY\\nlexicon; in this book, we use dictionary for the data structure and vocabulary\\nLEXICON\\nfor the set of terms). Then for each term, we have a list that records which\\ndocuments the term occurs in. Each item in the list – which records that a\\nterm appeared in a document (and, later, often, the positions in the docu-\\nment) – is conventionally called a posting.4 The list is then called a postings\\nPOSTING\\nPOSTINGS LIST\\nlist (or inverted list), and all the postings lists taken together are referred to as\\nthe postings. The dictionary in Figure 1.3 has been sorted alphabetically and\\nPOSTINGS\\neach postings list is sorted by document ID. We will see why this is useful in\\nSection 1.3, below, but later we will also consider alternatives to doing this\\n(Section 7.1.5).\\n1.2\\nA ﬁrst take at building an inverted index\\nTo gain the speed beneﬁts of indexing at retrieval time, we have to build the\\nindex in advance. The major steps in this are:\\n1. Collect the documents to be indexed:\\nFriends, Romans, countrymen.\\nSo let it be with Caesar ...\\n2. Tokenize the text, turning each document into a list of tokens:\\nFriends\\nRomans\\ncountrymen\\nSo ...\\n3. Some information retrieval researchers prefer the term inverted ﬁle, but expressions like in-\\ndex construction and index compression are much more common than inverted ﬁle construction and\\ninverted ﬁle compression. For consistency, we use (inverted) index throughout this book.\\n4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherently\\nassociated with a term, via the postings list it is placed on; sometimes we will also talk of a\\n(term, docID) pair as a posting.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.2\\nA ﬁrst take at building an inverted index\\n7\\nBrutus\\n−→\\n1\\n2\\n4\\n11\\n31\\n45\\n173\\n174\\nCaesar\\n−→\\n1\\n2\\n4\\n5\\n6\\n16\\n57\\n132\\n...\\nCalpurnia\\n−→\\n2\\n31\\n54\\n101\\n...\\n|\\n{z\\n}\\n|\\n{z\\n}\\nDictionary\\nPostings\\n◮Figure 1.3\\nThe two parts of an inverted index. The dictionary is commonly kept\\nin memory, with pointers to each postings list, which is stored on disk.\\n3. Do linguistic preprocessing, producing a list of normalized tokens, which\\nare the indexing terms: friend\\nroman\\ncountryman\\nso ...\\n4. Index the documents that each term occurs in by creating an inverted in-\\ndex, consisting of a dictionary and postings.\\nWe will deﬁne and discuss the earlier stages of processing, that is, steps 1–3,\\nin Section 2.2 (page 22). Until then you can think of tokens and normalized\\ntokens as also loosely equivalent to words. Here, we assume that the ﬁrst\\n3 steps have already been done, and we examine building a basic inverted\\nindex by sort-based indexing.\\nWithin a document collection, we assume that each document has a unique\\nserial number, known as the document identiﬁer (docID). During index con-\\nDOCID\\nstruction, we can simply assign successive integers to each new document\\nwhen it is ﬁrst encountered. The input to indexing is a list of normalized\\ntokens for each document, which we can equally think of as a list of pairs of\\nterm and docID, as in Figure 1.4. The core indexing step is sorting this list\\nSORTING\\nso that the terms are alphabetical, giving us the representation in the middle\\ncolumn of Figure 1.4. Multiple occurrences of the same term from the same\\ndocument are then merged.5 Instances of the same term are then grouped,\\nand the result is split into a dictionary and postings, as shown in the right\\ncolumn of Figure 1.4. Since a term generally occurs in a number of docu-\\nments, this data organization already reduces the storage requirements of\\nthe index. The dictionary also records some statistics, such as the number of\\ndocuments which contain each term (the document frequency, which is here\\nDOCUMENT\\nFREQUENCY\\nalso the length of each postings list). This information is not vital for a ba-\\nsic Boolean search engine, but it allows us to improve the efﬁciency of the\\n5. Unix users can note that these steps are similar to use of the sort and then uniq commands.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8\\n1\\nBoolean retrieval\\nDoc 1\\nDoc 2\\nI did enact Julius Caesar: I was killed\\ni’ the Capitol; Brutus killed me.\\nSo let it be with Caesar. The noble Brutus\\nhath told you Caesar was ambitious:\\nterm\\ndocID\\nI\\n1\\ndid\\n1\\nenact\\n1\\njulius\\n1\\ncaesar\\n1\\nI\\n1\\nwas\\n1\\nkilled\\n1\\ni’\\n1\\nthe\\n1\\ncapitol\\n1\\nbrutus\\n1\\nkilled\\n1\\nme\\n1\\nso\\n2\\nlet\\n2\\nit\\n2\\nbe\\n2\\nwith\\n2\\ncaesar\\n2\\nthe\\n2\\nnoble\\n2\\nbrutus\\n2\\nhath\\n2\\ntold\\n2\\nyou\\n2\\ncaesar\\n2\\nwas\\n2\\nambitious\\n2\\n=⇒\\nterm\\ndocID\\nambitious\\n2\\nbe\\n2\\nbrutus\\n1\\nbrutus\\n2\\ncapitol\\n1\\ncaesar\\n1\\ncaesar\\n2\\ncaesar\\n2\\ndid\\n1\\nenact\\n1\\nhath\\n1\\nI\\n1\\nI\\n1\\ni’\\n1\\nit\\n2\\njulius\\n1\\nkilled\\n1\\nkilled\\n1\\nlet\\n2\\nme\\n1\\nnoble\\n2\\nso\\n2\\nthe\\n1\\nthe\\n2\\ntold\\n2\\nyou\\n2\\nwas\\n1\\nwas\\n2\\nwith\\n2\\n=⇒\\nterm\\ndoc. freq.\\n→\\npostings lists\\nambitious\\n1\\n→\\n2\\nbe\\n1\\n→\\n2\\nbrutus\\n2\\n→\\n1 →2\\ncapitol\\n1\\n→\\n1\\ncaesar\\n2\\n→\\n1 →2\\ndid\\n1\\n→\\n1\\nenact\\n1\\n→\\n1\\nhath\\n1\\n→\\n2\\nI\\n1\\n→\\n1\\ni’\\n1\\n→\\n1\\nit\\n1\\n→\\n2\\njulius\\n1\\n→\\n1\\nkilled\\n1\\n→\\n1\\nlet\\n1\\n→\\n2\\nme\\n1\\n→\\n1\\nnoble\\n1\\n→\\n2\\nso\\n1\\n→\\n2\\nthe\\n2\\n→\\n1 →2\\ntold\\n1\\n→\\n2\\nyou\\n1\\n→\\n2\\nwas\\n2\\n→\\n1 →2\\nwith\\n1\\n→\\n2\\n◮Figure 1.4\\nBuilding an index by sorting and grouping. The sequence of terms\\nin each document, tagged by their documentID (left) is sorted alphabetically (mid-\\ndle). Instances of the same term are then grouped by word and then by documentID.\\nThe terms and documentIDs are then separated out (right). The dictionary stores\\nthe terms, and has a pointer to the postings list for each term. It commonly also\\nstores other summary information such as, here, the document frequency of each\\nterm. We use this information for improving query time efﬁciency and, later, for\\nweighting in ranked retrieval models. Each postings list stores the list of documents\\nin which a term occurs, and may store other information such as the term frequency\\n(the frequency of each term in each document) or the position(s) of the term in each\\ndocument.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.2\\nA ﬁrst take at building an inverted index\\n9\\nsearch engine at query time, and it is a statistic later used in many ranked re-\\ntrieval models. The postings are secondarily sorted by docID. This provides\\nthe basis for efﬁcient query processing. This inverted index structure is es-\\nsentially without rivals as the most efﬁcient structure for supporting ad hoc\\ntext search.\\nIn the resulting index, we pay for storage of both the dictionary and the\\npostings lists. The latter are much larger, but the dictionary is commonly\\nkept in memory, while postings lists are normally kept on disk, so the size\\nof each is important, and in Chapter 5 we will examine how each can be\\noptimized for storage and access efﬁciency. What data structure should be\\nused for a postings list? A ﬁxed length array would be wasteful as some\\nwords occur in many documents, and others in very few. For an in-memory\\npostings list, two good alternatives are singly linked lists or variable length\\narrays. Singly linked lists allow cheap insertion of documents into postings\\nlists (following updates, such as when recrawling the web for updated doc-\\numents), and naturally extend to more advanced indexing strategies such as\\nskip lists (Section 2.3), which require additional pointers. Variable length ar-\\nrays win in space requirements by avoiding the overhead for pointers and in\\ntime requirements because their use of contiguous memory increases speed\\non modern processors with memory caches. Extra pointers can in practice be\\nencoded into the lists as offsets. If updates are relatively infrequent, variable\\nlength arrays will be more compact and faster to traverse. We can also use a\\nhybrid scheme with a linked list of ﬁxed length arrays for each term. When\\npostings lists are stored on disk, they are stored (perhaps compressed) as a\\ncontiguous run of postings without explicit pointers (as in Figure 1.3), so as\\nto minimize the size of the postings list and the number of disk seeks to read\\na postings list into memory.\\n?\\nExercise 1.1\\n[⋆]\\nDraw the inverted index that would be built for the following document collection.\\n(See Figure 1.3 for an example.)\\nDoc 1\\nnew home sales top forecasts\\nDoc 2\\nhome sales rise in july\\nDoc 3\\nincrease in home sales in july\\nDoc 4\\njuly new home sales rise\\nExercise 1.2\\n[⋆]\\nConsider these documents:\\nDoc 1\\nbreakthrough drug for schizophrenia\\nDoc 2\\nnew schizophrenia drug\\nDoc 3\\nnew approach for treatment of schizophrenia\\nDoc 4\\nnew hopes for schizophrenia patients\\na. Draw the term-document incidence matrix for this document collection.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10\\n1\\nBoolean retrieval\\nBrutus\\n−→\\n1 →2 →4 →11 →31 →45 →173 →174\\nCalpurnia\\n−→\\n2 →31 →54 →101\\nIntersection\\n=⇒\\n2 →31\\n◮Figure 1.5\\nIntersecting the postings lists for Brutus and Calpurnia from Figure 1.3.\\nb. Draw the inverted index representation for this collection, as in Figure 1.3 (page 7).\\nExercise 1.3\\n[⋆]\\nFor the document collection shown in Exercise 1.2, what are the returned results for\\nthese queries:\\na. schizophrenia AND drug\\nb. for AND NOT(drug OR approach)\\n1.3\\nProcessing Boolean queries\\nHow do we process a query using an inverted index and the basic Boolean\\nretrieval model? Consider processing the simple conjunctive query:\\nSIMPLE CONJUNCTIVE\\nQUERIES\\n(1.1)\\nBrutus AND Calpurnia\\nover the inverted index partially shown in Figure 1.3 (page 7). We:\\n1. Locate Brutus in the Dictionary\\n2. Retrieve its postings\\n3. Locate Calpurnia in the Dictionary\\n4. Retrieve its postings\\n5. Intersect the two postings lists, as shown in Figure 1.5.\\nThe intersection operation is the crucial one: we need to efﬁciently intersect\\nPOSTINGS LIST\\nINTERSECTION\\npostings lists so as to be able to quickly ﬁnd documents that contain both\\nterms. (This operation is sometimes referred to as merging postings lists:\\nPOSTINGS MERGE\\nthis slightly counterintuitive name reﬂects using the term merge algorithm for\\na general family of algorithms that combine multiple sorted lists by inter-\\nleaved advancing of pointers through each; here we are merging the lists\\nwith a logical AND operation.)\\nThere is a simple and effective method of intersecting postings lists using\\nthe merge algorithm (see Figure 1.6): we maintain pointers into both lists\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.3\\nProcessing Boolean queries\\n11\\nINTERSECT(p1, p2)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen ADD(answer, docID(p1))\\n5\\np1 ←next(p1)\\n6\\np2 ←next(p2)\\n7\\nelse if docID(p1) < docID(p2)\\n8\\nthen p1 ←next(p1)\\n9\\nelse p2 ←next(p2)\\n10\\nreturn answer\\n◮Figure 1.6\\nAlgorithm for the intersection of two postings lists p1 and p2.\\nand walk through the two postings lists simultaneously, in time linear in\\nthe total number of postings entries. At each step, we compare the docID\\npointed to by both pointers. If they are the same, we put that docID in the\\nresults list, and advance both pointers. Otherwise we advance the pointer\\npointing to the smaller docID. If the lengths of the postings lists are x and\\ny, the intersection takes O(x + y) operations. Formally, the complexity of\\nquerying is Θ(N), where N is the number of documents in the collection.6\\nOur indexing methods gain us just a constant, not a difference in Θ time\\ncomplexity compared to a linear scan, but in practice the constant is huge.\\nTo use this algorithm, it is crucial that postings be sorted by a single global\\nordering. Using a numeric sort by docID is one simple way to achieve this.\\nWe can extend the intersection operation to process more complicated queries\\nlike:\\n(1.2)\\n(Brutus OR Caesar) AND NOT Calpurnia\\nQuery optimization is the process of selecting how to organize the work of an-\\nQUERY OPTIMIZATION\\nswering a query so that the least total amount of work needs to be done by\\nthe system. A major element of this for Boolean queries is the order in which\\npostings lists are accessed. What is the best order for query processing? Con-\\nsider a query that is an AND of t terms, for instance:\\n(1.3)\\nBrutus AND Caesar AND Calpurnia\\nFor each of the t terms, we need to get its postings, then AND them together.\\nThe standard heuristic is to process terms in order of increasing document\\n6. The notation Θ(·) is used to express an asymptotically tight bound on the complexity of\\nan algorithm. Informally, this is often written as O(·), but this notation really expresses an\\nasymptotic upper bound, which need not be tight (Cormen et al. 1990).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12\\n1\\nBoolean retrieval\\nINTERSECT(⟨t1, . . . , tn⟩)\\n1\\nterms ←SORTBYINCREASINGFREQUENCY(⟨t1, . . . , tn⟩)\\n2\\nresult ←postings( first(terms))\\n3\\nterms ←rest(terms)\\n4\\nwhile terms ̸= NIL and result ̸= NIL\\n5\\ndo result ←INTERSECT(result, postings( first(terms)))\\n6\\nterms ←rest(terms)\\n7\\nreturn result\\n◮Figure 1.7\\nAlgorithm for conjunctive queries that returns the set of documents\\ncontaining each term in the input list of terms.\\nfrequency: if we start by intersecting the two smallest postings lists, then all\\nintermediate results must be no bigger than the smallest postings list, and we\\nare therefore likely to do the least amount of total work. So, for the postings\\nlists in Figure 1.3 (page 7), we execute the above query as:\\n(1.4)\\n(Calpurnia AND Brutus) AND Caesar\\nThis is a ﬁrst justiﬁcation for keeping the frequency of terms in the dictionary:\\nit allows us to make this ordering decision based on in-memory data before\\naccessing any postings list.\\nConsider now the optimization of more general queries, such as:\\n(1.5)\\n(madding OR crowd) AND (ignoble OR strife) AND (killed OR slain)\\nAs before, we will get the frequencies for all terms, and we can then (con-\\nservatively) estimate the size of each OR by the sum of the frequencies of its\\ndisjuncts. We can then process the query in increasing order of the size of\\neach disjunctive term.\\nFor arbitrary Boolean queries, we have to evaluate and temporarily store\\nthe answers for intermediate expressions in a complex expression. However,\\nin many circumstances, either because of the nature of the query language,\\nor just because this is the most common type of query that users submit, a\\nquery is purely conjunctive. In this case, rather than viewing merging post-\\nings lists as a function with two inputs and a distinct output, it is more ef-\\nﬁcient to intersect each retrieved postings list with the current intermediate\\nresult in memory, where we initialize the intermediate result by loading the\\npostings list of the least frequent term. This algorithm is shown in Figure 1.7.\\nThe intersection operation is then asymmetric: the intermediate results list\\nis in memory while the list it is being intersected with is being read from\\ndisk. Moreover the intermediate results list is always at least as short as the\\nother list, and in many cases it is orders of magnitude shorter. The postings\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.3\\nProcessing Boolean queries\\n13\\nintersection can still be done by the algorithm in Figure 1.6, but when the\\ndifference between the list lengths is very large, opportunities to use alter-\\nnative techniques open up. The intersection can be calculated in place by\\ndestructively modifying or marking invalid items in the intermediate results\\nlist. Or the intersection can be done as a sequence of binary searches in the\\nlong postings lists for each posting in the intermediate results list. Another\\npossibility is to store the long postings list as a hashtable, so that membership\\nof an intermediate result item can be calculated in constant rather than linear\\nor log time. However, such alternative techniques are difﬁcult to combine\\nwith postings list compression of the sort discussed in Chapter 5. Moreover,\\nstandard postings list intersection operations remain necessary when both\\nterms of a query are very common.\\n?\\nExercise 1.4\\n[⋆]\\nFor the queries below, can we still run through the intersection in time O(x + y),\\nwhere x and y are the lengths of the postings lists for Brutus and Caesar? If not, what\\ncan we achieve?\\na. Brutus AND NOT Caesar\\nb. Brutus OR NOT Caesar\\nExercise 1.5\\n[⋆]\\nExtend the postings merge algorithm to arbitrary Boolean query formulas. What is\\nits time complexity? For instance, consider:\\nc. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\\nCan we always merge in linear time? Linear in what? Can we do better than this?\\nExercise 1.6\\n[⋆⋆]\\nWe can use distributive laws for AND and OR to rewrite queries.\\na. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using\\nthe distributive laws.\\nb. Would the resulting query be more or less efﬁciently evaluated than the original\\nform of this query?\\nc. Is this result true in general or does it depend on the words and the contents of\\nthe document collection?\\nExercise 1.7\\n[⋆]\\nRecommend a query processing order for\\nd. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)\\ngiven the following postings list sizes:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14\\n1\\nBoolean retrieval\\nTerm\\nPostings size\\neyes\\n213312\\nkaleidoscope\\n87009\\nmarmalade\\n107913\\nskies\\n271658\\ntangerine\\n46653\\ntrees\\n316812\\nExercise 1.8\\n[⋆]\\nIf the query is:\\ne. friends AND romans AND (NOT countrymen)\\nhow could we use the frequency of countrymen in evaluating the best query evaluation\\norder? In particular, propose a way of handling negation in determining the order of\\nquery processing.\\nExercise 1.9\\n[⋆⋆]\\nFor a conjunctive query, is processing postings lists in order of size guaranteed to be\\noptimal? Explain why it is, or give an example where it isn’t.\\nExercise 1.10\\n[⋆⋆]\\nWrite out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an x OR y\\nquery.\\nExercise 1.11\\n[⋆⋆]\\nHow should the Boolean query x AND NOT y be handled? Why is naive evaluation\\nof this query normally very expensive? Write out a postings merge algorithm that\\nevaluates this query efﬁciently.\\n1.4\\nThe extended Boolean model versus ranked retrieval\\nThe Boolean retrieval model contrasts with ranked retrieval models such as the\\nRANKED RETRIEVAL\\nMODEL\\nvector space model (Section 6.3), in which users largely use free text queries,\\nFREE TEXT QUERIES\\nthat is, just typing one or more words rather than using a precise language\\nwith operators for building up query expressions, and the system decides\\nwhich documents best satisfy the query. Despite decades of academic re-\\nsearch on the advantages of ranked retrieval, systems implementing the Boo-\\nlean retrieval model were the main or only search option provided by large\\ncommercial information providers for three decades until the early 1990s (ap-\\nproximately the date of arrival of the World Wide Web). However, these\\nsystems did not have just the basic Boolean operations (AND, OR, and NOT)\\nwhich we have presented so far. A strict Boolean expression over terms with\\nan unordered results set is too limited for many of the information needs\\nthat people have, and these systems implemented extended Boolean retrieval\\nmodels by incorporating additional operators such as term proximity oper-\\nators. A proximity operator is a way of specifying that two terms in a query\\nPROXIMITY OPERATOR\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.4\\nThe extended Boolean model versus ranked retrieval\\n15\\nmust occur close to each other in a document, where closeness may be mea-\\nsured by limiting the allowed number of intervening words or by reference\\nto a structural unit such as a sentence or paragraph.\\n\\x0f\\nExample 1.1: Commercial Boolean searching: Westlaw.\\nWestlaw (http://www.westlaw.com/)\\nis the largest commercial legal search service (in terms of the number of paying sub-\\nscribers), with over half a million subscribers performing millions of searches a day\\nover tens of terabytes of text data. The service was started in 1975. In 2005, Boolean\\nsearch (called “Terms and Connectors” by Westlaw) was still the default, and used\\nby a large percentage of users, although ranked free text querying (called “Natural\\nLanguage” by Westlaw) was added in 1992. Here are some example Boolean queries\\non Westlaw:\\nInformation need: Information on the legal theories involved in preventing the\\ndisclosure of trade secrets by employees formerly employed by a competing\\ncompany. Query: \"trade secret\" /s disclos! /s prevent /s employe!\\nInformation need: Requirements for disabled people to be able to access a work-\\nplace.\\nQuery: disab! /p access! /s work-site work-place (employment /3 place)\\nInformation need: Cases about a host’s responsibility for drunk guests.\\nQuery: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\\nNote the long, precise queries and the use of proximity operators, both uncommon\\nin web search. Submitted queries average about ten words in length. Unlike web\\nsearch conventions, a space between words represents disjunction (the tightest bind-\\ning operator), & is AND and /s, /p, and /k ask for matches in the same sentence,\\nsame paragraph or within k words respectively. Double quotes give a phrase search\\n(consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trail-\\ning wildcard query (see Section 3.2, page 51); thus liab! matches all words starting\\nwith liab. Additionally work-site matches any of worksite, work-site or work site; see\\nSection 2.2.1 (page 22). Typical expert queries are usually carefully deﬁned and incre-\\nmentally developed until they obtain what look to be good results to the user.\\nMany users, particularly professionals, prefer Boolean query models.\\nBoolean\\nqueries are precise: a document either matches the query or it does not. This of-\\nfers the user greater control and transparency over what is retrieved. And some do-\\nmains, such as legal materials, allow an effective means of document ranking within a\\nBoolean model: Westlaw returns documents in reverse chronological order, which is\\nin practice quite effective. In 2007, the majority of law librarians still seem to rec-\\nommend terms and connectors for high recall searches, and the majority of legal\\nusers think they are getting greater control by using them. However, this does not\\nmean that Boolean queries are more effective for professional searchers. Indeed, ex-\\nperimenting on a Westlaw subcollection, Turtle (1994) found that free text queries\\nproduced better results than Boolean queries prepared by Westlaw’s own reference\\nlibrarians for the majority of the information needs in his experiments. A general\\nproblem with Boolean search is that using AND operators tends to produce high pre-\\ncision but low recall searches, while using OR operators gives low precision but high\\nrecall searches, and it is difﬁcult or impossible to ﬁnd a satisfactory middle ground.\\nIn this chapter, we have looked at the structure and construction of a basic\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16\\n1\\nBoolean retrieval\\ninverted index, comprising a dictionary and postings lists. We introduced\\nthe Boolean retrieval model, and examined how to do efﬁcient retrieval via\\nlinear time merges and simple query optimization. In Chapters 2–7 we will\\nconsider in detail richer query models and the sort of augmented index struc-\\ntures that are needed to handle them efﬁciently. Here we just mention a few\\nof the main additional things we would like to be able to do:\\n1. We would like to better determine the set of terms in the dictionary and\\nto provide retrieval that is tolerant to spelling mistakes and inconsistent\\nchoice of words.\\n2. It is often useful to search for compounds or phrases that denote a concept\\nsuch as “operating system”. As the Westlaw examples show, we might also\\nwish to do proximity queries such as Gates NEAR Microsoft. To answer\\nsuch queries, the index has to be augmented to capture the proximities of\\nterms in documents.\\n3. A Boolean model only records term presence or absence, but often we\\nwould like to accumulate evidence, giving more weight to documents that\\nhave a term several times as opposed to ones that contain it only once. To\\nbe able to do this we need term frequency information (the number of times\\nTERM FREQUENCY\\na term occurs in a document) in postings lists.\\n4. Boolean queries just retrieve a set of matching documents, but commonly\\nwe wish to have an effective method to order (or “rank”) the returned\\nresults. This requires having a mechanism for determining a document\\nscore which encapsulates how good a match a document is for a query.\\nWith these additional ideas, we will have seen most of the basic technol-\\nogy that supports ad hoc searching over unstructured information. Ad hoc\\nsearching over documents has recently conquered the world, powering not\\nonly web search engines but the kind of unstructured search that lies behind\\nthe large eCommerce websites. Although the main web search engines differ\\nby emphasizing free text querying, most of the basic issues and technologies\\nof indexing and querying remain the same, as we will see in later chapters.\\nMoreover, over time, web search engines have added at least partial imple-\\nmentations of some of the most popular operators from extended Boolean\\nmodels: phrase search is especially popular and most have a very partial\\nimplementation of Boolean operators. Nevertheless, while these options are\\nliked by expert searchers, they are little used by most people and are not the\\nmain focus in work on trying to improve web search engine performance.\\n?\\nExercise 1.12\\n[⋆]\\nWrite a query using Westlaw syntax which would ﬁnd any of the words professor,\\nteacher, or lecturer in the same sentence as a form of the verb explain.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n1.5\\nReferences and further reading\\n17\\nExercise 1.13\\n[⋆]\\nTry using the Boolean search features on a couple of major web search engines. For\\ninstance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar\\nAND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and\\ntop hits. Do they make sense in terms of Boolean logic? Often they haven’t for major\\nsearch engines. Can you make sense of what is going on? What about if you try\\ndifferent words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR\\nconquer. What bound should the number of results from the ﬁrst two queries place\\non the third query? Is this bound observed?\\n1.5\\nReferences and further reading\\nThe practical pursuit of computerized information retrieval began in the late\\n1940s (Cleverdon 1991, Liddy 2005). A great increase in the production of\\nscientiﬁc literature, much in the form of less formal technical reports rather\\nthan traditional journal articles, coupled with the availability of computers,\\nled to interest in automatic document retrieval. However, in those days, doc-\\nument retrieval was always based on author, title, and keywords; full-text\\nsearch came much later.\\nThe article of Bush (1945) provided lasting inspiration for the new ﬁeld:\\n“Consider a future device for individual use, which is a sort of mech-\\nanized private ﬁle and library. It needs a name, and, to coin one at\\nrandom, ‘memex’ will do. A memex is a device in which an individual\\nstores all his books, records, and communications, and which is mech-\\nanized so that it may be consulted with exceeding speed and ﬂexibility.\\nIt is an enlarged intimate supplement to his memory.”\\nThe term Information Retrieval was coined by Calvin Mooers in 1948/1950\\n(Mooers 1950).\\nIn 1958, much newspaper attention was paid to demonstrations at a con-\\nference (see Taube and Wooster 1958) of IBM “auto-indexing” machines, based\\nprimarily on the work of H. P. Luhn. Commercial interest quickly gravitated\\ntowards Boolean retrieval systems, but the early years saw a heady debate\\nover various disparate technologies for retrieval systems. For example Moo-\\ners (1961) dissented:\\n“It is a common fallacy, underwritten at this date by the investment of\\nseveral million dollars in a variety of retrieval hardware, that the al-\\ngebra of George Boole (1847) is the appropriate formalism for retrieval\\nsystem design. This view is as widely and uncritically accepted as it is\\nwrong.”\\nThe observation of AND vs. OR giving you opposite extremes in a precision/\\nrecall tradeoff, but not the middle ground comes from (Lee and Fox 1988).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18\\n1\\nBoolean retrieval\\nThe book (Witten et al. 1999) is the standard reference for an in-depth com-\\nparison of the space and time efﬁciency of the inverted index versus other\\npossible data structures; a more succinct and up-to-date presentation ap-\\npears in Zobel and Moffat (2006). We further discuss several approaches in\\nChapter 5.\\nFriedl (2006) covers the practical usage of regular expressions for searching.\\nREGULAR EXPRESSIONS\\nThe underlying computer science appears in (Hopcroft et al. 2000).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n19\\n2\\nThe term vocabulary and postings\\nlists\\nRecall the major steps in inverted index construction:\\n1. Collect the documents to be indexed.\\n2. Tokenize the text.\\n3. Do linguistic preprocessing of tokens.\\n4. Index the documents that each term occurs in.\\nIn this chapter we ﬁrst brieﬂy mention how the basic unit of a document can\\nbe deﬁned and how the character sequence that it comprises is determined\\n(Section 2.1). We then examine in detail some of the substantive linguis-\\ntic issues of tokenization and linguistic preprocessing, which determine the\\nvocabulary of terms which a system uses (Section 2.2). Tokenization is the\\nprocess of chopping character streams into tokens, while linguistic prepro-\\ncessing then deals with building equivalence classes of tokens which are the\\nset of terms that are indexed. Indexing itself is covered in Chapters 1 and 4.\\nThen we return to the implementation of postings lists. In Section 2.3, we\\nexamine an extended postings list data structure that supports faster query-\\ning, while Section 2.4 covers building postings data structures suitable for\\nhandling phrase and proximity queries, of the sort that commonly appear in\\nboth extended Boolean models and on the web.\\n2.1\\nDocument delineation and character sequence decoding\\n2.1.1\\nObtaining the character sequence in a document\\nDigital documents that are the input to an indexing process are typically\\nbytes in a ﬁle or on a web server. The ﬁrst step of processing is to convert this\\nbyte sequence into a linear sequence of characters. For the case of plain En-\\nglish text in ASCII encoding, this is trivial. But often things get much more\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20\\n2\\nThe term vocabulary and postings lists\\ncomplex. The sequence of characters may be encoded by one of various sin-\\ngle byte or multibyte encoding schemes, such as Unicode UTF-8, or various\\nnational or vendor-speciﬁc standards. We need to determine the correct en-\\ncoding. This can be regarded as a machine learning classiﬁcation problem,\\nas discussed in Chapter 13,1 but is often handled by heuristic methods, user\\nselection, or by using provided document metadata. Once the encoding is\\ndetermined, we decode the byte sequence to a character sequence. We might\\nsave the choice of encoding because it gives some evidence about what lan-\\nguage the document is written in.\\nThe characters may have to be decoded out of some binary representation\\nlike Microsoft Word DOC ﬁles and/or a compressed format such as zip ﬁles.\\nAgain, we must determine the document format, and then an appropriate\\ndecoder has to be used. Even for plain text documents, additional decoding\\nmay need to be done. In XML documents (Section 10.1, page 197), charac-\\nter entities, such as &amp;, need to be decoded to give the correct character,\\nnamely & for &amp;. Finally, the textual part of the document may need to\\nbe extracted out of other material that will not be processed. This might be\\nthe desired handling for XML ﬁles, if the markup is going to be ignored; we\\nwould almost certainly want to do this with postscript or PDF ﬁles. We will\\nnot deal further with these issues in this book, and will assume henceforth\\nthat our documents are a list of characters. Commercial products usually\\nneed to support a broad range of document types and encodings, since users\\nwant things to just work with their data as is. Often, they just think of docu-\\nments as text inside applications and are not even aware of how it is encoded\\non disk. This problem is usually solved by licensing a software library that\\nhandles decoding document formats and character encodings.\\nThe idea that text is a linear sequence of characters is also called into ques-\\ntion by some writing systems, such as Arabic, where text takes on some\\ntwo dimensional and mixed order characteristics, as shown in Figures 2.1\\nand 2.2. But, despite some complicated writing system conventions, there\\nis an underlying sequence of sounds being represented and hence an essen-\\ntially linear structure remains, and this is what is represented in the digital\\nrepresentation of Arabic, as shown in Figure 2.1.\\n2.1.2\\nChoosing a document unit\\nThe next phase is to determine what the document unit for indexing is. Thus\\nDOCUMENT UNIT\\nfar we have assumed that documents are ﬁxed units for the purposes of in-\\ndexing. For example, we take each ﬁle in a folder as a document. But there\\n1. A classiﬁer is a function that takes objects of some sort and assigns them to one of a number\\nof distinct classes (see Chapter 13). Usually classiﬁcation is done by machine learning methods\\nsuch as probabilistic models, but it can also be done by hand-written rules.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.1\\nDocument delineation and character sequence decoding\\n21\\nٌ     ك   ِ  ت  ا   ب   ٌ     ⇐   آِ\\x05َ\\x03ب\\n             un b ā  t  i k  \\n/kitābun/ ‘a book’ \\n \\n◮Figure 2.1\\nAn example of a vocalized Modern Standard Arabic word. The writing\\nis from right to left and letters undergo complex mutations as they are combined. The\\nrepresentation of short vowels (here, /i/ and /u/) and the ﬁnal /n/ (nunation) de-\\nparts from strict linearity by being represented as diacritics above and below letters.\\nNevertheless, the represented text is still clearly a linear ordering of characters repre-\\nsenting sounds. Full vocalization, as here, normally appears only in the Koran and\\nchildren’s books. Day-to-day text is unvocalized (short vowels are not represented\\nbut the letter for ¯a would still appear) or partially vocalized, with short vowels in-\\nserted in places where the writer perceives ambiguities. These choices add further\\ncomplexities to indexing.\\n \\x02\\x03\\x04 \\x05\\x06 \\x07\\x08ا\\x04\\x10\\x0f\\x0e\\r ا\\x0c\\x0b\\nا1962\\n \\x15\\x16\\x17 \\n132\\n\\x05\\x19\\x1a\\x07\\x1b\\x0c #\"!\" !  ا\\x1f\\x1e\\x10\\x1dل ا\\n. \\n \\n ←                   → ←   → ←                               START \\n‘Algeria achieved its independence in 1962 after 132 years of French occupation.’ \\n \\n◮Figure 2.2\\nThe conceptual linear order of characters is not necessarily the order\\nthat you see on the page. In languages that are written right-to-left, such as Hebrew\\nand Arabic, it is quite common to also have left-to-right text interspersed, such as\\nnumbers and dollar amounts. With modern Unicode representation concepts, the\\norder of characters in ﬁles matches the conceptual order, and the reversal of displayed\\ncharacters is handled by the rendering system, but this may not be true for documents\\nin older encodings.\\nare many cases in which you might want to do something different. A tra-\\nditional Unix (mbox-format) email ﬁle stores a sequence of email messages\\n(an email folder) in one ﬁle, but you might wish to regard each email mes-\\nsage as a separate document. Many email messages now contain attached\\ndocuments, and you might then want to regard the email message and each\\ncontained attachment as separate documents. If an email message has an\\nattached zip ﬁle, you might want to decode the zip ﬁle and regard each ﬁle\\nit contains as a separate document. Going in the opposite direction, various\\npieces of web software (such as latex2html) take things that you might regard\\nas a single document (e.g., a Powerpoint ﬁle or a LATEX document) and split\\nthem into separate HTML pages for each slide or subsection, stored as sep-\\narate ﬁles. In these cases, you might want to combine multiple ﬁles into a\\nsingle document.\\nMore generally, for very long documents, the issue of indexing granularity\\nINDEXING\\nGRANULARITY\\narises. For a collection of books, it would usually be a bad idea to index an\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n22\\n2\\nThe term vocabulary and postings lists\\nentire book as a document. A search for Chinese toys might bring up a book\\nthat mentions China in the ﬁrst chapter and toys in the last chapter, but this\\ndoes not make it relevant to the query. Instead, we may well wish to index\\neach chapter or paragraph as a mini-document. Matches are then more likely\\nto be relevant, and since the documents are smaller it will be much easier for\\nthe user to ﬁnd the relevant passages in the document. But why stop there?\\nWe could treat individual sentences as mini-documents. It becomes clear\\nthat there is a precision/recall tradeoff here. If the units get too small, we\\nare likely to miss important passages because terms were distributed over\\nseveral mini-documents, while if units are too large we tend to get spurious\\nmatches and the relevant information is hard for the user to ﬁnd.\\nThe problems with large document units can be alleviated by use of ex-\\nplicit or implicit proximity search (Sections 2.4.2 and 7.2.2), and the trade-\\noffs in resulting system performance that we are hinting at are discussed\\nin Chapter 8. The issue of index granularity, and in particular a need to\\nsimultaneously index documents at multiple levels of granularity, appears\\nprominently in XML retrieval, and is taken up again in Chapter 10. An IR\\nsystem should be designed to offer choices of granularity. For this choice to\\nbe made well, the person who is deploying the system must have a good\\nunderstanding of the document collection, the users, and their likely infor-\\nmation needs and usage patterns. For now, we will henceforth assume that\\na suitable size document unit has been chosen, together with an appropriate\\nway of dividing or aggregating ﬁles, if needed.\\n2.2\\nDetermining the vocabulary of terms\\n2.2.1\\nTokenization\\nGiven a character sequence and a deﬁned document unit, tokenization is the\\ntask of chopping it up into pieces, called tokens, perhaps at the same time\\nthrowing away certain characters, such as punctuation. Here is an example\\nof tokenization:\\nInput: Friends, Romans, Countrymen, lend me your ears;\\nOutput: Friends\\nRomans\\nCountrymen\\nlend\\nme\\nyour\\nears\\nThese tokens are often loosely referred to as terms or words, but it is some-\\ntimes important to make a type/token distinction. A token is an instance\\nTOKEN\\nof a sequence of characters in some particular document that are grouped\\ntogether as a useful semantic unit for processing. A type is the class of all\\nTYPE\\ntokens containing the same character sequence. A term is a (perhaps nor-\\nTERM\\nmalized) type that is included in the IR system’s dictionary. The set of index\\nterms could be entirely distinct from the tokens, for instance, they could be\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n23\\nsemantic identiﬁers in a taxonomy, but in practice in modern IR systems they\\nare strongly related to the tokens in the document. However, rather than be-\\ning exactly the tokens that appear in the document, they are usually derived\\nfrom them by various normalization processes which are discussed in Sec-\\ntion 2.2.3.2 For example, if the document to be indexed is to sleep perchance to\\ndream, then there are 5 tokens, but only 4 types (since there are 2 instances of\\nto). However, if to is omitted from the index (as a stop word, see Section 2.2.2\\n(page 27)), then there will be only 3 terms: sleep, perchance, and dream.\\nThe major question of the tokenization phase is what are the correct tokens\\nto use? In this example, it looks fairly trivial: you chop on whitespace and\\nthrow away punctuation characters. This is a starting point, but even for\\nEnglish there are a number of tricky cases. For example, what do you do\\nabout the various uses of the apostrophe for possession and contractions?\\nMr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t\\namusing.\\nFor O’Neill, which of the following is the desired tokenization?\\nneill\\noneill\\no’neill\\no’\\nneill\\no\\nneill ?\\nAnd for aren’t, is it:\\naren’t\\narent\\nare\\nn’t\\naren\\nt ?\\nA simple strategy is to just split on all non-alphanumeric characters, but\\nwhile o\\nneill looks okay, aren\\nt looks intuitively bad. For all of them,\\nthe choices determine which Boolean queries will match. A query of neill\\nAND capital will match in three cases but not the other two. In how many\\ncases would a query of o’neill AND capital match? If no preprocessing of a\\nquery is done, then it would match in only one of the ﬁve cases. For either\\n2. That is, as deﬁned here, tokens that are not indexed (stop words) are not terms, and if mul-\\ntiple tokens are collapsed together via normalization, they are indexed as one term, under the\\nnormalized form. However, we later relax this deﬁnition when discussing classiﬁcation and\\nclustering in Chapters 13–18, where there is no index. In these chapters, we drop the require-\\nment of inclusion in the dictionary. A term means a normalized word.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n24\\n2\\nThe term vocabulary and postings lists\\nBoolean or free text queries, you always want to do the exact same tokeniza-\\ntion of document and query words, generally by processing queries with the\\nsame tokenizer. This guarantees that a sequence of characters in a text will\\nalways match the same sequence typed in a query.3\\nThese issues of tokenization are language-speciﬁc. It thus requires the lan-\\nguage of the document to be known. Language identiﬁcation based on clas-\\nLANGUAGE\\nIDENTIFICATION\\nsiﬁers that use short character subsequences as features is highly effective;\\nmost languages have distinctive signature patterns (see page 46 for refer-\\nences).\\nFor most languages and particular domains within them there are unusual\\nspeciﬁc tokens that we wish to recognize as terms, such as the programming\\nlanguages C++ and C#, aircraft names like B-52, or a T.V. show name such\\nas M*A*S*H – which is sufﬁciently integrated into popular culture that you\\nﬁnd usages such as M*A*S*H-style hospitals. Computer technology has in-\\ntroduced new types of character sequences that a tokenizer should probably\\ntokenize as a single token, including email addresses (jblack@mail.yahoo.com),\\nweb URLs (http://stuff.big.com/new/specials.html),numeric IP addresses (142.32.48.231),\\npackage tracking numbers (1Z9999W99845399981), and more. One possible\\nsolution is to omit from indexing tokens such as monetary amounts, num-\\nbers, and URLs, since their presence greatly expands the size of the vocab-\\nulary. However, this comes at a large cost in restricting what people can\\nsearch for. For instance, people might want to search in a bug database for\\nthe line number where an error occurs. Items such as the date of an email,\\nwhich have a clear semantic type, are often indexed separately as document\\nmetadata (see Section 6.1, page 110).\\nIn English, hyphenation is used for various purposes ranging from split-\\nHYPHENS\\nting up vowels in words (co-education) to joining nouns as names (Hewlett-\\nPackard) to a copyediting device to show word grouping (the hold-him-back-\\nand-drag-him-away maneuver). It is easy to feel that the ﬁrst example should be\\nregarded as one token (and is indeed more commonly written as just coedu-\\ncation), the last should be separated into words, and that the middle case is\\nunclear. Handling hyphens automatically can thus be complex: it can either\\nbe done as a classiﬁcation problem, or more commonly by some heuristic\\nrules, such as allowing short hyphenated preﬁxes on words, but not longer\\nhyphenated forms.\\nConceptually, splitting on white space can also split what should be re-\\ngarded as a single token. This occurs most commonly with names (San Fran-\\ncisco, Los Angeles) but also with borrowed foreign phrases (au fait) and com-\\n3. For the free text case, this is straightforward. The Boolean case is more complex: this tok-\\nenization may produce multiple terms from one query word. This can be handled by combining\\nthe terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system\\nto handle the opposite case where the user entered as two terms something that was tokenized\\ntogether in the document processing.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n25\\npounds that are sometimes written as a single word and sometimes space\\nseparated (such as white space vs. whitespace). Other cases with internal spaces\\nthat we might wish to regard as a single token include phone numbers ((800) 234-\\n2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad\\nretrieval results, for example, if a search for York University mainly returns\\ndocuments containing New York University. The problems of hyphens and\\nnon-separating whitespace can even interact. Advertisements for air fares\\nfrequently contain items like San Francisco-Los Angeles, where simply doing\\nwhitespace splitting would give unfortunate results. In such cases, issues of\\ntokenization interact with handling phrase queries (which we discuss in Sec-\\ntion 2.4 (page 39)), particularly if we would like queries for all of lowercase,\\nlower-case and lower case to return the same results. The last two can be han-\\ndled by splitting on hyphens and using a phrase index. Getting the ﬁrst case\\nright would depend on knowing that it is sometimes written as two words\\nand also indexing it in this way. One effective strategy in practice, which\\nis used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis\\n(Example 1.1), is to encourage users to enter hyphens wherever they may be\\npossible, and whenever there is a hyphenated form, the system will general-\\nize the query to cover all three of the one word, hyphenated, and two word\\nforms, so that a query for over-eager will search for over-eager OR “over eager”\\nOR overeager. However, this strategy depends on user training, since if you\\nquery using either of the other two forms, you get no generalization.\\nEach new language presents some new issues. For instance, French has a\\nvariant use of the apostrophe for a reduced deﬁnite article ‘the’ before a word\\nbeginning with a vowel (e.g., l’ensemble) and has some uses of the hyphen\\nwith postposed clitic pronouns in imperatives and questions (e.g., donne-\\nmoi ‘give me’). Getting the ﬁrst case correct will affect the correct indexing\\nof a fair percentage of nouns and adjectives: you would want documents\\nmentioning both l’ensemble and un ensemble to be indexed under ensemble.\\nOther languages make the problem harder in new ways. German writes\\ncompound nouns without spaces (e.g., Computerlinguistik ‘computational lin-\\nCOMPOUNDS\\nguistics’; Lebensversicherungsgesellschaftsangestellter ‘life insurance company\\nemployee’). Retrieval systems for German greatly beneﬁt from the use of a\\ncompound-splitter module, which is usually implemented by seeing if a word\\nCOMPOUND-SPLITTER\\ncan be subdivided into multiple words that appear in a vocabulary. This phe-\\nnomenon reaches its limit case with major East Asian Languages (e.g., Chi-\\nnese, Japanese, Korean, and Thai), where text is written without any spaces\\nbetween words. An example is shown in Figure 2.3. One approach here is to\\nperform word segmentation as prior linguistic processing. Methods of word\\nWORD SEGMENTATION\\nsegmentation vary from having a large vocabulary and taking the longest\\nvocabulary match with some heuristics for unknown words to the use of\\nmachine learning sequence models, such as hidden Markov models or condi-\\ntional random ﬁelds, trained over hand-segmented words (see the references\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n26\\n2\\nThe term vocabulary and postings lists\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x04\\n\\x05\\n\\x06\\n\\x07\\n\\x05\\n\\x08\\n\\t\\n\\n\\x0b\\n\\x0c\\n\\r\\n\\x0e\\n\\x0f\\n\\x10\\n\\x11\\n\\x12\\n\\x13\\n\\x14\\n\\x15\\n\\x16\\n\\x17\\n\\x18\\n\\x19\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x05\\n\\x08\\n\\t\\n\\x1a\\n\\x1b\\n\\x1c\\n\\x1d\\n\\x1e\\n\\x1f\\n \\n!\\n\"\\n#\\n$\\n%\\n&\\n\\'\\n\\x18\\n\\x12\\n\\'\\n\\x18\\n(\\n)\\n*\\n\\x19\\n\\x00\\x01\\n\\x02\\n\\x03\\n+\\n,\\n#\\n-\\n\\x08\\n\\r\\n.\\n/\\n\\x12\\n◮Figure 2.3\\nThe standard unsegmented form of Chinese text using the simpliﬁed\\ncharacters of mainland China. There is no whitespace between words, not even be-\\ntween sentences – the apparent space after the Chinese period (◦) is just a typograph-\\nical illusion caused by placing the character on the left side of its square box. The\\nﬁrst sentence is just words in Chinese characters with no spaces between them. The\\nsecond and third sentences include Arabic numerals and punctuation breaking up\\nthe Chinese characters.\\n◮Figure 2.4\\nAmbiguities in Chinese word segmentation. The two characters can\\nbe treated as one word meaning ‘monk’ or as a sequence of two words meaning ‘and’\\nand ‘still’.\\na\\nan\\nand\\nare\\nas\\nat\\nbe\\nby\\nfor\\nfrom\\nhas\\nhe\\nin\\nis\\nit\\nits\\nof\\non\\nthat\\nthe\\nto\\nwas\\nwere\\nwill\\nwith\\n◮Figure 2.5\\nA stop list of 25 semantically non-selective words which are common\\nin Reuters-RCV1.\\nin Section 2.5). Since there are multiple possible segmentations of character\\nsequences (see Figure 2.4), all such methods make mistakes sometimes, and\\nso you are never guaranteed a consistent unique tokenization. The other ap-\\nproach is to abandon word-based indexing and to do all indexing via just\\nshort subsequences of characters (character k-grams), regardless of whether\\nparticular sequences cross word boundaries or not. Three reasons why this\\napproach is appealing are that an individual Chinese character is more like a\\nsyllable than a letter and usually has some semantic content, that most words\\nare short (the commonest length is 2 characters), and that, given the lack of\\nstandardization of word breaking in the writing system, it is not always clear\\nwhere word boundaries should be placed anyway. Even in English, some\\ncases of where to put word boundaries are just orthographic conventions –\\nthink of notwithstanding vs. not to mention or into vs. on to – but people are\\neducated to write the words with consistent use of spaces.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n27\\n2.2.2\\nDropping common terms: stop words\\nSometimes, some extremely common words which would appear to be of\\nlittle value in helping select documents matching a user need are excluded\\nfrom the vocabulary entirely. These words are called stop words. The general\\nSTOP WORDS\\nstrategy for determining a stop list is to sort the terms by collection frequency\\nCOLLECTION\\nFREQUENCY\\n(the total number of times each term appears in the document collection),\\nand then to take the most frequent terms, often hand-ﬁltered for their se-\\nmantic content relative to the domain of the documents being indexed, as\\na stop list, the members of which are then discarded during indexing. An\\nSTOP LIST\\nexample of a stop list is shown in Figure 2.5. Using a stop list signiﬁcantly\\nreduces the number of postings that a system has to store; we will present\\nsome statistics on this in Chapter 5 (see Table 5.1, page 87). And a lot of\\nthe time not indexing stop words does little harm: keyword searches with\\nterms like the and by don’t seem very useful. However, this is not true for\\nphrase searches. The phrase query “President of the United States”, which con-\\ntains two stop words, is more precise than President AND “United States”. The\\nmeaning of ﬂights to London is likely to be lost if the word to is stopped out. A\\nsearch for Vannevar Bush’s article As we may think will be difﬁcult if the ﬁrst\\nthree words are stopped out, and the system searches simply for documents\\ncontaining the word think. Some special query types are disproportionately\\naffected. Some song titles and well known pieces of verse consist entirely of\\nwords that are commonly on stop lists (To be or not to be, Let It Be, I don’t want\\nto be, ...).\\nThe general trend in IR systems over time has been from standard use of\\nquite large stop lists (200–300 terms) to very small stop lists (7–12 terms)\\nto no stop list whatsoever. Web search engines generally do not use stop\\nlists. Some of the design of modern IR systems has focused precisely on\\nhow we can exploit the statistics of language so as to be able to cope with\\ncommon words in better ways. We will show in Section 5.3 (page 95) how\\ngood compression techniques greatly reduce the cost of storing the postings\\nfor common words. Section 6.2.1 (page 117) then discusses how standard\\nterm weighting leads to very common words having little impact on doc-\\nument rankings. Finally, Section 7.1.5 (page 140) shows how an IR system\\nwith impact-sorted indexes can terminate scanning a postings list early when\\nweights get small, and hence common words do not cause a large additional\\nprocessing cost for the average query, even though postings lists for stop\\nwords are very long. So for most modern IR systems, the additional cost of\\nincluding stop words is not that big – neither in terms of index size nor in\\nterms of query processing time.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n28\\n2\\nThe term vocabulary and postings lists\\nQuery term\\nTerms in documents that should be matched\\nWindows\\nWindows\\nwindows\\nWindows, windows, window\\nwindow\\nwindow, windows\\n◮Figure 2.6\\nAn example of how asymmetric expansion of query terms can usefully\\nmodel users’ expectations.\\n2.2.3\\nNormalization (equivalence classing of terms)\\nHaving broken up our documents (and also our query) into tokens, the easy\\ncase is if tokens in the query just match tokens in the token list of the doc-\\nument. However, there are many cases when two character sequences are\\nnot quite the same but you would like a match to occur. For instance, if you\\nsearch for USA, you might hope to also match documents containing U.S.A.\\nToken normalization is the process of canonicalizing tokens so that matches\\nTOKEN\\nNORMALIZATION\\noccur despite superﬁcial differences in the character sequences of the to-\\nkens.4 The most standard way to normalize is to implicitly create equivalence\\nEQUIVALENCE CLASSES\\nclasses, which are normally named after one member of the set. For instance,\\nif the tokens anti-discriminatory and antidiscriminatory are both mapped onto\\nthe term antidiscriminatory, in both the document text and queries, then searches\\nfor one term will retrieve documents that contain either.\\nThe advantage of just using mapping rules that remove characters like hy-\\nphens is that the equivalence classing to be done is implicit, rather than being\\nfully calculated in advance: the terms that happen to become identical as the\\nresult of these rules are the equivalence classes. It is only easy to write rules\\nof this sort that remove characters. Since the equivalence classes are implicit,\\nit is not obvious when you might want to add characters. For instance, it\\nwould be hard to know to turn antidiscriminatory into anti-discriminatory.\\nAn alternative to creating equivalence classes is to maintain relations be-\\ntween unnormalized tokens. This method can be extended to hand-constructed\\nlists of synonyms such as car and automobile, a topic we discuss further in\\nChapter 9. These term relationships can be achieved in two ways. The usual\\nway is to index unnormalized tokens and to maintain a query expansion list\\nof multiple vocabulary entries to consider for a certain query term. A query\\nterm is then effectively a disjunction of several postings lists. The alterna-\\ntive is to perform the expansion during index construction. When the doc-\\nument contains automobile, we index it under car as well (and, usually, also\\nvice-versa). Use of either of these methods is considerably less efﬁcient than\\nequivalence classing, as there are more postings to store and merge. The ﬁrst\\n4. It is also often referred to as term normalization, but we prefer to reserve the name term for the\\noutput of the normalization process.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n29\\nmethod adds a query expansion dictionary and requires more processing at\\nquery time, while the second method requires more space for storing post-\\nings. Traditionally, expanding the space required for the postings lists was\\nseen as more disadvantageous, but with modern storage costs, the increased\\nﬂexibility that comes from distinct postings lists is appealing.\\nThese approaches are more ﬂexible than equivalence classes because the\\nexpansion lists can overlap while not being identical. This means there can\\nbe an asymmetry in expansion. An example of how such an asymmetry can\\nbe exploited is shown in Figure 2.6: if the user enters windows, we wish to\\nallow matches with the capitalized Windows operating system, but this is not\\nplausible if the user enters window, even though it is plausible for this query\\nto also match lowercase windows.\\nThe best amount of equivalence classing or query expansion to do is a\\nfairly open question. Doing some deﬁnitely seems a good idea. But doing a\\nlot can easily have unexpected consequences of broadening queries in unin-\\ntended ways. For instance, equivalence-classing U.S.A. and USA to the latter\\nby deleting periods from tokens might at ﬁrst seem very reasonable, given\\nthe prevalent pattern of optional use of periods in acronyms. However, if I\\nput in as my query term C.A.T., I might be rather upset if it matches every\\nappearance of the word cat in documents.5\\nBelow we present some of the forms of normalization that are commonly\\nemployed and how they are implemented. In many cases they seem helpful,\\nbut they can also do harm. In fact, you can worry about many details of\\nequivalence classing, but it often turns out that providing processing is done\\nconsistently to the query and to documents, the ﬁne details may not have\\nmuch aggregate effect on performance.\\nAccents and diacritics.\\nDiacritics on characters in English have a fairly\\nmarginal status, and we might well want cliché and cliche to match, or naive\\nand naïve. This can be done by normalizing tokens to remove diacritics. In\\nmany other languages, diacritics are a regular part of the writing system and\\ndistinguish different sounds. Occasionally words are distinguished only by\\ntheir accents. For instance, in Spanish, peña is ‘a cliff’, while pena is ‘sorrow’.\\nNevertheless, the important question is usually not prescriptive or linguistic\\nbut is a question of how users are likely to write queries for these words. In\\nmany cases, users will enter queries for words without diacritics, whether\\nfor reasons of speed, laziness, limited software, or habits born of the days\\nwhen it was hard to use non-ASCII text on many computer systems. In these\\ncases, it might be best to equate all words to a form without diacritics.\\n5. At the time we wrote this chapter (Aug. 2005), this was actually the case on Google: the top\\nresult for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n30\\n2\\nThe term vocabulary and postings lists\\nCapitalization/case-folding.\\nA common strategy is to do case-folding by re-\\nCASE-FOLDING\\nducing all letters to lower case. Often this is a good idea: it will allow in-\\nstances of Automobile at the beginning of a sentence to match with a query of\\nautomobile. It will also help on a web search engine when most of your users\\ntype in ferrari when they are interested in a Ferrari car. On the other hand,\\nsuch case folding can equate words that might better be kept apart. Many\\nproper nouns are derived from common nouns and so are distinguished only\\nby case, including companies (General Motors, The Associated Press), govern-\\nment organizations (the Fed vs. fed) and person names (Bush, Black). We al-\\nready mentioned an example of unintended query expansion with acronyms,\\nwhich involved not only acronym normalization (C.A.T. →CAT) but also\\ncase-folding (CAT →cat).\\nFor English, an alternative to making every token lowercase is to just make\\nsome tokens lowercase. The simplest heuristic is to convert to lowercase\\nwords at the beginning of a sentence and all words occurring in a title that is\\nall uppercase or in which most or all words are capitalized. These words are\\nusually ordinary words that have been capitalized. Mid-sentence capitalized\\nwords are left as capitalized (which is usually correct). This will mostly avoid\\ncase-folding in cases where distinctions should be kept apart. The same task\\ncan be done more accurately by a machine learning sequence model which\\nuses more features to make the decision of when to case-fold. This is known\\nas truecasing. However, trying to get capitalization right in this way probably\\nTRUECASING\\ndoesn’t help if your users usually use lowercase regardless of the correct case\\nof words. Thus, lowercasing everything often remains the most practical\\nsolution.\\nOther issues in English.\\nOther possible normalizations are quite idiosyn-\\ncratic and particular to English.\\nFor instance, you might wish to equate\\nne’er and never or the British spelling colour and the American spelling color.\\nDates, times and similar items come in multiple formats, presenting addi-\\ntional challenges. You might wish to collapse together 3/12/91 and Mar. 12,\\n1991. However, correct processing here is complicated by the fact that in the\\nU.S., 3/12/91 is Mar. 12, 1991, whereas in Europe it is 3 Dec 1991.\\nOther languages.\\nEnglish has maintained a dominant position on the WWW;\\napproximately 60% of web pages are in English (Gerrand 2007). But that still\\nleaves 40% of the web, and the non-English portion might be expected to\\ngrow over time, since less than one third of Internet users and less than 10%\\nof the world’s population primarily speak English. And there are signs of\\nchange: Sifry (2007) reports that only about one third of blog posts are in\\nEnglish.\\nOther languages again present distinctive issues in equivalence classing.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n31\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x04\\n\\x05\\n\\x06\\n\\x07\\n\\x08\\n\\x06\\n\\t\\n\\n\\x0b\\n\\x0c\\n\\r\\n\\x0e\\n\\x0f\\n\\x10\\n\\x01\\n\\x11\\n\\x12\\n\\x13\\n\\x14\\n\\x15\\n\\x16\\n\\x17\\n\\x18\\n\\x19\\n\\x07\\n\\x1a\\n\\x1b\\n\\x1c\\n\\x1d\\n\\x1e\\n\\x1f\\n\\x1f\\n \\n!\\n\"\\n \\n!\\n#\\n$\\n\\x0c\\n%\\n\\x01\\n\\x0c\\n&\\n\\'\\n(\\n)\\n\\t\\n*\\n+\\n,\\n-\\n.\\n/\\n0\\n)\\n\\x10\\n\\r\\n1\\n\\x0c\\n2\\n3\\n4\\n5\\n6\\n7\\n&\\n+\\n8\\n9\\n\\n:\\n;\\n:\\n<\\n\\x07\\n=\\n>\\n\\t\\n?\\n@\\nA\\nB\\nC\\n\\x15\\n-\\nD\\nE\\n6\\n8\\n9\\n\\n:\\n;\\n:\\n<\\n)\\nF\\nG\\n*\\nH\\nI\\n\\t\\n*\\n:\\n\\x1c\\nJ\\n)\\nK\\n+\\nL\\nM\\nN\\n?\\nO\\nP\\n\\x1c\\nQ\\nR\\nS\\n\\x01\\nT\\n\\x07\\nU\\nV\\nV\\nW\\nX\\nY\\n&\\nZ\\n[\\nN\\n?\\n)\\n\\x1b\\n+\\n\\\\\\n]\\n;\\n^\\n_\\n+\\n\\x12\\n`\\n4\\na\\n+\\nb\\n;\\nc\\n\\x07\\nd\\ne\\n*\\nf\\nV\\ng\\nh\\nV\\n-\\n?\\ni\\nN\\nj\\nk\\nl\\nm\\nn\\n\\x13\\n:\\nA\\no\\n\\x06\\n\\x08\\n\\x06\\np\\nN\\n5\\n+\\nq\\nV\\nr\\ns\\nt\\nu\\n&\\nv\\nw\\nx\\n)\\nQ\\ny\\nz\\n{\\nh\\n|\\n&\\n}\\n\\x06\\n\\x15\\n~\\n\\x7f\\nM\\n?\\n@\\nA\\n◮Figure 2.7\\nJapanese makes use of multiple intermingled writing systems and,\\nlike Chinese, does not segment words. The text is mainly Chinese characters with\\nthe hiragana syllabary for inﬂectional endings and function words. The part in latin\\nletters is actually a Japanese expression, but has been taken up as the name of an\\nenvironmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His\\nname is written using the katakana syllabary in the middle of the ﬁrst line. The ﬁrst\\nfour characters of the ﬁnal line express a monetary amount that we would want to\\nmatch with ¥500,000 (500,000 Japanese yen).\\nThe French word for the has distinctive forms based not only on the gender\\n(masculine or feminine) and number of the following noun, but also depend-\\ning on whether the following word begins with a vowel: le, la, l’, les. We may\\nwell wish to equivalence class these various forms of the. German has a con-\\nvention whereby vowels with an umlaut can be rendered instead as a two\\nvowel digraph. We would want to treat Schütze and Schuetze as equivalent.\\nJapanese is a well-known difﬁcult writing system, as illustrated in Fig-\\nure 2.7. Modern Japanese is standardly an intermingling of multiple alpha-\\nbets, principally Chinese characters, two syllabaries (hiragana and katakana)\\nand western characters (Latin letters, Arabic numerals, and various sym-\\nbols). While there are strong conventions and standardization through the\\neducation system over the choice of writing system, in many cases the same\\nword can be written with multiple writing systems. For example, a word\\nmay be written in katakana for emphasis (somewhat like italics). Or a word\\nmay sometimes be written in hiragana and sometimes in Chinese charac-\\nters. Successful retrieval thus requires complex equivalence classing across\\nthe writing systems. In particular, an end user might commonly present a\\nquery entirely in hiragana, because it is easier to type, just as Western end\\nusers commonly use all lowercase.\\nDocument collections being indexed can include documents from many\\ndifferent languages. Or a single document can easily contain text from mul-\\ntiple languages. For instance, a French email might quote clauses from a\\ncontract document written in English. Most commonly, the language is de-\\ntected and language-particular tokenization and normalization rules are ap-\\nplied at a predetermined granularity, such as whole documents or individual\\nparagraphs, but this still will not correctly deal with cases where language\\nchanges occur for brief quotations. When document collections contain mul-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n32\\n2\\nThe term vocabulary and postings lists\\ntiple languages, a single index may have to contain terms of several lan-\\nguages. One option is to run a language identiﬁcation classiﬁer on docu-\\nments and then to tag terms in the vocabulary for their language. Or this\\ntagging can simply be omitted, since it is relatively rare for the exact same\\ncharacter sequence to be a word in different languages.\\nWhen dealing with foreign or complex words, particularly foreign names,\\nthe spelling may be unclear or there may be variant transliteration standards\\ngiving different spellings (for example, Chebyshev and Tchebycheff or Beijing\\nand Peking). One way of dealing with this is to use heuristics to equiva-\\nlence class or expand terms with phonetic equivalents. The traditional and\\nbest known such algorithm is the Soundex algorithm, which we cover in\\nSection 3.4 (page 63).\\n2.2.4\\nStemming and lemmatization\\nFor grammatical reasons, documents are going to use different forms of a\\nword, such as organize, organizes, and organizing. Additionally, there are fami-\\nlies of derivationally related words with similar meanings, such as democracy,\\ndemocratic, and democratization. In many situations, it seems as if it would be\\nuseful for a search for one of these words to return documents that contain\\nanother word in the set.\\nThe goal of both stemming and lemmatization is to reduce inﬂectional\\nforms and sometimes derivationally related forms of a word to a common\\nbase form. For instance:\\nam, are, is ⇒be\\ncar, cars, car’s, cars’ ⇒car\\nThe result of this mapping of text will be something like:\\nthe boy’s cars are different colors ⇒\\nthe boy car be differ color\\nHowever, the two words differ in their ﬂavor. Stemming usually refers to\\nSTEMMING\\na crude heuristic process that chops off the ends of words in the hope of\\nachieving this goal correctly most of the time, and often includes the re-\\nmoval of derivational afﬁxes. Lemmatization usually refers to doing things\\nLEMMATIZATION\\nproperly with the use of a vocabulary and morphological analysis of words,\\nnormally aiming to remove inﬂectional endings only and to return the base\\nor dictionary form of a word, which is known as the lemma. If confronted\\nLEMMA\\nwith the token saw, stemming might return just s, whereas lemmatization\\nwould attempt to return either see or saw depending on whether the use of\\nthe token was as a verb or a noun. The two may also differ in that stemming\\nmost commonly collapses derivationally related words, whereas lemmatiza-\\ntion commonly only collapses the different inﬂectional forms of a lemma.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n33\\nLinguistic processing for stemming or lemmatization is often done by an ad-\\nditional plug-in component to the indexing process, and a number of such\\ncomponents exist, both commercial and open-source.\\nThe most common algorithm for stemming English, and one that has re-\\npeatedly been shown to be empirically very effective, is Porter’s algorithm\\nPORTER STEMMER\\n(Porter 1980). The entire algorithm is too long and intricate to present here,\\nbut we will indicate its general nature. Porter’s algorithm consists of 5 phases\\nof word reductions, applied sequentially. Within each phase there are var-\\nious conventions to select rules, such as selecting the rule from each rule\\ngroup that applies to the longest sufﬁx. In the ﬁrst phase, this convention is\\nused with the following rule group:\\n(2.1)\\nRule\\nExample\\nSSES\\n→\\nSS\\ncaresses\\n→\\ncaress\\nIES\\n→\\nI\\nponies\\n→\\nponi\\nSS\\n→\\nSS\\ncaress\\n→\\ncaress\\nS\\n→\\ncats\\n→\\ncat\\nMany of the later rules use a concept of the measure of a word, which loosely\\nchecks the number of syllables to see whether a word is long enough that it\\nis reasonable to regard the matching portion of a rule as a sufﬁx rather than\\nas part of the stem of a word. For example, the rule:\\n(m > 1)\\nEMENT\\n→\\nwould map replacement to replac, but not cement to c. The ofﬁcial site for the\\nPorter Stemmer is:\\nhttp://www.tartarus.org/˜martin/PorterStemmer/\\nOther stemmers exist, including the older, one-pass Lovins stemmer (Lovins\\n1968), and newer entrants like the Paice/Husk stemmer (Paice 1990); see:\\nhttp://www.cs.waikato.ac.nz/˜eibe/stemmers/\\nhttp://www.comp.lancs.ac.uk/computing/research/stemming/\\nFigure 2.8 presents an informal comparison of the different behaviors of these\\nstemmers. Stemmers use language-speciﬁc rules, but they require less know-\\nledge than a lemmatizer, which needs a complete vocabulary and morpho-\\nlogical analysis to correctly lemmatize words. Particular domains may also\\nrequire special stemming rules. However, the exact stemmed form does not\\nmatter, only the equivalence classes it forms.\\nRather than using a stemmer, you can use a lemmatizer, a tool from Nat-\\nLEMMATIZER\\nural Language Processing which does full morphological analysis to accu-\\nrately identify the lemma for each word. Doing full morphological analysis\\nproduces at most very modest beneﬁts for retrieval. It is hard to say more,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n34\\n2\\nThe term vocabulary and postings lists\\nSample text: Such an analysis can reveal features that are not easily visible\\nfrom the variations in the individual genes and can lead to a picture of\\nexpression that is more biologically transparent and accessible to\\ninterpretation\\nLovins stemmer: such an analys can reve featur that ar not eas vis from th\\nvari in th individu gen and can lead to a pictur of expres that is mor\\nbiolog transpar and acces to interpres\\nPorter stemmer: such an analysi can reveal featur that ar not easili visibl\\nfrom the variat in the individu gene and can lead to a pictur of express\\nthat is more biolog transpar and access to interpret\\nPaice stemmer: such an analys can rev feat that are not easy vis from the\\nvary in the individ gen and can lead to a pict of express that is mor\\nbiolog transp and access to interpret\\n◮Figure 2.8\\nA comparison of three stemming algorithms on a sample text.\\nbecause either form of normalization tends not to improve English informa-\\ntion retrieval performance in aggregate – at least not by very much. While\\nit helps a lot for some queries, it equally hurts performance a lot for others.\\nStemming increases recall while harming precision. As an example of what\\ncan go wrong, note that the Porter stemmer stems all of the following words:\\noperate operating operates operation operative operatives operational\\nto oper. However, since operate in its various forms is a common verb, we\\nwould expect to lose considerable precision on queries such as the following\\nwith Porter stemming:\\noperational AND research\\noperating AND system\\noperative AND dentistry\\nFor a case like this, moving to using a lemmatizer would not completely ﬁx\\nthe problem because particular inﬂectional forms are used in particular col-\\nlocations: a sentence with the words operate and system is not a good match\\nfor the query operating AND system. Getting better value from term normaliza-\\ntion depends more on pragmatic issues of word use than on formal issues of\\nlinguistic morphology.\\nThe situation is different for languages with much more morphology (such\\nas Spanish, German, and Finnish). Results in the European CLEF evaluations\\nhave repeatedly shown quite large gains from the use of stemmers (and com-\\npound splitting for languages like German); see the references in Section 2.5.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n35\\n?\\nExercise 2.1\\n[⋆]\\nAre the following statements true or false?\\na. In a Boolean retrieval system, stemming never lowers precision.\\nb. In a Boolean retrieval system, stemming never lowers recall.\\nc. Stemming increases the size of the vocabulary.\\nd. Stemming should be invoked at indexing time but not while processing a query.\\nExercise 2.2\\n[⋆]\\nSuggest what normalized form should be used for these words (including the word\\nitself as a possibility):\\na. ’Cos\\nb. Shi’ite\\nc. cont’d\\nd. Hawai’i\\ne. O’Rourke\\nExercise 2.3\\n[⋆]\\nThe following pairs of words are stemmed to the same form by the Porter stemmer.\\nWhich pairs would you argue shouldn’t be conﬂated. Give your reasoning.\\na. abandon/abandonment\\nb. absorbency/absorbent\\nc. marketing/markets\\nd. university/universe\\ne. volume/volumes\\nExercise 2.4\\n[⋆]\\nFor the Porter stemmer rule group shown in (2.1):\\na. What is the purpose of including an identity rule such as SS →SS?\\nb. Applying just this rule group, what will the following words be stemmed to?\\ncircus\\ncanaries\\nboss\\nc. What rule should be added to correctly stem pony?\\nd. The stemming for ponies and pony might seem strange. Does it have a deleterious\\neffect on retrieval? Why or why not?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n36\\n2\\nThe term vocabulary and postings lists\\n◮Figure 2.9\\nPostings lists with skip pointers. The postings intersection can use a\\nskip pointer when the end point is still less than the item on the other list.\\n2.3\\nFaster postings list intersection via skip pointers\\nIn the remainder of this chapter, we will discuss extensions to postings list\\ndata structures and ways to increase the efﬁciency of using postings lists. Re-\\ncall the basic postings list intersection operation from Section 1.3 (page 10):\\nwe walk through the two postings lists simultaneously, in time linear in the\\ntotal number of postings entries. If the list lengths are m and n, the intersec-\\ntion takes O(m + n) operations. Can we do better than this? That is, empiri-\\ncally, can we usually process postings list intersection in sublinear time? We\\ncan, if the index isn’t changing too fast.\\nOne way to do this is to use a skip list by augmenting postings lists with\\nSKIP LIST\\nskip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are\\neffectively shortcuts that allow us to avoid processing parts of the postings\\nlist that will not ﬁgure in the search results. The two questions are then where\\nto place skip pointers and how to do efﬁcient merging using skip pointers.\\nConsider ﬁrst efﬁcient merging, with Figure 2.9 as an example. Suppose\\nwe’ve stepped through the lists in the ﬁgure until we have matched 8 on\\neach list and moved it to the results list. We advance both pointers, giving us\\n16 on the upper list and 41 on the lower list. The smallest item is then the\\nelement 16 on the top list. Rather than simply advancing the upper pointer,\\nwe ﬁrst check the skip list pointer and note that 28 is also less than 41. Hence\\nwe can follow the skip list pointer, and then we advance the upper pointer\\nto 28 . We thus avoid stepping to 19 and 23 on the upper list. A number\\nof variant versions of postings list intersection with skip pointers is possible\\ndepending on when exactly you check the skip pointer. One version is shown\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.3\\nFaster postings list intersection via skip pointers\\n37\\nINTERSECTWITHSKIPS(p1, p2)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen ADD(answer, docID(p1))\\n5\\np1 ←next(p1)\\n6\\np2 ←next(p2)\\n7\\nelse if docID(p1) < docID(p2)\\n8\\nthen if hasSkip(p1) and (docID(skip(p1)) ≤docID(p2))\\n9\\nthen while hasSkip(p1) and (docID(skip(p1)) ≤docID(p2))\\n10\\ndo p1 ←skip(p1)\\n11\\nelse p1 ←next(p1)\\n12\\nelse if hasSkip(p2) and (docID(skip(p2)) ≤docID(p1))\\n13\\nthen while hasSkip(p2) and (docID(skip(p2)) ≤docID(p1))\\n14\\ndo p2 ←skip(p2)\\n15\\nelse p2 ←next(p2)\\n16\\nreturn answer\\n◮Figure 2.10\\nPostings lists intersection with skip pointers.\\nin Figure 2.10. Skip pointers will only be available for the original postings\\nlists. For an intermediate result in a complex query, the call hasSkip(p) will\\nalways return false. Finally, note that the presence of skip pointers only helps\\nfor AND queries, not for OR queries.\\nWhere do we place skips? There is a tradeoff. More skips means shorter\\nskip spans, and that we are more likely to skip. But it also means lots of\\ncomparisons to skip pointers, and lots of space storing skip pointers. Fewer\\nskips means few pointer comparisons, but then long skip spans which means\\nthat there will be fewer opportunities to skip. A simple heuristic for placing\\nskips, which has been found to work well in practice, is that for a postings\\nlist of length P, use\\n√\\nP evenly-spaced skip pointers. This heuristic can be\\nimproved upon; it ignores any details of the distribution of query terms.\\nBuilding effective skip pointers is easy if an index is relatively static; it\\nis harder if a postings list keeps changing because of updates. A malicious\\ndeletion strategy can render skip lists ineffective.\\nChoosing the optimal encoding for an inverted index is an ever-changing\\ngame for the system builder, because it is strongly dependent on underly-\\ning computer technologies and their relative speeds and sizes. Traditionally,\\nCPUs were slow, and so highly compressed techniques were not optimal.\\nNow CPUs are fast and disk is slow, so reducing disk postings list size dom-\\ninates. However, if you’re running a search engine with everything in mem-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n38\\n2\\nThe term vocabulary and postings lists\\nory then the equation changes again. We discuss the impact of hardware\\nparameters on index construction time in Section 4.1 (page 68) and the im-\\npact of index size on system speed in Chapter 5.\\n?\\nExercise 2.5\\n[⋆]\\nWhy are skip pointers not useful for queries of the form x OR y?\\nExercise 2.6\\n[⋆]\\nWe have a two-word query. For one term the postings list consists of the following 16\\nentries:\\n[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\\nand for the other it is the one entry postings list:\\n[47].\\nWork out how many comparisons would be done to intersect the two postings lists\\nwith the following two strategies. Brieﬂy justify your answers:\\na. Using standard postings lists\\nb. Using postings lists stored with skip pointers, with a skip length of\\n√\\nP, as sug-\\ngested in Section 2.3.\\nExercise 2.7\\n[⋆]\\nConsider a postings intersection between this postings list, with skip pointers:\\n3\\n5\\n9\\n15\\n24\\n39\\n60\\n68\\n75\\n81\\n84\\n89\\n92\\n96\\n97 100 115\\nand the following intermediate result postings list (which hence has no skip pointers):\\n3\\n5\\n89\\n95\\n97\\n99\\n100\\n101\\nTrace through the postings intersection algorithm in Figure 2.10 (page 37).\\na. How often is a skip pointer followed (i.e., p1 is advanced to skip(p1))?\\nb. How many postings comparisons will be made by this algorithm while intersect-\\ning the two lists?\\nc. How many postings comparisons would be made if the postings lists are inter-\\nsected without the use of skip pointers?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n39\\n2.4\\nPositional postings and phrase queries\\nMany complex or technical concepts and many organization and product\\nnames are multiword compounds or phrases. We would like to be able to\\npose a query such as Stanford University by treating it as a phrase so that a\\nsentence in a document like The inventor Stanford Ovshinsky never went to uni-\\nversity. is not a match. Most recent search engines support a double quotes\\nsyntax (“stanford university”) for phrase queries, which has proven to be very\\nPHRASE QUERIES\\neasily understood and successfully used by users. As many as 10% of web\\nqueries are phrase queries, and many more are implicit phrase queries (such\\nas person names), entered without use of double quotes. To be able to sup-\\nport such queries, it is no longer sufﬁcient for postings lists to be simply lists\\nof documents that contain individual terms. In this section we consider two\\napproaches to supporting phrase queries and their combination. A search\\nengine should not only support phrase queries, but implement them efﬁ-\\nciently. A related but distinct concept is term proximity weighting, where a\\ndocument is preferred to the extent that the query terms appear close to each\\nother in the text. This technique is covered in Section 7.2.2 (page 144) in the\\ncontext of ranked retrieval.\\n2.4.1\\nBiword indexes\\nOne approach to handling phrases is to consider every pair of consecutive\\nterms in a document as a phrase. For example, the text Friends, Romans,\\nCountrymen would generate the biwords:\\nBIWORD INDEX\\nfriends romans\\nromans countrymen\\nIn this model, we treat each of these biwords as a vocabulary term. Being\\nable to process two-word phrase queries is immediate. Longer phrases can\\nbe processed by breaking them down. The query stanford university palo alto\\ncan be broken into the Boolean query on biwords:\\n“stanford university” AND “university palo” AND “palo alto”\\nThis query could be expected to work fairly well in practice, but there can\\nand will be occasional false positives. Without examining the documents,\\nwe cannot verify that the documents matching the above Boolean query do\\nactually contain the original 4 word phrase.\\nAmong possible queries, nouns and noun phrases have a special status in\\ndescribing the concepts people are interested in searching for. But related\\nnouns can often be divided from each other by various function words, in\\nphrases such as the abolition of slavery or renegotiation of the constitution. These\\nneeds can be incorporated into the biword indexing model in the following\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n40\\n2\\nThe term vocabulary and postings lists\\nway. First, we tokenize the text and perform part-of-speech-tagging.6 We\\ncan then group terms into nouns, including proper nouns, (N) and function\\nwords, including articles and prepositions, (X), among other classes. Now\\ndeem any string of terms of the form NX*N to be an extended biword. Each\\nsuch extended biword is made a term in the vocabulary. For example:\\nrenegotiation\\nof\\nthe\\nconstitution\\nN\\nX\\nX\\nN\\nTo process a query using such an extended biword index, we need to also\\nparse it into N’s and X’s, and then segment the query into extended biwords,\\nwhich can be looked up in the index.\\nThis algorithm does not always work in an intuitively optimal manner\\nwhen parsing longer queries into Boolean queries. Using the above algo-\\nrithm, the query\\ncost overruns on a power plant\\nis parsed into\\n“cost overruns” AND “overruns power” AND “power plant”\\nwhereas it might seem a better query to omit the middle biword. Better\\nresults can be obtained by using more precise part-of-speech patterns that\\ndeﬁne which extended biwords should be indexed.\\nThe concept of a biword index can be extended to longer sequences of\\nwords, and if the index includes variable length word sequences, it is gen-\\nerally referred to as a phrase index. Indeed, searches for a single term are\\nPHRASE INDEX\\nnot naturally handled in a biword index (you would need to scan the dic-\\ntionary for all biwords containing the term), and so we also need to have an\\nindex of single-word terms. While there is always a chance of false positive\\nmatches, the chance of a false positive match on indexed phrases of length 3\\nor more becomes very small indeed. But on the other hand, storing longer\\nphrases has the potential to greatly expand the vocabulary size. Maintain-\\ning exhaustive phrase indexes for phrases of length greater than two is a\\ndaunting prospect, and even use of an exhaustive biword dictionary greatly\\nexpands the size of the vocabulary. However, towards the end of this sec-\\ntion we discuss the utility of the strategy of using a partial phrase index in a\\ncompound indexing scheme.\\n6. Part of speech taggers classify words as nouns, verbs, etc. – or, in practice, often as ﬁner-\\ngrained classes like “plural proper noun”. Many fairly accurate (c. 96% per-tag accuracy) part-\\nof-speech taggers now exist, usually trained by machine learning methods on hand-tagged text.\\nSee, for instance, Manning and Schütze (1999, ch. 10).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n41\\nto, 993427:\\n⟨1, 6: ⟨7, 18, 33, 72, 86, 231⟩;\\n2, 5: ⟨1, 17, 74, 222, 255⟩;\\n4, 5: ⟨8, 16, 190, 429, 433⟩;\\n5, 2: ⟨363, 367⟩;\\n7, 3: ⟨13, 23, 191⟩; ... ⟩\\nbe, 178239:\\n⟨1, 2: ⟨17, 25⟩;\\n4, 5: ⟨17, 191, 291, 430, 434⟩;\\n5, 3: ⟨14, 19, 101⟩; ... ⟩\\n◮Figure 2.11\\nPositional index example. The word to has a document frequency\\n993,477, and occurs 6 times in document 1 at positions 7, 18, 33, etc.\\n2.4.2\\nPositional indexes\\nFor the reasons given, a biword index is not the standard solution. Rather,\\na positional index is most commonly employed. Here, for each term in the\\nPOSITIONAL INDEX\\nvocabulary, we store postings of the form docID: ⟨position1, position2, ... ⟩,\\nas shown in Figure 2.11, where each position is a token index in the docu-\\nment. Each posting will also usually record the term frequency, for reasons\\ndiscussed in Chapter 6.\\nTo process a phrase query, you still need to access the inverted index en-\\ntries for each distinct term. As before, you would start with the least frequent\\nterm and then work to further restrict the list of possible candidates. In the\\nmerge operation, the same general technique is used as before, but rather\\nthan simply checking that both terms are in a document, you also need to\\ncheck that their positions of appearance in the document are compatible with\\nthe phrase query being evaluated. This requires working out offsets between\\nthe words.\\n\\x0f\\nExample 2.1: Satisfying phrase queries.\\nSuppose the postings lists for to and\\nbe are as in Figure 2.11, and the query is “to be or not to be”. The postings lists to access\\nare: to, be, or, not. We will examine intersecting the postings lists for to and be. We\\nﬁrst look for documents that contain both terms. Then, we look for places in the lists\\nwhere there is an occurrence of be with a token index one higher than a position of to,\\nand then we look for another occurrence of each word with token index 4 higher than\\nthe ﬁrst occurrence. In the above lists, the pattern of occurrences that is a possible\\nmatch is:\\nto: ⟨. . . ; 4:⟨. . . ,429,433⟩; . . . ⟩\\nbe: ⟨. . . ; 4:⟨. . . ,430,434⟩; . . . ⟩\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n42\\n2\\nThe term vocabulary and postings lists\\nPOSITIONALINTERSECT(p1, p2, k)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen l ←⟨⟩\\n5\\npp1 ←positions(p1)\\n6\\npp2 ←positions(p2)\\n7\\nwhile pp1 ̸= NIL\\n8\\ndo while pp2 ̸= NIL\\n9\\ndo if |pos(pp1) −pos(pp2)| ≤k\\n10\\nthen ADD(l, pos(pp2))\\n11\\nelse if pos(pp2) > pos(pp1)\\n12\\nthen break\\n13\\npp2 ←next(pp2)\\n14\\nwhile l ̸= ⟨⟩and |l[0] −pos(pp1)| > k\\n15\\ndo DELETE(l[0])\\n16\\nfor each ps ∈l\\n17\\ndo ADD(answer, ⟨docID(p1), pos(pp1), ps⟩)\\n18\\npp1 ←next(pp1)\\n19\\np1 ←next(p1)\\n20\\np2 ←next(p2)\\n21\\nelse if docID(p1) < docID(p2)\\n22\\nthen p1 ←next(p1)\\n23\\nelse p2 ←next(p2)\\n24\\nreturn answer\\n◮Figure 2.12\\nAn algorithm for proximity intersection of postings lists p1 and p2.\\nThe algorithm ﬁnds places where the two terms appear within k words of each other\\nand returns a list of triples giving docID and the term position in p1 and p2.\\nThe same general method is applied for within k word proximity searches,\\nof the sort we saw in Example 1.1 (page 15):\\nemployment /3 place\\nHere, /k means “within k words of (on either side)”. Clearly, positional in-\\ndexes can be used for such queries; biword indexes cannot. We show in\\nFigure 2.12 an algorithm for satisfying within k word proximity searches; it\\nis further discussed in Exercise 2.12.\\nPositional index size.\\nAdopting a positional index expands required post-\\nings storage signiﬁcantly, even if we compress position values/offsets as we\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n43\\nwill discuss in Section 5.3 (page 95). Indeed, moving to a positional index\\nalso changes the asymptotic complexity of a postings intersection operation,\\nbecause the number of items to check is now bounded not by the number of\\ndocuments but by the total number of tokens in the document collection T.\\nThat is, the complexity of a Boolean query is Θ(T) rather than Θ(N). How-\\never, most applications have little choice but to accept this, since most users\\nnow expect to have the functionality of phrase and proximity searches.\\nLet’s examine the space implications of having a positional index. A post-\\ning now needs an entry for each occurrence of a term. The index size thus\\ndepends on the average document size. The average web page has less than\\n1000 terms, but documents like SEC stock ﬁlings, books, and even some epic\\npoems easily reach 100,000 terms. Consider a term with frequency 1 in 1000\\nterms on average. The result is that large documents cause an increase of two\\norders of magnitude in the space required to store the postings list:\\nExpected\\nExpected entries\\nDocument size\\npostings\\nin positional posting\\n1000\\n1\\n1\\n100,000\\n1\\n100\\nWhile the exact numbers depend on the type of documents and the language\\nbeing indexed, some rough rules of thumb are to expect a positional index to\\nbe 2 to 4 times as large as a non-positional index, and to expect a compressed\\npositional index to be about one third to one half the size of the raw text\\n(after removal of markup, etc.) of the original uncompressed documents.\\nSpeciﬁc numbers for an example collection are given in Table 5.1 (page 87)\\nand Table 5.6 (page 103).\\n2.4.3\\nCombination schemes\\nThe strategies of biword indexes and positional indexes can be fruitfully\\ncombined. If users commonly query on particular phrases, such as Michael\\nJackson, it is quite inefﬁcient to keep merging positional postings lists. A\\ncombination strategy uses a phrase index, or just a biword index, for certain\\nqueries and uses a positional index for other phrase queries. Good queries\\nto include in the phrase index are ones known to be common based on re-\\ncent querying behavior. But this is not the only criterion: the most expensive\\nphrase queries to evaluate are ones where the individual words are com-\\nmon but the desired phrase is comparatively rare. Adding Britney Spears as\\na phrase index entry may only give a speedup factor to that query of about\\n3, since most documents that mention either word are valid results, whereas\\nadding The Who as a phrase index entry may speed up that query by a factor\\nof 1000. Hence, having the latter is more desirable, even if it is a relatively\\nless common query.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n44\\n2\\nThe term vocabulary and postings lists\\nWilliams et al. (2004) evaluate an even more sophisticated scheme which\\nemploys indexes of both these sorts and additionally a partial next word\\nindex as a halfway house between the ﬁrst two strategies. For each term, a\\nnext word index records terms that follow it in a document. They conclude\\nNEXT WORD INDEX\\nthat such a strategy allows a typical mixture of web phrase queries to be\\ncompleted in one quarter of the time taken by use of a positional index alone,\\nwhile taking up 26% more space than use of a positional index alone.\\n?\\nExercise 2.8\\n[⋆]\\nAssume a biword index. Give an example of a document which will be returned\\nfor a query of New York University but is actually a false positive which should not be\\nreturned.\\nExercise 2.9\\n[⋆]\\nShown below is a portion of a positional index in the format: term: doc1: ⟨position1,\\nposition2, . . . ⟩; doc2: ⟨position1, position2, . . . ⟩; etc.\\nangels: 2: ⟨36,174,252,651⟩; 4: ⟨12,22,102,432⟩; 7: ⟨17⟩;\\nfools: 2: ⟨1,17,74,222⟩; 4: ⟨8,78,108,458⟩; 7: ⟨3,13,23,193⟩;\\nfear: 2: ⟨87,704,722,901⟩; 4: ⟨13,43,113,433⟩; 7: ⟨18,328,528⟩;\\nin: 2: ⟨3,37,76,444,851⟩; 4: ⟨10,20,110,470,500⟩; 7: ⟨5,15,25,195⟩;\\nrush: 2: ⟨2,66,194,321,702⟩; 4: ⟨9,69,149,429,569⟩; 7: ⟨4,14,404⟩;\\nto: 2: ⟨47,86,234,999⟩; 4: ⟨14,24,774,944⟩; 7: ⟨199,319,599,709⟩;\\ntread: 2: ⟨57,94,333⟩; 4: ⟨15,35,155⟩; 7: ⟨20,320⟩;\\nwhere: 2: ⟨67,124,393,1001⟩; 4: ⟨11,41,101,421,431⟩; 7: ⟨16,36,736⟩;\\nWhich document(s) if any match each of the following queries, where each expression\\nwithin quotes is a phrase query?\\na. “fools rush in”\\nb. “fools rush in” AND “angels fear to tread”\\nExercise 2.10\\n[⋆]\\nConsider the following fragment of a positional index with the format:\\nword: document: ⟨position, position, . . .⟩; document: ⟨position, . . .⟩\\n. . .\\nGates: 1: ⟨3⟩; 2: ⟨6⟩; 3: ⟨2,17⟩; 4: ⟨1⟩;\\nIBM: 4: ⟨3⟩; 7: ⟨14⟩;\\nMicrosoft: 1: ⟨1⟩; 2: ⟨1,21⟩; 3: ⟨3⟩; 5: ⟨16,22,51⟩;\\nThe /k operator, word1 /k word2 ﬁnds occurrences of word1 within k words of word2 (on\\neither side), where k is a positive integer argument. Thus k = 1 demands that word1\\nbe adjacent to word2.\\na. Describe the set of documents that satisfy the query Gates /2 Microsoft.\\nb. Describe each set of values for k for which the query Gates /k Microsoft returns a\\ndifferent set of documents as the answer.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.5\\nReferences and further reading\\n45\\nExercise 2.11\\n[⋆⋆]\\nConsider the general procedure for merging two positional postings lists for a given\\ndocument, to determine the document positions where a document satisﬁes a /k\\nclause (in general there can be multiple positions at which each term occurs in a sin-\\ngle document). We begin with a pointer to the position of occurrence of each term\\nand move each pointer along the list of occurrences in the document, checking as we\\ndo so whether we have a hit for /k. Each move of either pointer counts as a step. Let\\nL denote the total number of occurrences of the two terms in the document. What is\\nthe big-O complexity of the merge procedure, if we wish to have postings including\\npositions in the result?\\nExercise 2.12\\n[⋆⋆]\\nConsider the adaptation of the basic algorithm for intersection of two postings lists\\n(Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\\nqueries. A naive algorithm for this operation could be O(PLmax2), where P is the\\nsum of the lengths of the postings lists (i.e., the sum of document frequencies) and\\nLmax is the maximum length of a document (in tokens).\\na. Go through this algorithm carefully and explain how it works.\\nb. What is the complexity of this algorithm? Justify your answer carefully.\\nc. For certain queries and data distributions, would another algorithm be more efﬁ-\\ncient? What complexity does it have?\\nExercise 2.13\\n[⋆⋆]\\nSuppose we wish to use a postings intersection procedure to determine simply the\\nlist of documents that satisfy a /k clause, rather than returning the list of positions,\\nas in Figure 2.12 (page 42). For simplicity, assume k ≥2. Let L denote the total\\nnumber of occurrences of the two terms in the document collection (i.e., the sum of\\ntheir collection frequencies). Which of the following is true? Justify your answer.\\na. The merge can be accomplished in a number of steps linear in L and independent\\nof k, and we can ensure that each pointer moves only to the right.\\nb. The merge can be accomplished in a number of steps linear in L and independent\\nof k, but a pointer may be forced to move non-monotonically (i.e., to sometimes\\nback up)\\nc. The merge can require kL steps in some cases.\\nExercise 2.14\\n[⋆⋆]\\nHow could an IR system combine use of a positional index and use of stop words?\\nWhat is the potential problem, and how could it be handled?\\n2.5\\nReferences and further reading\\nExhaustive discussion of the character-level processing of East Asian lan-\\nEAST ASIAN\\nLANGUAGES\\nguages can be found in Lunde (1998). Character bigram indexes are perhaps\\nthe most standard approach to indexing Chinese, although some systems use\\nword segmentation. Due to differences in the language and writing system,\\nword segmentation is most usual for Japanese (Luk and Kwok 2002, Kishida\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n46\\n2\\nThe term vocabulary and postings lists\\net al. 2005). The structure of a character k-gram index over unsegmented text\\ndiffers from that in Section 3.2.2 (page 54): there the k-gram dictionary points\\nto postings lists of entries in the regular dictionary, whereas here it points\\ndirectly to document postings lists. For further discussion of Chinese word\\nsegmentation, see Sproat et al. (1996), Sproat and Emerson (2003), Tseng et al.\\n(2005), and Gao et al. (2005).\\nLita et al. (2003) present a method for truecasing. Natural language pro-\\ncessing work on computational morphology is presented in (Sproat 1992,\\nBeesley and Karttunen 2003).\\nLanguage identiﬁcation was perhaps ﬁrst explored in cryptography; for\\nexample, Konheim (1981) presents a character-level k-gram language identi-\\nﬁcation algorithm. While other methods such as looking for particular dis-\\ntinctive function words and letter combinations have been used, with the\\nadvent of widespread digital text, many people have explored the charac-\\nter n-gram technique, and found it to be highly successful (Beesley 1998,\\nDunning 1994, Cavnar and Trenkle 1994). Written language identiﬁcation\\nis regarded as a fairly easy problem, while spoken language identiﬁcation\\nremains more difﬁcult; see Hughes et al. (2006) for a recent survey.\\nExperiments on and discussion of the positive and negative impact of\\nstemming in English can be found in the following works: Salton (1989), Har-\\nman (1991), Krovetz (1995), Hull (1996). Hollink et al. (2004) provide detailed\\nresults for the effectiveness of language-speciﬁc methods on 8 European lan-\\nguages. In terms of percent change in mean average precision (see page 159)\\nover a baseline system, diacritic removal gains up to 23% (being especially\\nhelpful for Finnish, French, and Swedish). Stemming helped markedly for\\nFinnish (30% improvement) and Spanish (10% improvement), but for most\\nlanguages, including English, the gain from stemming was in the range 0–\\n5%, and results from a lemmatizer were poorer still. Compound splitting\\ngained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather\\nthan language-particular methods, indexing character k-grams (as we sug-\\ngested for Chinese) could often give as good or better results: using within-\\nword character 4-grams rather than words gave gains of 37% in Finnish, 27%\\nin Swedish, and 20% in German, while even being slightly positive for other\\nlanguages, such as Dutch, Spanish, and English. Tomlinson (2003) presents\\nbroadly similar results.\\nBar-Ilan and Gutman (2005) suggest that, at the\\ntime of their study (2003), the major commercial web search engines suffered\\nfrom lacking decent language-particular processing; for example, a query on\\nwww.google.fr for l’électricité did not separate off the article l’ but only matched\\npages with precisely this string of article+noun.\\nThe classic presentation of skip pointers for IR can be found in Moffat and\\nSKIP LIST\\nZobel (1996). Extended techniques are discussed in Boldi and Vigna (2005).\\nThe main paper in the algorithms literature is Pugh (1990), which uses mul-\\ntilevel skip pointers to give expected O(log P) list access (the same expected\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n2.5\\nReferences and further reading\\n47\\nefﬁciency as using a tree data structure) with less implementational complex-\\nity. In practice, the effectiveness of using skip pointers depends on various\\nsystem parameters. Moffat and Zobel (1996) report conjunctive queries run-\\nning about ﬁve times faster with the use of skip pointers, but Bahle et al.\\n(2002, p. 217) report that, with modern CPUs, using skip lists instead slows\\ndown search because it expands the size of the postings list (i.e., disk I/O\\ndominates performance). In contrast, Strohman and Croft (2007) again show\\ngood performance gains from skipping, in a system architecture designed to\\noptimize for the large memory spaces and multiple cores of recent CPUs.\\nJohnson et al. (2006) report that 11.7% of all queries in two 2002 web query\\nlogs contained phrase queries, though Kammenhuber et al. (2006) report\\nonly 3% phrase queries for a different data set. Silverstein et al. (1999) note\\nthat many queries without explicit phrase operators are actually implicit\\nphrase searches.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n49\\n3\\nDictionaries and tolerant\\nretrieval\\nIn Chapters 1 and 2 we developed the ideas underlying inverted indexes\\nfor handling Boolean and proximity queries. Here, we develop techniques\\nthat are robust to typographical errors in the query, as well as alternative\\nspellings.\\nIn Section 3.1 we develop data structures that help the search\\nfor terms in the vocabulary in an inverted index. In Section 3.2 we study\\nthe idea of a wildcard query: a query such as *a*e*i*o*u*, which seeks doc-\\nWILDCARD QUERY\\numents containing any term that includes all the ﬁve vowels in sequence.\\nThe * symbol indicates any (possibly empty) string of characters. Users pose\\nsuch queries to a search engine when they are uncertain about how to spell\\na query term, or seek documents containing variants of a query term; for in-\\nstance, the query automat* would seek documents containing any of the terms\\nautomatic, automation and automated.\\nWe then turn to other forms of imprecisely posed queries, focusing on\\nspelling errors in Section 3.3. Users make spelling errors either by accident,\\nor because the term they are searching for (e.g., Herman) has no unambiguous\\nspelling in the collection. We detail a number of techniques for correcting\\nspelling errors in queries, one term at a time as well as for an entire string\\nof query terms. Finally, in Section 3.4 we study a method for seeking vo-\\ncabulary terms that are phonetically close to the query term(s). This can be\\nespecially useful in cases like the Herman example, where the user may not\\nknow how a proper name is spelled in documents in the collection.\\nBecause we will develop many variants of inverted indexes in this chapter,\\nwe will use sometimes the phrase standard inverted index to mean the inverted\\nindex developed in Chapters 1 and 2, in which each vocabulary term has a\\npostings list with the documents in the collection.\\n3.1\\nSearch structures for dictionaries\\nGiven an inverted index and a query, our ﬁrst task is to determine whether\\neach query term exists in the vocabulary and if so, identify the pointer to the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n50\\n3\\nDictionaries and tolerant retrieval\\ncorresponding postings. This vocabulary lookup operation uses a classical\\ndata structure called the dictionary and has two broad classes of solutions:\\nhashing, and search trees. In the literature of data structures, the entries in\\nthe vocabulary (in our case, terms) are often referred to as keys. The choice\\nof solution (hashing, or search trees) is governed by a number of questions:\\n(1) How many keys are we likely to have? (2) Is the number likely to remain\\nstatic, or change a lot – and in the case of changes, are we likely to only have\\nnew keys inserted, or to also have some keys in the dictionary be deleted? (3)\\nWhat are the relative frequencies with which various keys will be accessed?\\nHashing has been used for dictionary lookup in some search engines. Each\\nvocabulary term (key) is hashed into an integer over a large enough space\\nthat hash collisions are unlikely; collisions if any are resolved by auxiliary\\nstructures that can demand care to maintain.1 At query time, we hash each\\nquery term separately and following a pointer to the corresponding post-\\nings, taking into account any logic for resolving hash collisions. There is no\\neasy way to ﬁnd minor variants of a query term (such as the accented and\\nnon-accented versions of a word like resume), since these could be hashed to\\nvery different integers. In particular, we cannot seek (for instance) all terms\\nbeginning with the preﬁx automat, an operation that we will require below\\nin Section 3.2. Finally, in a setting (such as the Web) where the size of the\\nvocabulary keeps growing, a hash function designed for current needs may\\nnot sufﬁce in a few years’ time.\\nSearch trees overcome many of these issues – for instance, they permit us\\nto enumerate all vocabulary terms beginning with automat. The best-known\\nsearch tree is the binary tree, in which each internal node has two children.\\nBINARY TREE\\nThe search for a term begins at the root of the tree. Each internal node (in-\\ncluding the root) represents a binary test, based on whose outcome the search\\nproceeds to one of the two sub-trees below that node. Figure 3.1 gives an ex-\\nample of a binary search tree used for a dictionary. Efﬁcient search (with a\\nnumber of comparisons that is O(log M)) hinges on the tree being balanced:\\nthe numbers of terms under the two sub-trees of any node are either equal\\nor differ by one. The principal issue here is that of rebalancing: as terms are\\ninserted into or deleted from the binary search tree, it needs to be rebalanced\\nso that the balance property is maintained.\\nTo mitigate rebalancing, one approach is to allow the number of sub-trees\\nunder an internal node to vary in a ﬁxed interval. A search tree commonly\\nused for a dictionary is the B-tree – a search tree in which every internal node\\nB-TREE\\nhas a number of children in the interval [a, b], where a and b are appropriate\\npositive integers; Figure 3.2 shows an example with a = 2 and b = 4. Each\\nbranch under an internal node again represents a test for a range of char-\\n1. So-called perfect hash functions are designed to preclude collisions, but are rather more com-\\nplicated both to implement and to compute.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n51\\n◮Figure 3.1\\nA binary search tree. In this example the branch at the root partitions\\nvocabulary terms into two subtrees, those whose ﬁrst letter is between a and m, and\\nthe rest.\\nacter sequences, as in the binary tree example of Figure 3.1. A B-tree may\\nbe viewed as “collapsing” multiple levels of the binary tree into one; this\\nis especially advantageous when some of the dictionary is disk-resident, in\\nwhich case this collapsing serves the function of pre-fetching imminent bi-\\nnary tests. In such cases, the integers a and b are determined by the sizes of\\ndisk blocks. Section 3.5 contains pointers to further background on search\\ntrees and B-trees.\\nIt should be noted that unlike hashing, search trees demand that the char-\\nacters used in the document collection have a prescribed ordering; for in-\\nstance, the 26 letters of the English alphabet are always listed in the speciﬁc\\norder A through Z. Some Asian languages such as Chinese do not always\\nhave a unique ordering, although by now all languages (including Chinese\\nand Japanese) have adopted a standard ordering system for their character\\nsets.\\n3.2\\nWildcard queries\\nWildcard queries are used in any of the following situations: (1) the user\\nis uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n52\\n3\\nDictionaries and tolerant retrieval\\n◮Figure 3.2\\nA B-tree. In this example every internal node has between 2 and 4\\nchildren.\\nleads to the wildcard query S*dney); (2) the user is aware of multiple vari-\\nants of spelling a term and (consciously) seeks documents containing any of\\nthe variants (e.g., color vs. colour); (3) the user seeks documents containing\\nvariants of a term that would be caught by stemming, but is unsure whether\\nthe search engine performs stemming (e.g., judicial vs. judiciary, leading to the\\nwildcard query judicia*); (4) the user is uncertain of the correct rendition of a\\nforeign word or phrase (e.g., the query Universit* Stuttgart).\\nA query such as mon* is known as a trailing wildcard query, because the *\\nWILDCARD QUERY\\nsymbol occurs only once, at the end of the search string. A search tree on\\nthe dictionary is a convenient way of handling trailing wildcard queries: we\\nwalk down the tree following the symbols m, o and n in turn, at which point\\nwe can enumerate the set W of terms in the dictionary with the preﬁx mon.\\nFinally, we use |W| lookups on the standard inverted index to retrieve all\\ndocuments containing any term in W.\\nBut what about wildcard queries in which the * symbol is not constrained\\nto be at the end of the search string? Before handling this general case, we\\nmention a slight generalization of trailing wildcard queries. First, consider\\nleading wildcard queries, or queries of the form *mon. Consider a reverse B-tree\\non the dictionary – one in which each root-to-leaf path of the B-tree corre-\\nsponds to a term in the dictionary written backwards: thus, the term lemon\\nwould, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down\\nthe reverse B-tree then enumerates all terms R in the vocabulary with a given\\npreﬁx.\\nIn fact, using a regular B-tree together with a reverse B-tree, we can handle\\nan even more general case: wildcard queries in which there is a single * sym-\\nbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set\\nW of dictionary terms beginning with the preﬁx se, then the reverse B-tree to\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n53\\nenumerate the set R of terms ending with the sufﬁx mon. Next, we take the\\nintersection W ∩R of these two sets, to arrive at the set of terms that begin\\nwith the preﬁx se and end with the sufﬁx mon. Finally, we use the standard\\ninverted index to retrieve all documents containing any terms in this inter-\\nsection. We can thus handle wildcard queries that contain a single * symbol\\nusing two B-trees, the normal B-tree and a reverse B-tree.\\n3.2.1\\nGeneral wildcard queries\\nWe now study two techniques for handling general wildcard queries. Both\\ntechniques share a common strategy: express the given wildcard query qw as\\na Boolean query Q on a specially constructed index, such that the answer to\\nQ is a superset of the set of vocabulary terms matching qw. Then, we check\\neach term in the answer to Q against qw, discarding those vocabulary terms\\nthat do not match qw. At this point we have the vocabulary terms matching\\nqw and can resort to the standard inverted index.\\nPermuterm indexes\\nOur ﬁrst special index for general wildcard queries is the permuterm index,\\nPERMUTERM INDEX\\na form of inverted index. First, we introduce a special symbol $ into our\\ncharacter set, to mark the end of a term. Thus, the term hello is shown here as\\nthe augmented term hello$. Next, we construct a permuterm index, in which\\nthe various rotations of each term (augmented with $) all link to the original\\nvocabulary term. Figure 3.3 gives an example of such a permuterm index\\nentry for the term hello.\\nWe refer to the set of rotated terms in the permuterm index as the per-\\nmuterm vocabulary.\\nHow does this index help us with wildcard queries? Consider the wildcard\\nquery m*n. The key is to rotate such a wildcard query so that the * symbol\\nappears at the end of the string – thus the rotated wildcard query becomes\\nn$m*. Next, we look up this string in the permuterm index, where seeking\\nn$m* (via a search tree) leads to rotations of (among others) the terms man\\nand moron.\\nNow that the permuterm index enables us to identify the original vocab-\\nulary terms matching a wildcard query, we look up these terms in the stan-\\ndard inverted index to retrieve matching documents. We can thus handle\\nany wildcard query with a single * symbol. But what about a query such as\\nﬁ*mo*er? In this case we ﬁrst enumerate the terms in the dictionary that are\\nin the permuterm index of er$ﬁ*. Not all such dictionary terms will have\\nthe string mo in the middle - we ﬁlter these out by exhaustive enumera-\\ntion, checking each candidate to see if it contains mo. In this example, the\\nterm ﬁshmonger would survive this ﬁltering but ﬁlibuster would not. We then\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n54\\n3\\nDictionaries and tolerant retrieval\\n◮Figure 3.3\\nA portion of a permuterm index.\\nrun the surviving terms through the standard inverted index for document\\nretrieval. One disadvantage of the permuterm index is that its dictionary\\nbecomes quite large, including as it does all rotations of each term.\\nNotice the close interplay between the B-tree and the permuterm index\\nabove. Indeed, it suggests that the structure should perhaps be viewed as\\na permuterm B-tree. However, we follow traditional terminology here in\\ndescribing the permuterm index as distinct from the B-tree that allows us to\\nselect the rotations with a given preﬁx.\\n3.2.2\\nk-gram indexes for wildcard queries\\nWhereas the permuterm index is simple, it can lead to a considerable blowup\\nfrom the number of rotations per term; for a dictionary of English terms, this\\ncan represent an almost ten-fold space increase. We now present a second\\ntechnique, known as the k-gram index, for processing wildcard queries. We\\nwill also use k-gram indexes in Section 3.3.4. A k-gram is a sequence of k\\ncharacters. Thus cas, ast and stl are all 3-grams occurring in the term castle.\\nWe use a special character $ to denote the beginning or end of a term, so the\\nfull set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$.\\nIn a k-gram index, the dictionary contains all k-grams that occur in any term\\nk-GRAM INDEX\\nin the vocabulary. Each postings list points from a k-gram to all vocabulary\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n55\\netr\\nbeetroot\\nmetric\\npetrify\\nretrieval\\n-\\n-\\n-\\n-\\n◮Figure 3.4\\nExample of a postings list in a 3-gram index. Here the 3-gram etr is\\nillustrated. Matching vocabulary terms are lexicographically ordered in the postings.\\nterms containing that k-gram. For instance, the 3-gram etr would point to vo-\\ncabulary terms such as metric and retrieval. An example is given in Figure 3.4.\\nHow does such an index help us with wildcard queries? Consider the\\nwildcard query re*ve. We are seeking documents containing any term that\\nbegins with re and ends with ve. Accordingly, we run the Boolean query $re\\nAND ve$. This is looked up in the 3-gram index and yields a list of matching\\nterms such as relive, remove and retrieve. Each of these matching terms is then\\nlooked up in the standard inverted index to yield documents matching the\\nquery.\\nThere is however a difﬁculty with the use of k-gram indexes, that demands\\none further step of processing. Consider using the 3-gram index described\\nabove for the query red*. Following the process described above, we ﬁrst\\nissue the Boolean query $re AND red to the 3-gram index. This leads to a\\nmatch on terms such as retired, which contain the conjunction of the two 3-\\ngrams $re and red, yet do not match the original wildcard query red*.\\nTo cope with this, we introduce a post-ﬁltering step, in which the terms enu-\\nmerated by the Boolean query on the 3-gram index are checked individually\\nagainst the original query red*. This is a simple string-matching operation\\nand weeds out terms such as retired that do not match the original query.\\nTerms that survive are then searched in the standard inverted index as usual.\\nWe have seen that a wildcard query can result in multiple terms being\\nenumerated, each of which becomes a single-term query on the standard in-\\nverted index. Search engines do allow the combination of wildcard queries\\nusing Boolean operators, for example, re*d AND fe*ri. What is the appropriate\\nsemantics for such a query? Since each wildcard query turns into a disjunc-\\ntion of single-term queries, the appropriate interpretation of this example\\nis that we have a conjunction of disjunctions: we seek all documents that\\ncontain any term matching re*d and any term matching fe*ri.\\nEven without Boolean combinations of wildcard queries, the processing of\\na wildcard query can be quite expensive, because of the added lookup in the\\nspecial index, ﬁltering and ﬁnally the standard inverted index. A search en-\\ngine may support such rich functionality, but most commonly, the capability\\nis hidden behind an interface (say an “Advanced Query” interface) that most\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n56\\n3\\nDictionaries and tolerant retrieval\\nusers never use. Exposing such functionality in the search interface often en-\\ncourages users to invoke it even when they do not require it (say, by typing\\na preﬁx of their query followed by a *), increasing the processing load on the\\nsearch engine.\\n?\\nExercise 3.1\\nIn the permuterm index, each permuterm vocabulary term points to the original vo-\\ncabulary term(s) from which it was derived. How many original vocabulary terms\\ncan there be in the postings list of a permuterm vocabulary term?\\nExercise 3.2\\nWrite down the entries in the permuterm index dictionary that are generated by the\\nterm mama.\\nExercise 3.3\\nIf you wanted to search for s*ng in a permuterm wildcard index, what key(s) would\\none do the lookup on?\\nExercise 3.4\\nRefer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the\\npostings are lexicographically ordered. Why is this ordering useful?\\nExercise 3.5\\nConsider again the query ﬁ*mo*er from Section 3.2.1. What Boolean query on a bigram\\nindex would be generated for this query? Can you think of a term that matches the\\npermuterm query in Section 3.2.1, but does not satisfy this Boolean query?\\nExercise 3.6\\nGive an example of a sentence that falsely matches the wildcard query mon*h if the\\nsearch were to simply use a conjunction of bigrams.\\n3.3\\nSpelling correction\\nWe next look at the problem of correcting spelling errors in queries. For in-\\nstance, we may wish to retrieve documents containing the term carrot when\\nthe user types the query carot. Google reports (http://www.google.com/jobs/britney.html)\\nthat the following are all treated as misspellings of the query britney spears:\\nbritian spears, britney’s spears, brandy spears and prittany spears. We look at two\\nsteps to solving this problem: the ﬁrst based on edit distance and the second\\nbased on k-gram overlap. Before getting into the algorithmic details of these\\nmethods, we ﬁrst review how search engines provide spell-correction as part\\nof a user experience.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n57\\n3.3.1\\nImplementing spelling correction\\nThere are two basic principles underlying most spelling correction algorithms.\\n1. Of various alternative correct spellings for a mis-spelled query, choose\\nthe “nearest” one. This demands that we have a notion of nearness or\\nproximity between a pair of queries. We will develop these proximity\\nmeasures in Section 3.3.3.\\n2. When two correctly spelled queries are tied (or nearly tied), select the one\\nthat is more common. For instance, grunt and grant both seem equally\\nplausible as corrections for grnt. Then, the algorithm should choose the\\nmore common of grunt and grant as the correction. The simplest notion\\nof more common is to consider the number of occurrences of the term\\nin the collection; thus if grunt occurs more often than grant, it would be\\nthe chosen correction. A different notion of more common is employed\\nin many search engines, especially on the web. The idea is to use the\\ncorrection that is most common among queries typed in by other users.\\nThe idea here is that if grunt is typed as a query more often than grant, then\\nit is more likely that the user who typed grnt intended to type the query\\ngrunt.\\nBeginning in Section 3.3.3 we describe notions of proximity between queries,\\nas well as their efﬁcient computation. Spelling correction algorithms build on\\nthese computations of proximity; their functionality is then exposed to users\\nin one of several ways:\\n1. On the query carot always retrieve documents containing carot as well as\\nany “spell-corrected” version of carot, including carrot and tarot.\\n2. As in (1) above, but only when the query term carot is not in the dictionary.\\n3. As in (1) above, but only when the original query returned fewer than a\\npreset number of documents (say fewer than ﬁve documents).\\n4. When the original query returns fewer than a preset number of docu-\\nments, the search interface presents a spelling suggestion to the end user:\\nthis suggestion consists of the spell-corrected query term(s). Thus, the\\nsearch engine might respond to the user: “Did you mean carrot?”\\n3.3.2\\nForms of spelling correction\\nWe focus on two speciﬁc forms of spelling correction that we refer to as\\nisolated-term correction and context-sensitive correction. In isolated-term cor-\\nrection, we attempt to correct a single query term at a time – even when we\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n58\\n3\\nDictionaries and tolerant retrieval\\nhave a multiple-term query. The carot example demonstrates this type of cor-\\nrection. Such isolated-term correction would fail to detect, for instance, that\\nthe query ﬂew form Heathrow contains a mis-spelling of the term from – because\\neach term in the query is correctly spelled in isolation.\\nWe begin by examining two techniques for addressing isolated-term cor-\\nrection: edit distance, and k-gram overlap.\\nWe then proceed to context-\\nsensitive correction.\\n3.3.3\\nEdit distance\\nGiven two character strings s1 and s2, the edit distance between them is the\\nEDIT DISTANCE\\nminimum number of edit operations required to transform s1 into s2. Most\\ncommonly, the edit operations allowed for this purpose are: (i) insert a char-\\nacter into a string; (ii) delete a character from a string and (iii) replace a char-\\nacter of a string by another character; for these operations, edit distance is\\nsometimes known as Levenshtein distance. For example, the edit distance be-\\nLEVENSHTEIN\\nDISTANCE\\ntween cat and dog is 3. In fact, the notion of edit distance can be generalized\\nto allowing different weights for different kinds of edit operations, for in-\\nstance a higher weight may be placed on replacing the character s by the\\ncharacter p, than on replacing it by the character a (the latter being closer to s\\non the keyboard). Setting weights in this way depending on the likelihood of\\nletters substituting for each other is very effective in practice (see Section 3.4\\nfor the separate issue of phonetic similarity). However, the remainder of our\\ntreatment here will focus on the case in which all edit operations have the\\nsame weight.\\nIt is well-known how to compute the (weighted) edit distance between\\ntwo strings in time O(|s1| × |s2|), where |si| denotes the length of a string si.\\nThe idea is to use the dynamic programming algorithm in Figure 3.5, where\\nthe characters in s1 and s2 are given in array form. The algorithm ﬁlls the\\n(integer) entries in a matrix m whose two dimensions equal the lengths of\\nthe two strings whose edit distances is being computed; the (i, j) entry of the\\nmatrix will hold (after the algorithm is executed) the edit distance between\\nthe strings consisting of the ﬁrst i characters of s1 and the ﬁrst j characters\\nof s2. The central dynamic programming step is depicted in Lines 8-10 of\\nFigure 3.5, where the three quantities whose minimum is taken correspond\\nto substituting a character in s1, inserting a character in s1 and inserting a\\ncharacter in s2.\\nFigure 3.6 shows an example Levenshtein distance computation of Fig-\\nure 3.5. The typical cell [i, j] has four entries formatted as a 2 × 2 cell. The\\nlower right entry in each cell is the min of the other three, corresponding to\\nthe main dynamic programming step in Figure 3.5. The other three entries\\nare the three entries m[i −1, j −1] + 0 or 1 depending on whether s1[i] =\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n59\\nEDITDISTANCE(s1, s2)\\n1\\nint m[i, j] = 0\\n2\\nfor i ←1 to |s1|\\n3\\ndo m[i, 0] = i\\n4\\nfor j ←1 to |s2|\\n5\\ndo m[0, j] = j\\n6\\nfor i ←1 to |s1|\\n7\\ndo for j ←1 to |s2|\\n8\\ndo m[i, j] = min{m[i −1, j −1] + if (s1[i] = s2[j]) then 0 else 1ﬁ,\\n9\\nm[i −1, j] + 1,\\n10\\nm[i, j −1] + 1}\\n11\\nreturn m[|s1|, |s2|]\\n◮Figure 3.5\\nDynamic programming algorithm for computing the edit distance be-\\ntween strings s1 and s2.\\nf\\na\\ns\\nt\\n0\\n1\\n1\\n2\\n2\\n3\\n3\\n4\\n4\\nc\\n1\\n1\\n1\\n2\\n2\\n1\\n2\\n3\\n2\\n2\\n3\\n4\\n3\\n3\\n4\\n5\\n4\\n4\\na\\n2\\n2\\n2\\n2\\n3\\n2\\n1\\n3\\n3\\n1\\n3\\n4\\n2\\n2\\n4\\n5\\n3\\n3\\nt\\n3\\n3\\n3\\n3\\n4\\n3\\n3\\n2\\n4\\n2\\n2\\n3\\n3\\n2\\n2\\n4\\n3\\n2\\ns\\n4\\n4\\n4\\n4\\n5\\n4\\n4\\n3\\n5\\n3\\n2\\n3\\n4\\n2\\n3\\n3\\n3\\n3\\n◮Figure 3.6\\nExample Levenshtein distance computation. The 2 × 2 cell in the [i, j]\\nentry of the table shows the three numbers whose minimum yields the fourth. The\\ncells in italics determine the edit distance in this example.\\ns2[j], m[i −1, j] + 1 and m[i, j −1] + 1. The cells with numbers in italics depict\\nthe path by which we determine the Levenshtein distance.\\nThe spelling correction problem however demands more than computing\\nedit distance: given a set S of strings (corresponding to terms in the vocab-\\nulary) and a query string q, we seek the string(s) in V of least edit distance\\nfrom q. We may view this as a decoding problem, in which the codewords\\n(the strings in V) are prescribed in advance. The obvious way of doing this\\nis to compute the edit distance from q to each string in V, before selecting the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n60\\n3\\nDictionaries and tolerant retrieval\\nstring(s) of minimum edit distance. This exhaustive search is inordinately\\nexpensive. Accordingly, a number of heuristics are used in practice to efﬁ-\\nciently retrieve vocabulary terms likely to have low edit distance to the query\\nterm(s).\\nThe simplest such heuristic is to restrict the search to dictionary terms be-\\nginning with the same letter as the query string; the hope would be that\\nspelling errors do not occur in the ﬁrst character of the query. A more sophis-\\nticated variant of this heuristic is to use a version of the permuterm index,\\nin which we omit the end-of-word symbol $. Consider the set of all rota-\\ntions of the query string q. For each rotation r from this set, we traverse the\\nB-tree into the permuterm index, thereby retrieving all dictionary terms that\\nhave a rotation beginning with r. For instance, if q is mase and we consider\\nthe rotation r = sema, we would retrieve dictionary terms such as semantic\\nand semaphore that do not have a small edit distance to q. Unfortunately, we\\nwould miss more pertinent dictionary terms such as mare and mane. To ad-\\ndress this, we reﬁne this rotation scheme: for each rotation, we omit a sufﬁx\\nof ℓcharacters before performing the B-tree traversal. This ensures that each\\nterm in the set R of terms retrieved from the dictionary includes a “long”\\nsubstring in common with q. The value of ℓcould depend on the length of q.\\nAlternatively, we may set it to a ﬁxed constant such as 2.\\n3.3.4\\nk-gram indexes for spelling correction\\nTo further limit the set of vocabulary terms for which we compute edit dis-\\ntances to the query term, we now show how to invoke the k-gram index of\\nSection 3.2.2 (page 54) to assist with retrieving vocabulary terms with low\\nedit distance to the query q. Once we retrieve such terms, we can then ﬁnd\\nthe ones of least edit distance from q.\\nIn fact, we will use the k-gram index to retrieve vocabulary terms that\\nhave many k-grams in common with the query. We will argue that for rea-\\nsonable deﬁnitions of “many k-grams in common,” the retrieval process is\\nessentially that of a single scan through the postings for the k-grams in the\\nquery string q.\\nThe 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the post-\\nings for the three bigrams in the query bord. Suppose we wanted to retrieve\\nvocabulary terms that contained at least two of these three bigrams. A single\\nscan of the postings (much as in Chapter 1) would let us enumerate all such\\nterms; in the example of Figure 3.7 we would enumerate aboard, boardroom\\nand border.\\nThis straightforward application of the linear scan intersection of postings\\nimmediately reveals the shortcoming of simply requiring matched vocabu-\\nlary terms to contain a ﬁxed number of k-grams from the query q: terms\\nlike boardroom, an implausible “correction” of bord, get enumerated. Conse-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n61\\nrd\\naboard\\nardent\\nboardroom\\nborder\\nor\\nborder\\nlord\\nmorbid\\nsordid\\nbo\\naboard\\nabout\\nboardroom\\nborder\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n◮Figure 3.7\\nMatching at least two of the three 2-grams in the query bord.\\nquently, we require more nuanced measures of the overlap in k-grams be-\\ntween a vocabulary term and q. The linear scan intersection can be adapted\\nwhen the measure of overlap is the Jaccard coefﬁcient for measuring the over-\\nJACCARD COEFFICIENT\\nlap between two sets A and B, deﬁned to be |A ∩B|/|A ∪B|. The two sets we\\nconsider are the set of k-grams in the query q, and the set of k-grams in a vo-\\ncabulary term. As the scan proceeds, we proceed from one vocabulary term\\nt to the next, computing on the ﬂy the Jaccard coefﬁcient between q and t. If\\nthe coefﬁcient exceeds a preset threshold, we add t to the output; if not, we\\nmove on to the next term in the postings. To compute the Jaccard coefﬁcient,\\nwe need the set of k-grams in q and t.\\nSince we are scanning the postings for all k-grams in q, we immediately\\nhave these k-grams on hand. What about the k-grams of t? In principle,\\nwe could enumerate these on the ﬂy from t; in practice this is not only slow\\nbut potentially infeasible since, in all likelihood, the postings entries them-\\nselves do not contain the complete string t but rather some encoding of t. The\\ncrucial observation is that to compute the Jaccard coefﬁcient, we only need\\nthe length of the string t. To see this, recall the example of Figure 3.7 and\\nconsider the point when the postings scan for query q = bord reaches term\\nt = boardroom. We know that two bigrams match. If the postings stored the\\n(pre-computed) number of bigrams in boardroom (namely, 8), we have all the\\ninformation we require to compute the Jaccard coefﬁcient to be 2/(8 + 3 −2);\\nthe numerator is obtained from the number of postings hits (2, from bo and\\nrd) while the denominator is the sum of the number of bigrams in bord and\\nboardroom, less the number of postings hits.\\nWe could replace the Jaccard coefﬁcient by other measures that allow ef-\\nﬁcient on the ﬂy computation during postings scans. How do we use these\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n62\\n3\\nDictionaries and tolerant retrieval\\nfor spelling correction? One method that has some empirical support is to\\nﬁrst use the k-gram index to enumerate a set of candidate vocabulary terms\\nthat are potential corrections of q. We then compute the edit distance from q\\nto each term in this set, selecting terms from the set with small edit distance\\nto q.\\n3.3.5\\nContext sensitive spelling correction\\nIsolated-term correction would fail to correct typographical errors such as\\nﬂew form Heathrow, where all three query terms are correctly spelled. When\\na phrase such as this retrieves few documents, a search engine may like to\\noffer the corrected query ﬂew from Heathrow. The simplest way to do this is to\\nenumerate corrections of each of the three query terms (using the methods\\nleading up to Section 3.3.4) even though each query term is correctly spelled,\\nthen try substitutions of each correction in the phrase. For the example ﬂew\\nform Heathrow, we enumerate such phrases as ﬂed form Heathrow and ﬂew fore\\nHeathrow. For each such substitute phrase, the search engine runs the query\\nand determines the number of matching results.\\nThis enumeration can be expensive if we ﬁnd many corrections of the in-\\ndividual terms, since we could encounter a large number of combinations of\\nalternatives. Several heuristics are used to trim this space. In the example\\nabove, as we expand the alternatives for ﬂew and form, we retain only the\\nmost frequent combinations in the collection or in the query logs, which con-\\ntain previous queries by users. For instance, we would retain ﬂew from as an\\nalternative to try and extend to a three-term corrected query, but perhaps not\\nﬂed fore or ﬂea form. In this example, the biword ﬂed fore is likely to be rare\\ncompared to the biword ﬂew from. Then, we only attempt to extend the list of\\ntop biwords (such as ﬂew from), to corrections of Heathrow. As an alternative\\nto using the biword statistics in the collection, we may use the logs of queries\\nissued by users; these could of course include queries with spelling errors.\\n?\\nExercise 3.7\\nIf |si| denotes the length of string si, show that the edit distance between s1 and s2 is\\nnever more than max{|s1|, |s2|}.\\nExercise 3.8\\nCompute the edit distance between paris and alice. Write down the 5 × 5 array of\\ndistances between all preﬁxes as computed by the algorithm in Figure 3.5.\\nExercise 3.9\\nWrite pseudocode showing the details of computing on the ﬂy the Jaccard coefﬁcient\\nwhile scanning the postings of the k-gram index, as mentioned on page 61.\\nExercise 3.10\\nCompute the Jaccard coefﬁcients between the query bord and each of the terms in\\nFigure 3.7 that contain the bigram or.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.4\\nPhonetic correction\\n63\\nExercise 3.11\\nConsider the four-term query catched in the rye and suppose that each of the query\\nterms has ﬁve alternative terms suggested by isolated-term correction. How many\\npossible corrected phrases must we consider if we do not trim the space of corrected\\nphrases, but instead try all six variants for each of the terms?\\nExercise 3.12\\nFor each of the preﬁxes of the query — catched, catched in and catched in the — we have\\na number of substitute preﬁxes arising from each term and its alternatives. Suppose\\nthat we were to retain only the top 10 of these substitute preﬁxes, as measured by\\nits number of occurrences in the collection. We eliminate the rest from consideration\\nfor extension to longer preﬁxes: thus, if batched in is not one of the 10 most common\\n2-term queries in the collection, we do not consider any extension of batched in as pos-\\nsibly leading to a correction of catched in the rye. How many of the possible substitute\\npreﬁxes are we eliminating at each phase?\\nExercise 3.13\\nAre we guaranteed that retaining and extending only the 10 commonest substitute\\npreﬁxes of catched in will lead to one of the 10 commonest substitute preﬁxes of catched\\nin the?\\n3.4\\nPhonetic correction\\nOur ﬁnal technique for tolerant retrieval has to do with phonetic correction:\\nmisspellings that arise because the user types a query that sounds like the tar-\\nget term. Such algorithms are especially applicable to searches on the names\\nof people. The main idea here is to generate, for each term, a “phonetic hash”\\nso that similar-sounding terms hash to the same value. The idea owes its\\norigins to work in international police departments from the early 20th cen-\\ntury, seeking to match names for wanted criminals despite the names being\\nspelled differently in different countries. It is mainly used to correct phonetic\\nmisspellings in proper nouns.\\nAlgorithms for such phonetic hashing are commonly collectively known as\\nsoundex algorithms. However, there is an original soundex algorithm, with\\nSOUNDEX\\nvarious variants, built on the following scheme:\\n1. Turn every term to be indexed into a 4-character reduced form. Build an\\ninverted index from these reduced forms to the original terms; call this\\nthe soundex index.\\n2. Do the same with query terms.\\n3. When the query calls for a soundex match, search this soundex index.\\nThe variations in different soundex algorithms have to do with the conver-\\nsion of terms to 4-character forms. A commonly used conversion results in\\na 4-character code, with the ﬁrst character being a letter of the alphabet and\\nthe other three being digits between 0 and 9.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n64\\n3\\nDictionaries and tolerant retrieval\\n1. Retain the ﬁrst letter of the term.\\n2. Change all occurrences of the following letters to ’0’ (zero): ’A’, E’, ’I’, ’O’,\\n’U’, ’H’, ’W’, ’Y’.\\n3. Change letters to digits as follows:\\nB, F, P, V to 1.\\nC, G, J, K, Q, S, X, Z to 2.\\nD,T to 3.\\nL to 4.\\nM, N to 5.\\nR to 6.\\n4. Repeatedly remove one out of each pair of consecutive identical digits.\\n5. Remove all zeros from the resulting string. Pad the resulting string with\\ntrailing zeros and return the ﬁrst four positions, which will consist of a\\nletter followed by three digits.\\nFor an example of a soundex map, Hermann maps to H655. Given a query\\n(say herman), we compute its soundex code and then retrieve all vocabulary\\nterms matching this soundex code from the soundex index, before running\\nthe resulting query on the standard inverted index.\\nThis algorithm rests on a few observations: (1) vowels are viewed as inter-\\nchangeable, in transcribing names; (2) consonants with similar sounds (e.g.,\\nD and T) are put in equivalence classes. This leads to related names often\\nhaving the same soundex codes. While these rules work for many cases,\\nespecially European languages, such rules tend to be writing system depen-\\ndent. For example, Chinese names can be written in Wade-Giles or Pinyin\\ntranscription. While soundex works for some of the differences in the two\\ntranscriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2,\\nit fails in other cases, for example Wade-Giles j and Pinyin r are mapped\\ndifferently.\\n?\\nExercise 3.14\\nFind two differently spelled proper nouns whose soundex codes are the same.\\nExercise 3.15\\nFind two phonetically similar proper nouns whose soundex codes are different.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n3.5\\nReferences and further reading\\n65\\n3.5\\nReferences and further reading\\nKnuth (1997) is a comprehensive source for information on search trees, in-\\ncluding B-trees and their use in searching through dictionaries.\\nGarﬁeld (1976) gives one of the ﬁrst complete descriptions of the permuterm\\nindex. Ferragina and Venturini (2007) give an approach to addressing the\\nspace blowup in permuterm indexes.\\nOne of the earliest formal treatments of spelling correction was due to\\nDamerau (1964). The notion of edit distance that we have used is due to Lev-\\nenshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer\\n(1974). Peterson (1980) and Kukich (1992) developed variants of methods\\nbased on edit distances, culminating in a detailed empirical study of sev-\\neral methods by Zobel and Dart (1995), which shows that k-gram indexing\\nis very effective for ﬁnding candidate mismatches, but should be combined\\nwith a more ﬁne-grained technique such as edit distance to determine the\\nmost likely misspellings. Gusﬁeld (1997) is a standard reference on string\\nalgorithms such as edit distance.\\nProbabilistic models (“noisy channel” models) for spelling correction were\\npioneered by Kernighan et al. (1990) and further developed by Brill and\\nMoore (2000) and Toutanova and Moore (2002). In these models, the mis-\\nspelled query is viewed as a probabilistic corruption of a correct query. They\\nhave a similar mathematical basis to the language model methods presented\\nin Chapter 12, and also provide ways of incorporating phonetic similarity,\\ncloseness on the keyboard, and data from the actual spelling mistakes of\\nusers. Many would regard them as the state-of-the-art approach. Cucerzan\\nand Brill (2004) show how this work can be extended to learning spelling\\ncorrection models based on query reformulations in search engine logs.\\nThe soundex algorithm is attributed to Margaret K. Odell and Robert C.\\nRusselli (from U.S. patents granted in 1918 and 1922); the version described\\nhere draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate var-\\nious phonetic matching algorithms, ﬁnding that a variant of the soundex\\nalgorithm performs poorly for general spelling correction, but that other al-\\ngorithms based on the phonetic similarity of term pronunciations perform\\nwell.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n67\\n4\\nIndex construction\\nIn this chapter, we look at how to construct an inverted index. We call this\\nprocess index construction or indexing; the process or machine that performs it\\nINDEXING\\nthe indexer. The design of indexing algorithms is governed by hardware con-\\nINDEXER\\nstraints. We therefore begin this chapter with a review of the basics of com-\\nputer hardware that are relevant for indexing. We then introduce blocked\\nsort-based indexing (Section 4.2), an efﬁcient single-machine algorithm de-\\nsigned for static collections that can be viewed as a more scalable version of\\nthe basic sort-based indexing algorithm we introduced in Chapter 1. Sec-\\ntion 4.3 describes single-pass in-memory indexing, an algorithm that has\\neven better scaling properties because it does not hold the vocabulary in\\nmemory. For very large collections like the web, indexing has to be dis-\\ntributed over computer clusters with hundreds or thousands of machines.\\nWe discuss this in Section 4.4. Collections with frequent changes require dy-\\nnamic indexing introduced in Section 4.5 so that changes in the collection are\\nimmediately reﬂected in the index. Finally, we cover some complicating is-\\nsues that can arise in indexing – such as security and indexes for ranked\\nretrieval – in Section 4.6.\\nIndex construction interacts with several topics covered in other chapters.\\nThe indexer needs raw text, but documents are encoded in many ways (see\\nChapter 2). Indexers compress and decompress intermediate ﬁles and the\\nﬁnal index (see Chapter 5). In web search, documents are not on a local\\nﬁle system, but have to be spidered or crawled (see Chapter 20). In enter-\\nprise search, most documents are encapsulated in varied content manage-\\nment systems, email applications, and databases. We give some examples\\nin Section 4.7. Although most of these applications can be accessed via http,\\nnative Application Programming Interfaces (APIs) are usually more efﬁcient.\\nThe reader should be aware that building the subsystem that feeds raw text\\nto the indexing process can in itself be a challenging problem.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n68\\n4\\nIndex construction\\n◮Table 4.1\\nTypical system parameters in 2007. The seek time is the time needed\\nto position the disk head in a new position. The transfer time per byte is the rate of\\ntransfer from disk to memory when the head is in the right position.\\nSymbol\\nStatistic\\nValue\\ns\\naverage seek time\\n5 ms = 5 × 10−3 s\\nb\\ntransfer time per byte\\n0.02 µs = 2 × 10−8 s\\nprocessor’s clock rate\\n109 s−1\\np\\nlowlevel operation\\n(e.g., compare & swap a word)\\n0.01 µs = 10−8 s\\nsize of main memory\\nseveral GB\\nsize of disk space\\n1 TB or more\\n4.1\\nHardware basics\\nWhen building an information retrieval (IR) system, many decisions are based\\non the characteristics of the computer hardware on which the system runs.\\nWe therefore begin this chapter with a brief review of computer hardware.\\nPerformance characteristics typical of systems in 2007 are shown in Table 4.1.\\nA list of hardware basics that we need in this book to motivate IR system\\ndesign follows.\\n• Access to data in memory is much faster than access to data on disk. It\\ntakes a few clock cycles (perhaps 5 × 10−9 seconds) to access a byte in\\nmemory, but much longer to transfer it from disk (about 2 × 10−8 sec-\\nonds). Consequently, we want to keep as much data as possible in mem-\\nory, especially those data that we need to access frequently. We call the\\ntechnique of keeping frequently used disk data in main memory caching.\\nCACHING\\n• When doing a disk read or write, it takes a while for the disk head to\\nmove to the part of the disk where the data are located. This time is called\\nthe seek time and it averages 5 ms for typical disks. No data are being\\nSEEK TIME\\ntransferred during the seek. To maximize data transfer rates, chunks of\\ndata that will be read together should therefore be stored contiguously on\\ndisk. For example, using the numbers in Table 4.1 it may take as little as\\n0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is\\nstored as one chunk, but up to 0.2 + 100 × (5 × 10−3) = 0.7 seconds if it\\nis stored in 100 noncontiguous chunks because we need to move the disk\\nhead up to 100 times.\\n• Operating systems generally read and write entire blocks. Thus, reading\\na single byte from disk can take as much time as reading the entire block.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.2\\nBlocked sort-based indexing\\n69\\nBlock sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part\\nof main memory where a block being read or written is stored a buffer.\\nBUFFER\\n• Data transfers from disk to memory are handled by the system bus, not by\\nthe processor. This means that the processor is available to process data\\nduring disk I/O. We can exploit this fact to speed up data transfers by\\nstoring compressed data on disk. Assuming an efﬁcient decompression\\nalgorithm, the total time of reading and then decompressing compressed\\ndata is usually less than reading uncompressed data.\\n• Servers used in IR systems typically have several gigabytes (GB) of main\\nmemory, sometimes tens of GB. Available disk space is several orders of\\nmagnitude larger.\\n4.2\\nBlocked sort-based indexing\\nThe basic steps in constructing a nonpositional index are depicted in Fig-\\nure 1.4 (page 8). We ﬁrst make a pass through the collection assembling all\\nterm–docID pairs. We then sort the pairs with the term as the dominant key\\nand docID as the secondary key. Finally, we organize the docIDs for each\\nterm into a postings list and compute statistics like term and document fre-\\nquency. For small collections, all this can be done in memory. In this chapter,\\nwe describe methods for large collections that require the use of secondary\\nstorage.\\nTo make index construction more efﬁcient, we represent terms as termIDs\\n(instead of strings as we did in Figure 1.4), where each termID is a unique\\nTERMID\\nserial number. We can build the mapping from terms to termIDs on the ﬂy\\nwhile we are processing the collection; or, in a two-pass approach, we com-\\npile the vocabulary in the ﬁrst pass and construct the inverted index in the\\nsecond pass. The index construction algorithms described in this chapter all\\ndo a single pass through the data. Section 4.7 gives references to multipass\\nalgorithms that are preferable in certain applications, for example, when disk\\nspace is scarce.\\nWe work with the Reuters-RCV1 collection as our model collection in this\\nREUTERS-RCV1\\nchapter, a collection with roughly 1 GB of text. It consists of about 800,000\\ndocuments that were sent over the Reuters newswire during a 1-year pe-\\nriod between August 20, 1996, and August 19, 1997. A typical document is\\nshown in Figure 4.1, but note that we ignore multimedia information like\\nimages in this book and are only concerned with text. Reuters-RCV1 covers\\na wide range of international topics, including politics, business, sports, and\\n(as in this example) science. Some key statistics of the collection are shown\\nin Table 4.2.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n70\\n4\\nIndex construction\\n◮Table 4.2\\nCollection statistics for Reuters-RCV1. Values are rounded for the com-\\nputations in this book. The unrounded values are: 806,791 documents, 222 tokens\\nper document, 391,523 (distinct) terms, 6.04 bytes per token with spaces and punc-\\ntuation, 4.5 bytes per token without spaces and punctuation, 7.5 bytes per term, and\\n96,969,056 tokens. The numbers in this table correspond to the third line (“case fold-\\ning”) in Table 5.1 (page 87).\\nSymbol\\nStatistic\\nValue\\nN\\ndocuments\\n800,000\\nLave\\navg. # tokens per document\\n200\\nM\\nterms\\n400,000\\navg. # bytes per token (incl. spaces/punct.)\\n6\\navg. # bytes per token (without spaces/punct.)\\n4.5\\navg. # bytes per term\\n7.5\\nT\\ntokens\\n100,000,000\\nREUTERS\\nExtreme conditions create rare Antarctic clouds\\nYou are here:  Home > News > Science > Article\\nGo to a Section:      U.S.     International      Business      Markets\\nPolitics\\nEntertainment\\nTechnology\\nTue Aug 1, 2006 3:20am ET\\nEmail This Article | Print This Article | Reprints\\nSYDNEY (Reuters) - Rare, mother-of-pearl colored clouds\\ncaused by extreme weather conditions above Antarctica are a\\npossible indication of global warming, Australian scientists said on\\nTuesday.\\nKnown as nacreous clouds, the spectacular formations showing delicate\\nwisps of colors were photographed in the sky over an Australian\\nmeteorological base at Mawson Station on July 25.\\nSports\\nOddly Enough\\n[-] Text [+]\\n◮Figure 4.1\\nDocument from the Reuters newswire.\\nReuters-RCV1 has 100 million tokens. Collecting all termID–docID pairs of\\nthe collection using 4 bytes each for termID and docID therefore requires 0.8\\nGB of storage. Typical collections today are often one or two orders of mag-\\nnitude larger than Reuters-RCV1. You can easily see how such collections\\noverwhelm even large computers if we try to sort their termID–docID pairs\\nin memory. If the size of the intermediate ﬁles during index construction is\\nwithin a small factor of available memory, then the compression techniques\\nintroduced in Chapter 5 can help; however, the postings ﬁle of many large\\ncollections cannot ﬁt into memory even after compression.\\nWith main memory insufﬁcient, we need to use an external sorting algo-\\nEXTERNAL SORTING\\nALGORITHM\\nrithm, that is, one that uses disk. For acceptable speed, the central require-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.2\\nBlocked sort-based indexing\\n71\\nBSBINDEXCONSTRUCTION()\\n1\\nn ←0\\n2\\nwhile (all documents have not been processed)\\n3\\ndo n ←n + 1\\n4\\nblock ←PARSENEXTBLOCK()\\n5\\nBSBI-INVERT(block)\\n6\\nWRITEBLOCKTODISK(block, fn)\\n7\\nMERGEBLOCKS( f1, . . . , fn; f merged)\\n◮Figure 4.2\\nBlocked sort-based indexing. The algorithm stores inverted blocks in\\nﬁles f1, . . . , fn and the merged index in f merged.\\nment of such an algorithm is that it minimize the number of random disk\\nseeks during sorting – sequential disk reads are far faster than seeks as we\\nexplained in Section 4.1. One solution is the blocked sort-based indexing algo-\\nBLOCKED SORT-BASED\\nINDEXING ALGORITHM\\nrithm or BSBI in Figure 4.2. BSBI (i) segments the collection into parts of equal\\nsize, (ii) sorts the termID–docID pairs of each part in memory, (iii) stores in-\\ntermediate sorted results on disk, and (iv) merges all intermediate results\\ninto the ﬁnal index.\\nThe algorithm parses documents into termID–docID pairs and accumu-\\nlates the pairs in memory until a block of a ﬁxed size is full (PARSENEXTBLOCK\\nin Figure 4.2). We choose the block size to ﬁt comfortably into memory to\\npermit a fast in-memory sort. The block is then inverted and written to disk.\\nInversion involves two steps. First, we sort the termID–docID pairs. Next,\\nINVERSION\\nwe collect all termID–docID pairs with the same termID into a postings list,\\nwhere a posting is simply a docID. The result, an inverted index for the block\\nPOSTING\\nwe have just read, is then written to disk. Applying this to Reuters-RCV1 and\\nassuming we can ﬁt 10 million termID–docID pairs into memory, we end up\\nwith ten blocks, each an inverted index of one part of the collection.\\nIn the ﬁnal step, the algorithm simultaneously merges the ten blocks into\\none large merged index. An example with two blocks is shown in Figure 4.3,\\nwhere we use di to denote the ith document of the collection. To do the merg-\\ning, we open all block ﬁles simultaneously, and maintain small read buffers\\nfor the ten blocks we are reading and a write buffer for the ﬁnal merged in-\\ndex we are writing. In each iteration, we select the lowest termID that has\\nnot been processed yet using a priority queue or a similar data structure. All\\npostings lists for this termID are read and merged, and the merged list is\\nwritten back to disk. Each read buffer is reﬁlled from its ﬁle when necessary.\\nHow expensive is BSBI? Its time complexity is Θ(T log T) because the step\\nwith the highest time complexity is sorting and T is an upper bound for the\\nnumber of items we must sort (i.e., the number of termID–docID pairs). But\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n72\\n4\\nIndex construction\\nbrutus\\nd1,d3\\ncaesar\\nd1,d2,d4\\nnoble\\nd5\\nwith\\nd1,d2,d3,d5\\nbrutus\\nd6,d7\\ncaesar\\nd8,d9\\njulius\\nd10\\nkilled\\nd8\\npostings lists\\nto be merged\\nbrutus\\nd1,d3,d6,d7\\ncaesar\\nd1,d2,d4,d8,d9\\njulius\\nd10\\nkilled\\nd8\\nnoble\\nd5\\nwith\\nd1,d2,d3,d5\\nmerged\\npostings lists\\ndisk\\n◮Figure 4.3\\nMerging in blocked sort-based indexing. Two blocks (“postings lists to\\nbe merged”) are loaded from disk into memory, merged in memory (“merged post-\\nings lists”) and written back to disk. We show terms instead of termIDs for better\\nreadability.\\nthe actual indexing time is usually dominated by the time it takes to parse the\\ndocuments (PARSENEXTBLOCK) and to do the ﬁnal merge (MERGEBLOCKS).\\nExercise 4.6 asks you to compute the total index construction time for RCV1\\nthat includes these steps as well as inverting the blocks and writing them to\\ndisk.\\nNotice that Reuters-RCV1 is not particularly large in an age when one or\\nmore GB of memory are standard on personal computers. With appropriate\\ncompression (Chapter 5), we could have created an inverted index for RCV1\\nin memory on a not overly beefy server. The techniques we have described\\nare needed, however, for collections that are several orders of magnitude\\nlarger.\\n?\\nExercise 4.1\\nIf we need T log2 T comparisons (where T is the number of termID–docID pairs) and\\ntwo disk seeks for each comparison, how much time would index construction for\\nReuters-RCV1 take if we used disk instead of memory for storage and an unopti-\\nmized sorting algorithm (i.e., not an external sorting algorithm)? Use the system\\nparameters in Table 4.1.\\nExercise 4.2\\n[⋆]\\nHow would you create the dictionary in blocked sort-based indexing on the ﬂy to\\navoid an extra pass through the data?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.3\\nSingle-pass in-memory indexing\\n73\\nSPIMI-INVERT(token_stream)\\n1\\noutput_file = NEWFILE()\\n2\\ndictionary = NEWHASH()\\n3\\nwhile (free memory available)\\n4\\ndo token ←next(token_stream)\\n5\\nif term(token) /∈dictionary\\n6\\nthen postings_list = ADDTODICTIONARY(dictionary, term(token))\\n7\\nelse postings_list = GETPOSTINGSLIST(dictionary, term(token))\\n8\\nif f ull(postings_list)\\n9\\nthen postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token))\\n10\\nADDTOPOSTINGSLIST(postings_list, docID(token))\\n11\\nsorted_terms ←SORTTERMS(dictionary)\\n12\\nWRITEBLOCKTODISK(sorted_terms, dictionary, output_file)\\n13\\nreturn output_file\\n◮Figure 4.4\\nInversion of a block in single-pass in-memory indexing\\n4.3\\nSingle-pass in-memory indexing\\nBlocked sort-based indexing has excellent scaling properties, but it needs\\na data structure for mapping terms to termIDs. For very large collections,\\nthis data structure does not ﬁt into memory. A more scalable alternative is\\nsingle-pass in-memory indexing or SPIMI. SPIMI uses terms instead of termIDs,\\nSINGLE-PASS\\nIN-MEMORY INDEXING\\nwrites each block’s dictionary to disk, and then starts a new dictionary for the\\nnext block. SPIMI can index collections of any size as long as there is enough\\ndisk space available.\\nThe SPIMI algorithm is shown in Figure 4.4. The part of the algorithm that\\nparses documents and turns them into a stream of term–docID pairs, which\\nwe call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on\\nthe token stream until the entire collection has been processed.\\nTokens are processed one by one (line 4) during each successive call of\\nSPIMI-INVERT. When a term occurs for the ﬁrst time, it is added to the\\ndictionary (best implemented as a hash), and a new postings list is created\\n(line 6). The call in line 7 returns this postings list for subsequent occurrences\\nof the term.\\nA difference between BSBI and SPIMI is that SPIMI adds a posting di-\\nrectly to its postings list (line 10). Instead of ﬁrst collecting all termID–docID\\npairs and then sorting them (as we did in BSBI), each postings list is dynamic\\n(i.e., its size is adjusted as it grows) and it is immediately available to collect\\npostings. This has two advantages: It is faster because there is no sorting\\nrequired, and it saves memory because we keep track of the term a postings\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n74\\n4\\nIndex construction\\nlist belongs to, so the termIDs of postings need not be stored. As a result, the\\nblocks that individual calls of SPIMI-INVERT can process are much larger\\nand the index construction process as a whole is more efﬁcient.\\nBecause we do not know how large the postings list of a term will be when\\nwe ﬁrst encounter it, we allocate space for a short postings list initially and\\ndouble the space each time it is full (lines 8–9). This means that some mem-\\nory is wasted, which counteracts the memory savings from the omission of\\ntermIDs in intermediate data structures. However, the overall memory re-\\nquirements for the dynamically constructed index of a block in SPIMI are\\nstill lower than in BSBI.\\nWhen memory has been exhausted, we write the index of the block (which\\nconsists of the dictionary and the postings lists) to disk (line 12). We have to\\nsort the terms (line 11) before doing this because we want to write postings\\nlists in lexicographic order to facilitate the ﬁnal merging step. If each block’s\\npostings lists were written in unsorted order, merging blocks could not be\\naccomplished by a simple linear scan through each block.\\nEach call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last\\nstep of SPIMI (corresponding to line 7 in Figure 4.2; not shown in Figure 4.4)\\nis then to merge the blocks into the ﬁnal inverted index.\\nIn addition to constructing a new dictionary structure for each block and\\neliminating the expensive sorting step, SPIMI has a third important compo-\\nnent: compression. Both the postings and the dictionary terms can be stored\\ncompactly on disk if we employ compression. Compression increases the ef-\\nﬁciency of the algorithm further because we can process even larger blocks,\\nand because the individual blocks require less space on disk. We refer readers\\nto the literature for this aspect of the algorithm (Section 4.7).\\nThe time complexity of SPIMI is Θ(T) because no sorting of tokens is re-\\nquired and all operations are at most linear in the size of the collection.\\n4.4\\nDistributed indexing\\nCollections are often so large that we cannot perform index construction efﬁ-\\nciently on a single machine. This is particularly true of the World Wide Web\\nfor which we need large computer clusters1 to construct any reasonably sized\\nweb index. Web search engines, therefore, use distributed indexing algorithms\\nfor index construction. The result of the construction process is a distributed\\nindex that is partitioned across several machines – either according to term\\nor according to document. In this section, we describe distributed indexing\\nfor a term-partitioned index. Most large search engines prefer a document-\\n1. A cluster in this chapter is a group of tightly coupled computers that work together closely.\\nThis sense of the word is different from the use of cluster as a group of documents that are\\nsemantically similar in Chapters 16–18.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.4\\nDistributed indexing\\n75\\npartitioned index (which can be easily generated from a term-partitioned\\nindex). We discuss this topic further in Section 20.3 (page 454).\\nThe distributed index construction method we describe in this section is an\\napplication of MapReduce, a general architecture for distributed computing.\\nMAPREDUCE\\nMapReduce is designed for large computer clusters. The point of a cluster is\\nto solve large computing problems on cheap commodity machines or nodes\\nthat are built from standard parts (processor, memory, disk) as opposed to on\\na supercomputer with specialized hardware. Although hundreds or thou-\\nsands of machines are available in such clusters, individual machines can\\nfail at any time. One requirement for robust distributed indexing is, there-\\nfore, that we divide the work up into chunks that we can easily assign and\\n– in case of failure – reassign. A master node directs the process of assigning\\nMASTER NODE\\nand reassigning tasks to individual worker nodes.\\nThe map and reduce phases of MapReduce split up the computing job\\ninto chunks that standard machines can process in a short time. The various\\nsteps of MapReduce are shown in Figure 4.5 and an example on a collection\\nconsisting of two documents is shown in Figure 4.6. First, the input data,\\nin our case a collection of web pages, are split into n splits where the size of\\nSPLITS\\nthe split is chosen to ensure that the work can be distributed evenly (chunks\\nshould not be too large) and efﬁciently (the total number of chunks we need\\nto manage should not be too large); 16 or 64 MB are good sizes in distributed\\nindexing. Splits are not preassigned to machines, but are instead assigned\\nby the master node on an ongoing basis: As a machine ﬁnishes processing\\none split, it is assigned the next one. If a machine dies or becomes a laggard\\ndue to hardware problems, the split it is working on is simply reassigned to\\nanother machine.\\nIn general, MapReduce breaks a large computing problem into smaller\\nparts by recasting it in terms of manipulation of key-value pairs. For index-\\nKEY-VALUE PAIRS\\ning, a key-value pair has the form (termID,docID). In distributed indexing,\\nthe mapping from terms to termIDs is also distributed and therefore more\\ncomplex than in single-machine indexing. A simple solution is to maintain\\na (perhaps precomputed) mapping for frequent terms that is copied to all\\nnodes and to use terms directly (instead of termIDs) for infrequent terms.\\nWe do not address this problem here and assume that all nodes share a con-\\nsistent term →termID mapping.\\nThe map phase of MapReduce consists of mapping splits of the input data\\nMAP PHASE\\nto key-value pairs. This is the same parsing task we also encountered in BSBI\\nand SPIMI, and we therefore call the machines that execute the map phase\\nparsers. Each parser writes its output to local intermediate ﬁles, the segment\\nPARSER\\nSEGMENT FILE\\nﬁles (shown as a-f g-p q-z in Figure 4.5).\\nFor the reduce phase, we want all values for a given key to be stored close\\nREDUCE PHASE\\ntogether, so that they can be read and processed quickly. This is achieved by\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n76\\n4\\nIndex construction\\nmaster\\nassign\\nmap\\nphase\\nreduce\\nphase\\nassign\\nparser\\nsplits\\nparser\\nparser\\ninverter\\npostings\\ninverter\\ninverter\\na-f\\ng-p\\nq-z\\na-f g-p q-z\\na-f g-p q-z\\na-f\\nsegment\\nfiles\\ng-p q-z\\n◮Figure 4.5\\nAn example of distributed indexing with MapReduce. Adapted from\\nDean and Ghemawat (2004).\\npartitioning the keys into j term partitions and having the parsers write key-\\nvalue pairs for each term partition into a separate segment ﬁle. In Figure 4.5,\\nthe term partitions are according to ﬁrst letter: a–f, g–p, q–z, and j = 3. (We\\nchose these key ranges for ease of exposition. In general, key ranges need not\\ncorrespond to contiguous terms or termIDs.) The term partitions are deﬁned\\nby the person who operates the indexing system (Exercise 4.10). The parsers\\nthen write corresponding segment ﬁles, one for each term partition. Each\\nterm partition thus corresponds to r segments ﬁles, where r is the number\\nof parsers. For instance, Figure 4.5 shows three a–f segment ﬁles of the a–f\\npartition, corresponding to the three parsers shown in the ﬁgure.\\nCollecting all values (here: docIDs) for a given key (here: termID) into one\\nlist is the task of the inverters in the reduce phase. The master assigns each\\nINVERTER\\nterm partition to a different inverter – and, as in the case of parsers, reas-\\nsigns term partitions in case of failing or slow inverters. Each term partition\\n(corresponding to r segment ﬁles, one on each parser) is processed by one in-\\nverter. We assume here that segment ﬁles are of a size that a single machine\\ncan handle (Exercise 4.9). Finally, the list of values is sorted for each key and\\nwritten to the ﬁnal sorted postings list (“postings” in the ﬁgure). (Note that\\npostings in Figure 4.6 include term frequencies, whereas each posting in the\\nother sections of this chapter is simply a docID without term frequency in-\\nformation.) The data ﬂow is shown for a–f in Figure 4.5. This completes the\\nconstruction of the inverted index.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.4\\nDistributed indexing\\n77\\nSchema of map and reduce functions\\nmap: input\\nlist(k,v)\\nreduce: (k,list(v))\\noutput\\nInstantiation of the schema for index construction\\nmap: web collection\\nlist(termID, docID)\\nreduce: ( termID ,\\n1 list(docID) ,  termID  ,\\n2 list(docID) , . . . )\\n(postings list\\nlist\\n1, postings\\n2, ...)\\nExample for index construction\\nmap: d2 : C died. d1 : C came, C c’ed.\\n( C, d2 ,  died,d2 ,  C,d1 ,  came,d1 ,  C,d1 ,  〈c’ed,d1〉)\\nreduce: ( C,(d2,d1,d1) , died,(d2) , came,(d1) , c’ed,(d1) )\\n(〈C,(d1:2,d2:1)〉, 〈died,(d2:1)〉, 〈came,(d1:1)〉, 〈c’ed,(d1:1)〉 )\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n→\\n→\\n→\\n→\\n→\\n→\\n◮Figure 4.6\\nMap and reduce functions in MapReduce. In general, the map func-\\ntion produces a list of key-value pairs. All values for a key are collected into one\\nlist in the reduce phase. This list is then processed further. The instantiations of the\\ntwo functions and an example are shown for index construction. Because the map\\nphase processes documents in a distributed fashion, termID–docID pairs need not be\\nordered correctly initially as in this example. The example shows terms instead of\\ntermIDs for better readability. We abbreviate Caesar as C and conquered as c’ed.\\nParsers and inverters are not separate sets of machines. The master iden-\\ntiﬁes idle machines and assigns tasks to them. The same machine can be a\\nparser in the map phase and an inverter in the reduce phase. And there are\\noften other jobs that run in parallel with index construction, so in between\\nbeing a parser and an inverter a machine might do some crawling or another\\nunrelated task.\\nTo minimize write times before inverters reduce the data, each parser writes\\nits segment ﬁles to its local disk. In the reduce phase, the master communi-\\ncates to an inverter the locations of the relevant segment ﬁles (e.g., of the r\\nsegment ﬁles of the a–f partition). Each segment ﬁle only requires one se-\\nquential read because all data relevant to a particular inverter were written\\nto a single segment ﬁle by the parser. This setup minimizes the amount of\\nnetwork trafﬁc needed during indexing.\\nFigure 4.6 shows the general schema of the MapReduce functions. In-\\nput and output are often lists of key-value pairs themselves, so that several\\nMapReduce jobs can run in sequence. In fact, this was the design of the\\nGoogle indexing system in 2004. What we describe in this section corre-\\nsponds to only one of ﬁve to ten MapReduce operations in that indexing\\nsystem. Another MapReduce operation transforms the term-partitioned in-\\ndex we just created into a document-partitioned one.\\nMapReduce offers a robust and conceptually simple framework for imple-\\nmenting index construction in a distributed environment. By providing a\\nsemiautomatic method for splitting index construction into smaller tasks, it\\ncan scale to almost arbitrarily large collections, given computer clusters of\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n78\\n4\\nIndex construction\\nsufﬁcient size.\\n?\\nExercise 4.3\\nFor n = 15 splits, r = 10 segments, and j = 3 term partitions, how long would\\ndistributed index creation take for Reuters-RCV1 in a MapReduce architecture? Base\\nyour assumptions about cluster machines on Table 4.1.\\n4.5\\nDynamic indexing\\nThus far, we have assumed that the document collection is static. This is ﬁne\\nfor collections that change infrequently or never (e.g., the Bible or Shake-\\nspeare). But most collections are modiﬁed frequently with documents being\\nadded, deleted, and updated. This means that new terms need to be added\\nto the dictionary, and postings lists need to be updated for existing terms.\\nThe simplest way to achieve this is to periodically reconstruct the index\\nfrom scratch. This is a good solution if the number of changes over time is\\nsmall and a delay in making new documents searchable is acceptable – and\\nif enough resources are available to construct a new index while the old one\\nis still available for querying.\\nIf there is a requirement that new documents be included quickly, one solu-\\ntion is to maintain two indexes: a large main index and a small auxiliary index\\nAUXILIARY INDEX\\nthat stores new documents. The auxiliary index is kept in memory. Searches\\nare run across both indexes and results merged. Deletions are stored in an in-\\nvalidation bit vector. We can then ﬁlter out deleted documents before return-\\ning the search result. Documents are updated by deleting and reinserting\\nthem.\\nEach time the auxiliary index becomes too large, we merge it into the main\\nindex. The cost of this merging operation depends on how we store the index\\nin the ﬁle system. If we store each postings list as a separate ﬁle, then the\\nmerge simply consists of extending each postings list of the main index by\\nthe corresponding postings list of the auxiliary index. In this scheme, the\\nreason for keeping the auxiliary index is to reduce the number of disk seeks\\nrequired over time. Updating each document separately requires up to Mave\\ndisk seeks, where Mave is the average size of the vocabulary of documents in\\nthe collection. With an auxiliary index, we only put additional load on the\\ndisk when we merge auxiliary and main indexes.\\nUnfortunately, the one-ﬁle-per-postings-list scheme is infeasible because\\nmost ﬁle systems cannot efﬁciently handle very large numbers of ﬁles. The\\nsimplest alternative is to store the index as one large ﬁle, that is, as a concate-\\nnation of all postings lists. In reality, we often choose a compromise between\\nthe two extremes (Section 4.7). To simplify the discussion, we choose the\\nsimple option of storing the index as one large ﬁle here.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.5\\nDynamic indexing\\n79\\nLMERGEADDTOKEN(indexes, Z0, token)\\n1\\nZ0 ←MERGE(Z0, {token})\\n2\\nif |Z0| = n\\n3\\nthen for i ←0 to ∞\\n4\\ndo if Ii ∈indexes\\n5\\nthen Zi+1 ←MERGE(Ii, Zi)\\n6\\n(Zi+1 is a temporary index on disk.)\\n7\\nindexes ←indexes −{Ii}\\n8\\nelse Ii ←Zi\\n(Zi becomes the permanent index Ii.)\\n9\\nindexes ←indexes ∪{Ii}\\n10\\nBREAK\\n11\\nZ0 ←∅\\nLOGARITHMICMERGE()\\n1\\nZ0 ←∅\\n(Z0 is the in-memory index.)\\n2\\nindexes ←∅\\n3\\nwhile true\\n4\\ndo LMERGEADDTOKEN(indexes, Z0, GETNEXTTOKEN())\\n◮Figure 4.7\\nLogarithmic merging. Each token (termID,docID) is initially added to\\nin-memory index Z0 by LMERGEADDTOKEN. LOGARITHMICMERGE initializes Z0\\nand indexes.\\nIn this scheme, we process each posting ⌊T/n⌋times because we touch it\\nduring each of ⌊T/n⌋merges where n is the size of the auxiliary index and T\\nthe total number of postings. Thus, the overall time complexity is Θ(T2/n).\\n(We neglect the representation of terms here and consider only the docIDs.\\nFor the purpose of time complexity, a postings list is simply a list of docIDs.)\\nWe can do better than Θ(T2/n) by introducing log2(T/n) indexes I0, I1,\\nI2, ...of size 20 × n, 21 × n, 22 × n .... Postings percolate up this sequence of\\nindexes and are processed only once on each level. This scheme is called log-\\nLOGARITHMIC\\nMERGING\\narithmic merging (Figure 4.7). As before, up to n postings are accumulated in\\nan in-memory auxiliary index, which we call Z0. When the limit n is reached,\\nthe 20 × n postings in Z0 are transferred to a new index I0 that is created on\\ndisk. The next time Z0 is full, it is merged with I0 to create an index Z1 of size\\n21× n. Then Z1 is either stored as I1 (if there isn’t already an I1) or merged\\nwith I1 into Z2 (if I1 exists); and so on. We service search requests by query-\\ning in-memory Z0 and all currently valid indexes Ii on disk and merging the\\nresults. Readers familiar with the binomial heap data structure2 will recog-\\n2. See, for example, (Cormen et al. 1990, Chapter 19).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n80\\n4\\nIndex construction\\nnize its similarity with the structure of the inverted indexes in logarithmic\\nmerging.\\nOverall index construction time is Θ(T log(T/n)) because each posting\\nis processed only once on each of the log(T/n) levels. We trade this efﬁ-\\nciency gain for a slow down of query processing; we now need to merge\\nresults from log(T/n) indexes as opposed to just two (the main and auxil-\\niary indexes). As in the auxiliary index scheme, we still need to merge very\\nlarge indexes occasionally (which slows down the search system during the\\nmerge), but this happens less frequently and the indexes involved in a merge\\non average are smaller.\\nHaving multiple indexes complicates the maintenance of collection-wide\\nstatistics. For example, it affects the spelling correction algorithm in Sec-\\ntion 3.3 (page 56) that selects the corrected alternative with the most hits.\\nWith multiple indexes and an invalidation bit vector, the correct number of\\nhits for a term is no longer a simple lookup. In fact, all aspects of an IR\\nsystem – index maintenance, query processing, distribution, and so on – are\\nmore complex in logarithmic merging.\\nBecause of this complexity of dynamic indexing, some large search engines\\nadopt a reconstruction-from-scratch strategy. They do not construct indexes\\ndynamically. Instead, a new index is built from scratch periodically. Query\\nprocessing is then switched from the new index and the old index is deleted.\\n?\\nExercise 4.4\\nFor n = 2 and 1 ≤T ≤30, perform a step-by-step simulation of the algorithm in\\nFigure 4.7. Create a table that shows, for each point in time at which T = 2 ∗k tokens\\nhave been processed (1 ≤k ≤15), which of the three indexes I0, . . . , I3 are in use. The\\nﬁrst three lines of the table are given below.\\nI3\\nI2\\nI1\\nI0\\n2\\n0\\n0\\n0\\n0\\n4\\n0\\n0\\n0\\n1\\n6\\n0\\n0\\n1\\n0\\n4.6\\nOther types of indexes\\nThis chapter only describes construction of nonpositional indexes. Except\\nfor the much larger data volume we need to accommodate, the main differ-\\nence for positional indexes is that (termID, docID, (position1, position2, ...))\\ntriples, instead of (termID, docID) pairs have to be processed and that tokens\\nand postings contain positional information in addition to docIDs. With this\\nchange, the algorithms discussed here can all be applied to positional in-\\ndexes.\\nIn the indexes we have considered so far, postings lists are ordered with\\nrespect to docID. As we see in Chapter 5, this is advantageous for compres-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.6\\nOther types of indexes\\n81\\nusers\\ndocuments\\n0/1\\ndoc\\ne.\\n, 1 otherwis\\n0 if user can’t read\\n◮Figure 4.8\\nA user-document matrix for access control lists. Element (i, j) is 1 if\\nuser i has access to document j and 0 otherwise. During query processing, a user’s\\naccess postings list is intersected with the results list returned by the text part of the\\nindex.\\nsion – instead of docIDs we can compress smaller gaps between IDs, thus\\nreducing space requirements for the index. However, this structure for the\\nindex is not optimal when we build ranked (Chapters 6 and 7) – as opposed to\\nRANKED\\nBoolean – retrieval systems. In ranked retrieval, postings are often ordered ac-\\nRETRIEVAL SYSTEMS\\ncording to weight or impact, with the highest-weighted postings occurring\\nﬁrst. With this organization, scanning of long postings lists during query\\nprocessing can usually be terminated early when weights have become so\\nsmall that any further documents can be predicted to be of low similarity\\nto the query (see Chapter 6). In a docID-sorted index, new documents are\\nalways inserted at the end of postings lists. In an impact-sorted index (Sec-\\ntion 7.1.5, page 140), the insertion can occur anywhere, thus complicating the\\nupdate of the inverted index.\\nSecurityis an important consideration for retrieval systems in corporations.\\nSECURITY\\nA low-level employee should not be able to ﬁnd the salary roster of the cor-\\nporation, but authorized managers need to be able to search for it. Users’\\nresults lists must not contain documents they are barred from opening; the\\nvery existence of a document can be sensitive information.\\nUser authorization is often mediated through access control lists or ACLs.\\nACCESS CONTROL LISTS\\nACLs can be dealt with in an information retrieval system by representing\\neach document as the set of users that can access them (Figure 4.8) and then\\ninverting the resulting user-document matrix. The inverted ACL index has,\\nfor each user, a “postings list” of documents they can access – the user’s ac-\\ncess list. Search results are then intersected with this list. However, such\\nan index is difﬁcult to maintain when access permissions change – we dis-\\ncussed these difﬁculties in the context of incremental indexing for regular\\npostings lists in Section 4.5. It also requires the processing of very long post-\\nings lists for users with access to large document subsets. User membership\\nis therefore often veriﬁed by retrieving access information directly from the\\nﬁle system at query time – even though this slows down retrieval.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n82\\n4\\nIndex construction\\n◮Table 4.3\\nThe ﬁve steps in constructing an index for Reuters-RCV1 in blocked\\nsort-based indexing. Line numbers refer to Figure 4.2.\\nStep\\nTime\\n1\\nreading of collection (line 4)\\n2\\n10 initial sorts of 107 records each (line 5)\\n3\\nwriting of 10 blocks (line 6)\\n4\\ntotal disk transfer time for merging (line 7)\\n5\\ntime of actual merging (line 7)\\ntotal\\n◮Table 4.4\\nCollection statistics for a large collection.\\nSymbol\\nStatistic\\nValue\\nN\\n# documents\\n1,000,000,000\\nLave\\n# tokens per document\\n1000\\nM\\n# distinct terms\\n44,000,000\\nWe discussed indexes for storing and retrieving terms (as opposed to doc-\\numents) in Chapter 3.\\n?\\nExercise 4.5\\nCan spelling correction compromise document-level security? Consider the case where\\na spelling correction is based on documents to which the user does not have access.\\n?\\nExercise 4.6\\nTotal index construction time in blocked sort-based indexing is broken down in Ta-\\nble 4.3. Fill out the time column of the table for Reuters-RCV1 assuming a system\\nwith the parameters given in Table 4.1.\\nExercise 4.7\\nRepeat Exercise 4.6 for the larger collection in Table 4.4. Choose a block size that is\\nrealistic for current technology (remember that a block should easily ﬁt into main\\nmemory). How many blocks do you need?\\nExercise 4.8\\nAssume that we have a collection of modest size whose index can be constructed with\\nthe simple in-memory indexing algorithm in Figure 1.4 (page 8). For this collection,\\ncompare memory, disk and time requirements of the simple algorithm in Figure 1.4\\nand blocked sort-based indexing.\\nExercise 4.9\\nAssume that machines in MapReduce have 100 GB of disk space each. Assume fur-\\nther that the postings list of the term the has a size of 200 GB. Then the MapReduce\\nalgorithm as described cannot be run to construct the index. How would you modify\\nMapReduce so that it can handle this case?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n4.7\\nReferences and further reading\\n83\\nExercise 4.10\\nFor optimal load balancing, the inverters in MapReduce must get segmented postings\\nﬁles of similar sizes. For a new collection, the distribution of key-value pairs may not\\nbe known in advance. How would you solve this problem?\\nExercise 4.11\\nApply MapReduce to the problem of counting how often each term occurs in a set of\\nﬁles. Specify map and reduce operations for this task. Write down an example along\\nthe lines of Figure 4.6.\\nExercise 4.12\\nWe claimed (on page 80) that an auxiliary index can impair the quality of collec-\\ntion statistics. An example is the term weighting method idf, which is deﬁned as\\nlog(N/dfi) where N is the total number of documents and dfi is the number of docu-\\nments that term i occurs in (Section 6.2.1, page 117). Show that even a small auxiliary\\nindex can cause signiﬁcant error in idf when it is computed on the main index only.\\nConsider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm\\nFlossie).\\n4.7\\nReferences and further reading\\nWitten et al. (1999, Chapter 5) present an extensive treatment of the subject of\\nindex construction and additional indexing algorithms with different trade-\\noffs of memory, disk space, and time. In general, blocked sort-based indexing\\ndoes well on all three counts. However, if conserving memory or disk space\\nis the main criterion, then other algorithms may be a better choice. See Wit-\\nten et al. (1999), Tables 5.4 and 5.5; BSBI is closest to “sort-based multiway\\nmerge,” but the two algorithms differ in dictionary structure and use of com-\\npression.\\nMoffat and Bell (1995) show how to construct an index “in situ,” that\\nis, with disk space usage close to what is needed for the ﬁnal index and\\nwith a minimum of additional temporary ﬁles (cf. also Harman and Candela\\n(1990)). They give Lesk (1988) and Somogyi (1990) credit for being among\\nthe ﬁrst to employ sorting for index construction.\\nThe SPIMI method in Section 4.3 is from (Heinz and Zobel 2003). We have\\nsimpliﬁed several aspects of the algorithm, including compression and the\\nfact that each term’s data structure also contains, in addition to the postings\\nlist, its document frequency and house keeping information. We recommend\\nHeinz and Zobel (2003) and Zobel and Moffat (2006) as up-do-date, in-depth\\ntreatments of index construction. Other algorithms with good scaling prop-\\nerties with respect to vocabulary size require several passes through the data,\\ne.g., FAST-INV (Fox and Lee 1991, Harman et al. 1992).\\nThe MapReduce architecture was introduced by Dean and Ghemawat (2004).\\nAn open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/.\\nRibeiro-Neto et al. (1999) and Melnik et al. (2001) describe other approaches\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n84\\n4\\nIndex construction\\nto distributed indexing. Introductory chapters on distributed IR are (Baeza-\\nYates and Ribeiro-Neto 1999, Chapter 9) and (Grossman and Frieder 2004,\\nChapter 8). See also Callan (2000).\\nLester et al. (2005) and Büttcher and Clarke (2005a) analyze the proper-\\nties of logarithmic merging and compare it with other construction methods.\\nOne of the ﬁrst uses of this method was in Lucene (http://lucene.apache.org).\\nOther dynamic indexing methods are discussed by Büttcher et al. (2006) and\\nLester et al. (2006). The latter paper also discusses the strategy of replacing\\nthe old index by one built from scratch.\\nHeinz et al. (2002) compare data structures for accumulating the vocabu-\\nlary in memory. Büttcher and Clarke (2005b) discuss security models for a\\ncommon inverted index for multiple users. A detailed characterization of the\\nReuters-RCV1 collection can be found in (Lewis et al. 2004). NIST distributes\\nthe collection (see http://trec.nist.gov/data/reuters/reuters.html).\\nGarcia-Molina et al. (1999, Chapter 2) review computer hardware relevant\\nto system design in depth.\\nAn effective indexer for enterprise search needs to be able to communicate\\nefﬁciently with a number of applications that hold text data in corporations,\\nincluding Microsoft Outlook, IBM’s Lotus software, databases like Oracle\\nand MySQL, content management systems like Open Text, and enterprise\\nresource planning software like SAP.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n85\\n5\\nIndex compression\\nChapter 1 introduced the dictionary and the inverted index as the central\\ndata structures in information retrieval (IR). In this chapter, we employ a\\nnumber of compression techniques for dictionary and inverted index that\\nare essential for efﬁcient IR systems.\\nOne beneﬁt of compression is immediately clear. We need less disk space.\\nAs we will see, compression ratios of 1:4 are easy to achieve, potentially cut-\\nting the cost of storing the index by 75%.\\nThere are two more subtle beneﬁts of compression. The ﬁrst is increased\\nuse of caching. Search systems use some parts of the dictionary and the index\\nmuch more than others. For example, if we cache the postings list of a fre-\\nquently used query term t, then the computations necessary for responding\\nto the one-term query t can be entirely done in memory. With compression,\\nwe can ﬁt a lot more information into main memory. Instead of having to\\nexpend a disk seek when processing a query with t, we instead access its\\npostings list in memory and decompress it. As we will see below, there are\\nsimple and efﬁcient decompression methods, so that the penalty of having to\\ndecompress the postings list is small. As a result, we are able to decrease the\\nresponse time of the IR system substantially. Because memory is a more ex-\\npensive resource than disk space, increased speed owing to caching – rather\\nthan decreased space requirements – is often the prime motivator for com-\\npression.\\nThe second more subtle advantage of compression is faster transfer of data\\nfrom disk to memory. Efﬁcient decompression algorithms run so fast on\\nmodern hardware that the total time of transferring a compressed chunk of\\ndata from disk and then decompressing it is usually less than transferring\\nthe same chunk of data in uncompressed form. For instance, we can reduce\\ninput/output (I/O) time by loading a much smaller compressed postings\\nlist, even when you add on the cost of decompression. So, in most cases,\\nthe retrieval system runs faster on compressed postings lists than on uncom-\\npressed postings lists.\\nIf the main goal of compression is to conserve disk space, then the speed\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n86\\n5\\nIndex compression\\nof compression algorithms is of no concern. But for improved cache uti-\\nlization and faster disk-to-memory transfer, decompression speeds must be\\nhigh. The compression algorithms we discuss in this chapter are highly efﬁ-\\ncient and can therefore serve all three purposes of index compression.\\nIn this chapter, we deﬁne a posting as a docID in a postings list. For exam-\\nPOSTING\\nple, the postings list (6; 20, 45, 100), where 6 is the termID of the list’s term,\\ncontains three postings. As discussed in Section 2.4.2 (page 41), postings in\\nmost search systems also contain frequency and position information; but we\\nwill only consider simple docID postings here. See Section 5.4 for references\\non compressing frequencies and positions.\\nThis chapter ﬁrst gives a statistical characterization of the distribution of\\nthe entities we want to compress – terms and postings in large collections\\n(Section 5.1). We then look at compression of the dictionary, using the dictionary-\\nas-a-string method and blocked storage (Section 5.2). Section 5.3 describes\\ntwo techniques for compressing the postings ﬁle, variable byte encoding and\\nγ encoding.\\n5.1\\nStatistical properties of terms in information retrieval\\nAs in the last chapter, we use Reuters-RCV1 as our model collection (see Ta-\\nble 4.2, page 70). We give some term and postings statistics for the collection\\nin Table 5.1. “∆%” indicates the reduction in size from the previous line.\\n“T%” is the cumulative reduction from unﬁltered.\\nThe table shows the number of terms for different levels of preprocessing\\n(column 2). The number of terms is the main factor in determining the size\\nof the dictionary. The number of nonpositional postings (column 3) is an\\nindicator of the expected size of the nonpositional index of the collection.\\nThe expected size of a positional index is related to the number of positions\\nit must encode (column 4).\\nIn general, the statistics in Table 5.1 show that preprocessing affects the size\\nof the dictionary and the number of nonpositional postings greatly. Stem-\\nming and case folding reduce the number of (distinct) terms by 17% each\\nand the number of nonpositional postings by 4% and 3%, respectively. The\\ntreatment of the most frequent words is also important. The rule of 30 states\\nRULE OF 30\\nthat the 30 most common words account for 30% of the tokens in written text\\n(31% in the table). Eliminating the 150 most common words from indexing\\n(as stop words; cf. Section 2.2.2, page 27) cuts 25% to 30% of the nonpositional\\npostings. But, although a stop list of 150 words reduces the number of post-\\nings by a quarter or more, this size reduction does not carry over to the size\\nof the compressed index. As we will see later in this chapter, the postings\\nlists of frequent words require only a few bits per posting after compression.\\nThe deltas in the table are in a range typical of large collections. Note,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.1\\nStatistical properties of terms in information retrieval\\n87\\n◮Table 5.1\\nThe effect of preprocessing on the number of terms, nonpositional post-\\nings, and tokens for Reuters-RCV1. “∆%” indicates the reduction in size from the pre-\\nvious line, except that “30 stop words” and “150 stop words” both use “case folding”\\nas their reference line. “T%” is the cumulative (“total”) reduction from unﬁltered. We\\nperformed stemming with the Porter stemmer (Chapter 2, page 33).\\ntokens (= number of positio\\n(distinct) terms\\nnonpositional postings\\nentries in postings)\\nnumber\\n∆%\\nT%\\nnumber\\n∆%\\nT%\\nnumber\\n∆%\\nT%\\nunﬁltered\\n484,494\\n109,971,179\\n197,879,290\\nno numbers\\n473,723\\n−2\\n−2\\n100,680,242\\n−8\\n−8\\n179,158,204\\n−9\\n−9\\ncase folding\\n391,523\\n−17\\n−19\\n96,969,056\\n−3\\n−12\\n179,158,204\\n−0\\n−9\\n30 stop words\\n391,493\\n−0\\n−19\\n83,390,443\\n−14\\n−24\\n121,857,825\\n−31\\n−38\\n150 stop words\\n391,373\\n−0\\n−19\\n67,001,847\\n−30\\n−39\\n94,516,599\\n−47\\n−52\\nstemming\\n322,383\\n−17\\n−33\\n63,812,300\\n−4\\n−42\\n94,516,599\\n−0\\n−52\\nhowever, that the percentage reductions can be very different for some text\\ncollections. For example, for a collection of web pages with a high proportion\\nof French text, a lemmatizer for French reduces vocabulary size much more\\nthan the Porter stemmer does for an English-only collection because French\\nis a morphologically richer language than English.\\nThe compression techniques we describe in the remainder of this chapter\\nare lossless, that is, all information is preserved. Better compression ratios\\nLOSSLESS\\ncan be achieved with lossy compression, which discards some information.\\nLOSSY COMPRESSION\\nCase folding, stemming, and stop word elimination are forms of lossy com-\\npression. Similarly, the vector space model (Chapter 6) and dimensionality\\nreduction techniques like latent semantic indexing (Chapter 18) create com-\\npact representations from which we cannot fully restore the original collec-\\ntion. Lossy compression makes sense when the “lost” information is unlikely\\never to be used by the search system. For example, web search is character-\\nized by a large number of documents, short queries, and users who only look\\nat the ﬁrst few pages of results. As a consequence, we can discard postings of\\ndocuments that would only be used for hits far down the list. Thus, there are\\nretrieval scenarios where lossy methods can be used for compression without\\nany reduction in effectiveness.\\nBefore introducing techniques for compressing the dictionary, we want to\\nestimate the number of distinct terms M in a collection. It is sometimes said\\nthat languages have a vocabulary of a certain size. The second edition of\\nthe Oxford English Dictionary (OED) deﬁnes more than 600,000 words. But\\nthe vocabulary of most large collections is much larger than the OED. The\\nOED does not include most names of people, locations, products, or scientiﬁc\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n88\\n5\\nIndex compression\\n0\\n2\\n4\\n6\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nlog10 T\\nlog10 M\\n◮Figure 5.1\\nHeaps’ law.\\nVocabulary size M as a function of collection size T\\n(number of tokens) for Reuters-RCV1. For these data, the dashed line log10 M =\\n0.49 ∗log10 T + 1.64 is the best least-squares ﬁt. Thus, k = 101.64 ≈44 and b = 0.49.\\nentities like genes. These names need to be included in the inverted index,\\nso our users can search for them.\\n5.1.1\\nHeaps’ law: Estimating the number of terms\\nA better way of getting a handle on M is Heaps’ law, which estimates vocab-\\nHEAPS’ LAW\\nulary size as a function of collection size:\\nM = kTb\\n(5.1)\\nwhere T is the number of tokens in the collection. Typical values for the\\nparameters k and b are: 30 ≤k ≤100 and b ≈0.5. The motivation for\\nHeaps’ law is that the simplest possible relationship between collection size\\nand vocabulary size is linear in log–log space and the assumption of linearity\\nis usually born out in practice as shown in Figure 5.1 for Reuters-RCV1. In\\nthis case, the ﬁt is excellent for T > 105 = 100,000, for the parameter values\\nb = 0.49 and k = 44. For example, for the ﬁrst 1,000,020 tokens Heaps’ law\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.1\\nStatistical properties of terms in information retrieval\\n89\\npredicts 38,323 terms:\\n44 × 1,000,0200.49 ≈38,323.\\nThe actual number is 38,365 terms, very close to the prediction.\\nThe parameter k is quite variable because vocabulary growth depends a\\nlot on the nature of the collection and how it is processed. Case-folding and\\nstemming reduce the growth rate of the vocabulary, whereas including num-\\nbers and spelling errors increase it. Regardless of the values of the param-\\neters for a particular collection, Heaps’ law suggests that (i) the dictionary\\nsize continues to increase with more documents in the collection, rather than\\na maximum vocabulary size being reached, and (ii) the size of the dictionary\\nis quite large for large collections. These two hypotheses have been empir-\\nically shown to be true of large text collections (Section 5.4). So dictionary\\ncompression is important for an effective information retrieval system.\\n5.1.2\\nZipf’s law: Modeling the distribution of terms\\nWe also want to understand how terms are distributed across documents.\\nThis helps us to characterize the properties of the algorithms for compressing\\npostings lists in Section 5.3.\\nA commonly used model of the distribution of terms in a collection is Zipf’s\\nZIPF’S LAW\\nlaw. It states that, if t1 is the most common term in the collection, t2 is the\\nnext most common, and so on, then the collection frequency cfi of the ith\\nmost common term is proportional to 1/i:\\ncfi ∝1\\ni .\\n(5.2)\\nSo if the most frequent term occurs cf1 times, then the second most frequent\\nterm has half as many occurrences, the third most frequent term a third as\\nmany occurrences, and so on. The intuition is that frequency decreases very\\nrapidly with rank. Equation (5.2) is one of the simplest ways of formalizing\\nsuch a rapid decrease and it has been found to be a reasonably good model.\\nEquivalently, we can write Zipf’s law as cfi = cik or as log cfi = log c +\\nk log i where k = −1 and c is a constant to be deﬁned in Section 5.3.2. It\\nis therefore a power law with exponent k = −1. See Chapter 19, page 426,\\nPOWER LAW\\nfor another power law, a law characterizing the distribution of links on web\\npages.\\nThe log–log graph in Figure 5.2 plots the collection frequency of a term as\\na function of its rank for Reuters-RCV1. A line with slope –1, corresponding\\nto the Zipf function log cfi = log c −log i, is also shown. The ﬁt of the data\\nto the law is not particularly good, but good enough to serve as a model for\\nterm distributions in our calculations in Section 5.3.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n90\\n5\\nIndex compression\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nlog10 rank\\n7\\nlog10 cf\\n◮Figure 5.2\\nZipf’s law for Reuters-RCV1.\\nFrequency is plotted as a function of\\nfrequency rank for the terms in the collection. The line is the distribution predicted\\nby Zipf’s law (weighted least-squares ﬁt; intercept is 6.95).\\n?\\nExercise 5.1\\n[⋆]\\nAssuming one machine word per posting, what is the size of the uncompressed (non-\\npositional) index for different tokenizations based on Table 5.1? How do these num-\\nbers compare with Table 5.6?\\n5.2\\nDictionary compression\\nThis section presents a series of dictionary data structures that achieve in-\\ncreasingly higher compression ratios. The dictionary is small compared with\\nthe postings ﬁle as suggested by Table 5.1. So why compress it if it is respon-\\nsible for only a small percentage of the overall space requirements of the IR\\nsystem?\\nOne of the primary factors in determining the response time of an IR sys-\\ntem is the number of disk seeks necessary to process a query. If parts of the\\ndictionary are on disk, then many more disk seeks are necessary in query\\nevaluation. Thus, the main goal of compressing the dictionary is to ﬁt it in\\nmain memory, or at least a large portion of it, to support high query through-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.2\\nDictionary compression\\n91\\nterm\\ndocument\\nfrequency\\npointer\\nto\\npostings list\\na\\n656,265\\n−→\\naachen\\n65\\n−→\\n...\\n...\\n...\\nzulu\\n221\\n−→\\nspace needed:\\n20 bytes\\n4 bytes\\n4 bytes\\n◮Figure 5.3\\nStoring the dictionary as an array of ﬁxed-width entries.\\nput. Although dictionaries of very large collections ﬁt into the memory of a\\nstandard desktop machine, this is not true of many other application scenar-\\nios. For example, an enterprise search server for a large corporation may\\nhave to index a multiterabyte collection with a comparatively large vocab-\\nulary because of the presence of documents in many different languages.\\nWe also want to be able to design search systems for limited hardware such\\nas mobile phones and onboard computers. Other reasons for wanting to\\nconserve memory are fast startup time and having to share resources with\\nother applications. The search system on your PC must get along with the\\nmemory-hogging word processing suite you are using at the same time.\\n5.2.1\\nDictionary as a string\\nThe simplest data structure for the dictionary is to sort the vocabulary lex-\\nicographically and store it in an array of ﬁxed-width entries as shown in\\nFigure 5.3. We allocate 20 bytes for the term itself (because few terms have\\nmore than twenty characters in English), 4 bytes for its document frequency,\\nand 4 bytes for the pointer to its postings list. Four-byte pointers resolve a\\n4 gigabytes (GB) address space. For large collections like the web, we need\\nto allocate more bytes per pointer. We look up terms in the array by binary\\nsearch.\\nFor Reuters-RCV1, we need M × (20 + 4 + 4) = 400,000 × 28 =\\n11.2megabytes (MB) for storing the dictionary in this scheme.\\nUsing ﬁxed-width entries for terms is clearly wasteful. The average length\\nof a term in English is about eight characters (Table 4.2, page 70), so on av-\\nerage we are wasting twelve characters in the ﬁxed-width scheme. Also,\\nwe have no way of storing terms with more than twenty characters like\\nhydrochloroﬂuorocarbons and supercalifragilisticexpialidocious. We can overcome\\nthese shortcomings by storing the dictionary terms as one long string of char-\\nacters, as shown in Figure 5.4. The pointer to the next term is also used to\\ndemarcate the end of the current term. As before, we locate terms in the data\\nstructure by way of binary search in the (now smaller) table. This scheme\\nsaves us 60% compared to ﬁxed-width storage – 12 bytes on average of the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n92\\n5\\nIndex compression\\n. . . s y s t i l e s y z y g e t i c s y z y g i a l s y z y g y s z a i b e l y i t e s z e c i n s z o n o .  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n...\\n4 bytes\\npostings ptr.\\n \\n \\n \\n \\n \\n...\\n4 bytes\\nterm ptr.\\n3 bytes\\n. . .\\n→\\n→\\n→\\n→\\n→\\n◮Figure 5.4\\nDictionary-as-a-string storage. Pointers mark the end of the preceding\\nterm and the beginning of the next. For example, the ﬁrst three terms in this example\\nare systile, syzygetic, and syzygial.\\n20 bytes we allocated for terms before. However, we now also need to store\\nterm pointers. The term pointers resolve 400,000 × 8 = 3.2 × 106 positions,\\nso they need to be log2 3.2 × 106 ≈22 bits or 3 bytes long.\\nIn this new scheme, we need 400,000 × (4 + 4 + 3 + 8) = 7.6 MB for the\\nReuters-RCV1 dictionary: 4 bytes each for frequency and postings pointer, 3\\nbytes for the term pointer, and 8 bytes on average for the term. So we have\\nreduced the space requirements by one third from 11.2 to 7.6 MB.\\n5.2.2\\nBlocked storage\\nWe can further compress the dictionary by grouping terms in the string into\\nblocks of size k and keeping a term pointer only for the ﬁrst term of each\\nblock (Figure 5.5). We store the length of the term in the string as an ad-\\nditional byte at the beginning of the term. We thus eliminate k −1 term\\npointers, but need an additional k bytes for storing the length of each term.\\nFor k = 4, we save (k −1) × 3 = 9 bytes for term pointers, but need an ad-\\nditional k = 4 bytes for term lengths. So the total space requirements for the\\ndictionary of Reuters-RCV1 are reduced by 5 bytes per four-term block, or a\\ntotal of 400,000 × 1/4 × 5 = 0.5 MB, bringing us down to 7.1 MB.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.2\\nDictionary compression\\n93\\n. . . 7 s y s t i l e 9 s y z y g e t i c 8 s y z y g i a l 6 s y z y g y11s z a i b e l y i t e 6 s z e c i n .  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n. . .\\npostings ptr.\\n \\n \\n \\n \\n \\n. . .\\nterm ptr.\\n. . .\\n→\\n→\\n→\\n→\\n→\\n◮Figure 5.5\\nBlocked storage with four terms per block. The ﬁrst block consists of\\nsystile, syzygetic, syzygial, and syzygy with lengths of seven, nine, eight, and six charac-\\nters, respectively. Each term is preceded by a byte encoding its length that indicates\\nhow many bytes to skip to reach subsequent terms.\\nBy increasing the block size k, we get better compression. However, there\\nis a tradeoff between compression and the speed of term lookup. For the\\neight-term dictionary in Figure 5.6, steps in binary search are shown as dou-\\nble lines and steps in list search as simple lines. We search for terms in the un-\\ncompressed dictionary by binary search (a). In the compressed dictionary, we\\nﬁrst locate the term’s block by binary search and then its position within the\\nlist by linear search through the block (b). Searching the uncompressed dic-\\ntionary in (a) takes on average (0 + 1 + 2 + 3 + 2 + 1 + 2 + 2)/8 ≈1.6 steps,\\nassuming each term is equally likely to come up in a query. For example,\\nﬁnding the two terms, aid and box, takes three and two steps, respectively.\\nWith blocks of size k = 4 in (b), we need (0 + 1 + 2 + 3 + 4 + 1 + 2 + 3)/8 = 2\\nsteps on average, ≈25% more. For example, ﬁnding den takes one binary\\nsearch step and two steps through the block. By increasing k, we can get\\nthe size of the compressed dictionary arbitrarily close to the minimum of\\n400,000 × (4 + 4 + 1 + 8) = 6.8 MB, but term lookup becomes prohibitively\\nslow for large values of k.\\nOne source of redundancy in the dictionary we have not exploited yet is\\nthe fact that consecutive entries in an alphabetically sorted list share common\\npreﬁxes. This observation leads to front coding (Figure 5.7). A common preﬁx\\nFRONT CODING\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n94\\n5\\nIndex compression\\n(a)\\naid\\nbox\\nden\\nex\\njob\\nox\\npit\\nwin\\n(b)\\naid\\nbox\\nden\\nex\\njob\\nox\\npit\\nwin\\n◮Figure 5.6\\nSearch of the uncompressed dictionary (a) and a dictionary com-\\npressed by blocking with k = 4 (b).\\nOne block in blocked compression (k = 4) ...\\n8automata8automate9automatic10automation\\n⇓\\n...further compressed with front coding.\\n8automat∗a1⋄e2 ⋄ic3⋄ion\\n◮Figure 5.7\\nFront coding. A sequence of terms with identical preﬁx (“automat”) is\\nencoded by marking the end of the preﬁx with ∗and replacing it with ⋄in subsequent\\nterms. As before, the ﬁrst byte of each entry encodes the number of characters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n95\\n◮Table 5.2\\nDictionary compression for Reuters-RCV1.\\ndata structure\\nsize in MB\\ndictionary, ﬁxed-width\\n11.2\\ndictionary, term pointers into string\\n7.6\\n∼, with blocking, k = 4\\n7.1\\n∼, with blocking & front coding\\n5.9\\nis identiﬁed for a subsequence of the term list and then referred to with a\\nspecial character. In the case of Reuters, front coding saves another 1.2 MB,\\nas we found in an experiment.\\nOther schemes with even greater compression rely on minimal perfect\\nhashing, that is, a hash function that maps M terms onto [1, . . . , M] without\\ncollisions. However, we cannot adapt perfect hashes incrementally because\\neach new term causes a collision and therefore requires the creation of a new\\nperfect hash function. Therefore, they cannot be used in a dynamic environ-\\nment.\\nEven with the best compression scheme, it may not be feasible to store\\nthe entire dictionary in main memory for very large text collections and for\\nhardware with limited memory. If we have to partition the dictionary onto\\npages that are stored on disk, then we can index the ﬁrst term of each page\\nusing a B-tree. For processing most queries, the search system has to go to\\ndisk anyway to fetch the postings. One additional seek for retrieving the\\nterm’s dictionary page from disk is a signiﬁcant, but tolerable increase in the\\ntime it takes to process a query.\\nTable 5.2 summarizes the compression achieved by the four dictionary\\ndata structures.\\n?\\nExercise 5.2\\nEstimate the space usage of the Reuters-RCV1 dictionary with blocks of size k = 8\\nand k = 16 in blocked dictionary storage.\\nExercise 5.3\\nEstimate the time needed for term lookup in the compressed dictionary of Reuters-\\nRCV1 with block sizes of k = 4 (Figure 5.6, b), k = 8, and k = 16. What is the\\nslowdown compared with k = 1 (Figure 5.6, a)?\\n5.3\\nPostings ﬁle compression\\nRecall from Table 4.2 (page 70) that Reuters-RCV1 has 800,000 documents,\\n200 tokens per document, six characters per token, and 100,000,000 post-\\nings where we deﬁne a posting in this chapter as a docID in a postings\\nlist, that is, excluding frequency and position information. These numbers\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n96\\n5\\nIndex compression\\n◮Table 5.3\\nEncoding gaps instead of document IDs. For example, we store gaps\\n107, 5, 43, . . . , instead of docIDs 283154, 283159, 283202, . . . for computer. The ﬁrst\\ndocID is left unchanged (only shown for arachnocentric).\\nencoding\\npostings list\\nthe\\ndocIDs\\n...\\n283042\\n283043\\n283044\\n283045\\ngaps\\n1\\n1\\n1\\ncomputer\\ndocIDs\\n...\\n283047\\n283154\\n283159\\n283202\\ngaps\\n107\\n5\\n43\\narachnocentric\\ndocIDs\\n252000\\n500100\\ngaps\\n252000\\n248100\\ncorrespond to line 3 (“case folding”) in Table 5.1. Document identiﬁers are\\nlog2 800,000 ≈20 bits long. Thus, the size of the collection is about 800,000 ×\\n200 × 6 bytes = 960 MB and the size of the uncompressed postings ﬁle is\\n100,000,000 × 20/8 = 250 MB.\\nTo devise a more efﬁcient representation of the postings ﬁle, one that uses\\nfewer than 20 bits per document, we observe that the postings for frequent\\nterms are close together. Imagine going through the documents of a collec-\\ntion one by one and looking for a frequent term like computer. We will ﬁnd\\na document containing computer, then we skip a few documents that do not\\ncontain it, then there is again a document with the term and so on (see Ta-\\nble 5.3). The key idea is that the gaps between postings are short, requiring a\\nlot less space than 20 bits to store. In fact, gaps for the most frequent terms\\nsuch as the and for are mostly equal to 1. But the gaps for a rare term that\\noccurs only once or twice in a collection (e.g., arachnocentric in Table 5.3) have\\nthe same order of magnitude as the docIDs and need 20 bits. For an econom-\\nical representation of this distribution of gaps, we need a variable encoding\\nmethod that uses fewer bits for short gaps.\\nTo encode small numbers in less space than large numbers, we look at two\\ntypes of methods: bytewise compression and bitwise compression. As the\\nnames suggest, these methods attempt to encode gaps with the minimum\\nnumber of bytes and bits, respectively.\\n5.3.1\\nVariable byte codes\\nVariable byte (VB) encoding uses an integral number of bytes to encode a gap.\\nVARIABLE BYTE\\nENCODING\\nThe last 7 bits of a byte are “payload” and encode part of the gap. The ﬁrst\\nbit of the byte is a continuation bit.It is set to 1 for the last byte of the encoded\\nCONTINUATION BIT\\ngap and to 0 otherwise. To decode a variable byte code, we read a sequence\\nof bytes with continuation bit 0 terminated by a byte with continuation bit 1.\\nWe then extract and concatenate the 7-bit parts. Figure 5.8 gives pseudocode\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n97\\nVBENCODENUMBER(n)\\n1\\nbytes ←⟨⟩\\n2\\nwhile true\\n3\\ndo PREPEND(bytes, n mod 128)\\n4\\nif n < 128\\n5\\nthen BREAK\\n6\\nn ←n div 128\\n7\\nbytes[LENGTH(bytes)] += 128\\n8\\nreturn bytes\\nVBENCODE(numbers)\\n1\\nbytestream ←⟨⟩\\n2\\nfor each n ∈numbers\\n3\\ndo bytes ←VBENCODENUMBER(n)\\n4\\nbytestream ←EXTEND(bytestream, bytes)\\n5\\nreturn bytestream\\nVBDECODE(bytestream)\\n1\\nnumbers ←⟨⟩\\n2\\nn ←0\\n3\\nfor i ←1 to LENGTH(bytestream)\\n4\\ndo if bytestream[i] < 128\\n5\\nthen n ←128 × n + bytestream[i]\\n6\\nelse n ←128 × n + (bytestream[i] −128)\\n7\\nAPPEND(numbers, n)\\n8\\nn ←0\\n9\\nreturn numbers\\n◮Figure 5.8\\nVB encoding and decoding.\\nThe functions div and mod compute\\ninteger division and remainder after integer division, respectively. PREPEND adds an\\nelement to the beginning of a list, for example, PREPEND(⟨1,2⟩, 3) = ⟨3, 1, 2⟩. EXTEND\\nextends a list, for example, EXTEND(⟨1,2⟩, ⟨3, 4⟩) = ⟨1, 2, 3, 4⟩.\\n◮Table 5.4\\nVB encoding.\\nGaps are encoded using an integral number of bytes.\\nThe ﬁrst bit, the continuation bit, of each byte indicates whether the code ends with\\nthis byte (1) or not (0).\\ndocIDs\\n824\\n829\\n215406\\ngaps\\n5\\n214577\\nVB code\\n00000110 10111000\\n10000101\\n00001101 00001100 10110001\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n98\\n5\\nIndex compression\\n◮Table 5.5\\nSome examples of unary and γ codes. Unary codes are only shown for\\nthe smaller numbers. Commas in γ codes are for readability only and are not part of\\nthe actual codes.\\nnumber\\nunary code\\nlength\\noffset\\nγ code\\n0\\n0\\n1\\n10\\n0\\n0\\n2\\n110\\n10\\n0\\n10,0\\n3\\n1110\\n10\\n1\\n10,1\\n4\\n11110\\n110\\n00\\n110,00\\n9\\n1111111110\\n1110\\n001\\n1110,001\\n13\\n1110\\n101\\n1110,101\\n24\\n11110\\n1000\\n11110,1000\\n511\\n111111110\\n11111111\\n111111110,11111111\\n1025\\n11111111110\\n0000000001\\n11111111110,0000000001\\nfor VB encoding and decoding and Table 5.4 an example of a VB-encoded\\npostings list. 1\\nWith VB compression, the size of the compressed index for Reuters-RCV1\\nis 116 MB as we veriﬁed in an experiment. This is a more than 50% reduction\\nof the size of the uncompressed index (see Table 5.6).\\nThe idea of VB encoding can also be applied to larger or smaller units than\\nbytes: 32-bit words, 16-bit words, and 4-bit words or nibbles. Larger words\\nNIBBLE\\nfurther decrease the amount of bit manipulation necessary at the cost of less\\neffective (or no) compression. Word sizes smaller than bytes get even better\\ncompression ratios at the cost of more bit manipulation. In general, bytes\\noffer a good compromise between compression ratio and speed of decom-\\npression.\\nFor most IR systems variable byte codes offer an excellent tradeoff between\\ntime and space. They are also simple to implement – most of the alternatives\\nreferred to in Section 5.4 are more complex. But if disk space is a scarce\\nresource, we can achieve better compression ratios by using bit-level encod-\\nings, in particular two closely related encodings: γ codes, which we will turn\\nto next, and δ codes (Exercise 5.9).\\n$\\n5.3.2\\nγ codes\\nVB codes use an adaptive number of bytes depending on the size of the gap.\\nBit-level codes adapt the length of the code on the ﬁner grained bit level. The\\n1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of\\n0, in practice the origin is usually 1, so that 10000000 encodes 1, 10000101 encodes 6 (not 5 as in\\nthe table), and so on.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n99\\nsimplest bit-level code is unary code. The unary code of n is a string of n 1s\\nUNARY CODE\\nfollowed by a 0 (see the ﬁrst two columns of Table 5.5). Obviously, this is not\\na very efﬁcient code, but it will come in handy in a moment.\\nHow efﬁcient can a code be in principle? Assuming the 2n gaps G with\\n1 ≤G ≤2n are all equally likely, the optimal encoding uses n bits for each\\nG. So some gaps (G = 2n in this case) cannot be encoded with fewer than\\nlog2 G bits. Our goal is to get as close to this lower bound as possible.\\nA method that is within a factor of optimal is γ encoding. γ codes im-\\nγ ENCODING\\nplement variable-length encoding by splitting the representation of a gap G\\ninto a pair of length and offset. Offset is G in binary, but with the leading 1\\nremoved.2 For example, for 13 (binary 1101) offset is 101. Length encodes the\\nlength of offset in unary code. For 13, the length of offset is 3 bits, which is 1110\\nin unary. The γ code of 13 is therefore 1110101, the concatenation of length\\n1110 and offset 101. The right hand column of Table 5.5 gives additional\\nexamples of γ codes.\\nA γ code is decoded by ﬁrst reading the unary code up to the 0 that ter-\\nminates it, for example, the four bits 1110 when decoding 1110101. Now we\\nknow how long the offset is: 3 bits. The offset 101 can then be read correctly\\nand the 1 that was chopped off in encoding is prepended: 101 →1101 = 13.\\nThe length of offset is ⌊log2 G⌋bits and the length of length is ⌊log2 G⌋+ 1\\nbits, so the length of the entire code is 2 × ⌊log2 G⌋+ 1 bits. γ codes are\\nalways of odd length and they are within a factor of 2 of what we claimed\\nto be the optimal encoding length log2 G. We derived this optimum from\\nthe assumption that the 2n gaps between 1 and 2n are equiprobable. But this\\nneed not be the case. In general, we do not know the probability distribution\\nover gaps a priori.\\nThe characteristic of a discrete probability distribution3 P that determines\\nits coding properties (including whether a code is optimal) is its entropy H(P),\\nENTROPY\\nwhich is deﬁned as follows:\\nH(P) = −∑\\nx∈X\\nP(x) log2 P(x)\\nwhere X is the set of all possible numbers we need to be able to encode\\n(and therefore ∑x∈X P(x) = 1.0). Entropy is a measure of uncertainty as\\nshown in Figure 5.9 for a probability distribution P over two possible out-\\ncomes, namely, X = {x1, x2}. Entropy is maximized (H(P) = 1) for P(x1) =\\nP(x2) = 0.5 when uncertainty about which xi will appear next is largest; and\\n2. We assume here that G has no leading 0s. If there are any, they are removed before deleting\\nthe leading 1.\\n3. Readers who want to review basic concepts of probability theory may want to consult Rice\\n(2006) or Ross (2006). Note that we are interested in probability distributions over integers (gaps,\\nfrequencies, etc.), but that the coding properties of a probability distribution are independent of\\nwhether the outcomes are integers or something else.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n100\\n5\\nIndex compression\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nP(x1)\\nH(P)\\n◮Figure 5.9\\nEntropy H(P) as a function of P(x1) for a sample space with two\\noutcomes x1 and x2.\\nminimized (H(P) = 0) for P(x1) = 1, P(x2) = 0 and for P(x1) = 0, P(x2) = 1\\nwhen there is absolute certainty.\\nIt can be shown that the lower bound for the expected length E(L) of a\\ncode L is H(P) if certain conditions hold (see the references). It can further\\nbe shown that for 1 < H(P) < ∞, γ encoding is within a factor of 3 of this\\noptimal encoding, approaching 2 for large H(P):\\nE(Lγ)\\nH(P) ≤2 +\\n1\\nH(P) ≤3.\\nWhat is remarkable about this result is that it holds for any probability distri-\\nbution P. So without knowing anything about the properties of the distribu-\\ntion of gaps, we can apply γ codes and be certain that they are within a factor\\nof ≈2 of the optimal code for distributions of large entropy. A code like γ\\ncode with the property of being within a factor of optimal for an arbitrary\\ndistribution P is called universal.\\nUNIVERSAL CODE\\nIn addition to universality, γ codes have two other properties that are use-\\nful for index compression. First, they are preﬁx free, namely, no γ code is the\\nPREFIX FREE\\npreﬁx of another. This means that there is always a unique decoding of a\\nsequence of γ codes – and we do not need delimiters between them, which\\nwould decrease the efﬁciency of the code. The second property is that γ\\ncodes are parameter free. For many other efﬁcient codes, we have to ﬁt the\\nPARAMETER FREE\\nparameters of a model (e.g., the binomial distribution) to the distribution\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n101\\nof gaps in the index. This complicates the implementation of compression\\nand decompression. For instance, the parameters need to be stored and re-\\ntrieved. And in dynamic indexing, the distribution of gaps can change, so\\nthat the original parameters are no longer appropriate. These problems are\\navoided with a parameter-free code.\\nHow much compression of the inverted index do γ codes achieve? To\\nanswer this question we use Zipf’s law, the term distribution model intro-\\nduced in Section 5.1.2. According to Zipf’s law, the collection frequency cfi\\nis proportional to the inverse of the rank i, that is, there is a constant c′ such\\nthat:\\ncfi = c′\\ni .\\n(5.3)\\nWe can choose a different constant c such that the fractions c/i are relative\\nfrequencies and sum to 1 (that is, c/i = cfi/T):\\n1 =\\nM\\n∑\\ni=1\\nc\\ni = c\\nM\\n∑\\ni=1\\n1\\ni\\n=\\nc HM\\n(5.4)\\nc =\\n1\\nHM\\n(5.5)\\nwhere M is the number of distinct terms and HM is the Mth harmonic num-\\nber. 4 Reuters-RCV1 has M = 400,000 distinct terms and HM ≈ln M, so we\\nhave\\nc =\\n1\\nHM\\n≈\\n1\\nln M =\\n1\\nln 400,000 ≈1\\n13.\\nThus the ith term has a relative frequency of roughly 1/(13i), and the ex-\\npected average number of occurrences of term i in a document of length L\\nis:\\nLc\\ni ≈200 × 1\\n13\\ni\\n≈15\\ni\\nwhere we interpret the relative frequency as a term occurrence probability.\\nRecall that 200 is the average number of tokens per document in Reuters-\\nRCV1 (Table 4.2).\\nNow we have derived term statistics that characterize the distribution of\\nterms in the collection and, by extension, the distribution of gaps in the post-\\nings lists. From these statistics, we can calculate the space requirements for\\nan inverted index compressed with γ encoding. We ﬁrst stratify the vocab-\\nulary into blocks of size Lc = 15. On average, term i occurs 15/i times per\\n4. Note that, unfortunately, the conventional symbol for both entropy and harmonic number is\\nH. Context should make clear which is meant in this chapter.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n102\\n5\\nIndex compression\\nN documents\\nLc most\\nfrequent\\nN gaps of 1 each\\nterms\\nLc next most\\nfrequent\\nN/2 gaps of 2 each\\nterms\\nLc next most\\nfrequent\\nN/3 gaps of 3 each\\nterms\\n...\\n...\\n◮Figure 5.10\\nStratiﬁcation of terms for estimating the size of a γ encoded inverted\\nindex.\\ndocument. So the average number of occurrences f per document is 1 ≤f for\\nterms in the ﬁrst block, corresponding to a total number of N gaps per term.\\nThe average is 1\\n2 ≤f < 1 for terms in the second block, corresponding to\\nN/2 gaps per term, and 1\\n3 ≤f < 1\\n2 for terms in the third block, correspond-\\ning to N/3 gaps per term, and so on. (We take the lower bound because it\\nsimpliﬁes subsequent calculations. As we will see, the ﬁnal estimate is too\\npessimistic, even with this assumption.) We will make the somewhat unre-\\nalistic assumption that all gaps for a given term have the same size as shown\\nin Figure 5.10. Assuming such a uniform distribution of gaps, we then have\\ngaps of size 1 in block 1, gaps of size 2 in block 2, and so on.\\nEncoding the N/j gaps of size j with γ codes, the number of bits needed\\nfor the postings list of a term in the jth block (corresponding to one row in\\nthe ﬁgure) is:\\nbits-per-row\\n=\\nN\\nj × (2 × ⌊log2 j⌋+ 1)\\n≈\\n2N log2 j\\nj\\n.\\nTo encode the entire block, we need (Lc) · (2N log2 j)/j bits. There are M/(Lc)\\nblocks, so the postings ﬁle as a whole will take up:\\nM\\nLc\\n∑\\nj=1\\n2NLc log2 j\\nj\\n.\\n(5.6)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n103\\n◮Table 5.6\\nIndex and dictionary compression for Reuters-RCV1. The compression\\nratio depends on the proportion of actual text in the collection. Reuters-RCV1 con-\\ntains a large amount of XML markup. Using the two best compression schemes, γ\\nencoding and blocking with front coding, the ratio compressed index to collection\\nsize is therefore especially small for Reuters-RCV1: (101 + 5.9)/3600 ≈0.03.\\ndata structure\\nsize in MB\\ndictionary, ﬁxed-width\\n11.2\\ndictionary, term pointers into string\\n7.6\\n∼, with blocking, k = 4\\n7.1\\n∼, with blocking & front coding\\n5.9\\ncollection (text, xml markup etc)\\n3600.0\\ncollection (text)\\n960.0\\nterm incidence matrix\\n40,000.0\\npostings, uncompressed (32-bit words)\\n400.0\\npostings, uncompressed (20 bits)\\n250.0\\npostings, variable byte encoded\\n116.0\\npostings, γ encoded\\n101.0\\nFor Reuters-RCV1, M\\nLc ≈400,000/15 ≈27,000 and\\n27,000\\n∑\\nj=1\\n2 × 106 × 15 log2 j\\nj\\n≈224 MB.\\n(5.7)\\nSo the postings ﬁle of the compressed inverted index for our 960 MB collec-\\ntion has a size of 224 MB, one fourth the size of the original collection.\\nWhen we run γ compression on Reuters-RCV1, the actual size of the com-\\npressed index is even lower: 101 MB, a bit more than one tenth of the size of\\nthe collection. The reason for the discrepancy between predicted and actual\\nvalue is that (i) Zipf’s law is not a very good approximation of the actual dis-\\ntribution of term frequencies for Reuters-RCV1 and (ii) gaps are not uniform.\\nThe Zipf model predicts an index size of 251 MB for the unrounded numbers\\nfrom Table 4.2. If term frequencies are generated from the Zipf model and\\na compressed index is created for these artiﬁcial terms, then the compressed\\nsize is 254 MB. So to the extent that the assumptions about the distribution\\nof term frequencies are accurate, the predictions of the model are correct.\\nTable 5.6 summarizes the compression techniques covered in this chapter.\\nThe term incidence matrix (Figure 1.1, page 4) for Reuters-RCV1 has size\\n400,000 × 800,000 = 40 × 8 × 109 bits or 40 GB.\\nγ codes achieve great compression ratios – about 15% better than vari-\\nable byte codes for Reuters-RCV1. But they are expensive to decode. This is\\nbecause many bit-level operations – shifts and masks – are necessary to de-\\ncode a sequence of γ codes as the boundaries between codes will usually be\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n104\\n5\\nIndex compression\\nsomewhere in the middle of a machine word. As a result, query processing is\\nmore expensive for γ codes than for variable byte codes. Whether we choose\\nvariable byte or γ encoding depends on the characteristics of an application,\\nfor example, on the relative weights we give to conserving disk space versus\\nmaximizing query response time.\\nThe compression ratio for the index in Table 5.6 is about 25%: 400 MB (un-\\ncompressed, each posting stored as a 32-bit word) versus 101 MB (γ) and 116\\nMB (VB). This shows that both γ and VB codes meet the objectives we stated\\nin the beginning of the chapter. Index compression substantially improves\\ntime and space efﬁciency of indexes by reducing the amount of disk space\\nneeded, increasing the amount of information that can be kept in the cache,\\nand speeding up data transfers from disk to memory.\\n?\\nExercise 5.4\\n[⋆]\\nCompute variable byte codes for the numbers in Tables 5.3 and 5.5.\\nExercise 5.5\\n[⋆]\\nCompute variable byte and γ codes for the postings list ⟨777, 17743, 294068, 31251336⟩.\\nUse gaps instead of docIDs where possible. Write binary codes in 8-bit blocks.\\nExercise 5.6\\nConsider the postings list ⟨4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400⟩with a correspond-\\ning list of gaps ⟨4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130⟩. Assume that the length of the postings\\nlist is stored separately, so the system knows when a postings list is complete. Us-\\ning variable byte encoding: (i) What is the largest gap you can encode in 1 byte? (ii)\\nWhat is the largest gap you can encode in 2 bytes? (iii) How many bytes will the\\nabove postings list require under this encoding? (Count only space for encoding the\\nsequence of numbers.)\\nExercise 5.7\\nA little trick is to notice that a gap cannot be of length 0 and that the stuff left to encode\\nafter shifting cannot be 0. Based on these observations: (i) Suggest a modiﬁcation to\\nvariable byte encoding that allows you to encode slightly larger gaps in the same\\namount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What\\nis the largest gap you can encode in 2 bytes? (iv) How many bytes will the postings\\nlist in Exercise 5.6 require under this encoding? (Count only space for encoding the\\nsequence of numbers.)\\nExercise 5.8\\n[⋆]\\nFrom the following sequence of γ-coded gaps, reconstruct ﬁrst the gap sequence and\\nthen the postings sequence: 1110001110101011111101101111011.\\nExercise 5.9\\nγ codes are relatively inefﬁcient for large numbers (e.g., 1025 in Table 5.5) as they\\nencode the length of the offset in inefﬁcient unary code. δ codes differ from γ codes\\nδ CODES\\nin that they encode the ﬁrst part of the code (length) in γ code instead of unary code.\\nThe encoding of offset is the same. For example, the δ code of 7 is 10,0,11 (again, we\\nadd commas for readability). 10,0 is the γ code for length (2 in this case) and the\\nencoding of offset (11) is unchanged. (i) Compute the δ codes for the other numbers\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.4\\nReferences and further reading\\n105\\n◮Table 5.7\\nTwo gap sequences to be merged in blocked sort-based indexing\\nγ encoded gap sequence of run 1\\n1110110111111001011111111110100011111001\\nγ encoded gap sequence of run 2\\n11111010000111111000100011111110010000011111010101\\nin Table 5.5. For what range of numbers is the δ code shorter than the γ code? (ii) γ\\ncode beats variable byte code in Table 5.6 because the index contains stop words and\\nthus many small gaps. Show that variable byte code is more compact if larger gaps\\ndominate. (iii) Compare the compression ratios of δ code and variable byte code for\\na distribution of gaps dominated by large gaps.\\nExercise 5.10\\nGo through the above calculation of index size and explicitly state all the approxima-\\ntions that were made to arrive at Equation (5.6).\\nExercise 5.11\\nFor a collection of your choosing, determine the number of documents and terms and\\nthe average length of a document. (i) How large is the inverted index predicted to be\\nby Equation (5.6)? (ii) Implement an indexer that creates a γ-compressed inverted\\nindex for the collection. How large is the actual index? (iii) Implement an indexer\\nthat uses variable byte encoding. How large is the variable byte encoded index?\\nExercise 5.12\\nTo be able to hold as many postings as possible in main memory, it is a good idea to\\ncompress intermediate index ﬁles during index construction. (i) This makes merging\\nruns in blocked sort-based indexing more complicated. As an example, work out the\\nγ-encoded merged sequence of the gaps in Table 5.7. (ii) Index construction is more\\nspace efﬁcient when using compression. Would you also expect it to be faster?\\nExercise 5.13\\n(i) Show that the size of the vocabulary is ﬁnite according to Zipf’s law and inﬁnite\\naccording to Heaps’ law. (ii) Can we derive Heaps’ law from Zipf’s law?\\n5.4\\nReferences and further reading\\nHeaps’ law was discovered by Heaps (1978). See also Baeza-Yates and Ribeiro-\\nNeto (1999). A detailed study of vocabulary growth in large collections is\\n(Williams and Zobel 2005). Zipf’s law is due to Zipf (1949). Witten and Bell\\n(1990) investigate the quality of the ﬁt obtained by the law. Other term distri-\\nbution models, including K mixture and two-poisson model, are discussed\\nby Manning and Schütze (1999, Chapter 15). Carmel et al. (2001), Büttcher\\nand Clarke (2006), Blanco and Barreiro (2007), and Ntoulas and Cho (2007)\\nshow that lossy compression can achieve good compression with no or no\\nsigniﬁcant decrease in retrieval effectiveness.\\nDictionary compression is covered in detail by Witten et al. (1999, Chap-\\nter 4), which is recommended as additional reading.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n106\\n5\\nIndex compression\\nSubsection 5.3.1 is based on (Scholer et al. 2002). The authors ﬁnd that\\nvariable byte codes process queries two times faster than either bit-level\\ncompressed indexes or uncompressed indexes with a 30% penalty in com-\\npression ratio compared with the best bit-level compression method. They\\nalso show that compressed indexes can be superior to uncompressed indexes\\nnot only in disk usage, but also in query processing speed. Compared with\\nVB codes, “variable nibble” codes showed 5% to 10% better compression\\nand up to one third worse effectiveness in one experiment (Anh and Moffat\\n2005). Trotman (2003) also recommends using VB codes unless disk space is\\nat a premium. In recent work, Anh and Moffat (2005; 2006a) and Zukowski\\net al. (2006) have constructed word-aligned binary codes that are both faster\\nin decompression and at least as efﬁcient as VB codes. Zhang et al. (2007) in-\\nvestigate the increased effectiveness of caching when a number of different\\ncompression techniques for postings lists are used on modern hardware.\\nδ codes (Exercise 5.9) and γ codes were introduced by Elias (1975), who\\nproved that both codes are universal. In addition, δ codes are asymptotically\\noptimal for H(P) →∞. δ codes perform better than γ codes if large num-\\nbers (greater than 15) dominate. A good introduction to information theory,\\nincluding the concept of entropy, is (Cover and Thomas 1991). While Elias\\ncodes are only asymptotically optimal, arithmetic codes (Witten et al. 1999,\\nSection 2.4) can be constructed to be arbitrarily close to the optimum H(P)\\nfor any P.\\nSeveral additional index compression techniques are covered by Witten et\\nal. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommend using param-\\nPARAMETERIZED CODE\\neterized codes for index compression, codes that explicitly model the probabil-\\nity distribution of gaps for each term. For example, they show that Golomb\\nGOLOMB CODES\\ncodes achieve better compression ratios than γ codes for large collections.\\nMoffat and Zobel (1992) compare several parameterized methods, including\\nLLRUN (Fraenkel and Klein 1985).\\nThe distribution of gaps in a postings list depends on the assignment of\\ndocIDs to documents. A number of researchers have looked into assign-\\ning docIDs in a way that is conducive to the efﬁcient compression of gap\\nsequences (Moffat and Stuiver 1996; Blandford and Blelloch 2002; Silvestri\\net al. 2004; Blanco and Barreiro 2006; Silvestri 2007). These techniques assign\\ndocIDs in a small range to documents in a cluster where a cluster can consist\\nof all documents in a given time period, on a particular web site, or sharing\\nanother property. As a result, when a sequence of documents from a clus-\\nter occurs in a postings list, their gaps are small and can be more effectively\\ncompressed.\\nDifferent considerations apply to the compression of term frequencies and\\nword positions than to the compression of docIDs in postings lists. See Scho-\\nler et al. (2002) and Zobel and Moffat (2006). Zobel and Moffat (2006) is\\nrecommended in general as an in-depth and up-to-date tutorial on inverted\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n5.4\\nReferences and further reading\\n107\\nindexes, including index compression.\\nThis chapter only looks at index compression for Boolean retrieval. For\\nranked retrieval (Chapter 6), it is advantageous to order postings according\\nto term frequency instead of docID. During query processing, the scanning\\nof many postings lists can then be terminated early because smaller weights\\ndo not change the ranking of the highest ranked k documents found so far. It\\nis not a good idea to precompute and store weights in the index (as opposed\\nto frequencies) because they cannot be compressed as well as integers (see\\nSection 7.1.5, page 140).\\nDocument compression can also be important in an efﬁcient information re-\\ntrieval system. de Moura et al. (2000) and Brisaboa et al. (2007) describe\\ncompression schemes that allow direct searching of terms and phrases in the\\ncompressed text, which is infeasible with standard text compression utilities\\nlike gzip and compress.\\n?\\nExercise 5.14\\n[⋆]\\nWe have deﬁned unary codes as being “10”: sequences of 1s terminated by a 0. In-\\nterchanging the roles of 0s and 1s yields an equivalent “01” unary code. When this\\n01 unary code is used, the construction of a γ code can be stated as follows: (1) Write\\nG down in binary using b = ⌊log2 j⌋+ 1 bits. (2) Prepend (b −1) 0s. (i) Encode the\\nnumbers in Table 5.5 in this alternative γ code. (ii) Show that this method produces\\na well-deﬁned alternative γ code in the sense that it has the same length and can be\\nuniquely decoded.\\nExercise 5.15\\n[⋆⋆⋆]\\nUnary code is not a universal code in the sense deﬁned above. However, there exists\\na distribution over gaps for which unary code is optimal. Which distribution is this?\\nExercise 5.16\\nGive some examples of terms that violate the assumption that gaps all have the same\\nsize (which we made when estimating the space requirements of a γ-encoded index).\\nWhat are general characteristics of these terms?\\nExercise 5.17\\nConsider a term whose postings list has size n, say, n = 10,000. Compare the size of\\nthe γ-compressed gap-encoded postings list if the distribution of the term is uniform\\n(i.e., all gaps have the same size) versus its size when the distribution is not uniform.\\nWhich compressed postings list is smaller?\\nExercise 5.18\\nWork out the sum in Equation (5.7) and show it adds up to about 251 MB. Use the\\nnumbers in Table 4.2, but do not round Lc, c, and the number of vocabulary blocks.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n109\\n6\\nScoring, term weighting and the\\nvector space model\\nThus far we have dealt with indexes that support Boolean queries: a docu-\\nment either matches or does not match a query. In the case of large document\\ncollections, the resulting number of matching documents can far exceed the\\nnumber a human user could possibly sift through. Accordingly, it is essen-\\ntial for a search engine to rank-order the documents matching a query. To do\\nthis, the search engine computes, for each matching document, a score with\\nrespect to the query at hand. In this chapter we initiate the study of assigning\\na score to a (query, document) pair. This chapter consists of three main ideas.\\n1. We introduce parametric and zone indexes in Section 6.1, which serve\\ntwo purposes. First, they allow us to index and retrieve documents by\\nmetadata such as the language in which a document is written. Second,\\nthey give us a simple means for scoring (and thereby ranking) documents\\nin response to a query.\\n2. Next, in Section 6.2 we develop the idea of weighting the importance of a\\nterm in a document, based on the statistics of occurrence of the term.\\n3. In Section 6.3 we show that by viewing each document as a vector of such\\nweights, we can compute a score between a query and each document.\\nThis view is known as vector space scoring.\\nSection 6.4 develops several variants of term-weighting for the vector space\\nmodel. Chapter 7 develops computational aspects of vector space scoring,\\nand related topics.\\nAs we develop these ideas, the notion of a query will assume multiple\\nnuances. In Section 6.1 we consider queries in which speciﬁc query terms\\noccur in speciﬁed regions of a matching document. Beginning Section 6.2 we\\nwill in fact relax the requirement of matching speciﬁc regions of a document;\\ninstead, we will look at so-called free text queries that simply consist of query\\nterms with no speciﬁcation on their relative order, importance or where in a\\ndocument they should be found. The bulk of our study of scoring will be in\\nthis latter notion of a query being such a set of terms.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n110\\n6\\nScoring, term weighting and the vector space model\\n6.1\\nParametric and zone indexes\\nWe have thus far viewed a document as a sequence of terms. In fact, most\\ndocuments have additional structure. Digital documents generally encode,\\nin machine-recognizable form, certain metadata associated with each docu-\\nMETADATA\\nment. By metadata, we mean speciﬁc forms of data about a document, such\\nas its author(s), title and date of publication. This metadata would generally\\ninclude ﬁelds such as the date of creation and the format of the document, as\\nFIELD\\nwell the author and possibly the title of the document. The possible values\\nof a ﬁeld should be thought of as ﬁnite – for instance, the set of all dates of\\nauthorship.\\nConsider queries of the form “ﬁnd documents authored by William Shake-\\nspeare in 1601, containing the phrase alas poor Yorick”. Query processing then\\nconsists as usual of postings intersections, except that we may merge post-\\nings from standard inverted as well as parametric indexes. There is one para-\\nPARAMETRIC INDEX\\nmetric index for each ﬁeld (say, date of creation); it allows us to select only\\nthe documents matching a date speciﬁed in the query. Figure 6.1 illustrates\\nthe user’s view of such a parametric search. Some of the ﬁelds may assume\\nordered values, such as dates; in the example query above, the year 1601 is\\none such ﬁeld value. The search engine may support querying ranges on\\nsuch ordered values; to this end, a structure like a B-tree may be used for the\\nﬁeld’s dictionary.\\nZones are similar to ﬁelds, except the contents of a zone can be arbitrary\\nZONE\\nfree text. Whereas a ﬁeld may take on a relatively small set of values, a zone\\ncan be thought of as an arbitrary, unbounded amount of text. For instance,\\ndocument titles and abstracts are generally treated as zones. We may build a\\nseparate inverted index for each zone of a document, to support queries such\\nas “ﬁnd documents with merchant in the title and william in the author list and\\nthe phrase gentle rain in the body”. This has the effect of building an index\\nthat looks like Figure 6.2. Whereas the dictionary for a parametric index\\ncomes from a ﬁxed vocabulary (the set of languages, or the set of dates), the\\ndictionary for a zone index must structure whatever vocabulary stems from\\nthe text of that zone.\\nIn fact, we can reduce the size of the dictionary by encoding the zone in\\nwhich a term occurs in the postings. In Figure 6.3 for instance, we show how\\noccurrences of william in the title and author zones of various documents are\\nencoded. Such an encoding is useful when the size of the dictionary is a\\nconcern (because we require the dictionary to ﬁt in main memory). But there\\nis another important reason why the encoding of Figure 6.3 is useful: the\\nefﬁcient computation of scores using a technique we will call weighted zone\\nWEIGHTED ZONE\\nSCORING\\nscoring.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n111\\n◮Figure 6.1\\nParametric search. In this example we have a collection with ﬁelds al-\\nlowing us to select publications by zones such as Author and ﬁelds such as Language.\\nwilliam.author\\n2\\n3\\n5\\n8\\nwilliam.title\\n2\\n4\\n8\\n16\\nwilliam.abstract\\n11\\n121\\n1441\\n1729\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n◮Figure 6.2\\nBasic zone index ; zones are encoded as extensions of dictionary en-\\ntries.\\nwilliam\\n2.author,2.title\\n3.author\\n4.title\\n5.author\\n-\\n-\\n-\\n-\\n◮Figure 6.3\\nZone index in which the zone is encoded in the postings rather than\\nthe dictionary.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n112\\n6\\nScoring, term weighting and the vector space model\\n6.1.1\\nWeighted zone scoring\\nThus far in Section 6.1 we have focused on retrieving documents based on\\nBoolean queries on ﬁelds and zones. We now turn to a second application of\\nzones and ﬁelds.\\nGiven a Boolean query q and a document d, weighted zone scoring assigns\\nto the pair (q, d) a score in the interval [0, 1], by computing a linear combina-\\ntion of zone scores, where each zone of the document contributes a Boolean\\nvalue. More speciﬁcally, consider a set of documents each of which has ℓ\\nzones. Let g1, . . . , gℓ∈[0, 1] such that ∑ℓ\\ni=1 gi = 1. For 1 ≤i ≤ℓ, let si be the\\nBoolean score denoting a match (or absence thereof) between q and the ith\\nzone. For instance, the Boolean score from a zone could be 1 if all the query\\nterm(s) occur in that zone, and zero otherwise; indeed, it could be any Boo-\\nlean function that maps the presence of query terms in a zone to 0, 1. Then,\\nthe weighted zone score is deﬁned to be\\nℓ\\n∑\\ni=1\\ngisi.\\n(6.1)\\nWeighted zone scoring is sometimes referred to also as ranked Boolean re-\\nRANKED BOOLEAN\\nRETRIEVAL\\ntrieval.\\n\\x0f\\nExample 6.1:\\nConsider the query shakespeare in a collection in which each doc-\\nument has three zones: author, title and body. The Boolean score function for a zone\\ntakes on the value 1 if the query term shakespeare is present in the zone, and zero\\notherwise. Weighted zone scoring in such a collection would require three weights\\ng1, g2 and g3, respectively corresponding to the author, title and body zones. Suppose\\nwe set g1 = 0.2, g2 = 0.3 and g3 = 0.5 (so that the three weights add up to 1); this cor-\\nresponds to an application in which a match in the author zone is least important to\\nthe overall score, the title zone somewhat more, and the body contributes even more.\\nThus if the term shakespeare were to appear in the title and body zones but not the\\nauthor zone of a document, the score of this document would be 0.8.\\nHow do we implement the computation of weighted zone scores? A sim-\\nple approach would be to compute the score for each document in turn,\\nadding in all the contributions from the various zones. However, we now\\nshow how we may compute weighted zone scores directly from inverted in-\\ndexes. The algorithm of Figure 6.4 treats the case when the query q is a two-\\nterm query consisting of query terms q1 and q2, and the Boolean function is\\nAND: 1 if both query terms are present in a zone and 0 otherwise. Following\\nthe description of the algorithm, we describe the extension to more complex\\nqueries and Boolean functions.\\nThe reader may have noticed the close similarity between this algorithm\\nand that in Figure 1.6. Indeed, they represent the same postings traversal,\\nexcept that instead of merely adding a document to the set of results for\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n113\\nZONESCORE(q1, q2)\\n1\\nﬂoat scores[N] = [0]\\n2\\nconstant g[ℓ]\\n3\\np1 ←postings(q1)\\n4\\np2 ←postings(q2)\\n5\\n// scores[] is an array with a score entry for each document, initialized to zero.\\n6\\n//p1 and p2 are initialized to point to the beginning of their respective postings.\\n7\\n//Assume g[] is initialized to the respective zone weights.\\n8\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n9\\ndo if docID(p1) = docID(p2)\\n10\\nthen scores[docID(p1)] ←WEIGHTEDZONE(p1, p2, g)\\n11\\np1 ←next(p1)\\n12\\np2 ←next(p2)\\n13\\nelse if docID(p1) < docID(p2)\\n14\\nthen p1 ←next(p1)\\n15\\nelse p2 ←next(p2)\\n16\\nreturn scores\\n◮Figure 6.4\\nAlgorithm for computing the weighted zone score from two postings\\nlists. Function WEIGHTEDZONE (not shown here) is assumed to compute the inner\\nloop of Equation 6.1.\\na Boolean AND query, we now compute a score for each such document.\\nSome literature refers to the array scores[] above as a set of accumulators. The\\nACCUMULATOR\\nreason for this will be clear as we consider more complex Boolean functions\\nthan the AND; thus we may assign a non-zero score to a document even if it\\ndoes not contain all query terms.\\n6.1.2\\nLearning weights\\nHow do we determine the weights gi for weighted zone scoring? These\\nweights could be speciﬁed by an expert (or, in principle, the user); but in-\\ncreasingly, these weights are “learned” using training examples that have\\nbeen judged editorially. This latter methodology falls under a general class\\nof approaches to scoring and ranking in information retrieval, known as\\nmachine-learned relevance. We provide a brief introduction to this topic here\\nMACHINE-LEARNED\\nRELEVANCE\\nbecause weighted zone scoring presents a clean setting for introducing it; a\\ncomplete development demands an understanding of machine learning and\\nis deferred to Chapter 15.\\n1. We are provided with a set of training examples, each of which is a tu-\\nple consisting of a query q and a document d, together with a relevance\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n114\\n6\\nScoring, term weighting and the vector space model\\njudgment for d on q. In the simplest form, each relevance judgments is ei-\\nther Relevant or Non-relevant. More sophisticated implementations of the\\nmethodology make use of more nuanced judgments.\\n2. The weights gi are then “learned” from these examples, in order that the\\nlearned scores approximate the relevance judgments in the training exam-\\nples.\\nFor weighted zone scoring, the process may be viewed as learning a lin-\\near function of the Boolean match scores contributed by the various zones.\\nThe expensive component of this methodology is the labor-intensive assem-\\nbly of user-generated relevance judgments from which to learn the weights,\\nespecially in a collection that changes frequently (such as the Web). We now\\ndetail a simple example that illustrates how we can reduce the problem of\\nlearning the weights gi to a simple optimization problem.\\nWe now consider a simple case of weighted zone scoring, where each doc-\\nument has a title zone and a body zone. Given a query q and a document d, we\\nuse the given Boolean match function to compute Boolean variables sT(d, q)\\nand sB(d, q), depending on whether the title (respectively, body) zone of d\\nmatches query q. For instance, the algorithm in Figure 6.4 uses an AND of\\nthe query terms for this Boolean function. We will compute a score between\\n0 and 1 for each (document, query) pair using sT(d, q) and sB(d, q) by using\\na constant g ∈[0, 1], as follows:\\nscore(d, q) = g · sT(d, q) + (1 −g)sB(d, q).\\n(6.2)\\nWe now describe how to determine the constant g from a set of training ex-\\namples, each of which is a triple of the form Φj = (dj, qj, r(dj, qj)). In each\\ntraining example, a given training document dj and a given training query qj\\nare assessed by a human editor who delivers a relevance judgment r(dj, qj)\\nthat is either Relevant or Non-relevant. This is illustrated in Figure 6.5, where\\nseven training examples are shown.\\nFor each training example Φj we have Boolean values sT(dj, qj) and sB(dj, qj)\\nthat we use to compute a score from (6.2)\\nscore(dj, qj) = g · sT(dj, qj) + (1 −g)sB(dj, qj).\\n(6.3)\\nWe now compare this computed score to the human relevance judgment for\\nthe same document-query pair (dj, qj); to this end, we will quantize each\\nRelevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose\\nthat we deﬁne the error of the scoring function with weight g as\\nε(g, Φj) = (r(dj, qj) −score(dj, qj))2,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n115\\nExample\\nDocID\\nQuery\\nsT\\nsB\\nJudgment\\nΦ1\\n37\\nlinux\\n1\\n1\\nRelevant\\nΦ2\\n37\\npenguin\\n0\\n1\\nNon-relevant\\nΦ3\\n238\\nsystem\\n0\\n1\\nRelevant\\nΦ4\\n238\\npenguin\\n0\\n0\\nNon-relevant\\nΦ5\\n1741\\nkernel\\n1\\n1\\nRelevant\\nΦ6\\n2094\\ndriver\\n0\\n1\\nRelevant\\nΦ7\\n3191\\ndriver\\n1\\n0\\nNon-relevant\\n◮Figure 6.5\\nAn illustration of training examples.\\nsT\\nsB\\nScore\\n0\\n0\\n0\\n0\\n1\\n1 −g\\n1\\n0\\ng\\n1\\n1\\n1\\n◮Figure 6.6\\nThe four possible combinations of sT and sB.\\nwhere we have quantized the editorial relevance judgment r(dj, qj) to 0 or 1.\\nThen, the total error of a set of training examples is given by\\n∑\\nj\\nε(g, Φj).\\n(6.4)\\nThe problem of learning the constant g from the given training examples\\nthen reduces to picking the value of g that minimizes the total error in (6.4).\\nPicking the best value of g in (6.4) in the formulation of Section 6.1.3 re-\\nduces to the problem of minimizing a quadratic function of g over the inter-\\nval [0, 1]. This reduction is detailed in Section 6.1.3.\\n$\\n6.1.3\\nThe optimal weight g\\nWe begin by noting that for any training example Φj for which sT(dj, qj) = 0\\nand sB(dj, qj) = 1, the score computed by Equation (6.2) is 1 −g. In similar\\nfashion, we may write down the score computed by Equation (6.2) for the\\nthree other possible combinations of sT(dj, qj) and sB(dj, qj); this is summa-\\nrized in Figure 6.6.\\nLet n01r (respectively, n01n) denote the number of training examples for\\nwhich sT(dj, qj) = 0 and sB(dj, qj) = 1 and the editorial judgment is Relevant\\n(respectively, Non-relevant). Then the contribution to the total error in Equa-\\ntion (6.4) from training examples for which sT(dj, qj) = 0 and sB(dj, qj) = 1\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n116\\n6\\nScoring, term weighting and the vector space model\\nis\\n[1 −(1 −g)]2n01r + [0 −(1 −g)]2n01n.\\nBy writing in similar fashion the error contributions from training examples\\nof the other three combinations of values for sT(dj, qj) and sB(dj, qj) (and\\nextending the notation in the obvious manner), the total error corresponding\\nto Equation (6.4) is\\n(n01r + n10n)g2 + (n10r + n01n)(1 −g)2 + n00r + n11n.\\n(6.5)\\nBy differentiating Equation (6.5) with respect to g and setting the result to\\nzero, it follows that the optimal value of g is\\nn10r + n01n\\nn10r + n10n + n01r + n01n\\n.\\n(6.6)\\n?\\nExercise 6.1\\nWhen using weighted zone scoring, is it necessary for all zones to use the same Boo-\\nlean match function?\\nExercise 6.2\\nIn Example 6.1 above with weights g1 = 0.2, g2 = 0.31 and g3 = 0.49, what are all the\\ndistinct score values a document may get?\\nExercise 6.3\\nRewrite the algorithm in Figure 6.4 to the case of more than two query terms.\\nExercise 6.4\\nWrite pseudocode for the function WeightedZone for the case of two postings lists in\\nFigure 6.4.\\nExercise 6.5\\nApply Equation 6.6 to the sample training set in Figure 6.5 to estimate the best value\\nof g for this sample.\\nExercise 6.6\\nFor the value of g estimated in Exercise 6.5, compute the weighted zone score for each\\n(query, document) example. How do these scores relate to the relevance judgments\\nin Figure 6.5 (quantized to 0/1)?\\nExercise 6.7\\nWhy does the expression for g in (6.6) not involve training examples in which sT(dt, qt)\\nand sB(dt, qt) have the same value?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.2\\nTerm frequency and weighting\\n117\\n6.2\\nTerm frequency and weighting\\nThus far, scoring has hinged on whether or not a query term is present in\\na zone within a document. We take the next logical step: a document or\\nzone that mentions a query term more often has more to do with that query\\nand therefore should receive a higher score. To motivate this, we recall the\\nnotion of a free text query introduced in Section 1.4: a query in which the\\nterms of the query are typed freeform into the search interface, without any\\nconnecting search operators (such as Boolean operators). This query style,\\nwhich is extremely popular on the web, views the query as simply a set of\\nwords. A plausible scoring mechanism then is to compute a score that is the\\nsum, over the query terms, of the match scores between each query term and\\nthe document.\\nTowards this end, we assign to each term in a document a weight for that\\nterm, that depends on the number of occurrences of the term in the doc-\\nument. We would like to compute a score between a query term t and a\\ndocument d, based on the weight of t in d. The simplest approach is to assign\\nthe weight to be equal to the number of occurrences of term t in document d.\\nThis weighting scheme is referred to as term frequency and is denoted tft,d,\\nTERM FREQUENCY\\nwith the subscripts denoting the term and the document in order.\\nFor a document d, the set of weights determined by the tf weights above\\n(or indeed any weighting function that maps the number of occurrences of t\\nin d to a positive real value) may be viewed as a quantitative digest of that\\ndocument. In this view of a document, known in the literature as the bag\\nBAG OF WORDS\\nof words model, the exact ordering of the terms in a document is ignored but\\nthe number of occurrences of each term is material (in contrast to Boolean\\nretrieval). We only retain information on the number of occurrences of each\\nterm. Thus, the document “Mary is quicker than John” is, in this view, iden-\\ntical to the document “John is quicker than Mary”. Nevertheless, it seems\\nintuitive that two documents with similar bag of words representations are\\nsimilar in content. We will develop this intuition further in Section 6.3.\\nBefore doing so we ﬁrst study the question: are all words in a document\\nequally important? Clearly not; in Section 2.2.2 (page 27) we looked at the\\nidea of stop words – words that we decide not to index at all, and therefore do\\nnot contribute in any way to retrieval and scoring.\\n6.2.1\\nInverse document frequency\\nRaw term frequency as above suffers from a critical problem: all terms are\\nconsidered equally important when it comes to assessing relevancy on a\\nquery. In fact certain terms have little or no discriminating power in de-\\ntermining relevance. For instance, a collection of documents on the auto\\nindustry is likely to have the term auto in almost every document. To this\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n118\\n6\\nScoring, term weighting and the vector space model\\nWord\\ncf\\ndf\\ntry\\n10422\\n8760\\ninsurance\\n10440\\n3997\\n◮Figure 6.7\\nCollection frequency (cf) and document frequency (df) behave differ-\\nently, as in this example from the Reuters collection.\\nend, we introduce a mechanism for attenuating the effect of terms that occur\\ntoo often in the collection to be meaningful for relevance determination. An\\nimmediate idea is to scale down the term weights of terms with high collec-\\ntion frequency, deﬁned to be the total number of occurrences of a term in the\\ncollection. The idea would be to reduce the tf weight of a term by a factor\\nthat grows with its collection frequency.\\nInstead, it is more commonplace to use for this purpose the document fre-\\nDOCUMENT\\nFREQUENCY\\nquency dft, deﬁned to be the number of documents in the collection that con-\\ntain a term t. This is because in trying to discriminate between documents\\nfor the purpose of scoring it is better to use a document-level statistic (such\\nas the number of documents containing a term) than to use a collection-wide\\nstatistic for the term. The reason to prefer df to cf is illustrated in Figure 6.7,\\nwhere a simple example shows that collection frequency (cf) and document\\nfrequency (df) can behave rather differently. In particular, the cf values for\\nboth try and insurance are roughly equal, but their df values differ signiﬁ-\\ncantly. Intuitively, we want the few documents that contain insurance to get\\na higher boost for a query on insurance than the many documents containing\\ntry get from a query on try.\\nHow is the document frequency df of a term used to scale its weight? De-\\nnoting as usual the total number of documents in a collection by N, we deﬁne\\nthe inverse document frequency (idf) of a term t as follows:\\nINVERSE DOCUMENT\\nFREQUENCY\\nidft = log N\\ndft\\n.\\n(6.7)\\nThus the idf of a rare term is high, whereas the idf of a frequent term is\\nlikely to be low. Figure 6.8 gives an example of idf’s in the Reuters collection\\nof 806,791 documents; in this example logarithms are to the base 10. In fact,\\nas we will see in Exercise 6.12, the precise base of the logarithm is not material\\nto ranking. We will give on page 227 a justiﬁcation of the particular form in\\nEquation (6.7).\\n6.2.2\\nTf-idf weighting\\nWe now combine the deﬁnitions of term frequency and inverse document\\nfrequency, to produce a composite weight for each term in each document.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.2\\nTerm frequency and weighting\\n119\\nterm\\ndft\\nidft\\ncar\\n18,165\\n1.65\\nauto\\n6723\\n2.08\\ninsurance\\n19,241\\n1.62\\nbest\\n25,235\\n1.5\\n◮Figure 6.8\\nExample of idf values.\\nHere we give the idf’s of terms with various\\nfrequencies in the Reuters collection of 806,791 documents.\\nThe tf-idf weighting scheme assigns to term t a weight in document d given\\nTF-IDF\\nby\\ntf-idft,d = tft,d × idft.\\n(6.8)\\nIn other words, tf-idft,d assigns to term t a weight in document d that is\\n1. highest when t occurs many times within a small number of documents\\n(thus lending high discriminating power to those documents);\\n2. lower when the term occurs fewer times in a document, or occurs in many\\ndocuments (thus offering a less pronounced relevance signal);\\n3. lowest when the term occurs in virtually all documents.\\nAt this point, we may view each document as a vector with one component\\nDOCUMENT VECTOR\\ncorresponding to each term in the dictionary, together with a weight for each\\ncomponent that is given by (6.8). For dictionary terms that do not occur in\\na document, this weight is zero. This vector form will prove to be crucial to\\nscoring and ranking; we will develop these ideas in Section 6.3. As a ﬁrst\\nstep, we introduce the overlap score measure: the score of a document d is the\\nsum, over all query terms, of the number of times each of the query terms\\noccurs in d. We can reﬁne this idea so that we add up not the number of\\noccurrences of each query term t in d, but instead the tf-idf weight of each\\nterm in d.\\nScore(q, d) = ∑\\nt∈q\\ntf-idft,d.\\n(6.9)\\nIn Section 6.3 we will develop a more rigorous form of Equation (6.9).\\n?\\nExercise 6.8\\nWhy is the idf of a term always ﬁnite?\\nExercise 6.9\\nWhat is the idf of a term that occurs in every document? Compare this with the use\\nof stop word lists.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n120\\n6\\nScoring, term weighting and the vector space model\\nDoc1\\nDoc2\\nDoc3\\ncar\\n27\\n4\\n24\\nauto\\n3\\n33\\n0\\ninsurance\\n0\\n33\\n29\\nbest\\n14\\n0\\n17\\n◮Figure 6.9\\nTable of tf values for Exercise 6.10.\\nExercise 6.10\\nConsider the table of term frequencies for 3 documents denoted Doc1, Doc2, Doc3 in\\nFigure 6.9. Compute the tf-idf weights for the terms car, auto, insurance, best, for each\\ndocument, using the idf values from Figure 6.8.\\nExercise 6.11\\nCan the tf-idf weight of a term in a document exceed 1?\\nExercise 6.12\\nHow does the base of the logarithm in (6.7) affect the score calculation in (6.9)? How\\ndoes the base of the logarithm affect the relative scores of two documents on a given\\nquery?\\nExercise 6.13\\nIf the logarithm in (6.7) is computed base 2, suggest a simple approximation to the idf\\nof a term.\\n6.3\\nThe vector space model for scoring\\nIn Section 6.2 (page 117) we developed the notion of a document vector that\\ncaptures the relative importance of the terms in a document. The representa-\\ntion of a set of documents as vectors in a common vector space is known as\\nthe vector space model and is fundamental to a host of information retrieval op-\\nVECTOR SPACE MODEL\\nerations ranging from scoring documents on a query, document classiﬁcation\\nand document clustering. We ﬁrst develop the basic ideas underlying vector\\nspace scoring; a pivotal step in this development is the view (Section 6.3.2)\\nof queries as vectors in the same vector space as the document collection.\\n6.3.1\\nDot products\\nWe denote by ⃗V(d) the vector derived from document d, with one com-\\nponent in the vector for each dictionary term. Unless otherwise speciﬁed,\\nthe reader may assume that the components are computed using the tf-idf\\nweighting scheme, although the particular weighting scheme is immaterial\\nto the discussion that follows. The set of documents in a collection then may\\nbe viewed as a set of vectors in a vector space, in which there is one axis for\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n121\\n0\\n1\\n0\\n1\\njealous\\ngossip\\n⃗v(q)\\n⃗v(d1)\\n⃗v(d2)\\n⃗v(d3)\\nθ\\n◮Figure 6.10\\nCosine similarity illustrated. sim(d1, d2) = cos θ.\\neach term. This representation loses the relative ordering of the terms in each\\ndocument; recall our example from Section 6.2 (page 117), where we pointed\\nout that the documents Mary is quicker than John and John is quicker than Mary\\nare identical in such a bag of words representation.\\nHow do we quantify the similarity between two documents in this vector\\nspace? A ﬁrst attempt might consider the magnitude of the vector difference\\nbetween two document vectors. This measure suffers from a drawback: two\\ndocuments with very similar content can have a signiﬁcant vector difference\\nsimply because one is much longer than the other. Thus the relative distribu-\\ntions of terms may be identical in the two documents, but the absolute term\\nfrequencies of one may be far larger.\\nTo compensate for the effect of document length, the standard way of\\nquantifying the similarity between two documents d1 and d2 is to compute\\nthe cosine similarity of their vector representations ⃗V(d1) and ⃗V(d2)\\nCOSINE SIMILARITY\\nsim(d1, d2) =\\n⃗V(d1) · ⃗V(d2)\\n|⃗V(d1)||⃗V(d2)|\\n,\\n(6.10)\\nwhere the numerator represents the dot product (also known as the inner prod-\\nDOT PRODUCT\\nuct) of the vectors ⃗V(d1) and ⃗V(d2), while the denominator is the product of\\ntheir Euclidean lengths. The dot product ⃗x · ⃗y of two vectors is deﬁned as\\nEUCLIDEAN LENGTH\\n∑M\\ni=1 xiyi. Let ⃗V(d) denote the document vector for d, with M components\\n⃗V1(d) . . . ⃗VM(d). The Euclidean length of d is deﬁned to be\\nq\\n∑M\\ni=1 ⃗V2\\ni (d).\\nThe effect of the denominator of Equation (6.10) is thus to length-normalize\\nLENGTH-\\nNORMALIZATION\\nthe vectors ⃗V(d1) and ⃗V(d2) to unit vectors ⃗v(d1) = ⃗V(d1)/|⃗V(d1)| and\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n122\\n6\\nScoring, term weighting and the vector space model\\nDoc1\\nDoc2\\nDoc3\\ncar\\n0.88\\n0.09\\n0.58\\nauto\\n0.10\\n0.71\\n0\\ninsurance\\n0\\n0.71\\n0.70\\nbest\\n0.46\\n0\\n0.41\\n◮Figure 6.11\\nEuclidean normalized tf values for documents in Figure 6.9.\\nterm\\nSaS\\nPaP\\nWH\\naffection\\n115\\n58\\n20\\njealous\\n10\\n7\\n11\\ngossip\\n2\\n0\\n6\\n◮Figure 6.12\\nTerm frequencies in three novels. The novels are Austen’s Sense and\\nSensibility, Pride and Prejudice and Brontë’s Wuthering Heights.\\n⃗v(d2) = ⃗V(d2)/|⃗V(d2)|. We can then rewrite (6.10) as\\nsim(d1, d2) = ⃗v(d1) ·⃗v(d2).\\n(6.11)\\n\\x0f\\nExample 6.2:\\nConsider the documents in Figure 6.9. We now apply Euclidean\\nnormalization to the tf values from the table, for each of the three documents in the\\ntable. The quantity\\nq\\n∑M\\ni=1 ⃗V2\\ni (d) has the values 30.56, 46.84 and 41.30 respectively\\nfor Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these\\ndocuments are shown in Figure 6.11.\\nThus, (6.11) can be viewed as the dot product of the normalized versions of\\nthe two document vectors. This measure is the cosine of the angle θ between\\nthe two vectors, shown in Figure 6.10. What use is the similarity measure\\nsim(d1, d2)? Given a document d (potentially one of the di in the collection),\\nconsider searching for the documents in the collection most similar to d. Such\\na search is useful in a system where a user may identify a document and\\nseek others like it – a feature available in the results lists of search engines\\nas a more like this feature. We reduce the problem of ﬁnding the document(s)\\nmost similar to d to that of ﬁnding the di with the highest dot products (sim\\nvalues)⃗v(d) ·⃗v(di). We could do this by computing the dot products between\\n⃗v(d) and each of ⃗v(d1), . . . ,⃗v(dN), then picking off the highest resulting sim\\nvalues.\\n\\x0f\\nExample 6.3:\\nFigure 6.12 shows the number of occurrences of three terms (affection,\\njealous and gossip) in each of the following three novels: Jane Austen’s Sense and Sensi-\\nbility (SaS) and Pride and Prejudice (PaP) and Emily Brontë’s Wuthering Heights (WH).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n123\\nterm\\nSaS\\nPaP\\nWH\\naffection\\n0.996\\n0.993\\n0.847\\njealous\\n0.087\\n0.120\\n0.466\\ngossip\\n0.017\\n0\\n0.254\\n◮Figure 6.13\\nTerm vectors for the three novels of Figure 6.12. These are based on\\nraw term frequency only and are normalized as if these were the only terms in the\\ncollection. (Since affection and jealous occur in all three documents, their tf-idf weight\\nwould be 0 in most formulations.)\\nOf course, there are many other terms occurring in each of these novels. In this ex-\\nample we represent each of these novels as a unit vector in three dimensions, corre-\\nsponding to these three terms (only); we use raw term frequencies here, with no idf\\nmultiplier. The resulting weights are as shown in Figure 6.13.\\nNow consider the cosine similarities between pairs of the resulting three-dimensional\\nvectors. A simple computation shows that sim(⃗v(SAS), ⃗v(PAP)) is 0.999, whereas\\nsim(⃗v(SAS), ⃗v(WH)) is 0.888; thus, the two books authored by Austen (SaS and PaP)\\nare considerably closer to each other than to Brontë’s Wuthering Heights. In fact, the\\nsimilarity between the ﬁrst two is almost perfect (when restricted to the three terms\\nwe consider). Here we have considered tf weights, but we could of course use other\\nterm weight functions.\\nViewing a collection of N documents as a collection of vectors leads to a\\nnatural view of a collection as a term-document matrix: this is an M × N matrix\\nTERM-DOCUMENT\\nMATRIX\\nwhose rows represent the M terms (dimensions) of the N columns, each of\\nwhich corresponds to a document. As always, the terms being indexed could\\nbe stemmed before indexing; for instance, jealous and jealousy would under\\nstemming be considered as a single dimension. This matrix view will prove\\nto be useful in Chapter 18.\\n6.3.2\\nQueries as vectors\\nThere is a far more compelling reason to represent documents as vectors:\\nwe can also view a query as a vector. Consider the query q = jealous gossip.\\nThis query turns into the unit vector ⃗v(q) = (0, 0.707, 0.707) on the three\\ncoordinates of Figures 6.12 and 6.13. The key idea now: to assign to each\\ndocument d a score equal to the dot product\\n⃗v(q) ·⃗v(d).\\nIn the example of Figure 6.13, Wuthering Heights is the top-scoring docu-\\nment for this query with a score of 0.509, with Pride and Prejudice a distant\\nsecond with a score of 0.085, and Sense and Sensibility last with a score of\\n0.074. This simple example is somewhat misleading: the number of dimen-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n124\\n6\\nScoring, term weighting and the vector space model\\nsions in practice will be far larger than three: it will equal the vocabulary size\\nM.\\nTo summarize, by viewing a query as a “bag of words”, we are able to\\ntreat it as a very short document. As a consequence, we can use the cosine\\nsimilarity between the query vector and a document vector as a measure of\\nthe score of the document for that query. The resulting scores can then be\\nused to select the top-scoring documents for a query. Thus we have\\nscore(q, d) =\\n⃗V(q) · ⃗V(d)\\n|⃗V(q)||⃗V(d)|\\n.\\n(6.12)\\nA document may have a high cosine score for a query even if it does not\\ncontain all query terms. Note that the preceding discussion does not hinge\\non any speciﬁc weighting of terms in the document vector, although for the\\npresent we may think of them as either tf or tf-idf weights. In fact, a number\\nof weighting schemes are possible for query as well as document vectors, as\\nillustrated in Example 6.4 and developed further in Section 6.4.\\nComputing the cosine similarities between the query vector and each doc-\\nument vector in the collection, sorting the resulting scores and selecting the\\ntop K documents can be expensive — a single similarity computation can\\nentail a dot product in tens of thousands of dimensions, demanding tens of\\nthousands of arithmetic operations. In Section 7.1 we study how to use an in-\\nverted index for this purpose, followed by a series of heuristics for improving\\non this.\\n\\x0f\\nExample 6.4:\\nWe now consider the query best car insurance on a ﬁctitious collection\\nwith N = 1,000,000 documents where the document frequencies of auto, best, car and\\ninsurance are respectively 5000, 50000, 10000 and 1000.\\nterm\\nquery\\ndocument\\nproduct\\ntf\\ndf\\nidf\\nwt,q\\ntf\\nwf\\nwt,d\\nauto\\n0\\n5000\\n2.3\\n0\\n1\\n1\\n0.41\\n0\\nbest\\n1\\n50000\\n1.3\\n1.3\\n0\\n0\\n0\\n0\\ncar\\n1\\n10000\\n2.0\\n2.0\\n1\\n1\\n0.41\\n0.82\\ninsurance\\n1\\n1000\\n3.0\\n3.0\\n2\\n2\\n0.82\\n2.46\\nIn this example the weight of a term in the query is simply the idf (and zero for a\\nterm not in the query, such as auto); this is reﬂected in the column header wt,q (the en-\\ntry for auto is zero because the query does not contain the termauto). For documents,\\nwe use tf weighting with no use of idf but with Euclidean normalization. The former\\nis shown under the column headed wf, while the latter is shown under the column\\nheaded wt,d. Invoking (6.9) now gives a net score of 0 + 0 + 0.82 + 2.46 = 3.28.\\n6.3.3\\nComputing vector scores\\nIn a typical setting we have a collection of documents each represented by a\\nvector, a free text query represented by a vector, and a positive integer K. We\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n125\\nCOSINESCORE(q)\\n1\\nﬂoat Scores[N] = 0\\n2\\nInitialize Length[N]\\n3\\nfor each query term t\\n4\\ndo calculate wt,q and fetch postings list for t\\n5\\nfor each pair(d, tft,d) in postings list\\n6\\ndo Scores[d] += wft,d × wt,q\\n7\\nRead the array Length[d]\\n8\\nfor each d\\n9\\ndo Scores[d] = Scores[d]/Length[d]\\n10\\nreturn Top K components of Scores[]\\n◮Figure 6.14\\nThe basic algorithm for computing vector space scores.\\nseek the K documents of the collection with the highest vector space scores on\\nthe given query. We now initiate the study of determining the K documents\\nwith the highest vector space scores for a query. Typically, we seek these\\nK top documents in ordered by decreasing score; for instance many search\\nengines use K = 10 to retrieve and rank-order the ﬁrst page of the ten best\\nresults. Here we give the basic algorithm for this computation; we develop a\\nfuller treatment of efﬁcient techniques and approximations in Chapter 7.\\nFigure 6.14 gives the basic algorithm for computing vector space scores.\\nThe array Length holds the lengths (normalization factors) for each of the N\\ndocuments, whereas the array Scores holds the scores for each of the docu-\\nments. When the scores are ﬁnally computed in Step 9, all that remains in\\nStep 10 is to pick off the K documents with the highest scores.\\nThe outermost loop beginning Step 3 repeats the updating of Scores, iter-\\nating over each query term t in turn. In Step 5 we calculate the weight in\\nthe query vector for term t. Steps 6-8 update the score of each document by\\nadding in the contribution from term t. This process of adding in contribu-\\ntions one query term at a time is sometimes known as term-at-a-time scoring\\nTERM-AT-A-TIME\\nor accumulation, and the N elements of the array Scores are therefore known\\nas accumulators. For this purpose, it would appear necessary to store, with\\nACCUMULATOR\\neach postings entry, the weight wft,d of term t in document d (we have thus\\nfar used either tf or tf-idf for this weight, but leave open the possibility of\\nother functions to be developed in Section 6.4). In fact this is wasteful, since\\nstoring this weight may require a ﬂoating point number. Two ideas help alle-\\nviate this space problem. First, if we are using inverse document frequency,\\nwe need not precompute idft; it sufﬁces to store N/dft at the head of the\\npostings for t. Second, we store the term frequency tft,d for each postings en-\\ntry. Finally, Step 12 extracts the top K scores – this requires a priority queue\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n126\\n6\\nScoring, term weighting and the vector space model\\ndata structure, often implemented using a heap. Such a heap takes no more\\nthan 2N comparisons to construct, following which each of the K top scores\\ncan be extracted from the heap at a cost of O(log N) comparisons.\\nNote that the general algorithm of Figure 6.14 does not prescribe a speciﬁc\\nimplementation of how we traverse the postings lists of the various query\\nterms; we may traverse them one term at a time as in the loop beginning\\nat Step 3, or we could in fact traverse them concurrently as in Figure 1.6. In\\nsuch a concurrent postings traversal we compute the scores of one document\\nat a time, so that it is sometimes called document-at-a-time scoring. We will\\nDOCUMENT-AT-A-TIME\\nsay more about this in Section 7.1.5.\\n?\\nExercise 6.14\\nIf we were to stem jealous and jealousy to a common stem before setting up the vector\\nspace, detail how the deﬁnitions of tf and idf should be modiﬁed.\\nExercise 6.15\\nRecall the tf-idf weights computed in Exercise 6.10. Compute the Euclidean nor-\\nmalized document vectors for each of the documents, where each vector has four\\ncomponents, one for each of the four terms.\\nExercise 6.16\\nVerify that the sum of the squares of the components of each of the document vectors\\nin Exercise 6.15 is 1 (to within rounding error). Why is this the case?\\nExercise 6.17\\nWith term weights as computed in Exercise 6.15, rank the three documents by com-\\nputed score for the query car insurance, for each of the following cases of term weight-\\ning in the query:\\n1. The weight of a term is 1 if present in the query, 0 otherwise.\\n2. Euclidean normalized idf.\\n6.4\\nVariant tf-idf functions\\nFor assigning a weight for each term in each document, a number of alterna-\\ntives to tf and tf-idf have been considered. We discuss some of the principal\\nones here; a more complete development is deferred to Chapter 11. We will\\nsummarize these alternatives in Section 6.4.3 (page 128).\\n6.4.1\\nSublinear tf scaling\\nIt seems unlikely that twenty occurrences of a term in a document truly carry\\ntwenty times the signiﬁcance of a single occurrence. Accordingly, there has\\nbeen considerable research into variants of term frequency that go beyond\\ncounting the number of occurrences of a term. A common modiﬁcation is\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n127\\nto use instead the logarithm of the term frequency, which assigns a weight\\ngiven by\\nwft,d =\\n\\x1a\\n1 + log tft,d\\nif tft,d > 0\\n0\\notherwise .\\n(6.13)\\nIn this form, we may replace tf by some other function wf as in (6.13), to\\nobtain:\\nwf-idft,d = wft,d × idft.\\n(6.14)\\nEquation (6.9) can then be modiﬁed by replacing tf-idf by wf-idf as deﬁned\\nin (6.14).\\n6.4.2\\nMaximum tf normalization\\nOne well-studied technique is to normalize the tf weights of all terms occur-\\nring in a document by the maximum tf in that document. For each document\\nd, let tfmax(d) = maxτ∈d tfτ,d, where τ ranges over all terms in d. Then, we\\ncompute a normalized term frequency for each term t in document d by\\nntft,d = a + (1 −a)\\ntft,d\\ntfmax(d),\\n(6.15)\\nwhere a is a value between 0 and 1 and is generally set to 0.4, although some\\nearly work used the value 0.5. The term a in (6.15) is a smoothing term whose\\nSMOOTHING\\nrole is to damp the contribution of the second term – which may be viewed as\\na scaling down of tf by the largest tf value in d. We will encounter smoothing\\nfurther in Chapter 13 when discussing classiﬁcation; the basic idea is to avoid\\na large swing in ntft,d from modest changes in tft,d (say from 1 to 2). The main\\nidea of maximum tf normalization is to mitigate the following anomaly: we\\nobserve higher term frequencies in longer documents, merely because longer\\ndocuments tend to repeat the same words over and over again. To appreciate\\nthis, consider the following extreme example: supposed we were to take a\\ndocument d and create a new document d′ by simply appending a copy of d\\nto itself. While d′ should be no more relevant to any query than d is, the use\\nof (6.9) would assign it twice as high a score as d. Replacing tf-idft,d in (6.9) by\\nntf-idft,d eliminates the anomaly in this example. Maximum tf normalization\\ndoes suffer from the following issues:\\n1. The method is unstable in the following sense: a change in the stop word\\nlist can dramatically alter term weightings (and therefore ranking). Thus,\\nit is hard to tune.\\n2. A document may contain an outlier term with an unusually large num-\\nber of occurrences of that term, not representative of the content of that\\ndocument.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n128\\n6\\nScoring, term weighting and the vector space model\\nTerm frequency\\nDocument frequency\\nNormalization\\nn (natural)\\ntft,d\\nn (no)\\n1\\nn (none)\\n1\\nl (logarithm)\\n1 + log(tft,d)\\nt (idf)\\nlog N\\ndft\\nc (cosine)\\n1\\n√\\nw2\\n1+w2\\n2+...+w2\\nM\\na (augmented)\\n0.5 +\\n0.5×tft,d\\nmaxt(tft,d)\\np (prob idf)\\nmax{0, log N−dft\\ndft }\\nu (pivoted\\nunique)\\n1/u (Section 6.4.4)\\nb (boolean)\\n\\x1a1\\nif tft,d > 0\\n0\\notherwise\\nb (byte size)\\n1/CharLengthα, α < 1\\nL (log ave)\\n1+log(tft,d)\\n1+log(avet∈d(tft,d))\\n◮Figure 6.15\\nSMART notation for tf-idf variants. Here CharLength is the number\\nof characters in the document.\\n3. More generally, a document in which the most frequent term appears\\nroughly as often as many other terms should be treated differently from\\none with a more skewed distribution.\\n6.4.3\\nDocument and query weighting schemes\\nEquation (6.12) is fundamental to information retrieval systems that use any\\nform of vector space scoring. Variations from one vector space scoring method\\nto another hinge on the speciﬁc choices of weights in the vectors ⃗V(d) and\\n⃗V(q). Figure 6.15 lists some of the principal weighting schemes in use for\\neach of ⃗V(d) and ⃗V(q), together with a mnemonic for representing a spe-\\nciﬁc combination of weights; this system of mnemonics is sometimes called\\nSMART notation, following the authors of an early text retrieval system. The\\nmnemonic for representing a combination of weights takes the form ddd.qqq\\nwhere the ﬁrst triplet gives the term weighting of the document vector, while\\nthe second triplet gives the weighting in the query vector. The ﬁrst letter in\\neach triplet speciﬁes the term frequency component of the weighting, the\\nsecond the document frequency component, and the third the form of nor-\\nmalization used. It is quite common to apply different normalization func-\\ntions to ⃗V(d) and ⃗V(q). For example, a very standard weighting scheme\\nis lnc.ltc, where the document vector has log-weighted term frequency, no\\nidf (for both effectiveness and efﬁciency reasons), and cosine normalization,\\nwhile the query vector uses log-weighted term frequency, idf weighting, and\\ncosine normalization.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n129\\n$\\n6.4.4\\nPivoted normalized document length\\nIn Section 6.3.1 we normalized each document vector by the Euclidean length\\nof the vector, so that all document vectors turned into unit vectors. In doing\\nso, we eliminated all information on the length of the original document;\\nthis masks some subtleties about longer documents. First, longer documents\\nwill – as a result of containing more terms – have higher tf values. Second,\\nlonger documents contain more distinct terms. These factors can conspire to\\nraise the scores of longer documents, which (at least for some information\\nneeds) is unnatural. Longer documents can broadly be lumped into two cat-\\negories: (1) verbose documents that essentially repeat the same content – in\\nthese, the length of the document does not alter the relative weights of dif-\\nferent terms; (2) documents covering multiple different topics, in which the\\nsearch terms probably match small segments of the document but not all of\\nit – in this case, the relative weights of terms are quite different from a single\\nshort document that matches the query terms. Compensating for this phe-\\nnomenon is a form of document length normalization that is independent of\\nterm and document frequencies. To this end, we introduce a form of normal-\\nizing the vector representations of documents in the collection, so that the\\nresulting “normalized” documents are not necessarily of unit length. Then,\\nwhen we compute the dot product score between a (unit) query vector and\\nsuch a normalized document, the score is skewed to account for the effect\\nof document length on relevance. This form of compensation for document\\nlength is known as pivoted document length normalization.\\nPIVOTED DOCUMENT\\nLENGTH\\nNORMALIZATION\\nConsider a document collection together with an ensemble of queries for\\nthat collection. Suppose that we were given, for each query q and for each\\ndocument d, a Boolean judgment of whether or not d is relevant to the query\\nq; in Chapter 8 we will see how to procure such a set of relevance judgments\\nfor a query ensemble and a document collection. Given this set of relevance\\njudgments, we may compute a probability of relevance as a function of docu-\\nment length, averaged over all queries in the ensemble. The resulting plot\\nmay look like the curve drawn in thick lines in Figure 6.16. To compute this\\ncurve, we bucket documents by length and compute the fraction of relevant\\ndocuments in each bucket, then plot this fraction against the median docu-\\nment length of each bucket. (Thus even though the “curve” in Figure 6.16\\nappears to be continuous, it is in fact a histogram of discrete buckets of doc-\\nument length.)\\nOn the other hand, the curve in thin lines shows what might happen with\\nthe same documents and query ensemble if we were to use relevance as pre-\\nscribed by cosine normalization Equation (6.12) – thus, cosine normalization\\nhas a tendency to distort the computed relevance vis-à-vis the true relevance,\\nat the expense of longer documents. The thin and thick curves crossover at a\\npoint p corresponding to document length ℓp, which we refer to as the pivot\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n130\\n6\\nScoring, term weighting and the vector space model\\nDocument length\\nRelevance\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n\\x11\\n\\x08\\x08\\x08\\x08\\n\\x08\\x08\\x08\\x08\\nℓp\\np\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x08\\x08\\x08\\n\\x08\\x08\\x08\\x08\\n-\\n6\\n◮Figure 6.16\\nPivoted document length normalization.\\nlength; dashed lines mark this point on the x−and y−axes. The idea of\\npivoted document length normalization would then be to “rotate” the co-\\nsine normalization curve counter-clockwise about p so that it more closely\\nmatches thick line representing the relevance vs. document length curve.\\nAs mentioned at the beginning of this section, we do so by using in Equa-\\ntion (6.12) a normalization factor for each document vector ⃗V(d) that is not\\nthe Euclidean length of that vector, but instead one that is larger than the Eu-\\nclidean length for documents of length less than ℓp, and smaller for longer\\ndocuments.\\nTo this end, we ﬁrst note that the normalizing term for ⃗V(d) in the de-\\nnominator of Equation (6.12) is its Euclidean length, denoted |⃗V(d)|. In the\\nsimplest implementation of pivoted document length normalization, we use\\na normalization factor in the denominator that is linear in |⃗V(d)|, but one\\nof slope < 1 as in Figure 6.17. In this ﬁgure, the x−axis represents |⃗V(d)|,\\nwhile the y−axis represents possible normalization factors we can use. The\\nthin line y = x depicts the use of cosine normalization. Notice the following\\naspects of the thick line representing pivoted length normalization:\\n1. It is linear in the document length and has the form\\na|⃗V(d)| + (1 −a)piv,\\n(6.16)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n131\\n|⃗V(d)|\\nPivoted normalization\\ny = x; Cosine\\nPivoted\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00piv\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n-\\n6\\n◮Figure 6.17\\nImplementing pivoted document length normalization by linear scal-\\ning.\\nwhere piv is the cosine normalization value at which the two curves in-\\ntersect.\\n2. Its slope is a < 1 and (3) it crosses the y = x line at piv.\\nIt has been argued that in practice, Equation (6.16) is well approximated by\\naud + (1 −a)piv,\\nwhere ud is the number of unique terms in document d.\\nOf course, pivoted document length normalization is not appropriate for\\nall applications. For instance, in a collection of answers to frequently asked\\nquestions (say, at a customer service website), relevance may have little to\\ndo with document length. In other cases the dependency may be more com-\\nplex than can be accounted for by a simple linear pivoted normalization. In\\nsuch cases, document length can be used as a feature in the machine learning\\nbased scoring approach of Section 6.1.2.\\n?\\nExercise 6.18\\nOne measure of the similarity of two vectors is the Euclidean distance (or L2 distance)\\nEUCLIDEAN DISTANCE\\nbetween them:\\n|⃗x −⃗y| =\\nv\\nu\\nu\\nt\\nM\\n∑\\ni=1\\n(xi −yi)2\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n132\\n6\\nScoring, term weighting and the vector space model\\nquery\\ndocument\\nword\\ntf\\nwf\\ndf\\nidf\\nqi = wf-idf\\ntf\\nwf\\ndi = normalized wf\\nqi · di\\ndigital\\n10,000\\nvideo\\n100,000\\ncameras\\n50,000\\n◮Table 6.1\\nCosine computation for Exercise 6.19.\\nGiven a query q and documents d1, d2, . . ., we may rank the documents di in order\\nof increasing Euclidean distance from q. Show that if q and the di are all normalized\\nto unit vectors, then the rank ordering produced by Euclidean distance is identical to\\nthat produced by cosine similarities.\\nExercise 6.19\\nCompute the vector space similarity between the query “digital cameras” and the\\ndocument “digital cameras and video cameras” by ﬁlling out the empty columns in\\nTable 6.1. Assume N = 10,000,000, logarithmic term weighting (wf columns) for\\nquery and document, idf weighting for the query only and cosine normalization for\\nthe document only. Treat and as a stop word. Enter term counts in the tf columns.\\nWhat is the ﬁnal similarity score?\\nExercise 6.20\\nShow that for the query affection, the relative ordering of the scores of the three doc-\\numents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous\\ngossip.\\nExercise 6.21\\nIn turning a query into a unit vector in Figure 6.13, we assigned equal weights to each\\nof the query terms. What other principled approaches are plausible?\\nExercise 6.22\\nConsider the case of a query term that is not in the set of M indexed terms; thus our\\nstandard construction of the query vector results in ⃗V(q) not being in the vector space\\ncreated from the collection. How would one adapt the vector space representation to\\nhandle this case?\\nExercise 6.23\\nRefer to the tf and idf values for four terms and three documents in Exercise 6.10.\\nCompute the two top scoring documents on the query best car insurance for each of\\nthe following weighing schemes: (i) nnn.atc; (ii) ntc.atc.\\nExercise 6.24\\nSuppose that the word coyote does not occur in the collection used in Exercises 6.10\\nand 6.23. How would one compute ntc.atc scores for the query coyote insurance?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n6.5\\nReferences and further reading\\n133\\n6.5\\nReferences and further reading\\nChapter 7 develops the computational aspects of vector space scoring. Luhn\\n(1957; 1958) describes some of the earliest reported applications of term weight-\\ning. His paper dwells on the importance of medium frequency terms (terms\\nthat are neither too commonplace nor too rare) and may be thought of as an-\\nticipating tf-idf and related weighting schemes. Spärck Jones (1972) builds\\non this intuition through detailed experiments showing the use of inverse\\ndocument frequency in term weighting. A series of extensions and theoret-\\nical justiﬁcations of idf are due to Salton and Buckley (1987) Robertson and\\nJones (1976), Croft and Harper (1979) and Papineni (2001). Robertson main-\\ntains a web page (http://www.soi.city.ac.uk/˜ser/idf.html) containing the history\\nof idf, including soft copies of early papers that predated electronic versions\\nof journal article. Singhal et al. (1996a) develop pivoted document length\\nnormalization. Probabilistic language models (Chapter 11) develop weight-\\ning techniques that are more nuanced than tf-idf; the reader will ﬁnd this\\ndevelopment in Section 11.4.3.\\nWe observed that by assigning a weight for each term in a document, a\\ndocument may be viewed as a vector of term weights, one for each term in\\nthe collection. The SMART information retrieval system at Cornell (Salton\\n1971b) due to Salton and colleagues was perhaps the ﬁrst to view a doc-\\nument as a vector of weights. The basic computation of cosine scores as\\ndescribed in Section 6.3.3 is due to Zobel and Moffat (2006). The two query\\nevaluation strategies term-at-a-time and document-at-a-time are discussed\\nby Turtle and Flood (1995).\\nThe SMART notation for tf-idf term weighting schemes in Figure 6.15 is\\npresented in (Salton and Buckley 1988, Singhal et al. 1995; 1996b). Not all\\nversions of the notation are consistent; we most closely follow (Singhal et al.\\n1996b). A more detailed and exhaustive notation was developed in Moffat\\nand Zobel (1998), considering a larger palette of schemes for term and doc-\\nument frequency weighting. Beyond the notation, Moffat and Zobel (1998)\\nsought to set up a space of feasible weighting functions through which hill-\\nclimbing approaches could be used to begin with weighting schemes that\\nperformed well, then make local improvements to identify the best combi-\\nnations. However, they report that such hill-climbing methods failed to lead\\nto any conclusions on the best weighting schemes.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n135\\n7\\nComputing scores in a complete\\nsearch system\\nChapter 6 developed the theory underlying term weighting in documents\\nfor the purposes of scoring, leading up to vector space models and the basic\\ncosine scoring algorithm of Section 6.3.3 (page 124). In this chapter we be-\\ngin in Section 7.1 with heuristics for speeding up this computation; many of\\nthese heuristics achieve their speed at the risk of not ﬁnding quite the top K\\ndocuments matching the query. Some of these heuristics generalize beyond\\ncosine scoring. With Section 7.1 in place, we have essentially all the compo-\\nnents needed for a complete search engine. We therefore take a step back\\nfrom cosine scoring, to the more general problem of computing scores in a\\nsearch engine. In Section 7.2 we outline a complete search engine, includ-\\ning indexes and structures to support not only cosine scoring but also more\\ngeneral ranking factors such as query term proximity. We describe how all\\nof the various pieces ﬁt together in Section 7.2.4. We conclude this chapter\\nwith Section 7.3, where we discuss how the vector space model for free text\\nqueries interacts with common query operators.\\n7.1\\nEfﬁcient scoring and ranking\\nWe begin by recapping the algorithm of Figure 6.14. For a query such as q =\\njealous gossip, two observations are immediate:\\n1. The unit vector ⃗v(q) has only two non-zero components.\\n2. In the absence of any weighting for query terms, these non-zero compo-\\nnents are equal – in this case, both equal 0.707.\\nFor the purpose of ranking the documents matching this query, we are\\nreally interested in the relative (rather than absolute) scores of the documents\\nin the collection. To this end, it sufﬁces to compute the cosine similarity from\\neach document unit vector ⃗v(d) to ⃗V(q) (in which all non-zero components\\nof the query vector are set to 1), rather than to the unit vector ⃗v(q). For any\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n136\\n7\\nComputing scores in a complete search system\\nFASTCOSINESCORE(q)\\n1\\nﬂoat Scores[N] = 0\\n2\\nfor each d\\n3\\ndo Initialize Length[d] to the length of doc d\\n4\\nfor each query term t\\n5\\ndo calculate wt,q and fetch postings list for t\\n6\\nfor each pair(d, tft,d) in postings list\\n7\\ndo add wft,d to Scores[d]\\n8\\nRead the array Length[d]\\n9\\nfor each d\\n10\\ndo Divide Scores[d] by Length[d]\\n11\\nreturn Top K components of Scores[]\\n◮Figure 7.1\\nA faster algorithm for vector space scores.\\ntwo documents d1, d2\\n⃗V(q) ·⃗v(d1) > ⃗V(q) ·⃗v(d2) ⇔⃗v(q) ·⃗v(d1) > ⃗v(q) ·⃗v(d2).\\n(7.1)\\nFor any document d, the cosine similarity ⃗V(q) ·⃗v(d) is the weighted sum,\\nover all terms in the query q, of the weights of those terms in d. This in turn\\ncan be computed by a postings intersection exactly as in the algorithm of\\nFigure 6.14, with line 8 altered since we take wt,q to be 1 so that the multiply-\\nadd in that step becomes just an addition; the result is shown in Figure 7.1.\\nWe walk through the postings in the inverted index for the terms in q, accu-\\nmulating the total score for each document – very much as in processing a\\nBoolean query, except we assign a positive score to each document that ap-\\npears in any of the postings being traversed. As mentioned in Section 6.3.3\\nwe maintain an idf value for each dictionary term and a tf value for each\\npostings entry. This scheme computes a score for every document in the\\npostings of any of the query terms; the total number of such documents may\\nbe considerably smaller than N.\\nGiven these scores, the ﬁnal step before presenting results to a user is to\\npick out the K highest-scoring documents. While one could sort the complete\\nset of scores, a better approach is to use a heap to retrieve only the top K\\ndocuments in order. Where J is the number of documents with non-zero\\ncosine scores, constructing such a heap can be performed in 2J comparison\\nsteps, following which each of the K highest scoring documents can be “read\\noff” the heap with log J comparison steps.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n137\\n7.1.1\\nInexact top K document retrieval\\nThus far, we have focused on retrieving precisely the K highest-scoring doc-\\numents for a query. We now consider schemes by which we produce K doc-\\numents that are likely to be among the K highest scoring documents for a\\nquery. In doing so, we hope to dramatically lower the cost of computing\\nthe K documents we output, without materially altering the user’s perceived\\nrelevance of the top K results. Consequently, in most applications it sufﬁces\\nto retrieve K documents whose scores are very close to those of the K best.\\nIn the sections that follow we detail schemes that retrieve K such documents\\nwhile potentially avoiding computing scores for most of the N documents in\\nthe collection.\\nSuch inexact top-K retrieval is not necessarily, from the user’s perspective,\\na bad thing. The top K documents by the cosine measure are in any case not\\nnecessarily the K best for the query: cosine similarity is only a proxy for the\\nuser’s perceived relevance. In Sections 7.1.2–7.1.6 below, we give heuristics\\nusing which we are likely to retrieve K documents with cosine scores close\\nto those of the top K documents. The principal cost in computing the out-\\nput stems from computing cosine similarities between the query and a large\\nnumber of documents. Having a large number of documents in contention\\nalso increases the selection cost in the ﬁnal stage of culling the top K docu-\\nments from a heap. We now consider a series of ideas designed to eliminate\\na large number of documents without computing their cosine scores. The\\nheuristics have the following two-step scheme:\\n1. Find a set A of documents that are contenders, where K < |A| ≪N. A\\ndoes not necessarily contain the K top-scoring documents for the query,\\nbut is likely to have many documents with scores near those of the top K.\\n2. Return the K top-scoring documents in A.\\nFrom the descriptions of these ideas it will be clear that many of them require\\nparameters to be tuned to the collection and application at hand; pointers\\nto experience in setting these parameters may be found at the end of this\\nchapter. It should also be noted that most of these heuristics are well-suited\\nto free text queries, but not for Boolean or phrase queries.\\n7.1.2\\nIndex elimination\\nFor a multi-term query q, it is clear we only consider documents containing at\\nleast one of the query terms. We can take this a step further using additional\\nheuristics:\\n1. We only consider documents containing terms whose idf exceeds a preset\\nthreshold. Thus, in the postings traversal, we only traverse the postings\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n138\\n7\\nComputing scores in a complete search system\\nfor terms with high idf. This has a fairly signiﬁcant beneﬁt: the post-\\nings lists of low-idf terms are generally long; with these removed from\\ncontention, the set of documents for which we compute cosines is greatly\\nreduced. One way of viewing this heuristic: low-idf terms are treated as\\nstop words and do not contribute to scoring. For instance, on the query\\ncatcher in the rye, we only traverse the postings for catcher and rye. The\\ncutoff threshold can of course be adapted in a query-dependent manner.\\n2. We only consider documents that contain many (and as a special case,\\nall) of the query terms. This can be accomplished during the postings\\ntraversal; we only compute scores for documents containing all (or many)\\nof the query terms. A danger of this scheme is that by requiring all (or\\neven many) query terms to be present in a document before considering\\nit for cosine computation, we may end up with fewer than K candidate\\ndocuments in the output. This issue will discussed further in Section 7.2.1.\\n7.1.3\\nChampion lists\\nThe idea of champion lists (sometimes also called fancy lists or top docs) is to\\nprecompute, for each term t in the dictionary, the set of the r documents\\nwith the highest weights for t; the value of r is chosen in advance. For tf-\\nidf weighting, these would be the r documents with the highest tf values for\\nterm t. We call this set of r documents the champion list for term t.\\nNow, given a query q we create a set A as follows: we take the union\\nof the champion lists for each of the terms comprising q. We now restrict\\ncosine computation to only the documents in A. A critical parameter in this\\nscheme is the value r, which is highly application dependent. Intuitively, r\\nshould be large compared with K, especially if we use any form of the index\\nelimination described in Section 7.1.2. One issue here is that the value r is set\\nat the time of index construction, whereas K is application dependent and\\nmay not be available until the query is received; as a result we may (as in the\\ncase of index elimination) ﬁnd ourselves with a set A that has fewer than K\\ndocuments. There is no reason to have the same value of r for all terms in the\\ndictionary; it could for instance be set to be higher for rarer terms.\\n7.1.4\\nStatic quality scores and ordering\\nWe now further develop the idea of champion lists, in the somewhat more\\ngeneral setting of static quality scores. In many search engines, we have avail-\\nSTATIC QUALITY\\nSCORES\\nable a measure of quality g(d) for each document d that is query-independent\\nand thus static. This quality measure may be viewed as a number between\\nzero and one. For instance, in the context of news stories on the web, g(d)\\nmay be derived from the number of favorable reviews of the story by web\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n139\\n◮Figure 7.2\\nA static quality-ordered index. In this example we assume that Doc1,\\nDoc2 and Doc3 respectively have static quality scores g(1) = 0.25, g(2) = 0.5, g(3) =\\n1.\\nsurfers. Section 4.6 (page 80) provides further discussion on this topic, as\\ndoes Chapter 21 in the context of web search.\\nThe net score for a document d is some combination of g(d) together with\\nthe query-dependent score induced (say) by (6.12). The precise combination\\nmay be determined by the learning methods of Section 6.1.2, to be developed\\nfurther in Section 15.4.1; but for the purposes of our exposition here, let us\\nconsider a simple sum:\\nnet-score(q, d) = g(d) +\\n⃗V(q) · ⃗V(d)\\n|⃗V(q)||⃗V(d)|\\n.\\n(7.2)\\nIn this simple form, the static quality g(d) and the query-dependent score\\nfrom (6.10) have equal contributions, assuming each is between 0 and 1.\\nOther relative weightings are possible; the effectiveness of our heuristics will\\ndepend on the speciﬁc relative weighting.\\nFirst, consider ordering the documents in the postings list for each term by\\ndecreasing value of g(d). This allows us to perform the postings intersection\\nalgorithm of Figure 1.6. In order to perform the intersection by a single pass\\nthrough the postings of each query term, the algorithm of Figure 1.6 relied on\\nthe postings being ordered by document IDs. But in fact, we only required\\nthat all postings be ordered by a single common ordering; here we rely on the\\ng(d) values to provide this common ordering. This is illustrated in Figure 7.2,\\nwhere the postings are ordered in decreasing order of g(d).\\nThe ﬁrst idea is a direct extension of champion lists: for a well-chosen\\nvalue r, we maintain for each term t a global champion list of the r documents\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n140\\n7\\nComputing scores in a complete search system\\nwith the highest values for g(d) + tf-idft,d. The list itself is, like all the post-\\nings lists considered so far, sorted by a common order (either by document\\nIDs or by static quality). Then at query time, we only compute the net scores\\n(7.2) for documents in the union of these global champion lists. Intuitively,\\nthis has the effect of focusing on documents likely to have large net scores.\\nWe conclude the discussion of global champion lists with one further idea.\\nWe maintain for each term t two postings lists consisting of disjoint sets of\\ndocuments, each sorted by g(d) values. The ﬁrst list, which we call high,\\ncontains the m documents with the highest tf values for t. The second list,\\nwhich we call low, contains all other documents containing t. When process-\\ning a query, we ﬁrst scan only the high lists of the query terms, computing\\nnet scores for any document on the high lists of all (or more than a certain\\nnumber of) query terms. If we obtain scores for K documents in the process,\\nwe terminate. If not, we continue the scanning into the low lists, scoring doc-\\numents in these postings lists. This idea is developed further in Section 7.2.1.\\n7.1.5\\nImpact ordering\\nIn all the postings lists described thus far, we order the documents con-\\nsistently by some common ordering: typically by document ID but in Sec-\\ntion 7.1.4 by static quality scores. As noted at the end of Section 6.3.3, such a\\ncommon ordering supports the concurrent traversal of all of the query terms’\\npostings lists, computing the score for each document as we encounter it.\\nComputing scores in this manner is sometimes referred to as document-at-a-\\ntime scoring. We will now introduce a technique for inexact top-K retrieval\\nin which the postings are not all ordered by a common ordering, thereby\\nprecluding such a concurrent traversal. We will therefore require scores to\\nbe “accumulated” one term at a time as in the scheme of Figure 6.14, so that\\nwe have term-at-a-time scoring.\\nThe idea is to order the documents d in the postings list of term t by\\ndecreasing order of tft,d. Thus, the ordering of documents will vary from\\none postings list to another, and we cannot compute scores by a concurrent\\ntraversal of the postings lists of all query terms. Given postings lists ordered\\nby decreasing order of tft,d, two ideas have been found to signiﬁcantly lower\\nthe number of documents for which we accumulate scores: (1) when travers-\\ning the postings list for a query term t, we stop after considering a preﬁx\\nof the postings list – either after a ﬁxed number of documents r have been\\nseen, or after the value of tft,d has dropped below a threshold; (2) when ac-\\ncumulating scores in the outer loop of Figure 6.14, we consider the query\\nterms in decreasing order of idf, so that the query terms likely to contribute\\nthe most to the ﬁnal scores are considered ﬁrst. This latter idea too can be\\nadaptive at the time of processing a query: as we get to query terms with\\nlower idf, we can determine whether to proceed based on the changes in\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n141\\ndocument scores from processing the previous query term. If these changes\\nare minimal, we may omit accumulation from the remaining query terms, or\\nalternatively process shorter preﬁxes of their postings lists.\\nThese ideas form a common generalization of the methods introduced in\\nSections 7.1.2–7.1.4. We may also implement a version of static ordering in\\nwhich each postings list is ordered by an additive combination of static and\\nquery-dependent scores. We would again lose the consistency of ordering\\nacross postings, thereby having to process query terms one at time accumu-\\nlating scores for all documents as we go along. Depending on the particular\\nscoring function, the postings list for a document may be ordered by other\\nquantities than term frequency; under this more general setting, this idea is\\nknown as impact ordering.\\n7.1.6\\nCluster pruning\\nIn cluster pruning we have a preprocessing step during which we cluster the\\ndocument vectors. Then at query time, we consider only documents in a\\nsmall number of clusters as candidates for which we compute cosine scores.\\nSpeciﬁcally, the preprocessing step is as follows:\\n1. Pick\\n√\\nN documents at random from the collection. Call these leaders.\\n2. For each document that is not a leader, we compute its nearest leader.\\nWe refer to documents that are not leaders as followers. Intuitively, in the par-\\ntition of the followers induced by the use of\\n√\\nN randomly chosen leaders,\\nthe expected number of followers for each leader is ≈N/\\n√\\nN =\\n√\\nN. Next,\\nquery processing proceeds as follows:\\n1. Given a query q, ﬁnd the leader L that is closest to q. This entails comput-\\ning cosine similarities from q to each of the\\n√\\nN leaders.\\n2. The candidate set A consists of L together with its followers. We compute\\nthe cosine scores for all documents in this candidate set.\\nThe use of randomly chosen leaders for clustering is fast and likely to re-\\nﬂect the distribution of the document vectors in the vector space: a region\\nof the vector space that is dense in documents is likely to produce multi-\\nple leaders and thus a ﬁner partition into sub-regions. This illustrated in\\nFigure 7.3.\\nVariations of cluster pruning introduce additional parameters b1 and b2,\\nboth of which are positive integers. In the pre-processing step we attach\\neach follower to its b1 closest leaders, rather than a single closest leader. At\\nquery time we consider the b2 leaders closest to the query q. Clearly, the basic\\nscheme above corresponds to the case b1 = b2 = 1. Further, increasing b1 or\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n142\\n7\\nComputing scores in a complete search system\\n◮Figure 7.3\\nCluster pruning.\\nb2 increases the likelihood of ﬁnding K documents that are more likely to be\\nin the set of true top-scoring K documents, at the expense of more compu-\\ntation. We reiterate this approach when describing clustering in Chapter 16\\n(page 354).\\n?\\nExercise 7.1\\nWe suggested above (Figure 7.2) that the postings for static quality ordering be in\\ndecreasing order of g(d). Why do we use the decreasing rather than the increasing\\norder?\\nExercise 7.2\\nWhen discussing champion lists, we simply used the r documents with the largest tf\\nvalues to create the champion list for t. But when considering global champion lists,\\nwe used idf as well, identifying documents with the largest values of g(d) + tf-idft,d.\\nWhy do we differentiate between these two cases?\\nExercise 7.3\\nIf we were to only have one-term queries, explain why the use of global champion\\nlists with r = K sufﬁces for identifying the K highest scoring documents. What is a\\nsimple modiﬁcation to this idea if we were to only have s-term queries for any ﬁxed\\ninteger s > 1?\\nExercise 7.4\\nExplain how the common global ordering by g(d) values in all high and low lists\\nhelps make the score computation efﬁcient.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.2\\nComponents of an information retrieval system\\n143\\nExercise 7.5\\nConsider again the data of Exercise 6.23 with nnn.atc for the query-dependent scor-\\ning. Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2.\\nDetermine under Equation (7.2) what ranges of static quality score for Doc3 result in\\nit being the ﬁrst, second or third result for the query best car insurance.\\nExercise 7.6\\nSketch the frequency-ordered postings for the data in Figure 6.9.\\nExercise 7.7\\nLet the static quality scores for Doc1, Doc2 and Doc3 in Figure 6.11 be respectively\\n0.25, 0.5 and 1. Sketch the postings for impact ordering when each postings list is\\nordered by the sum of the static quality score and the Euclidean normalized tf values\\nin Figure 6.11.\\nExercise 7.8\\nThe nearest-neighbor problem in the plane is the following: given a set of N data\\npoints on the plane, we preprocess them into some data structure such that, given\\na query point Q, we seek the point in N that is closest to Q in Euclidean distance.\\nClearly cluster pruning can be used as an approach to the nearest-neighbor problem\\nin the plane, if we wished to avoid computing the distance from Q to every one of\\nthe query points. Devise a simple example on the plane so that with two leaders, the\\nanswer returned by cluster pruning is incorrect (it is not the data point closest to Q).\\n7.2\\nComponents of an information retrieval system\\nIn this section we combine the ideas developed so far to describe a rudimen-\\ntary search system that retrieves and scores documents. We ﬁrst develop\\nfurther ideas for scoring, beyond vector spaces. Following this, we will put\\ntogether all of these elements to outline a complete system. Because we con-\\nsider a complete system, we do not restrict ourselves to vector space retrieval\\nin this section. Indeed, our complete system will have provisions for vector\\nspace as well as other query operators and forms of retrieval. In Section 7.3\\nwe will return to how vector space queries interact with other query opera-\\ntors.\\n7.2.1\\nTiered indexes\\nWe mentioned in Section 7.1.2 that when using heuristics such as index elim-\\nination for inexact top-K retrieval, we may occasionally ﬁnd ourselves with\\na set A of contenders that has fewer than K documents. A common solution\\nto this issue is the user of tiered indexes, which may be viewed as a gener-\\nTIERED INDEXES\\nalization of champion lists. We illustrate this idea in Figure 7.4, where we\\nrepresent the documents and terms of Figure 6.9. In this example we set a tf\\nthreshold of 20 for tier 1 and 10 for tier 2, meaning that the tier 1 index only\\nhas postings entries with tf values exceeding 20, while the tier 2 index only\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n144\\n7\\nComputing scores in a complete search system\\n◮Figure 7.4\\nTiered indexes. If we fail to get K results from tier 1, query processing\\n“falls back” to tier 2, and so on. Within each tier, postings are ordered by document\\nID.\\nhas postings entries with tf values exceeding 10. In this example we have\\nchosen to order the postings entries within a tier by document ID.\\n7.2.2\\nQuery-term proximity\\nEspecially for free text queries on the web (Chapter 19), users prefer a doc-\\nument in which most or all of the query terms appear close to each other,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.2\\nComponents of an information retrieval system\\n145\\nbecause this is evidence that the document has text focused on their query\\nintent. Consider a query with two or more query terms, t1, t2, . . . , tk. Let ω\\nbe the width of the smallest window in a document d that contains all the\\nquery terms, measured in the number of words in the window. For instance,\\nif the document were to simply consist of the sentence The quality of mercy\\nis not strained, the smallest window for the query strained mercy would be 4.\\nIntuitively, the smaller that ω is, the better that d matches the query. In cases\\nwhere the document does not contain all of the query terms, we can set ω\\nto be some enormous number. We could also consider variants in which\\nonly words that are not stop words are considered in computing ω. Such\\nproximity-weighted scoring functions are a departure from pure cosine sim-\\nilarity and closer to the “soft conjunctive” semantics that Google and other\\nweb search engines evidently use.\\nHow can we design such a proximity-weighted scoring function to depend\\nPROXIMITY WEIGHTING\\non ω? The simplest answer relies on a “hand coding” technique we introduce\\nbelow in Section 7.2.3. A more scalable approach goes back to Section 6.1.2 –\\nwe treat the integer ω as yet another feature in the scoring function, whose\\nimportance is assigned by machine learning, as will be developed further in\\nSection 15.4.1.\\n7.2.3\\nDesigning parsing and scoring functions\\nCommon search interfaces, particularly for consumer-facing search applica-\\ntions on the web, tend to mask query operators from the end user. The intent\\nis to hide the complexity of these operators from the largely non-technical au-\\ndience for such applications, inviting free text queries. Given such interfaces,\\nhow should a search equipped with indexes for various retrieval operators\\ntreat a query such as rising interest rates? More generally, given the various fac-\\ntors we have studied that could affect the score of a document, how should\\nwe combine these features?\\nThe answer of course depends on the user population, the query distri-\\nbution and the collection of documents. Typically, a query parser is used to\\ntranslate the user-speciﬁed keywords into a query with various operators\\nthat is executed against the underlying indexes. Sometimes, this execution\\ncan entail multiple queries against the underlying indexes; for example, the\\nquery parser may issue a stream of queries:\\n1. Run the user-generated query string as a phrase query. Rank them by\\nvector space scoring using as query the vector consisting of the 3 terms\\nrising interest rates.\\n2. If fewer than ten documents contain the phrase rising interest rates, run the\\ntwo 2-term phrase queries rising interest and interest rates; rank these using\\nvector space scoring, as well.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n146\\n7\\nComputing scores in a complete search system\\n3. If we still have fewer than ten results, run the vector space query consist-\\ning of the three individual query terms.\\nEach of these steps (if invoked) may yield a list of scored documents, for\\neach of which we compute a score. This score must combine contributions\\nfrom vector space scoring, static quality, proximity weighting and potentially\\nother factors – particularly since a document may appear in the lists from\\nmultiple steps. This demands an aggregate scoring function that accumulates\\nEVIDENCE\\nACCUMULATION\\nevidence of a document’s relevance from multiple sources. How do we devise\\na query parser and how do we devise the aggregate scoring function?\\nThe answer depends on the setting. In many enterprise settings we have\\napplication builders who make use of a toolkit of available scoring opera-\\ntors, along with a query parsing layer, with which to manually conﬁgure\\nthe scoring function as well as the query parser. Such application builders\\nmake use of the available zones, metadata and knowledge of typical doc-\\numents and queries to tune the parsing and scoring. In collections whose\\ncharacteristics change infrequently (in an enterprise application, signiﬁcant\\nchanges in collection and query characteristics typically happen with infre-\\nquent events such as the introduction of new document formats or document\\nmanagement systems, or a merger with another company). Web search on\\nthe other hand is faced with a constantly changing document collection with\\nnew characteristics being introduced all the time. It is also a setting in which\\nthe number of scoring factors can run into the hundreds, making hand-tuned\\nscoring a difﬁcult exercise. To address this, it is becoming increasingly com-\\nmon to use machine-learned scoring, extending the ideas we introduced in\\nSection 6.1.2, as will be discussed further in Section 15.4.1.\\n7.2.4\\nPutting it all together\\nWe have now studied all the components necessary for a basic search system\\nthat supports free text queries as well as Boolean, zone and ﬁeld queries. We\\nbrieﬂy review how the various pieces ﬁt together into an overall system; this\\nis depicted in Figure 7.5.\\nIn this ﬁgure, documents stream in from the left for parsing and linguis-\\ntic processing (language and format detection, tokenization and stemming).\\nThe resulting stream of tokens feeds into two modules. First, we retain a\\ncopy of each parsed document in a document cache. This will enable us\\nto generate results snippets: snippets of text accompanying each document\\nin the results list for a query. This snippet tries to give a succinct explana-\\ntion to the user of why the document matches the query. The automatic\\ngeneration of such snippets is the subject of Section 8.7.\\nA second copy\\nof the tokens is fed to a bank of indexers that create a bank of indexes in-\\ncluding zone and ﬁeld indexes that store the metadata for each document,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.3\\nVector space scoring and query operator interaction\\n147\\n◮Figure 7.5\\nA complete search system. Data paths are shown primarily for a free\\ntext query.\\n(tiered) positional indexes, indexes for spelling correction and other tolerant\\nretrieval, and structures for accelerating inexact top-K retrieval. A free text\\nuser query (top center) is sent down to the indexes both directly and through\\na module for generating spelling-correction candidates. As noted in Chap-\\nter 3 the latter may optionally be invoked only when the original query fails\\nto retrieve enough results. Retrieved documents (dark arrow) are passed\\nto a scoring module that computes scores based on machine-learned rank-\\ning (MLR), a technique that builds on Section 6.1.2 (to be further developed\\nin Section 15.4.1) for scoring and ranking documents. Finally, these ranked\\ndocuments are rendered as a results page.\\n?\\nExercise 7.9\\nExplain how the postings intersection algorithm ﬁrst introduced in Section 1.3 can be\\nadapted to ﬁnd the smallest integer ω that contains all query terms.\\nExercise 7.10\\nAdapt this procedure to work when not all query terms are present in a document.\\n7.3\\nVector space scoring and query operator interaction\\nWe introduced the vector space model as a paradigm for free text queries.\\nWe conclude this chapter by discussing how the vector space scoring model\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n148\\n7\\nComputing scores in a complete search system\\nrelates to the query operators we have studied in earlier chapters. The re-\\nlationship should be viewed at two levels: in terms of the expressiveness\\nof queries that a sophisticated user may pose, and in terms of the index that\\nsupports the evaluation of the various retrieval methods. In building a search\\nengine, we may opt to support multiple query operators for an end user. In\\ndoing so we need to understand what components of the index can be shared\\nfor executing various query operators, as well as how to handle user queries\\nthat mix various query operators.\\nVector space scoring supports so-called free text retrieval, in which a query\\nis speciﬁed as a set of words without any query operators connecting them. It\\nallows documents matching the query to be scored and thus ranked, unlike\\nthe Boolean, wildcard and phrase queries studied earlier. Classically, the\\ninterpretation of such free text queries was that at least one of the query terms\\nbe present in any retrieved document. However more recently, web search\\nengines such as Google have popularized the notion that a set of terms typed\\ninto their query boxes (thus on the face of it, a free text query) carries the\\nsemantics of a conjunctive query that only retrieves documents containing\\nall or most query terms.\\nBoolean retrieval\\nClearly a vector space index can be used to answer Boolean queries, as long\\nas the weight of a term t in the document vector for d is non-zero when-\\never t occurs in d. The reverse is not true, since a Boolean index does not by\\ndefault maintain term weight information. There is no easy way of combin-\\ning vector space and Boolean queries from a user’s standpoint: vector space\\nqueries are fundamentally a form of evidence accumulation, where the pres-\\nence of more query terms in a document adds to the score of a document.\\nBoolean retrieval on the other hand, requires a user to specify a formula\\nfor selecting documents through the presence (or absence) of speciﬁc com-\\nbinations of keywords, without inducing any relative ordering among them.\\nMathematically, it is in fact possible to invoke so-called p-norms to combine\\nBoolean and vector space queries, but we know of no system that makes use\\nof this fact.\\nWildcard queries\\nWildcard and vector space queries require different indexes, except at the\\nbasic level that both can be implemented using postings and a dictionary\\n(e.g., a dictionary of trigrams for wildcard queries). If a search engine allows\\na user to specify a wildcard operator as part of a free text query (for instance,\\nthe query rom* restaurant), we may interpret the wildcard component of the\\nquery as spawning multiple terms in the vector space (in this example, rome\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n7.4\\nReferences and further reading\\n149\\nand roman would be two such terms) all of which are added to the query\\nvector. The vector space query is then executed as usual, with matching\\ndocuments being scored and ranked; thus a document containing both rome\\nand roma is likely to be scored higher than another containing only one of\\nthem. The exact score ordering will of course depend on the relative weights\\nof each term in matching documents.\\nPhrase queries\\nThe representation of documents as vectors is fundamentally lossy: the rel-\\native order of terms in a document is lost in the encoding of a document as\\na vector. Even if we were to try and somehow treat every biword as a term\\n(and thus an axis in the vector space), the weights on different axes not in-\\ndependent: for instance the phrase German shepherd gets encoded in the axis\\ngerman shepherd, but immediately has a non-zero weight on the axes german\\nand shepherd. Further, notions such as idf would have to be extended to such\\nbiwords. Thus an index built for vector space retrieval cannot, in general, be\\nused for phrase queries. Moreover, there is no way of demanding a vector\\nspace score for a phrase query — we only know the relative weights of each\\nterm in a document.\\nOn the query german shepherd, we could use vector space retrieval to iden-\\ntify documents heavy in these two terms, with no way of prescribing that\\nthey occur consecutively. Phrase retrieval, on the other hand, tells us of the\\nexistence of the phrase german shepherd in a document, without any indi-\\ncation of the relative frequency or weight of this phrase. While these two\\nretrieval paradigms (phrase and vector space) consequently have different\\nimplementations in terms of indexes and retrieval algorithms, they can in\\nsome cases be combined usefully, as in the three-step example of query pars-\\ning in Section 7.2.3.\\n7.4\\nReferences and further reading\\nHeuristics for fast query processing with early termination are described by\\nAnh et al. (2001), Garcia et al. (2004), Anh and Moffat (2006b), Persin et al.\\n(1996). Cluster pruning is investigated by Singitham et al. (2004) and by\\nChierichetti et al. (2007); see also Section 16.6 (page 372). Champion lists are\\ndescribed in Persin (1994) and (under the name top docs) in Brown (1995),\\nTOP DOCS\\nand further developed in Brin and Page (1998), Long and Suel (2003). While\\nthese heuristics are well-suited to free text queries that can be viewed as vec-\\ntors, they complicate phrase queries; see Anh and Moffat (2006c) for an index\\nstructure that supports both weighted and Boolean/phrase searches. Carmel\\net al. (2001) Clarke et al. (2000) and Song et al. (2005) treat the use of query\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n150\\n7\\nComputing scores in a complete search system\\nterm proximity in assessing relevance. Pioneering work on learning of rank-\\ning functions was done by Fuhr (1989), Fuhr and Pfeifer (1994), Cooper et al.\\n(1994), Bartell (1994), Bartell et al. (1998) and by Cohen et al. (1998).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n151\\n8\\nEvaluation in information\\nretrieval\\nWe have seen in the preceding chapters many alternatives in designing an IR\\nsystem. How do we know which of these techniques are effective in which\\napplications? Should we use stop lists? Should we stem? Should we use in-\\nverse document frequency weighting? Information retrieval has developed\\nas a highly empirical discipline, requiring careful and thorough evaluation to\\ndemonstrate the superior performance of novel techniques on representative\\ndocument collections.\\nIn this chapter we begin with a discussion of measuring the effectiveness\\nof IR systems (Section 8.1) and the test collections that are most often used\\nfor this purpose (Section 8.2). We then present the straightforward notion of\\nrelevant and nonrelevant documents and the formal evaluation methodol-\\nogy that has been developed for evaluating unranked retrieval results (Sec-\\ntion 8.3). This includes explaining the kinds of evaluation measures that\\nare standardly used for document retrieval and related tasks like text clas-\\nsiﬁcation and why they are appropriate. We then extend these notions and\\ndevelop further measures for evaluating ranked retrieval results (Section 8.4)\\nand discuss developing reliable and informative test collections (Section 8.5).\\nWe then step back to introduce the notion of user utility, and how it is ap-\\nproximated by the use of document relevance (Section 8.6). The key utility\\nmeasure is user happiness. Speed of response and the size of the index are\\nfactors in user happiness. It seems reasonable to assume that relevance of\\nresults is the most important factor: blindingly fast, useless answers do not\\nmake a user happy. However, user perceptions do not always coincide with\\nsystem designers’ notions of quality. For example, user happiness commonly\\ndepends very strongly on user interface design issues, including the layout,\\nclarity, and responsiveness of the user interface, which are independent of\\nthe quality of the results returned. We touch on other measures of the qual-\\nity of a system, in particular the generation of high-quality result summary\\nsnippets, which strongly inﬂuence user utility, but are not measured in the\\nbasic relevance ranking paradigm (Section 8.7).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n152\\n8\\nEvaluation in information retrieval\\n8.1\\nInformation retrieval system evaluation\\nTo measure ad hoc information retrieval effectiveness in the standard way,\\nwe need a test collection consisting of three things:\\n1. A document collection\\n2. A test suite of information needs, expressible as queries\\n3. A set of relevance judgments, standardly a binary assessment of either\\nrelevant or nonrelevant for each query-document pair.\\nThe standard approach to information retrieval system evaluation revolves\\naround the notion of relevant and nonrelevant documents. With respect to a\\nRELEVANCE\\nuser information need, a document in the test collection is given a binary\\nclassiﬁcation as either relevant or nonrelevant. This decision is referred to as\\nthe gold standard or ground truth judgment of relevance. The test document\\nGOLD STANDARD\\nGROUND TRUTH\\ncollection and suite of information needs have to be of a reasonable size:\\nyou need to average performance over fairly large test sets, as results are\\nhighly variable over different documents and information needs. As a rule\\nof thumb, 50 information needs has usually been found to be a sufﬁcient\\nminimum.\\nRelevance is assessed relative to an information need, not a query. For\\nINFORMATION NEED\\nexample, an information need might be:\\nInformation on whether drinking red wine is more effective at reduc-\\ning your risk of heart attacks than white wine.\\nThis might be translated into a query such as:\\nwine AND red AND white AND heart AND attack AND effective\\nA document is relevant if it addresses the stated information need, not be-\\ncause it just happens to contain all the words in the query. This distinction is\\noften misunderstood in practice, because the information need is not overt.\\nBut, nevertheless, an information need is present. If a user types python into a\\nweb search engine, they might be wanting to know where they can purchase\\na pet python. Or they might be wanting information on the programming\\nlanguage Python. From a one word query, it is very difﬁcult for a system to\\nknow what the information need is. But, nevertheless, the user has one, and\\ncan judge the returned results on the basis of their relevance to it. To evalu-\\nate a system, we require an overt expression of an information need, which\\ncan be used for judging returned documents as relevant or nonrelevant. At\\nthis point, we make a simpliﬁcation: relevance can reasonably be thought\\nof as a scale, with some documents highly relevant and others marginally\\nso. But for the moment, we will use just a binary decision of relevance. We\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.2\\nStandard test collections\\n153\\ndiscuss the reasons for using binary relevance judgments and alternatives in\\nSection 8.5.1.\\nMany systems contain various weights (often known as parameters) that\\ncan be adjusted to tune system performance. It is wrong to report results on\\na test collection which were obtained by tuning these parameters to maxi-\\nmize performance on that collection. That is because such tuning overstates\\nthe expected performance of the system, because the weights will be set to\\nmaximize performance on one particular set of queries rather than for a ran-\\ndom sample of queries. In such cases, the correct procedure is to have one\\nor more development test collections, and to tune the parameters on the devel-\\nDEVELOPMENT TEST\\nCOLLECTION\\nopment test collection. The tester then runs the system with those weights\\non the test collection and reports the results on that collection as an unbiased\\nestimate of performance.\\n8.2\\nStandard test collections\\nHere is a list of the most standard test collections and evaluation series. We\\nfocus particularly on test collections for ad hoc information retrieval system\\nevaluation, but also mention a couple of similar test collections for text clas-\\nsiﬁcation.\\nThe Cranﬁeld collection. This was the pioneering test collection in allowing\\nCRANFIELD\\nprecise quantitative measures of information retrieval effectiveness, but\\nis nowadays too small for anything but the most elementary pilot experi-\\nments. Collected in the United Kingdom starting in the late 1950s, it con-\\ntains 1398 abstracts of aerodynamics journal articles, a set of 225 queries,\\nand exhaustive relevance judgments of all (query, document) pairs.\\nText Retrieval Conference (TREC). The U.S. National Institute of Standards\\nTREC\\nand Technology (NIST) has run a large IR test bed evaluation series since\\n1992. Within this framework, there have been many tracks over a range\\nof different test collections, but the best known test collections are the\\nones used for the TREC Ad Hoc track during the ﬁrst 8 TREC evaluations\\nbetween 1992 and 1999. In total, these test collections comprise 6 CDs\\ncontaining 1.89 million documents (mainly, but not exclusively, newswire\\narticles) and relevance judgments for 450 information needs, which are\\ncalled topics and speciﬁed in detailed text passages. Individual test col-\\nlections are deﬁned over different subsets of this data. The early TRECs\\neach consisted of 50 information needs, evaluated over different but over-\\nlapping sets of documents. TRECs 6–8 provide 150 information needs\\nover about 528,000 newswire and Foreign Broadcast Information Service\\narticles. This is probably the best subcollection to use in future work, be-\\ncause it is the largest and the topics are more consistent. Because the test\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n154\\n8\\nEvaluation in information retrieval\\ndocument collections are so large, there are no exhaustive relevance judg-\\nments. Rather, NIST assessors’ relevance judgments are available only for\\nthe documents that were among the top k returned for some system which\\nwas entered in the TREC evaluation for which the information need was\\ndeveloped.\\nIn more recent years, NIST has done evaluations on larger document col-\\nlections, including the 25 million page GOV2 web page collection. From\\nGOV2\\nthe beginning, the NIST test document collections were orders of magni-\\ntude larger than anything available to researchers previously and GOV2\\nis now the largest Web collection easily available for research purposes.\\nNevertheless, the size of GOV2 is still more than 2 orders of magnitude\\nsmaller than the current size of the document collections indexed by the\\nlarge web search companies.\\nNII Test Collections for IR Systems (NTCIR). The NTCIR project has built\\nNTCIR\\nvarious test collections of similar sizes to the TREC collections, focus-\\ning on East Asian language and cross-language information retrieval, where\\nCROSS-LANGUAGE\\nINFORMATION\\nRETRIEVAL\\nqueries are made in one language over a document collection containing\\ndocuments in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-\\nen.html\\nCross Language Evaluation Forum (CLEF). This evaluation series has con-\\nCLEF\\ncentrated on European languages and cross-language information retrieval.\\nSee: http://www.clef-campaign.org/\\nReuters-21578 and Reuters-RCV1. For text classiﬁcation, the most used test\\nREUTERS\\ncollection has been the Reuters-21578 collection of 21578 newswire arti-\\ncles; see Chapter 13, page 279. More recently, Reuters released the much\\nlarger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents;\\nsee Chapter 4, page 69. Its scale and rich annotation makes it a better basis\\nfor future research.\\n20 Newsgroups. This is another widely used text classiﬁcation collection,\\n20 NEWSGROUPS\\ncollected by Ken Lang. It consists of 1000 articles from each of 20 Usenet\\nnewsgroups (the newsgroup name being regarded as the category). After\\nthe removal of duplicate articles, as it is usually used, it contains 18941\\narticles.\\n8.3\\nEvaluation of unranked retrieval sets\\nGiven these ingredients, how is system effectiveness measured? The two\\nmost frequent and basic measures for information retrieval effectiveness are\\nprecision and recall. These are ﬁrst deﬁned for the simple case where an\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.3\\nEvaluation of unranked retrieval sets\\n155\\nIR system returns a set of documents for a query. We will see later how to\\nextend these notions to ranked retrieval situations.\\nPrecision (P) is the fraction of retrieved documents that are relevant\\nPRECISION\\nPrecision = #(relevant items retrieved)\\n#(retrieved items)\\n= P(relevant|retrieved)\\n(8.1)\\nRecall (R) is the fraction of relevant documents that are retrieved\\nRECALL\\nRecall = #(relevant items retrieved)\\n#(relevant items)\\n= P(retrieved|relevant)\\n(8.2)\\nThese notions can be made clear by examining the following contingency\\ntable:\\n(8.3)\\nRelevant\\nNonrelevant\\nRetrieved\\ntrue positives (tp)\\nfalse positives (fp)\\nNot retrieved\\nfalse negatives (fn)\\ntrue negatives (tn)\\nThen:\\nP\\n=\\ntp/(tp + f p)\\n(8.4)\\nR\\n=\\ntp/(tp + f n)\\nAn obvious alternative that may occur to the reader is to judge an infor-\\nmation retrieval system by its accuracy, that is, the fraction of its classiﬁca-\\nACCURACY\\ntions that are correct. In terms of the contingency table above, accuracy =\\n(tp + tn)/(tp + f p + f n + tn). This seems plausible, since there are two ac-\\ntual classes, relevant and nonrelevant, and an information retrieval system\\ncan be thought of as a two-class classiﬁer which attempts to label them as\\nsuch (it retrieves the subset of documents which it believes to be relevant).\\nThis is precisely the effectiveness measure often used for evaluating machine\\nlearning classiﬁcation problems.\\nThere is a good reason why accuracy is not an appropriate measure for\\ninformation retrieval problems. In almost all circumstances, the data is ex-\\ntremely skewed: normally over 99.9% of the documents are in the nonrele-\\nvant category. A system tuned to maximize accuracy can appear to perform\\nwell by simply deeming all documents nonrelevant to all queries. Even if the\\nsystem is quite good, trying to label some documents as relevant will almost\\nalways lead to a high rate of false positives. However, labeling all documents\\nas nonrelevant is completely unsatisfying to an information retrieval system\\nuser. Users are always going to want to see some documents, and can be\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n156\\n8\\nEvaluation in information retrieval\\nassumed to have a certain tolerance for seeing some false positives provid-\\ning that they get some useful information. The measures of precision and\\nrecall concentrate the evaluation on the return of true positives, asking what\\npercentage of the relevant documents have been found and how many false\\npositives have also been returned.\\nThe advantage of having the two numbers for precision and recall is that\\none is more important than the other in many circumstances. Typical web\\nsurfers would like every result on the ﬁrst page to be relevant (high preci-\\nsion) but have not the slightest interest in knowing let alone looking at every\\ndocument that is relevant. In contrast, various professional searchers such as\\nparalegals and intelligence analysts are very concerned with trying to get as\\nhigh recall as possible, and will tolerate fairly low precision results in order to\\nget it. Individuals searching their hard disks are also often interested in high\\nrecall searches. Nevertheless, the two quantities clearly trade off against one\\nanother: you can always get a recall of 1 (but very low precision) by retriev-\\ning all documents for all queries! Recall is a non-decreasing function of the\\nnumber of documents retrieved. On the other hand, in a good system, preci-\\nsion usually decreases as the number of documents retrieved is increased. In\\ngeneral we want to get some amount of recall while tolerating only a certain\\npercentage of false positives.\\nA single measure that trades off precision versus recall is the F measure,\\nF MEASURE\\nwhich is the weighted harmonic mean of precision and recall:\\nF =\\n1\\nα 1\\nP + (1 −α) 1\\nR\\n= (β2 + 1)PR\\nβ2P + R\\nwhere\\nβ2 = 1 −α\\nα\\n(8.5)\\nwhere α ∈[0, 1] and thus β2 ∈[0, ∞]. The default balanced F measure equally\\nweights precision and recall, which means making α = 1/2 or β = 1. It is\\ncommonly written as F1, which is short for Fβ=1, even though the formula-\\ntion in terms of α more transparently exhibits the F measure as a weighted\\nharmonic mean. When using β = 1, the formula on the right simpliﬁes to:\\nFβ=1 = 2PR\\nP + R\\n(8.6)\\nHowever, using an even weighting is not the only choice. Values of β < 1\\nemphasize precision, while values of β > 1 emphasize recall. For example, a\\nvalue of β = 3 or β = 5 might be used if recall is to be emphasized. Recall,\\nprecision, and the F measure are inherently measures between 0 and 1, but\\nthey are also very commonly written as percentages, on a scale between 0\\nand 100.\\nWhy do we use a harmonic mean rather than the simpler average (arith-\\nmetic mean)? Recall that we can always get 100% recall by just returning all\\ndocuments, and therefore we can always get a 50% arithmetic mean by the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.3\\nEvaluation of unranked retrieval sets\\n157\\n0\\n2\\n0\\n4\\n0\\n6\\n0\\n8\\n0\\n1\\n0\\n0\\n0\\n2\\n0\\n4\\n0\\n6\\n0\\n8\\n0\\n1\\n0\\n0\\nP\\nr\\ne\\nc\\ni\\ns\\ni\\no\\nn\\n(\\nR\\ne\\nc\\na\\nl\\nl\\nf\\ni\\nx\\ne\\nd\\na\\nt\\n7\\n0\\n%\\n)\\nM\\ni\\nn\\ni\\nm\\nu\\nm\\nM\\na\\nx\\ni\\nm\\nu\\nm\\nA\\nr\\ni\\nt\\nh\\nm\\ne\\nt\\ni\\nc\\nG\\ne\\no\\nm\\ne\\nt\\nr\\ni\\nc\\nH\\na\\nr\\nm\\no\\nn\\ni\\nc\\n◮Figure 8.1\\nGraph comparing the harmonic mean to other means.\\nThe graph\\nshows a slice through the calculation of various means of precision and recall for\\nthe ﬁxed recall value of 70%. The harmonic mean is always less than either the arith-\\nmetic or geometric mean, and often quite close to the minimum of the two numbers.\\nWhen the precision is also 70%, all the measures coincide.\\nsame process. This strongly suggests that the arithmetic mean is an unsuit-\\nable measure to use. In contrast, if we assume that 1 document in 10,000 is\\nrelevant to the query, the harmonic mean score of this strategy is 0.02%. The\\nharmonic mean is always less than or equal to the arithmetic mean and the\\ngeometric mean. When the values of two numbers differ greatly, the har-\\nmonic mean is closer to their minimum than to their arithmetic mean; see\\nFigure 8.1.\\n?\\nExercise 8.1\\n[⋆]\\nAn IR system returns 8 relevant documents, and 10 nonrelevant documents. There\\nare a total of 20 relevant documents in the collection. What is the precision of the\\nsystem on this search, and what is its recall?\\nExercise 8.2\\n[⋆]\\nThe balanced F measure (a.k.a. F1) is deﬁned as the harmonic mean of precision and\\nrecall. What is the advantage of using the harmonic mean rather than “averaging”\\n(using the arithmetic mean)?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n158\\n8\\nEvaluation in information retrieval\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRecall\\nPrecision\\n◮Figure 8.2\\nPrecision/recall graph.\\nExercise 8.3\\n[⋆⋆]\\nDerive the equivalence between the two formulas for F measure shown in Equa-\\ntion (8.5), given that α = 1/(β2 + 1).\\n8.4\\nEvaluation of ranked retrieval results\\nPrecision, recall, and the F measure are set-based measures. They are com-\\nputed using unordered sets of documents. We need to extend these measures\\n(or to deﬁne new measures) if we are to evaluate the ranked retrieval results\\nthat are now standard with search engines. In a ranked retrieval context,\\nappropriate sets of retrieved documents are naturally given by the top k re-\\ntrieved documents. For each such set, precision and recall values can be\\nplotted to give a precision-recall curve, such as the one shown in Figure 8.2.\\nPRECISION-RECALL\\nCURVE\\nPrecision-recall curves have a distinctive saw-tooth shape: if the (k + 1)th\\ndocument retrieved is nonrelevant then recall is the same as for the top k\\ndocuments, but precision has dropped. If it is relevant, then both precision\\nand recall increase, and the curve jags up and to the right. It is often useful to\\nremove these jiggles and the standard way to do this is with an interpolated\\nprecision: the interpolated precision pinterp at a certain recall level r is deﬁned\\nINTERPOLATED\\nPRECISION\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n159\\nRecall\\nInterp.\\nPrecision\\n0.0\\n1.00\\n0.1\\n0.67\\n0.2\\n0.63\\n0.3\\n0.55\\n0.4\\n0.45\\n0.5\\n0.41\\n0.6\\n0.36\\n0.7\\n0.29\\n0.8\\n0.13\\n0.9\\n0.10\\n1.0\\n0.08\\n◮Table 8.1\\nCalculation of 11-point Interpolated Average Precision. This is for the\\nprecision-recall curve shown in Figure 8.2.\\nas the highest precision found for any recall level r′ ≥r:\\npinterp(r) = max\\nr′≥r p(r′)\\n(8.7)\\nThe justiﬁcation is that almost anyone would be prepared to look at a few\\nmore documents if it would increase the percentage of the viewed set that\\nwere relevant (that is, if the precision of the larger set is higher). Interpolated\\nprecision is shown by a thinner line in Figure 8.2. With this deﬁnition, the\\ninterpolated precision at a recall of 0 is well-deﬁned (Exercise 8.4).\\nExamining the entire precision-recall curve is very informative, but there\\nis often a desire to boil this information down to a few numbers, or perhaps\\neven a single number. The traditional way of doing this (used for instance\\nin the ﬁrst 8 TREC Ad Hoc evaluations) is the 11-point interpolated average\\n11-POINT\\nINTERPOLATED\\nAVERAGE PRECISION\\nprecision. For each information need, the interpolated precision is measured\\nat the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. For the precision-recall curve in\\nFigure 8.2, these 11 values are shown in Table 8.1. For each recall level, we\\nthen calculate the arithmetic mean of the interpolated precision at that recall\\nlevel for each information need in the test collection. A composite precision-\\nrecall curve showing 11 points can then be graphed. Figure 8.3 shows an\\nexample graph of such results from a representative good system at TREC 8.\\nIn recent years, other measures have become more common. Most stan-\\ndard among the TREC community is Mean Average Precision (MAP), which\\nMEAN AVERAGE\\nPRECISION\\nprovides a single-ﬁgure measure of quality across recall levels. Among eval-\\nuation measures, MAP has been shown to have especially good discrimina-\\ntion and stability. For a single information need, Average Precision is the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n160\\n8\\nEvaluation in information retrieval\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nRecall\\nPrecision\\n◮Figure 8.3\\nAveraged 11-point precision/recall graph across 50 queries for a rep-\\nresentative TREC system. The Mean Average Precision for this system is 0.2553.\\naverage of the precision value obtained for the set of top k documents exist-\\ning after each relevant document is retrieved, and this value is then averaged\\nover information needs. That is, if the set of relevant documents for an in-\\nformation need qj ∈Q is {d1, . . . dmj} and Rjk is the set of ranked retrieval\\nresults from the top result until you get to document dk, then\\nMAP(Q) =\\n1\\n|Q|\\n|Q|\\n∑\\nj=1\\n1\\nmj\\nmj\\n∑\\nk=1\\nPrecision(Rjk)\\n(8.8)\\nWhen a relevant document is not retrieved at all,1 the precision value in the\\nabove equation is taken to be 0. For a single information need, the average\\nprecision approximates the area under the uninterpolated precision-recall\\ncurve, and so the MAP is roughly the average area under the precision-recall\\ncurve for a set of queries.\\nUsing MAP, ﬁxed recall levels are not chosen, and there is no interpola-\\ntion. The MAP value for a test collection is the arithmetic mean of average\\n1. A system may not fully order all documents in the collection in response to a query or at\\nany rate an evaluation exercise may be based on submitting only the top k results for each\\ninformation need.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n161\\nprecision values for individual information needs. (This has the effect of\\nweighting each information need equally in the ﬁnal reported number, even\\nif many documents are relevant to some queries whereas very few are rele-\\nvant to other queries.) Calculated MAP scores normally vary widely across\\ninformation needs when measured within a single system, for instance, be-\\ntween 0.1 and 0.7. Indeed, there is normally more agreement in MAP for\\nan individual information need across systems than for MAP scores for dif-\\nferent information needs for the same system. This means that a set of test\\ninformation needs must be large and diverse enough to be representative of\\nsystem effectiveness across different queries.\\nThe above measures factor in precision at all recall levels. For many promi-\\nPRECISION AT k\\nnent applications, particularly web search, this may not be germane to users.\\nWhat matters is rather how many good results there are on the ﬁrst page or\\nthe ﬁrst three pages. This leads to measuring precision at ﬁxed low levels of\\nretrieved results, such as 10 or 30 documents. This is referred to as “Precision\\nat k”, for example “Precision at 10”. It has the advantage of not requiring any\\nestimate of the size of the set of relevant documents but the disadvantages\\nthat it is the least stable of the commonly used evaluation measures and that\\nit does not average well, since the total number of relevant documents for a\\nquery has a strong inﬂuence on precision at k.\\nAn alternative, which alleviates this problem, is R-precision. It requires\\nR-PRECISION\\nhaving a set of known relevant documents Rel, from which we calculate the\\nprecision of the top Rel documents returned. (The set Rel may be incomplete,\\nsuch as when Rel is formed by creating relevance judgments for the pooled\\ntop k results of particular systems in a set of experiments.) R-precision ad-\\njusts for the size of the set of relevant documents: A perfect system could\\nscore 1 on this metric for each query, whereas, even a perfect system could\\nonly achieve a precision at 20 of 0.4 if there were only 8 documents in the\\ncollection relevant to an information need. Averaging this measure across\\nqueries thus makes more sense. This measure is harder to explain to naive\\nusers than Precision at k but easier to explain than MAP. If there are |Rel|\\nrelevant documents for a query, we examine the top |Rel| results of a sys-\\ntem, and ﬁnd that r are relevant, then by deﬁnition, not only is the precision\\n(and hence R-precision) r/|Rel|, but the recall of this result set is also r/|Rel|.\\nThus, R-precision turns out to be identical to the break-even point, another\\nBREAK-EVEN POINT\\nmeasure which is sometimes used, deﬁned in terms of this equality relation-\\nship holding. Like Precision at k, R-precision describes only one point on\\nthe precision-recall curve, rather than attempting to summarize effectiveness\\nacross the curve, and it is somewhat unclear why you should be interested\\nin the break-even point rather than either the best point on the curve (the\\npoint with maximal F-measure) or a retrieval level of interest to a particular\\napplication (Precision at k). Nevertheless, R-precision turns out to be highly\\ncorrelated with MAP empirically, despite measuring only a single point on\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n162\\n8\\nEvaluation in information retrieval\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1 − specificity\\nsensitivity ( = recall)\\n◮Figure 8.4\\nThe ROC curve corresponding to the precision-recall curve in Fig-\\nure 8.2.\\n.\\nthe curve.\\nAnother concept sometimes used in evaluation is an ROC curve. (“ROC”\\nROC CURVE\\nstands for “Receiver Operating Characteristics”, but knowing that doesn’t\\nhelp most people.) An ROC curve plots the true positive rate or sensitiv-\\nity against the false positive rate or (1 −speciﬁcity). Here, sensitivity is just\\nSENSITIVITY\\nanother term for recall. The false positive rate is given by f p/( f p + tn). Fig-\\nure 8.4 shows the ROC curve corresponding to the precision-recall curve in\\nFigure 8.2. An ROC curve always goes from the bottom left to the top right of\\nthe graph. For a good system, the graph climbs steeply on the left side. For\\nunranked result sets, speciﬁcity, given by tn/( f p + tn), was not seen as a very\\nSPECIFICITY\\nuseful notion. Because the set of true negatives is always so large, its value\\nwould be almost 1 for all information needs (and, correspondingly, the value\\nof the false positive rate would be almost 0). That is, the “interesting” part of\\nFigure 8.2 is 0 < recall < 0.4, a part which is compressed to a small corner\\nof Figure 8.4. But an ROC curve could make sense when looking over the\\nfull retrieval spectrum, and it provides another way of looking at the data.\\nIn many ﬁelds, a common aggregate measure is to report the area under the\\nROC curve, which is the ROC analog of MAP. Precision-recall curves are\\nsometimes loosely referred to as ROC curves. This is understandable, but\\nnot accurate.\\nA ﬁnal approach that has seen increasing adoption, especially when em-\\nployed with machine learning approaches to ranking (see Section 15.4, page 341)\\nis measures of cumulative gain, and in particular normalized discounted cumu-\\nCUMULATIVE GAIN\\nNORMALIZED\\nDISCOUNTED\\nCUMULATIVE GAIN\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n163\\nlative gain (NDCG). NDCG is designed for situations of non-binary notions\\nNDCG\\nof relevance (cf. Section 8.5.1). Like precision at k, it is evaluated over some\\nnumber k of top search results. For a set of queries Q, let R(j, d) be the rele-\\nvance score assessors gave to document d for query j. Then,\\nNDCG(Q, k) =\\n1\\n|Q|\\n|Q|\\n∑\\nj=1\\nZkj\\nk\\n∑\\nm=1\\n2R(j,m) −1\\nlog2(1 + m),\\n(8.9)\\nwhere Zkj is a normalization factor calculated to make it so that a perfect\\nranking’s NDCG at k for query j is 1. For queries for which k′ < k documents\\nare retrieved, the last summation is done up to k′.\\n?\\nExercise 8.4\\n[⋆]\\nWhat are the possible values for interpolated precision at a recall level of 0?\\nExercise 8.5\\n[⋆⋆]\\nMust there always be a break-even point between precision and recall? Either show\\nthere must be or give a counter-example.\\nExercise 8.6\\n[⋆⋆]\\nWhat is the relationship between the value of F1 and the break-even point?\\nExercise 8.7\\n[⋆⋆]\\nThe Dice coefﬁcient of two sets is a measure of their intersection scaled by their size\\nDICE COEFFICIENT\\n(giving a value in the range 0 to 1):\\nDice(X, Y) = 2|X ∩Y|\\n|X| + |Y|\\nShow that the balanced F-measure (F1) is equal to the Dice coefﬁcient of the retrieved\\nand relevant document sets.\\nExercise 8.8\\n[⋆]\\nConsider an information need for which there are 4 relevant documents in the collec-\\ntion. Contrast two systems run on this collection. Their top 10 results are judged for\\nrelevance as follows (the leftmost item is the top ranked search result):\\nSystem 1\\nR N R N N\\nN N N R\\nR\\nSystem 2\\nN R N N R\\nR\\nR N N N\\na. What is the MAP of each system? Which has a higher MAP?\\nb. Does this result intuitively make sense? What does it say about what is important\\nin getting a good MAP score?\\nc. What is the R-precision of each system? (Does it rank the systems the same as\\nMAP?)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n164\\n8\\nEvaluation in information retrieval\\nExercise 8.9\\n[⋆⋆]\\nThe following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned\\ndocuments in a ranked list of 20 documents retrieved in response to a query from a\\ncollection of 10,000 documents. The top of the ranked list (the document the system\\nthinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant\\ndocuments. Assume that there are 8 relevant documents in total in the collection.\\nR R N N N\\nN N N R N\\nR N N N R\\nN N N N R\\na. What is the precision of the system on the top 20?\\nb. What is the F1 on the top 20?\\nc. What is the uninterpolated precision of the system at 25% recall?\\nd. What is the interpolated precision at 33% recall?\\ne. Assume that these 20 documents are the complete result set of the system. What\\nis the MAP for the query?\\nAssume, now, instead, that the system returned the entire 10,000 documents in a\\nranked list, and these are the ﬁrst 20 results returned.\\nf. What is the largest possible MAP that this system could have?\\ng. What is the smallest possible MAP that this system could have?\\nh. In a set of experiments, only the top 20 results are evaluated by hand. The result\\nin (e) is used to approximate the range (f)–(g). For this example, how large (in\\nabsolute terms) can the error for the MAP be by calculating (e) instead of (f) and\\n(g) for this query?\\n8.5\\nAssessing relevance\\nTo properly evaluate a system, your test information needs must be germane\\nto the documents in the test document collection, and appropriate for pre-\\ndicted usage of the system. These information needs are best designed by\\ndomain experts. Using random combinations of query terms as an informa-\\ntion need is generally not a good idea because typically they will not resem-\\nble the actual distribution of information needs.\\nGiven information needs and documents, you need to collect relevance\\nassessments. This is a time-consuming and expensive process involving hu-\\nman beings. For tiny collections like Cranﬁeld, exhaustive judgments of rel-\\nevance for each query and document pair were obtained. For large modern\\ncollections, it is usual for relevance to be assessed only for a subset of the\\ndocuments for each query. The most standard approach is pooling, where rel-\\nPOOLING\\nevance is assessed over a subset of the collection that is formed from the top\\nk documents returned by a number of different IR systems (usually the ones\\nto be evaluated), and perhaps other sources such as the results of Boolean\\nkeyword searches or documents found by expert searchers in an interactive\\nprocess.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.5\\nAssessing relevance\\n165\\nJudge 2 Relevance\\nYes\\nNo\\nTotal\\nJudge 1\\nYes\\n300\\n20\\n320\\nRelevance\\nNo\\n10\\n70\\n80\\nTotal\\n310\\n90\\n400\\nObserved proportion of the times the judges agreed\\nP(A) = (300 + 70)/400 = 370/400 = 0.925\\nPooled marginals\\nP(nonrelevant) = (80 + 90)/(400 + 400) = 170/800 = 0.2125\\nP(relevant) = (320 + 310)/(400 + 400) = 630/800 = 0.7878\\nProbability that the two judges agreed by chance\\nP(E) = P(nonrelevant)2 + P(relevant)2 = 0.21252 + 0.78782 = 0.665\\nKappa statistic\\nκ = (P(A) −P(E))/(1 −P(E)) = (0.925 −0.665)/(1 −0.665) = 0.776\\n◮Table 8.2\\nCalculating the kappa statistic.\\nA human is not a device that reliably reports a gold standard judgment\\nof relevance of a document to a query. Rather, humans and their relevance\\njudgments are quite idiosyncratic and variable. But this is not a problem\\nto be solved: in the ﬁnal analysis, the success of an IR system depends on\\nhow good it is at satisfying the needs of these idiosyncratic humans, one\\ninformation need at a time.\\nNevertheless, it is interesting to consider and measure how much agree-\\nment between judges there is on relevance judgments. In the social sciences,\\na common measure for agreement between judges is the kappa statistic. It is\\nKAPPA STATISTIC\\ndesigned for categorical judgments and corrects a simple agreement rate for\\nthe rate of chance agreement.\\nkappa = P(A) −P(E)\\n1 −P(E)\\n(8.10)\\nwhere P(A) is the proportion of the times the judges agreed, and P(E) is the\\nproportion of the times they would be expected to agree by chance. There\\nare choices in how the latter is estimated: if we simply say we are making\\na two-class decision and assume nothing more, then the expected chance\\nagreement rate is 0.5. However, normally the class distribution assigned is\\nskewed, and it is usual to use marginal statistics to calculate expected agree-\\nMARGINAL\\nment.2 There are still two ways to do it depending on whether one pools\\n2. For a contingency table, as in Table 8.2, a marginal statistic is formed by summing a row or\\ncolumn. The marginal ai.k = ∑j aijk.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n166\\n8\\nEvaluation in information retrieval\\nthe marginal distribution across judges or uses the marginals for each judge\\nseparately; both forms have been used, but we present the pooled version\\nbecause it is more conservative in the presence of systematic differences in as-\\nsessments across judges. The calculations are shown in Table 8.2. The kappa\\nvalue will be 1 if two judges always agree, 0 if they agree only at the rate\\ngiven by chance, and negative if they are worse than random. If there are\\nmore than two judges, it is normal to calculate an average pairwise kappa\\nvalue. As a rule of thumb, a kappa value above 0.8 is taken as good agree-\\nment, a kappa value between 0.67 and 0.8 is taken as fair agreement, and\\nagreement below 0.67 is seen as data providing a dubious basis for an evalu-\\nation, though the precise cutoffs depend on the purposes for which the data\\nwill be used.\\nInterjudge agreement of relevance has been measured within the TREC\\nevaluations and for medical IR collections. Using the above rules of thumb,\\nthe level of agreement normally falls in the range of “fair” (0.67–0.8). The fact\\nthat human agreement on a binary relevance judgment is quite modest is one\\nreason for not requiring more ﬁne-grained relevance labeling from the test\\nset creator. To answer the question of whether IR evaluation results are valid\\ndespite the variation of individual assessors’ judgments, people have exper-\\nimented with evaluations taking one or the other of two judges’ opinions as\\nthe gold standard. The choice can make a considerable absolute difference to\\nreported scores, but has in general been found to have little impact on the rel-\\native effectiveness ranking of either different systems or variants of a single\\nsystem which are being compared for effectiveness.\\n8.5.1\\nCritiques and justiﬁcations of the concept of relevance\\nThe advantage of system evaluation, as enabled by the standard model of\\nrelevant and nonrelevant documents, is that we have a ﬁxed setting in which\\nwe can vary IR systems and system parameters to carry out comparative ex-\\nperiments. Such formal testing is much less expensive and allows clearer\\ndiagnosis of the effect of changing system parameters than doing user stud-\\nies of retrieval effectiveness. Indeed, once we have a formal measure that\\nwe have conﬁdence in, we can proceed to optimize effectiveness by machine\\nlearning methods, rather than tuning parameters by hand. Of course, if the\\nformal measure poorly describes what users actually want, doing this will\\nnot be effective in improving user satisfaction. Our perspective is that, in\\npractice, the standard formal measures for IR evaluation, although a simpli-\\nﬁcation, are good enough, and recent work in optimizing formal evaluation\\nmeasures in IR has succeeded brilliantly. There are numerous examples of\\ntechniques developed in formal evaluation settings, which improve effec-\\ntiveness in operational settings, such as the development of document length\\nnormalization methods within the context of TREC (Sections 6.4.4 and 11.4.3)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.5\\nAssessing relevance\\n167\\nand machine learning methods for adjusting parameter weights in scoring\\n(Section 6.1.2).\\nThat is not to say that there are not problems latent within the abstrac-\\ntions used. The relevance of one document is treated as independent of the\\nrelevance of other documents in the collection. (This assumption is actually\\nbuilt into most retrieval systems – documents are scored against queries, not\\nagainst each other – as well as being assumed in the evaluation methods.)\\nAssessments are binary: there aren’t any nuanced assessments of relevance.\\nRelevance of a document to an information need is treated as an absolute,\\nobjective decision. But judgments of relevance are subjective, varying across\\npeople, as we discussed above. In practice, human assessors are also imper-\\nfect measuring instruments, susceptible to failures of understanding and at-\\ntention. We also have to assume that users’ information needs do not change\\nas they start looking at retrieval results. Any results based on one collection\\nare heavily skewed by the choice of collection, queries, and relevance judg-\\nment set: the results may not translate from one domain to another or to a\\ndifferent user population.\\nSome of these problems may be ﬁxable. A number of recent evaluations,\\nincluding INEX, some TREC tracks, and NTCIR have adopted an ordinal\\nnotion of relevance with documents divided into 3 or 4 classes, distinguish-\\ning slightly relevant documents from highly relevant documents. See Sec-\\ntion 10.4 (page 210) for a detailed discussion of how this is implemented in\\nthe INEX evaluations.\\nOne clear problem with the relevance-based assessment that we have pre-\\nsented is the distinction between relevance and marginal relevance: whether\\nMARGINAL RELEVANCE\\na document still has distinctive usefulness after the user has looked at cer-\\ntain other documents (Carbonell and Goldstein 1998). Even if a document\\nis highly relevant, its information can be completely redundant with other\\ndocuments which have already been examined. The most extreme case of\\nthis is documents that are duplicates – a phenomenon that is actually very\\ncommon on the World Wide Web – but it can also easily occur when sev-\\neral documents provide a similar precis of an event. In such circumstances,\\nmarginal relevance is clearly a better measure of utility to the user. Maximiz-\\ning marginal relevance requires returning documents that exhibit diversity\\nand novelty. One way to approach measuring this is by using distinct facts\\nor entities as evaluation units. This perhaps more directly measures true\\nutility to the user but doing this makes it harder to create a test collection.\\n?\\nExercise 8.10\\n[⋆⋆]\\nBelow is a table showing how two human judges rated the relevance of a set of 12\\ndocuments to a particular information need (0 = nonrelevant, 1 = relevant). Let us as-\\nsume that you’ve written an IR system that for this query returns the set of documents\\n{4, 5, 6, 7, 8}.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n168\\n8\\nEvaluation in information retrieval\\ndocID\\nJudge 1\\nJudge 2\\n1\\n0\\n0\\n2\\n0\\n0\\n3\\n1\\n1\\n4\\n1\\n1\\n5\\n1\\n0\\n6\\n1\\n0\\n7\\n1\\n0\\n8\\n1\\n0\\n9\\n0\\n1\\n10\\n0\\n1\\n11\\n0\\n1\\n12\\n0\\n1\\na. Calculate the kappa measure between the two judges.\\nb. Calculate precision, recall, and F1 of your system if a document is considered rel-\\nevant only if the two judges agree.\\nc. Calculate precision, recall, and F1 of your system if a document is considered rel-\\nevant if either judge thinks it is relevant.\\n8.6\\nA broader perspective: System quality and user utility\\nFormal evaluation measures are at some distance from our ultimate interest\\nin measures of human utility: how satisﬁed is each user with the results the\\nsystem gives for each information need that they pose? The standard way to\\nmeasure human satisfaction is by various kinds of user studies. These might\\ninclude quantitative measures, both objective, such as time to complete a\\ntask, as well as subjective, such as a score for satisfaction with the search\\nengine, and qualitative measures, such as user comments on the search in-\\nterface. In this section we will touch on other system aspects that allow quan-\\ntitative evaluation and the issue of user utility.\\n8.6.1\\nSystem issues\\nThere are many practical benchmarks on which to rate an information re-\\ntrieval system beyond its retrieval quality. These include:\\n• How fast does it index, that is, how many documents per hour does it\\nindex for a certain distribution over document lengths? (cf. Chapter 4)\\n• How fast does it search, that is, what is its latency as a function of index\\nsize?\\n• How expressive is its query language? How fast is it on complex queries?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.6\\nA broader perspective: System quality and user utility\\n169\\n• How large is its document collection, in terms of the number of doc-\\numents or the collection having information distributed across a broad\\nrange of topics?\\nAll these criteria apart from query language expressiveness are straightfor-\\nwardly measurable: we can quantify the speed or size. Various kinds of fea-\\nture checklists can make query language expressiveness semi-precise.\\n8.6.2\\nUser utility\\nWhat we would really like is a way of quantifying aggregate user happiness,\\nbased on the relevance, speed, and user interface of a system. One part of\\nthis is understanding the distribution of people we wish to make happy, and\\nthis depends entirely on the setting. For a web search engine, happy search\\nusers are those who ﬁnd what they want. One indirect measure of such users\\nis that they tend to return to the same engine. Measuring the rate of return\\nof users is thus an effective metric, which would of course be more effective\\nif you could also measure how much these users used other search engines.\\nBut advertisers are also users of modern web search engines. They are happy\\nif customers click through to their sites and then make purchases. On an\\neCommerce web site, a user is likely to be wanting to purchase something.\\nThus, we can measure the time to purchase, or the fraction of searchers who\\nbecome buyers. On a shopfront web site, perhaps both the user’s and the\\nstore owner’s needs are satisﬁed if a purchase is made. Nevertheless, in\\ngeneral, we need to decide whether it is the end user’s or the eCommerce\\nsite owner’s happiness that we are trying to optimize. Usually, it is the store\\nowner who is paying us.\\nFor an “enterprise” (company, government, or academic) intranet search\\nengine, the relevant metric is more likely to be user productivity: how much\\ntime do users spend looking for information that they need. There are also\\nmany other practical criteria concerning such matters as information secu-\\nrity, which we mentioned in Section 4.6 (page 80).\\nUser happiness is elusive to measure, and this is part of why the standard\\nmethodology uses the proxy of relevance of search results. The standard\\ndirect way to get at user satisfaction is to run user studies, where people en-\\ngage in tasks, and usually various metrics are measured, the participants are\\nobserved, and ethnographic interview techniques are used to get qualitative\\ninformation on satisfaction. User studies are very useful in system design,\\nbut they are time consuming and expensive to do. They are also difﬁcult to\\ndo well, and expertise is required to design the studies and to interpret the\\nresults. We will not discuss the details of human usability testing here.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n170\\n8\\nEvaluation in information retrieval\\n8.6.3\\nReﬁning a deployed system\\nIf an IR system has been built and is being used by a large number of users,\\nthe system’s builders can evaluate possible changes by deploying variant\\nversions of the system and recording measures that are indicative of user\\nsatisfaction with one variant vs. others as they are being used. This method\\nis frequently used by web search engines.\\nThe most common version of this is A/B testing, a term borrowed from the\\nA/B TEST\\nadvertising industry. For such a test, precisely one thing is changed between\\nthe current system and a proposed system, and a small proportion of traf-\\nﬁc (say, 1–10% of users) is randomly directed to the variant system, while\\nmost users use the current system. For example, if we wish to investigate a\\nchange to the ranking algorithm, we redirect a random sample of users to\\na variant system and evaluate measures such as the frequency with which\\npeople click on the top result, or any result on the ﬁrst page. (This particular\\nanalysis method is referred to as clickthrough log analysis or clickstream min-\\nCLICKTHROUGH LOG\\nANALYSIS\\nCLICKSTREAM MINING\\ning. It is further discussed as a method of implicit feedback in Section 9.1.7\\n(page 187).)\\nThe basis of A/B testing is running a bunch of single variable tests (either\\nin sequence or in parallel): for each test only one parameter is varied from the\\ncontrol (the current live system). It is therefore easy to see whether varying\\neach parameter has a positive or negative effect. Such testing of a live system\\ncan easily and cheaply gauge the effect of a change on users, and, with a\\nlarge enough user base, it is practical to measure even very small positive\\nand negative effects. In principle, more analytic power can be achieved by\\nvarying multiple things at once in an uncorrelated (random) way, and doing\\nstandard multivariate statistical analysis, such as multiple linear regression.\\nIn practice, though, A/B testing is widely used, because A/B tests are easy\\nto deploy, easy to understand, and easy to explain to management.\\n8.7\\nResults snippets\\nHaving chosen or ranked the documents matching a query, we wish to pre-\\nsent a results list that will be informative to the user. In many cases the\\nuser will not want to examine all the returned documents and so we want\\nto make the results list informative enough that the user can do a ﬁnal rank-\\ning of the documents for themselves based on relevance to their information\\nneed.3 The standard way of doing this is to provide a snippet, a short sum-\\nSNIPPET\\nmary of the document, which is designed so as to allow the user to decide\\nits relevance. Typically, the snippet consists of the document title and a short\\n3. There are exceptions, in domains where recall is emphasized. For instance, in many legal\\ndisclosure cases, a legal associate will review every document that matches a keyword search.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.7\\nResults snippets\\n171\\nsummary, which is automatically extracted. The question is how to design\\nthe summary so as to maximize its usefulness to the user.\\nThe two basic kinds of summaries are static, which are always the same\\nSTATIC SUMMARY\\nregardless of the query, and dynamic (or query-dependent), which are cus-\\nDYNAMIC SUMMARY\\ntomized according to the user’s information need as deduced from a query.\\nDynamic summaries attempt to explain why a particular document was re-\\ntrieved for the query at hand.\\nA static summary is generally comprised of either or both a subset of the\\ndocument and metadata associated with the document. The simplest form\\nof summary takes the ﬁrst two sentences or 50 words of a document, or ex-\\ntracts particular zones of a document, such as the title and author. Instead of\\nzones of a document, the summary can instead use metadata associated with\\nthe document. This may be an alternative way to provide an author or date,\\nor may include elements which are designed to give a summary, such as the\\ndescription metadata which can appear in the meta element of a web\\nHTML page. This summary is typically extracted and cached at indexing\\ntime, in such a way that it can be retrieved and presented quickly when dis-\\nplaying search results, whereas having to access the actual document content\\nmight be a relatively expensive operation.\\nThere has been extensive work within natural language processing (NLP)\\non better ways to do text summarization. Most such work still aims only to\\nTEXT SUMMARIZATION\\nchoose sentences from the original document to present and concentrates on\\nhow to select good sentences. The models typically combine positional fac-\\ntors, favoring the ﬁrst and last paragraphs of documents and the ﬁrst and last\\nsentences of paragraphs, with content factors, emphasizing sentences with\\nkey terms, which have low document frequency in the collection as a whole,\\nbut high frequency and good distribution across the particular document\\nbeing returned. In sophisticated NLP approaches, the system synthesizes\\nsentences for a summary, either by doing full text generation or by editing\\nand perhaps combining sentences used in the document. For example, it\\nmight delete a relative clause or replace a pronoun with the noun phrase\\nthat it refers to. This last class of methods remains in the realm of research\\nand is seldom used for search results: it is easier, safer, and often even better\\nto just use sentences from the original document.\\nDynamic summaries display one or more “windows” on the document,\\naiming to present the pieces that have the most utility to the user in evalu-\\nating the document with respect to their information need. Usually these\\nwindows contain one or several of the query terms, and so are often re-\\nferred to as keyword-in-context (KWIC) snippets, though sometimes they may\\nKEYWORD-IN-CONTEXT\\nstill be pieces of the text such as the title that are selected for their query-\\nindependent information value just as in the case of static summarization.\\nDynamic summaries are generated in conjunction with scoring. If the query\\nis found as a phrase, occurrences of the phrase in the document will be\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n172\\n8\\nEvaluation in information retrieval\\n... In recent years, Papua New Guinea has faced severe economic\\ndifﬁculties and economic growth has slowed, partly as a result of weak\\ngovernance and civil war, and partly as a result of external factors such as\\nthe Bougainville civil war which led to the closure in 1989 of the Panguna\\nmine (at that time the most important foreign exchange earner and\\ncontributor to Government ﬁnances), the Asian ﬁnancial crisis, a decline in\\nthe prices of gold and copper, and a fall in the production of oil. PNG’s\\neconomic development record over the past few years is evidence that\\ngovernance issues underly many of the country’s problems. Good\\ngovernance, which may be deﬁned as the transparent and accountable\\nmanagement of human, natural, economic and ﬁnancial resources for the\\npurposes of equitable and sustainable development, ﬂows from proper\\npublic sector management, efﬁcient ﬁscal and accounting mechanisms, and\\na willingness to make service delivery a priority in practice. ...\\n◮Figure 8.5\\nAn example of selecting text for a dynamic snippet. This snippet was\\ngenerated for a document in response to the query new guinea economic development.\\nThe ﬁgure shows in bold italic where the selected snippet text occurred in the original\\ndocument.\\nshown as the summary. If not, windows within the document that contain\\nmultiple query terms will be selected. Commonly these windows may just\\nstretch some number of words to the left and right of the query terms. This is\\na place where NLP techniques can usefully be employed: users prefer snip-\\npets that read well because they contain complete phrases.\\nDynamic summaries are generally regarded as greatly improving the us-\\nability of IR systems, but they present a complication for IR system design. A\\ndynamic summary cannot be precomputed, but, on the other hand, if a sys-\\ntem has only a positional index, then it cannot easily reconstruct the context\\nsurrounding search engine hits in order to generate such a dynamic sum-\\nmary. This is one reason for using static summaries. The standard solution\\nto this in a world of large and cheap disk drives is to locally cache all the\\ndocuments at index time (notwithstanding that this approach raises various\\nlegal, information security and control issues that are far from resolved) as\\nshown in Figure 7.5 (page 147). Then, a system can simply scan a document\\nwhich is about to appear in a displayed results list to ﬁnd snippets containing\\nthe query words. Beyond simply access to the text, producing a good KWIC\\nsnippet requires some care. Given a variety of keyword occurrences in a\\ndocument, the goal is to choose fragments which are: (i) maximally informa-\\ntive about the discussion of those terms in the document, (ii) self-contained\\nenough to be easy to read, and (iii) short enough to ﬁt within the normally\\nstrict constraints on the space available for summaries.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.8\\nReferences and further reading\\n173\\nGenerating snippets must be fast since the system is typically generating\\nmany snippets for each query that it handles. Rather than caching an entire\\ndocument, it is common to cache only a generous but ﬁxed size preﬁx of\\nthe document, such as perhaps 10,000 characters. For most common, short\\ndocuments, the entire document is thus cached, but huge amounts of local\\nstorage will not be wasted on potentially vast documents. Summaries of\\ndocuments whose length exceeds the preﬁx size will be based on material\\nin the preﬁx only, which is in general a useful zone in which to look for a\\ndocument summary anyway.\\nIf a document has been updated since it was last processed by a crawler\\nand indexer, these changes will be neither in the cache nor in the index. In\\nthese circumstances, neither the index nor the summary will accurately re-\\nﬂect the current contents of the document, but it is the differences between\\nthe summary and the actual document content that will be more glaringly\\nobvious to the end user.\\n8.8\\nReferences and further reading\\nDeﬁnition and implementation of the notion of relevance to a query got off\\nto a rocky start in 1953. Swanson (1988) reports that in an evaluation in that\\nyear between two teams, they agreed that 1390 documents were variously\\nrelevant to a set of 98 questions, but disagreed on a further 1577 documents,\\nand the disagreements were never resolved.\\nRigorous formal testing of IR systems was ﬁrst completed in the Cranﬁeld\\nexperiments, beginning in the late 1950s. A retrospective discussion of the\\nCranﬁeld test collection and experimentation with it can be found in (Clever-\\ndon 1991). The other seminal series of early IR experiments were those on the\\nSMART system by Gerard Salton and colleagues (Salton 1971b; 1991). The\\nTREC evaluations are described in detail by Voorhees and Harman (2005).\\nOnline information is available at http://trec.nist.gov/. Initially, few researchers\\ncomputed the statistical signiﬁcance of their experimental results, but the IR\\ncommunity increasingly demands this (Hull 1993). User studies of IR system\\neffectiveness began more recently (Saracevic and Kantor 1988; 1996).\\nThe notions of recall and precision were ﬁrst used by Kent et al. (1955),\\nalthough the term precision did not appear until later. The F measure (or,\\nF MEASURE\\nrather its complement E = 1 −F) was introduced by van Rijsbergen (1979).\\nHe provides an extensive theoretical discussion, which shows how adopting\\na principle of decreasing marginal relevance (at some point a user will be\\nunwilling to sacriﬁce a unit of precision for an added unit of recall) leads to\\nthe harmonic mean being the appropriate method for combining precision\\nand recall (and hence to its adoption rather than the minimum or geometric\\nmean).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n174\\n8\\nEvaluation in information retrieval\\nBuckley and Voorhees (2000) compare several evaluation measures, in-\\ncluding precision at k, MAP, and R-precision, and evaluate the error rate of\\neach measure. R-precision was adopted as the ofﬁcial evaluation metric in\\nR-PRECISION\\nthe TREC HARD track (Allan 2005). Aslam and Yilmaz (2005) examine its\\nsurprisingly close correlation to MAP, which had been noted in earlier stud-\\nies (Tague-Sutcliffe and Blustein 1995, Buckley and Voorhees 2000). A stan-\\ndard program for evaluating IR systems which computes many measures of\\nranked retrieval effectiveness is Chris Buckley’s trec_eval program used\\nin the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/.\\nKekäläinen and Järvelin (2002) argue for the superiority of graded rele-\\nvance judgments when dealing with very large document collections, and\\nJärvelin and Kekäläinen (2002) introduce cumulated gain-based methods for\\nIR system evaluation in this context. Sakai (2007) does a study of the stabil-\\nity and sensitivity of evaluation measures based on graded relevance judg-\\nments from NTCIR tasks, and concludes that NDCG is best for evaluating\\ndocument ranking.\\nSchamber et al. (1990) examine the concept of relevance, stressing its multi-\\ndimensional and context-speciﬁc nature, but also arguing that it can be mea-\\nsured effectively. (Voorhees 2000) is the standard article for examining vari-\\nation in relevance judgments and their effects on retrieval system scores and\\nranking for the TREC Ad Hoc task. Voorhees concludes that although the\\nnumbers change, the rankings are quite stable. Hersh et al. (1994) present\\nsimilar analysis for a medical IR collection. In contrast, Kekäläinen (2005)\\nanalyze some of the later TRECs, exploring a 4-way relevance judgment and\\nthe notion of cumulative gain, arguing that the relevance measure used does\\nsubstantially affect system rankings. See also Harter (1998). Zobel (1998)\\nstudies whether the pooling method used by TREC to collect a subset of doc-\\numents that will be evaluated for relevance is reliable and fair, and concludes\\nthat it is.\\nThe kappa statistic and its use for language-related purposes is discussed\\nKAPPA STATISTIC\\nby Carletta (1996). Many standard sources (e.g., Siegel and Castellan 1988)\\npresent pooled calculation of the expected agreement, but Di Eugenio and\\nGlass (2004) argue for preferring the unpooled agreement (though perhaps\\npresenting multiple measures). For further discussion of alternative mea-\\nsures of agreement, which may in fact be better, see Lombard et al. (2002)\\nand Krippendorff (2003).\\nText summarization has been actively explored for many years. Modern\\nwork on sentence selection was initiated by Kupiec et al. (1995). More recent\\nwork includes (Barzilay and Elhadad 1997) and (Jing 2000), together with\\na broad selection of work appearing at the yearly DUC conferences and at\\nother NLP venues. Tombros and Sanderson (1998) demonstrate the advan-\\ntages of dynamic summaries in the IR context. Turpin et al. (2007) address\\nhow to generate snippets efﬁciently.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n8.8\\nReferences and further reading\\n175\\nClickthrough log analysis is studied in (Joachims 2002b, Joachims et al.\\n2005).\\nIn a series of papers, Hersh, Turpin and colleagues show how improve-\\nments in formal retrieval effectiveness, as evaluated in batch experiments, do\\nnot always translate into an improved system for users (Hersh et al. 2000a;b;\\n2001, Turpin and Hersh 2001; 2002).\\nUser interfaces for IR and human factors such as models of human infor-\\nmation seeking and usability testing are outside the scope of what we cover\\nin this book. More information on these topics can be found in other text-\\nbooks, including (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) and (Korfhage\\n1997), and collections focused on cognitive aspects (Spink and Cole 2005).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n177\\n9\\nRelevance feedback and query\\nexpansion\\nIn most collections, the same concept may be referred to using different\\nwords. This issue, known as synonymy, has an impact on the recall of most\\nSYNONYMY\\ninformation retrieval systems. For example, you would want a search for\\naircraft to match plane (but only for references to an airplane, not a woodwork-\\ning plane), and for a search on thermodynamics to match references to heat in\\nappropriate discussions. Users often attempt to address this problem them-\\nselves by manually reﬁning a query, as was discussed in Section 1.4; in this\\nchapter we discuss ways in which a system can help with query reﬁnement,\\neither fully automatically or with the user in the loop.\\nThe methods for tackling this problem split into two major classes: global\\nmethods and local methods. Global methods are techniques for expanding\\nor reformulating query terms independent of the query and results returned\\nfrom it, so that changes in the query wording will cause the new query to\\nmatch other semantically similar terms. Global methods include:\\n• Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2)\\n• Query expansion via automatic thesaurus generation (Section 9.2.3)\\n• Techniques like spelling correction (discussed in Chapter 3)\\nLocal methods adjust a query relative to the documents that initially appear\\nto match the query. The basic methods here are:\\n• Relevance feedback (Section 9.1)\\n• Pseudo relevance feedback, also known as Blind relevance feedback (Sec-\\ntion 9.1.6)\\n• (Global) indirect relevance feedback (Section 9.1.7)\\nIn this chapter, we will mention all of these approaches, but we will concen-\\ntrate on relevance feedback, which is one of the most used and most success-\\nful approaches.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n178\\n9\\nRelevance feedback and query expansion\\n9.1\\nRelevance feedback and pseudo relevance feedback\\nThe idea of relevance feedback (RF) is to involve the user in the retrieval process\\nRELEVANCE FEEDBACK\\nso as to improve the ﬁnal result set. In particular, the user gives feedback on\\nthe relevance of documents in an initial set of results. The basic procedure is:\\n• The user issues a (short, simple) query.\\n• The system returns an initial set of retrieval results.\\n• The user marks some returned documents as relevant or nonrelevant.\\n• The system computes a better representation of the information need based\\non the user feedback.\\n• The system displays a revised set of retrieval results.\\nRelevance feedback can go through one or more iterations of this sort. The\\nprocess exploits the idea that it may be difﬁcult to formulate a good query\\nwhen you don’t know the collection well, but it is easy to judge particular\\ndocuments, and so it makes sense to engage in iterative query reﬁnement\\nof this sort. In such a scenario, relevance feedback can also be effective in\\ntracking a user’s evolving information need: seeing some documents may\\nlead users to reﬁne their understanding of the information they are seeking.\\nImage search provides a good example of relevance feedback. Not only is\\nit easy to see the results at work, but this is a domain where a user can easily\\nhave difﬁculty formulating what they want in words, but can easily indicate\\nrelevant or nonrelevant images. After the user enters an initial query for bike\\non the demonstration system at:\\nhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html\\nthe initial results (in this case, images) are returned. In Figure 9.1 (a), the\\nuser has selected some of them as relevant. These will be used to reﬁne the\\nquery, while other displayed results have no effect on the reformulation. Fig-\\nure 9.1 (b) then shows the new top-ranked results calculated after this round\\nof relevance feedback.\\nFigure 9.2 shows a textual IR example where the user wishes to ﬁnd out\\nabout new applications of space satellites.\\n9.1.1\\nThe Rocchio algorithm for relevance feedback\\nThe Rocchio Algorithm is the classic algorithm for implementing relevance\\nfeedback. It models a way of incorporating relevance feedback information\\ninto the vector space model of Section 6.3.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n179\\n(a)\\n(b)\\n◮Figure 9.1\\nRelevance feedback searching over images. (a) The user views the\\ninitial query results for a query of bike, selects the ﬁrst, third and fourth result in\\nthe top row and the fourth result in the bottom row as relevant, and submits this\\nfeedback. (b) The users sees the revised result set. Precision is greatly improved.\\nFrom http://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n180\\n9\\nRelevance feedback and query expansion\\n(a)\\nQuery: New space satellite applications\\n(b)\\n+\\n1. 0.539, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer\\n+\\n2. 0.533, 07/09/91, NASA Scratches Environment Gear From Satel-\\nlite Plan\\n3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan, But\\nUrges Launches of Smaller Probes\\n4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes Incredi-\\nble Feat: Staying Within Budget\\n5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Pro-\\nposes Satellites for Climate Research\\n6. 0.524, 08/22/90, Report Provides Support for the Critics Of Using\\nBig Satellites to Study Climate\\n7.\\n0.516, 04/13/87, Arianespace Receives Satellite Launch Pact\\nFrom Telesat Canada\\n+\\n8. 0.509, 12/02/87, Telecommunications Tale of Two Companies\\n(c)\\n2.074 new\\n15.106 space\\n30.816 satellite\\n5.660 application\\n5.991 nasa\\n5.196 eos\\n4.196 launch\\n3.972 aster\\n3.516 instrument\\n3.446 arianespace\\n3.004 bundespost\\n2.806 ss\\n2.790 rocket\\n2.053 scientist\\n2.003 broadcast\\n1.172 earth\\n0.836 oil\\n0.646 measure\\n(d)\\n*\\n1. 0.513, 07/09/91, NASA Scratches Environment Gear From Satel-\\nlite Plan\\n*\\n2. 0.500, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer\\n3. 0.493, 08/07/89, When the Pentagon Launches a Secret Satellite,\\nSpace Sleuths Do Some Spy Work of Their Own\\n4. 0.493, 07/31/89, NASA Uses ‘Warm’ Superconductors For Fast\\nCircuit\\n*\\n5. 0.492, 12/02/87, Telecommunications Tale of Two Companies\\n6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile For\\nCommercial Use\\n7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Match\\nthe Soviets In Rocket Launchers\\n8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Cost $90\\nMillion\\n◮Figure 9.2\\nExample of relevance feedback on a text collection. (a) The initial query\\n(a). (b) The user marks some relevant documents (shown with a plus sign). (c) The\\nquery is then expanded by 18 terms with weights as shown. (d) The revised top\\nresults are then shown. A * marks the documents which were judged relevant in the\\nrelevance feedback phase.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n181\\n◮Figure 9.3\\nThe Rocchio optimal query for separating relevant and nonrelevant\\ndocuments.\\nThe underlying theory.\\nWe want to ﬁnd a query vector, denoted as ⃗q, that\\nmaximizes similarity with relevant documents while minimizing similarity\\nwith nonrelevant documents. If Cr is the set of relevant documents and Cnr\\nis the set of nonrelevant documents, then we wish to ﬁnd:1\\n⃗qopt = arg max\\n⃗q\\n[sim(⃗q, Cr) −sim(⃗q, Cnr)],\\n(9.1)\\nwhere sim is deﬁned as in Equation 6.10. Under cosine similarity, the optimal\\nquery vector⃗qopt for separating the relevant and nonrelevant documents is:\\n⃗qopt =\\n1\\n|Cr| ∑\\n⃗dj∈Cr\\n⃗dj −\\n1\\n|Cnr| ∑\\n⃗dj∈Cnr\\n⃗dj\\n(9.2)\\nThat is, the optimal query is the vector difference between the centroids of the\\nrelevant and nonrelevant documents; see Figure 9.3. However, this observa-\\ntion is not terribly useful, precisely because the full set of relevant documents\\nis not known: it is what we want to ﬁnd.\\nThe Rocchio (1971) algorithm.\\nThis was the relevance feedback mecha-\\nROCCHIO ALGORITHM\\n1. In the equation, arg maxx f (x) returns a value of x which maximizes the value of the function\\nf (x). Similarly, arg minx f (x) returns a value of x which minimizes the value of the function\\nf (x).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n182\\n9\\nRelevance feedback and query expansion\\n◮Figure 9.4\\nAn application of Rocchio’s algorithm. Some documents have been\\nlabeled as relevant and nonrelevant and the initial query vector is moved in response\\nto this feedback.\\nnism introduced in and popularized by Salton’s SMART system around 1970.\\nIn a real IR query context, we have a user query and partial knowledge of\\nknown relevant and nonrelevant documents. The algorithm proposes using\\nthe modiﬁed query ⃗qm:\\n⃗qm = α⃗q0 + β 1\\n|Dr| ∑\\n⃗dj∈Dr\\n⃗dj −γ\\n1\\n|Dnr| ∑\\n⃗dj∈Dnr\\n⃗dj\\n(9.3)\\nwhere q0 is the original query vector, Dr and Dnr are the set of known rel-\\nevant and nonrelevant documents respectively, and α, β, and γ are weights\\nattached to each term. These control the balance between trusting the judged\\ndocument set versus the query: if we have a lot of judged documents, we\\nwould like a higher β and γ. Starting from q0, the new query moves you\\nsome distance toward the centroid of the relevant documents and some dis-\\ntance away from the centroid of the nonrelevant documents. This new query\\ncan be used for retrieval in the standard vector space model (see Section 6.3).\\nWe can easily leave the positive quadrant of the vector space by subtracting\\noff a nonrelevant document’s vector. In the Rocchio algorithm, negative term\\nweights are ignored. That is, the term weight is set to 0. Figure 9.4 shows the\\neffect of applying relevance feedback.\\nRelevance feedback can improve both recall and precision. But, in prac-\\ntice, it has been shown to be most useful for increasing recall in situations\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n183\\nwhere recall is important. This is partly because the technique expands the\\nquery, but it is also partly an effect of the use case: when they want high\\nrecall, users can be expected to take time to review results and to iterate on\\nthe search. Positive feedback also turns out to be much more valuable than\\nnegative feedback, and so most IR systems set γ < β. Reasonable values\\nmight be α = 1, β = 0.75, and γ = 0.15. In fact, many systems, such as\\nthe image search system in Figure 9.1, allow only positive feedback, which\\nis equivalent to setting γ = 0. Another alternative is to use only the marked\\nnonrelevant document which received the highest ranking from the IR sys-\\ntem as negative feedback (here, |Dnr| = 1 in Equation (9.3)). While many of\\nthe experimental results comparing various relevance feedback variants are\\nrather inconclusive, some studies have suggested that this variant, called Ide\\nIDE DEC-HI\\ndec-hi is the most effective or at least the most consistent performer.\\n$\\n9.1.2\\nProbabilistic relevance feedback\\nRather than reweighting the query in a vector space, if a user has told us\\nsome relevant and nonrelevant documents, then we can proceed to build a\\nclassiﬁer. One way of doing this is with a Naive Bayes probabilistic model.\\nIf R is a Boolean indicator variable expressing the relevance of a document,\\nthen we can estimate P(xt = 1|R), the probability of a term t appearing in a\\ndocument, depending on whether it is relevant or not, as:\\nˆP(xt = 1|R = 1)\\n=\\n|VRt|/|VR|\\n(9.4)\\nˆP(xt = 1|R = 0)\\n=\\n(d ft −|VRt|)/(N −|VR|)\\nwhere N is the total number of documents, d ft is the number that contain\\nt, VR is the set of known relevant documents, and VRt is the subset of this\\nset containing t. Even though the set of known relevant documents is a per-\\nhaps small subset of the true set of relevant documents, if we assume that\\nthe set of relevant documents is a small subset of the set of all documents\\nthen the estimates given above will be reasonable. This gives a basis for\\nanother way of changing the query term weights. We will discuss such prob-\\nabilistic approaches more in Chapters 11 and 13, and in particular outline\\nthe application to relevance feedback in Section 11.3.4 (page 228). For the\\nmoment, observe that using just Equation (9.4) as a basis for term-weighting\\nis likely insufﬁcient. The equations use only collection statistics and infor-\\nmation about the term distribution within the documents judged relevant.\\nThey preserve no memory of the original query.\\n9.1.3\\nWhen does relevance feedback work?\\nThe success of relevance feedback depends on certain assumptions. Firstly,\\nthe user has to have sufﬁcient knowledge to be able to make an initial query\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n184\\n9\\nRelevance feedback and query expansion\\nwhich is at least somewhere close to the documents they desire. This is\\nneeded anyhow for successful information retrieval in the basic case, but\\nit is important to see the kinds of problems that relevance feedback cannot\\nsolve alone. Cases where relevance feedback alone is not sufﬁcient include:\\n• Misspellings. If the user spells a term in a different way to the way it\\nis spelled in any document in the collection, then relevance feedback is\\nunlikely to be effective. This can be addressed by the spelling correction\\ntechniques of Chapter 3.\\n• Cross-language information retrieval. Documents in another language\\nare not nearby in a vector space based on term distribution. Rather, docu-\\nments in the same language cluster more closely together.\\n• Mismatch of searcher’s vocabulary versus collection vocabulary. If the\\nuser searches for laptop but all the documents use the term notebook com-\\nputer, then the query will fail, and relevance feedback is again most likely\\nineffective.\\nSecondly, the relevance feedback approach requires relevant documents to\\nbe similar to each other. That is, they should cluster. Ideally, the term dis-\\ntribution in all relevant documents will be similar to that in the documents\\nmarked by the users, while the term distribution in all nonrelevant docu-\\nments will be different from those in relevant documents. Things will work\\nwell if all relevant documents are tightly clustered around a single proto-\\ntype, or, at least, if there are different prototypes, if the relevant documents\\nhave signiﬁcant vocabulary overlap, while similarities between relevant and\\nnonrelevant documents are small. Implicitly, the Rocchio relevance feedback\\nmodel treats relevant documents as a single cluster, which it models via the\\ncentroid of the cluster. This approach does not work as well if the relevant\\ndocuments are a multimodal class, that is, they consist of several clusters of\\ndocuments within the vector space. This can happen with:\\n• Subsets of the documents using different vocabulary, such as Burma vs.\\nMyanmar\\n• A query for which the answer set is inherently disjunctive, such as Pop\\nstars who once worked at Burger King.\\n• Instances of a general concept, which often appear as a disjunction of\\nmore speciﬁc concepts, for example, felines.\\nGood editorial content in the collection can often provide a solution to this\\nproblem. For example, an article on the attitudes of different groups to the\\nsituation in Burma could introduce the terminology used by different parties,\\nthus linking the document clusters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n185\\nRelevance feedback is not necessarily popular with users. Users are often\\nreluctant to provide explicit feedback, or in general do not wish to prolong\\nthe search interaction. Furthermore, it is often harder to understand why a\\nparticular document was retrieved after relevance feedback is applied.\\nRelevance feedback can also have practical problems. The long queries\\nthat are generated by straightforward application of relevance feedback tech-\\nniques are inefﬁcient for a typical IR system. This results in a high computing\\ncost for the retrieval and potentially long response times for the user. A par-\\ntial solution to this is to only reweight certain prominent terms in the relevant\\ndocuments, such as perhaps the top 20 terms by term frequency. Some ex-\\nperimental results have also suggested that using a limited number of terms\\nlike this may give better results (Harman 1992) though other work has sug-\\ngested that using more terms is better in terms of retrieved document quality\\n(Buckley et al. 1994b).\\n9.1.4\\nRelevance feedback on the web\\nSome web search engines offer a similar/related pages feature: the user in-\\ndicates a document in the results set as exemplary from the standpoint of\\nmeeting his information need and requests more documents like it. This can\\nbe viewed as a particular simple form of relevance feedback. However, in\\ngeneral relevance feedback has been little used in web search. One exception\\nwas the Excite web search engine, which initially provided full relevance\\nfeedback. However, the feature was in time dropped, due to lack of use. On\\nthe web, few people use advanced search interfaces and most would like to\\ncomplete their search in a single interaction. But the lack of uptake also prob-\\nably reﬂects two other factors: relevance feedback is hard to explain to the\\naverage user, and relevance feedback is mainly a recall enhancing strategy,\\nand web search users are only rarely concerned with getting sufﬁcient recall.\\nSpink et al. (2000) present results from the use of relevance feedback in\\nthe Excite search engine. Only about 4% of user query sessions used the\\nrelevance feedback option, and these were usually exploiting the “More like\\nthis” link next to each result. About 70% of users only looked at the ﬁrst\\npage of results and did not pursue things any further. For people who used\\nrelevance feedback, results were improved about two thirds of the time.\\nAn important more recent thread of work is the use of clickstream data\\n(what links a user clicks on) to provide indirect relevance feedback. Use\\nof this data is studied in detail in (Joachims 2002b, Joachims et al. 2005).\\nThe very successful use of web link structure (see Chapter 21) can also be\\nviewed as implicit feedback, but provided by page authors rather than read-\\ners (though in practice most authors are also readers).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n186\\n9\\nRelevance feedback and query expansion\\n?\\nExercise 9.1\\nIn Rocchio’s algorithm, what weight setting for α/β/γ does a “Find pages like this\\none” search correspond to?\\nExercise 9.2\\n[⋆]\\nGive three reasons why relevance feedback has been little used in web search.\\n9.1.5\\nEvaluation of relevance feedback strategies\\nInteractive relevance feedback can give very substantial gains in retrieval\\nperformance. Empirically, one round of relevance feedback is often very\\nuseful. Two rounds is sometimes marginally more useful. Successful use of\\nrelevance feedback requires enough judged documents, otherwise the pro-\\ncess is unstable in that it may drift away from the user’s information need.\\nAccordingly, having at least ﬁve judged documents is recommended.\\nThere is some subtlety to evaluating the effectiveness of relevance feed-\\nback in a sound and enlightening way. The obvious ﬁrst strategy is to start\\nwith an initial query q0 and to compute a precision-recall graph. Following\\none round of feedback from the user, we compute the modiﬁed query qm\\nand again compute a precision-recall graph. Here, in both rounds we assess\\nperformance over all documents in the collection, which makes comparisons\\nstraightforward. If we do this, we ﬁnd spectacular gains from relevance feed-\\nback: gains on the order of 50% in mean average precision. But unfortunately\\nit is cheating. The gains are partly due to the fact that known relevant doc-\\numents (judged by the user) are now ranked higher. Fairness demands that\\nwe should only evaluate with respect to documents not seen by the user.\\nA second idea is to use documents in the residual collection (the set of doc-\\numents minus those assessed relevant) for the second round of evaluation.\\nThis seems like a more realistic evaluation. Unfortunately, the measured per-\\nformance can then often be lower than for the original query. This is partic-\\nularly the case if there are few relevant documents, and so a fair proportion\\nof them have been judged by the user in the ﬁrst round. The relative per-\\nformance of variant relevance feedback methods can be validly compared,\\nbut it is difﬁcult to validly compare performance with and without relevance\\nfeedback because the collection size and the number of relevant documents\\nchanges from before the feedback to after it.\\nThus neither of these methods is fully satisfactory. A third method is to\\nhave two collections, one which is used for the initial query and relevance\\njudgments, and the second that is then used for comparative evaluation. The\\nperformance of both q0 and qm can be validly compared on the second col-\\nlection.\\nPerhaps the best evaluation of the utility of relevance feedback is to do user\\nstudies of its effectiveness, in particular by doing a time-based comparison:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n187\\nPrecision at k = 50\\nTerm weighting\\nno RF\\npseudo RF\\nlnc.ltc\\n64.2%\\n72.7%\\nLnu.ltu\\n74.2%\\n87.0%\\n◮Figure 9.5\\nResults showing pseudo relevance feedback greatly improving perfor-\\nmance. These results are taken from the Cornell SMART system at TREC 4 (Buckley\\net al. 1995), and also contrast the use of two different length normalization schemes\\n(L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20\\nterms to each query.\\nhow fast does a user ﬁnd relevant documents with relevance feedback vs.\\nanother strategy (such as query reformulation), or alternatively, how many\\nrelevant documents does a user ﬁnd in a certain amount of time. Such no-\\ntions of user utility are fairest and closest to real system usage.\\n9.1.6\\nPseudo relevance feedback\\nPseudo relevance feedback, also known as blind relevance feedback, provides a\\nPSEUDO RELEVANCE\\nFEEDBACK\\nBLIND RELEVANCE\\nFEEDBACK\\nmethod for automatic local analysis. It automates the manual part of rele-\\nvance feedback, so that the user gets improved retrieval performance with-\\nout an extended interaction. The method is to do normal retrieval to ﬁnd an\\ninitial set of most relevant documents, to then assume that the top k ranked\\ndocuments are relevant, and ﬁnally to do relevance feedback as before under\\nthis assumption.\\nThis automatic technique mostly works. Evidence suggests that it tends\\nto work better than global analysis (Section 9.2). It has been found to im-\\nprove performance in the TREC ad hoc task. See for example the results in\\nFigure 9.5. But it is not without the dangers of an automatic process. For\\nexample, if the query is about copper mines and the top several documents\\nare all about mines in Chile, then there may be query drift in the direction of\\ndocuments on Chile.\\n9.1.7\\nIndirect relevance feedback\\nWe can also use indirect sources of evidence rather than explicit feedback on\\nrelevance as the basis for relevance feedback. This is often called implicit (rel-\\nIMPLICIT RELEVANCE\\nFEEDBACK\\nevance) feedback. Implicit feedback is less reliable than explicit feedback, but is\\nmore useful than pseudo relevance feedback, which contains no evidence of\\nuser judgments. Moreover, while users are often reluctant to provide explicit\\nfeedback, it is easy to collect implicit feedback in large quantities for a high\\nvolume system, such as a web search engine.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n188\\n9\\nRelevance feedback and query expansion\\nOn the web, DirectHit introduced the idea of ranking more highly docu-\\nments that users chose to look at more often. In other words, clicks on links\\nwere assumed to indicate that the page was likely relevant to the query. This\\napproach makes various assumptions, such as that the document summaries\\ndisplayed in results lists (on whose basis users choose which documents to\\nclick on) are indicative of the relevance of these documents. In the original\\nDirectHit search engine, the data about the click rates on pages was gathered\\nglobally, rather than being user or query speciﬁc. This is one form of the gen-\\neral area of clickstream mining. Today, a closely related approach is used in\\nCLICKSTREAM MINING\\nranking the advertisements that match a web search query (Chapter 19).\\n9.1.8\\nSummary\\nRelevance feedback has been shown to be very effective at improving rele-\\nvance of results. Its successful use requires queries for which the set of rele-\\nvant documents is medium to large. Full relevance feedback is often onerous\\nfor the user, and its implementation is not very efﬁcient in most IR systems.\\nIn many cases, other types of interactive retrieval may improve relevance by\\nabout as much with less work.\\nBeyond the core ad hoc retrieval scenario, other uses of relevance feedback\\ninclude:\\n• Following a changing information need (e.g., names of car models of in-\\nterest change over time)\\n• Maintaining an information ﬁlter (e.g., for a news feed). Such ﬁlters are\\ndiscussed further in Chapter 13.\\n• Active learning (deciding which examples it is most useful to know the\\nclass of to reduce annotation costs).\\n?\\nExercise 9.3\\nUnder what conditions would the modiﬁed query qm in Equation 9.3 be the same as\\nthe original query q0? In all other cases, is qm closer than q0 to the centroid of the\\nrelevant documents?\\nExercise 9.4\\nWhy is positive feedback likely to be more useful than negative feedback to an IR\\nsystem? Why might only using one nonrelevant document be more effective than\\nusing several?\\nExercise 9.5\\nSuppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs. The\\nuser examines two documents, d1 and d2. She judges d1, with the content CDs cheap\\nsoftware cheap CDs relevant and d2 with content cheap thrills DVDs nonrelevant. As-\\nsume that we are using direct term frequency (with no scaling and no document\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.2\\nGlobal methods for query reformulation\\n189\\nfrequency). There is no need to length-normalize vectors. Using Rocchio relevance\\nfeedback as in Equation (9.3) what would the revised query vector be after relevance\\nfeedback? Assume α = 1, β = 0.75, γ = 0.25.\\nExercise 9.6\\n[⋆]\\nOmar has implemented a relevance feedback web search system, where he is going\\nto do relevance feedback based only on words in the title text returned for a page (for\\nefﬁciency). The user is going to rank 3 results. The ﬁrst user, Jinxing, queries for:\\nbanana slug\\nand the top three titles returned are:\\nbanana slug Ariolimax columbianus\\nSanta Cruz mountains banana slug\\nSanta Cruz Campus Mascot\\nJinxing judges the ﬁrst two documents relevant, and the third nonrelevant. Assume\\nthat Omar’s search engine uses term frequency but no length normalization nor IDF.\\nAssume that he is using the Rocchio relevance feedback mechanism, with α = β =\\nγ = 1. Show the ﬁnal revised query that would be run. (Please list the vector elements\\nin alphabetical order.)\\n9.2\\nGlobal methods for query reformulation\\nIn this section we more brieﬂy discuss three global methods for expanding a\\nquery: by simply aiding the user in doing so, by using a manual thesaurus,\\nand through building a thesaurus automatically.\\n9.2.1\\nVocabulary tools for query reformulation\\nVarious user supports in the search process can help the user see how their\\nsearches are or are not working. This includes information about words that\\nwere omitted from the query because they were on stop lists, what words\\nwere stemmed to, the number of hits on each term or phrase, and whether\\nwords were dynamically turned into phrases. The IR system might also sug-\\ngest search terms by means of a thesaurus or a controlled vocabulary. A user\\ncan also be allowed to browse lists of the terms that are in the inverted index,\\nand thus ﬁnd good terms that appear in the collection.\\n9.2.2\\nQuery expansion\\nIn relevance feedback, users give additional input on documents (by mark-\\ning documents in the results set as relevant or not), and this input is used\\nto reweight the terms in the query for documents. In query expansion on the\\nQUERY EXPANSION\\nother hand, users give additional input on query words or phrases, possibly\\nsuggesting additional query terms. Some search engines (especially on the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n190\\n9\\nRelevance feedback and query expansion\\nY\\na\\nh\\no\\no\\n!\\nM\\ny\\nY\\na\\nh\\no\\no\\n!\\nM\\na\\ni\\nl\\nW\\ne\\nl\\nc\\no\\nm\\ne\\n,\\nG\\nu\\ne\\ns\\nt\\n[\\nS\\ni\\ng\\nn\\nI\\nn\\n]\\nH\\ne\\nl\\np\\nS\\ne\\na\\nr\\nc\\nh\\np\\na\\nl\\nm\\nW\\ne\\nb\\nI\\nm\\na\\ng\\ne\\ns\\nV\\ni\\nd\\ne\\no\\nL\\no\\nc\\na\\nl\\nS\\nh\\no\\np\\np\\ni\\nn\\ng\\nm\\no\\nr\\ne\\nO\\np\\nt\\ni\\no\\nn\\ns\\nA\\nl\\ns\\no\\nt\\nr\\ny\\n:\\nS\\nP\\nO\\nN\\nS\\nO\\nR\\nR\\nE\\nS\\nU\\nL\\nT\\nS\\np\\na\\nl\\nm\\nt\\nr\\ne\\ne\\ns\\n,\\np\\na\\nl\\nm\\ns\\np\\nr\\ni\\nn\\ng\\ns\\n,\\np\\na\\nl\\nm\\nc\\ne\\nn\\nt\\nr\\no\\n,\\np\\na\\nl\\nm\\nt\\nr\\ne\\no\\n,\\nM\\no\\nr\\ne\\n.\\n.\\n.\\nP\\na\\nl\\nm\\nb\\nA\\nT\\n&\\nT\\na\\nt\\nt\\n.\\nc\\no\\nm\\n/\\nw\\ni\\nr\\ne\\nl\\ne\\ns\\ns\\nl\\nG\\no\\nm\\no\\nb\\ni\\nl\\ne\\ne\\nf\\nf\\no\\nr\\nt\\nl\\ne\\ns\\ns\\nl\\ny\\nw\\ni\\nt\\nh\\nt\\nh\\ne\\nP\\nA\\nL\\nM\\nT\\nr\\ne\\no\\nf\\nr\\no\\nm\\nA\\nT\\n&\\nT\\n(\\nC\\ni\\nn\\ng\\nu\\nl\\na\\nr\\n)\\n.\\nP\\na\\nl\\nm\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\nP\\na\\nl\\nm\\n.\\nc\\no\\nm\\nl\\nO\\nr\\ng\\na\\nn\\ni\\nz\\ne\\nr\\n,\\nP\\nl\\na\\nn\\nn\\ne\\nr\\n,\\nW\\ni\\nF\\ni\\n,\\nM\\nu\\ns\\ni\\nc\\nB\\nl\\nu\\ne\\nt\\no\\no\\nt\\nh\\n,\\nG\\na\\nm\\ne\\ns\\n,\\nP\\nh\\no\\nt\\no\\ns\\n&\\nV\\ni\\nd\\ne\\no\\n.\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\nM\\na\\nk\\ne\\nr\\no\\nf\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\nP\\nD\\nA\\nd\\ne\\nv\\ni\\nc\\ne\\ns\\nt\\nh\\na\\nt\\na\\nl\\nl\\no\\nw\\nm\\no\\nb\\ni\\nl\\ne\\nu\\ns\\ne\\nr\\ns\\nt\\no\\nm\\na\\nn\\na\\ng\\ne\\ns\\nc\\nh\\ne\\nd\\nu\\nl\\ne\\ns\\n,\\nc\\no\\nn\\nt\\na\\nc\\nt\\ns\\n,\\na\\nn\\nd\\no\\nt\\nh\\ne\\nr\\np\\ne\\nr\\ns\\no\\nn\\na\\nl\\na\\nn\\nd\\nb\\nu\\ns\\ni\\nn\\ne\\ns\\ns\\ni\\nn\\nf\\no\\nr\\nm\\na\\nt\\ni\\no\\nn\\n.\\nw\\nw\\nw\\n.\\np\\na\\nl\\nm\\n.\\nc\\no\\nm\\nl\\nC\\na\\nc\\nh\\ne\\nd\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\nb\\nT\\nr\\ne\\no\\na\\nn\\nd\\nC\\ne\\nn\\nt\\nr\\no\\ns\\nm\\na\\nr\\nt\\np\\nh\\no\\nn\\ne\\ns\\n,\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\n,\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\n,\\ni\\nn\\nn\\no\\nv\\na\\nt\\no\\nr\\no\\nf\\ne\\na\\ns\\ny\\nl\\nt\\no\\nl\\nu\\ns\\ne\\nm\\no\\nb\\ni\\nl\\ne\\np\\nr\\no\\nd\\nu\\nc\\nt\\ns\\ni\\nn\\nc\\nl\\nu\\nd\\ni\\nn\\ng\\nP\\na\\nl\\nm\\n®\\nT\\nr\\ne\\no\\n_\\na\\nn\\nd\\nC\\ne\\nn\\nt\\nr\\no\\n_\\ns\\nm\\na\\nr\\nt\\np\\nh\\no\\nn\\ne\\ns\\n,\\nP\\na\\nl\\nm\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\n,\\ns\\ne\\nr\\nv\\ni\\nc\\ne\\ns\\n,\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\n.\\nw\\nw\\nw\\n.\\np\\na\\nl\\nm\\n.\\nc\\no\\nm\\n/\\nu\\ns\\nl\\nC\\na\\nc\\nh\\ne\\nd\\nS\\nP\\nO\\nN\\nS\\nO\\nR\\nR\\nE\\nS\\nU\\nL\\nT\\nS\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\na\\nt\\nD\\ne\\nl\\nl\\nS\\nt\\na\\ny\\nC\\no\\nn\\nn\\ne\\nc\\nt\\ne\\nd\\nw\\ni\\nt\\nh\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\nP\\nC\\ns\\n&\\nP\\nD\\nA\\ns\\n.\\nS\\nh\\no\\np\\na\\nt\\nD\\ne\\nl\\nl\\n™\\nO\\nf\\nf\\ni\\nc\\ni\\na\\nl\\nS\\ni\\nt\\ne\\n.\\nw\\nw\\nw\\n.\\nD\\ne\\nl\\nl\\n.\\nc\\no\\nm\\nB\\nu\\ny\\nP\\na\\nl\\nm\\nC\\ne\\nn\\nt\\nr\\no\\nC\\na\\ns\\ne\\ns\\nU\\nl\\nt\\ni\\nm\\na\\nt\\ne\\ns\\ne\\nl\\ne\\nc\\nt\\ni\\no\\nn\\no\\nf\\nc\\na\\ns\\ne\\ns\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\nf\\no\\nr\\nb\\nu\\ns\\ni\\nn\\ne\\ns\\ns\\nd\\ne\\nv\\ni\\nc\\ne\\ns\\n.\\nw\\nw\\nw\\n.\\nC\\na\\ns\\ne\\ns\\n.\\nc\\no\\nm\\nF\\nr\\ne\\ne\\nP\\nl\\na\\nm\\nT\\nr\\ne\\no\\nG\\ne\\nt\\nA\\nF\\nr\\ne\\ne\\nP\\na\\nl\\nm\\nT\\nr\\ne\\no\\n7\\n0\\n0\\nW\\nP\\nh\\no\\nn\\ne\\n.\\nP\\na\\nr\\nt\\ni\\nc\\ni\\np\\na\\nt\\ne\\nT\\no\\nd\\na\\ny\\n.\\nE\\nv\\na\\nl\\nu\\na\\nt\\ni\\no\\nn\\nN\\na\\nt\\ni\\no\\nn\\n.\\nc\\no\\nm\\n/\\nt\\nr\\ne\\no\\n1\\nª\\n1\\n0\\no\\nf\\na\\nb\\no\\nu\\nt\\n5\\n3\\n4\\n,\\n0\\n0\\n0\\n,\\n0\\n0\\n0\\nf\\no\\nr\\np\\na\\nl\\nm\\n(\\nA\\nb\\no\\nu\\nt\\nt\\nh\\ni\\ns\\np\\na\\ng\\ne\\n)\\nª\\n0\\n.\\n1\\n1\\ns\\ne\\nc\\n.\\n◮Figure 9.6\\nAn example of query expansion in the interface of the Yahoo! web\\nsearch engine in 2006. The expanded query suggestions appear just below the “Search\\nResults” bar.\\nweb) suggest related queries in response to a query; the users then opt to use\\none of these alternative query suggestions. Figure 9.6 shows an example of\\nquery suggestion options being presented in the Yahoo! web search engine.\\nThe central question in this form of query expansion is how to generate al-\\nternative or expanded queries for the user. The most common form of query\\nexpansion is global analysis, using some form of thesaurus. For each term\\nt in a query, the query can be automatically expanded with synonyms and\\nrelated words of t from the thesaurus. Use of a thesaurus can be combined\\nwith ideas of term weighting: for instance, one might weight added terms\\nless than original query terms.\\nMethods for building a thesaurus for query expansion include:\\n• Use of a controlled vocabulary that is maintained by human editors. Here,\\nthere is a canonical term for each concept. The subject headings of tra-\\nditional library subject indexes, such as the Library of Congress Subject\\nHeadings, or the Dewey Decimal system are examples of a controlled\\nvocabulary. Use of a controlled vocabulary is quite common for well-\\nresourced domains. A well-known example is the Uniﬁed Medical Lan-\\nguage System (UMLS) used with MedLine for querying the biomedical\\nresearch literature. For example, in Figure 9.7, neoplasms was added to a\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.2\\nGlobal methods for query reformulation\\n191\\n• User query: cancer\\n• PubMed query: (“neoplasms”[TIAB] NOT Medline[SB]) OR “neoplasms”[MeSH\\nTerms] OR cancer[Text Word]\\n• User query: skin itch\\n• PubMed query: (“skin”[MeSH Terms] OR (“integumentary system”[TIAB] NOT\\nMedline[SB]) OR “integumentary system”[MeSH Terms] OR skin[Text Word]) AND\\n((“pruritus”[TIAB] NOT Medline[SB]) OR “pruritus”[MeSH Terms] OR itch[Text\\nWord])\\n◮Figure 9.7\\nExamples of query expansion via the PubMed thesaurus. When a user\\nissues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/,\\ntheir query is mapped on to the Medline vocabulary as shown.\\nsearch for cancer. This Medline query expansion also contrasts with the\\nYahoo! example. The Yahoo! interface is a case of interactive query expan-\\nsion, whereas PubMed does automatic query expansion. Unless the user\\nchooses to examine the submitted query, they may not even realize that\\nquery expansion has occurred.\\n• A manual thesaurus. Here, human editors have built up sets of synony-\\nmous names for concepts, without designating a canonical term.\\nThe\\nUMLS metathesaurus is one example of a thesaurus. Statistics Canada\\nmaintains a thesaurus of preferred terms, synonyms, broader terms, and\\nnarrower terms for matters on which the government collects statistics,\\nsuch as goods and services. This thesaurus is also bilingual English and\\nFrench.\\n• An automatically derived thesaurus. Here, word co-occurrence statistics\\nover a collection of documents in a domain are used to automatically in-\\nduce a thesaurus; see Section 9.2.3.\\n• Query reformulations based on query log mining. Here, we exploit the\\nmanual query reformulations of other users to make suggestions to a new\\nuser. This requires a huge query volume, and is thus particularly appro-\\npriate to web search.\\nThesaurus-based query expansion has the advantage of not requiring any\\nuser input. Use of query expansion generally increases recall and is widely\\nused in many science and engineering ﬁelds. As well as such global analysis\\ntechniques, it is also possible to do query expansion by local analysis, for\\ninstance, by analyzing the documents in the result set. User input is now\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n192\\n9\\nRelevance feedback and query expansion\\nWord\\nNearest neighbors\\nabsolutely\\nabsurd, whatsoever, totally, exactly, nothing\\nbottomed\\ndip, copper, drops, topped, slide, trimmed\\ncaptivating\\nshimmer, stunningly, superbly, plucky, witty\\ndoghouse\\ndog, porch, crawling, beside, downstairs\\nmakeup\\nrepellent, lotion, glossy, sunscreen, skin, gel\\nmediating\\nreconciliation, negotiate, case, conciliation\\nkeeping\\nhoping, bring, wiping, could, some, would\\nlithographs\\ndrawings, Picasso, Dali, sculptures, Gauguin\\npathogens\\ntoxins, bacteria, organisms, bacterial, parasite\\nsenses\\ngrasp, psyche, truly, clumsy, naive, innate\\n◮Figure 9.8\\nAn example of an automatically generated thesaurus. This example\\nis based on the work in Schütze (1998), which employs latent semantic indexing (see\\nChapter 18).\\nusually required, but a distinction remains as to whether the user is giving\\nfeedback on documents or on query terms.\\n9.2.3\\nAutomatic thesaurus generation\\nAs an alternative to the cost of a manual thesaurus, we could attempt to\\ngenerate a thesaurus automatically by analyzing a collection of documents.\\nThere are two main approaches. One is simply to exploit word cooccurrence.\\nWe say that words co-occurring in a document or paragraph are likely to be\\nin some sense similar or related in meaning, and simply count text statistics\\nto ﬁnd the most similar words. The other approach is to use a shallow gram-\\nmatical analysis of the text and to exploit grammatical relations or grammat-\\nical dependencies. For example, we say that entities that are grown, cooked,\\neaten, and digested, are more likely to be food items. Simply using word\\ncooccurrence is more robust (it cannot be misled by parser errors), but using\\ngrammatical relations is more accurate.\\nThe simplest way to compute a co-occurrence thesaurus is based on term-\\nterm similarities. We begin with a term-document matrix A, where each cell\\nAt,d is a weighted count wt,d for term t and document d, with weighting so\\nA has length-normalized rows. If we then calculate C = AAT, then Cu,v is\\na similarity score between terms u and v, with a larger number being better.\\nFigure 9.8 shows an example of a thesaurus derived in basically this manner,\\nbut with an extra step of dimensionality reduction via Latent Semantic In-\\ndexing, which we discuss in Chapter 18. While some of the thesaurus terms\\nare good or at least suggestive, others are marginal or bad. The quality of the\\nassociations is typically a problem. Term ambiguity easily introduces irrel-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n9.3\\nReferences and further reading\\n193\\nevant statistically correlated terms. For example, a query for Apple computer\\nmay expand to Apple red fruit computer. In general these thesauri suffer from\\nboth false positives and false negatives. Moreover, since the terms in the au-\\ntomatic thesaurus are highly correlated in documents anyway (and often the\\ncollection used to derive the thesaurus is the same as the one being indexed),\\nthis form of query expansion may not retrieve many additional documents.\\nQuery expansion is often effective in increasing recall. However, there is\\na high cost to manually producing a thesaurus and then updating it for sci-\\nentiﬁc and terminological developments within a ﬁeld. In general a domain-\\nspeciﬁc thesaurus is required: general thesauri and dictionaries give far too\\nlittle coverage of the rich domain-particular vocabularies of most scientiﬁc\\nﬁelds. However, query expansion may also signiﬁcantly decrease precision,\\nparticularly when the query contains ambiguous terms. For example, if the\\nuser searches for interest rate, expanding the query to interest rate fascinate eval-\\nuate is unlikely to be useful. Overall, query expansion is less successful than\\nrelevance feedback, though it may be as good as pseudo relevance feedback.\\nIt does, however, have the advantage of being much more understandable to\\nthe system user.\\n?\\nExercise 9.7\\nIf A is simply a Boolean cooccurrence matrix, then what do you get as the entries in\\nC?\\n9.3\\nReferences and further reading\\nWork in information retrieval quickly confronted the problem of variant ex-\\npression which meant that the words in a query might not appear in a doc-\\nument, despite it being relevant to the query. An early experiment about\\n1960 cited by Swanson (1988) found that only 11 out of 23 documents prop-\\nerly indexed under the subject toxicity had any use of a word containing the\\nstem toxi. There is also the issue of translation, of users knowing what terms\\na document will use. Blair and Maron (1985) conclude that “it is impossibly\\ndifﬁcult for users to predict the exact words, word combinations, and phrases\\nthat are used by all (or most) relevant documents and only (or primarily) by\\nthose documents”.\\nThe main initial papers on relevance feedback using vector space models\\nall appear in Salton (1971b), including the presentation of the Rocchio al-\\ngorithm (Rocchio 1971) and the Ide dec-hi variant along with evaluation of\\nseveral variants (Ide 1971). Another variant is to regard all documents in\\nthe collection apart from those judged relevant as nonrelevant, rather than\\nonly ones that are explicitly judged nonrelevant. However, Schütze et al.\\n(1995) and Singhal et al. (1997) show that better results are obtained for rout-\\ning by using only documents close to the query of interest rather than all\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n194\\n9\\nRelevance feedback and query expansion\\ndocuments. Other later work includes Salton and Buckley (1990), Riezler\\net al. (2007) (a statistical NLP approach to RF) and the recent survey paper\\nRuthven and Lalmas (2003).\\nThe effectiveness of interactive relevance feedback systems is discussed in\\n(Salton 1989, Harman 1992, Buckley et al. 1994b). Koenemann and Belkin\\n(1996) do user studies of the effectiveness of relevance feedback.\\nTraditionally Roget’s thesaurus has been the best known English language\\nthesaurus (Roget 1946). In recent computational work, people almost always\\nuse WordNet (Fellbaum 1998), not only because it is free, but also because of\\nits rich link structure. It is available at: http://wordnet.princeton.edu.\\nQiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus gener-\\nation. Xu and Croft (1996) explore using both local and global query expan-\\nsion.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n195\\n10\\nXML retrieval\\nInformation retrieval systems are often contrasted with relational databases.\\nTraditionally, IR systems have retrieved information from unstructured text\\n– by which we mean “raw” text without markup. Databases are designed\\nfor querying relational data: sets of records that have values for predeﬁned\\nattributes such as employee number, title and salary. There are fundamental\\ndifferences between information retrieval and database systems in terms of\\nretrieval model, data structures and query language as shown in Table 10.1.1\\nSome highly structured text search problems are most efﬁciently handled\\nby a relational database, for example, if the employee table contains an at-\\ntribute for short textual job descriptions and you want to ﬁnd all employees\\nwho are involved with invoicing. In this case, the SQL query:\\nselect lastname from employees where job_desc like ’invoic%’;\\nmay be sufﬁcient to satisfy your information need with high precision and\\nrecall.\\nHowever, many structured data sources containing text are best modeled\\nas structured documents rather than relational data. We call the search over\\nsuch structured documents structured retrieval. Queries in structured retrieval\\nSTRUCTURED\\nRETRIEVAL\\ncan be either structured or unstructured, but we will assume in this chap-\\nter that the collection consists only of structured documents. Applications\\nof structured retrieval include digital libraries, patent databases, blogs, text\\nin which entities like persons and locations have been tagged (in a process\\ncalled named entity tagging) and output from ofﬁce suites like OpenOfﬁce\\nthat save documents as marked up text. In all of these applications, we want\\nto be able to run queries that combine textual criteria with structural criteria.\\nExamples of such queries are give me a full-length article on fast fourier transforms\\n(digital libraries), give me patents whose claims mention RSA public key encryption\\n1. In most modern database systems, one can enable full-text search for text columns. This\\nusually means that an inverted index is created and Boolean or vector space search enabled,\\neffectively combining core database with information retrieval technologies.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n196\\n10\\nXML retrieval\\nRDB search\\nunstructured retrieval\\nstructured retrieval\\nobjects\\nrecords\\nunstructured documents\\ntrees with text at leaves\\nmodel\\nrelational model\\nvector space & others\\n?\\nmain data structure\\ntable\\ninverted index\\n?\\nqueries\\nSQL\\nfree text queries\\n?\\n◮Table 10.1\\nRDB (relational database) search, unstructured information retrieval\\nand structured information retrieval. There is no consensus yet as to which methods\\nwork best for structured retrieval although many researchers believe that XQuery\\n(page 215) will become the standard for structured queries.\\nand that cite US patent 4,405,829 (patents), or give me articles about sightseeing\\ntours of the Vatican and the Coliseum (entity-tagged text). These three queries\\nare structured queries that cannot be answered well by an unranked retrieval\\nsystem. As we argued in Example 1.1 (page 15) unranked retrieval models\\nlike the Boolean model suffer from low recall. For instance, an unranked\\nsystem would return a potentially large number of articles that mention the\\nVatican, the Coliseum and sightseeing tours without ranking the ones that\\nare most relevant for the query ﬁrst. Most users are also notoriously bad at\\nprecisely stating structural constraints. For instance, users may not know\\nfor which structured elements the search system supports search. In our ex-\\nample, the user may be unsure whether to issue the query as sightseeing AND\\n(COUNTRY:Vatican OR LANDMARK:Coliseum) , as sightseeing AND (STATE:Vatican OR\\nBUILDING:Coliseum) or in some other form. Users may also be completely un-\\nfamiliar with structured search and advanced search interfaces or unwilling\\nto use them. In this chapter, we look at how ranked retrieval methods can be\\nadapted to structured documents to address these problems.\\nWe will only look at one standard for encoding structured documents: Ex-\\ntensible Markup Language or XML, which is currently the most widely used\\nXML\\nsuch standard. We will not cover the speciﬁcs that distinguish XML from\\nother types of markup such as HTML and SGML. But most of what we say\\nin this chapter is applicable to markup languages in general.\\nIn the context of information retrieval, we are only interested in XML as\\na language for encoding text and documents. A perhaps more widespread\\nuse of XML is to encode non-text data. For example, we may want to export\\ndata in XML format from an enterprise resource planning system and then\\nread them into an analytics program to produce graphs for a presentation.\\nThis type of application of XML is called data-centric because numerical and\\nDATA-CENTRIC XML\\nnon-text attribute-value data dominate and text is usually a small fraction of\\nthe overall data. Most data-centric XML is stored in databases – in contrast\\nto the inverted index-based methods for text-centric XML that we present in\\nthis chapter.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.1\\nBasic XML concepts\\n197\\nWe call XML retrieval structured retrieval in this chapter. Some researchers\\nprefer the term semistructured retrieval to distinguish XML retrieval from database\\nSEMISTRUCTURED\\nRETRIEVAL\\nquerying. We have adopted the terminology that is widespread in the XML\\nretrieval community. For instance, the standard way of referring to XML\\nqueries is structured queries, not semistructured queries. The term structured\\nretrieval is rarely used for database querying and it always refers to XML\\nretrieval in this book.\\nThere is a second type of information retrieval problem that is intermediate\\nbetween unstructured retrieval and querying a relational database: paramet-\\nric and zone search, which we discussed in Section 6.1 (page 110). In the\\ndata model of parametric and zone search, there are parametric ﬁelds (re-\\nlational attributes like date or ﬁle-size) and zones – text attributes that each\\ntake a chunk of unstructured text as value, e.g., author and title in Figure 6.1\\n(page 111). The data model is ﬂat, that is, there is no nesting of attributes.\\nThe number of attributes is small. In contrast, XML documents have the\\nmore complex tree structure that we see in Figure 10.2 in which attributes\\nare nested. The number of attributes and nodes is greater than in parametric\\nand zone search.\\nAfter presenting the basic concepts of XML in Section 10.1, this chapter\\nﬁrst discusses the challenges we face in XML retrieval (Section 10.2). Next we\\ndescribe a vector space model for XML retrieval (Section 10.3). Section 10.4\\npresents INEX, a shared task evaluation that has been held for a number of\\nyears and currently is the most important venue for XML retrieval research.\\nWe discuss the differences between data-centric and text-centric approaches\\nto XML in Section 10.5.\\n10.1\\nBasic XML concepts\\nAn XML document is an ordered, labeled tree. Each node of the tree is an\\nXML element and is written with an opening and closing tag. An element can\\nXML ELEMENT\\nhave one or more XML attributes. In the XML document in Figure 10.1, the\\nXML ATTRIBUTE\\nscene element is enclosed by the two tags <scene ...> and </scene>. It\\nhas an attribute number with value vii and two child elements, title and verse.\\nFigure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of\\ntext, e.g., Shakespeare, Macbeth, and Macbeth’s castle. The tree’s internal nodes\\nencode either the structure of the document (title, act, and scene) or metadata\\nfunctions (author).\\nThe standard for accessing and processing XML documents is the XML\\nDocument Object Model or DOM. The DOM represents elements, attributes\\nXML DOM\\nand text within elements as nodes in a tree. Figure 10.2 is a simpliﬁed DOM\\nrepresentation of the XML document in Figure 10.1.2 With a DOM API, we\\n2. The representation is simpliﬁed in a number of respects. For example, we do not show the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n198\\n10\\nXML retrieval\\n<play>\\n<author>Shakespeare</author>\\n<title>Macbeth</title>\\n<act number=\"I\">\\n<scene number=\"vii\">\\n<title>Macbeth’s castle</title>\\n<verse>Will I with wine and wassail ...</verse>\\n</scene>\\n</act>\\n</play>\\n◮Figure 10.1\\nAn XML document.\\nroot element\\nplay\\nelement\\nauthor\\nelement\\nact\\nelement\\ntitle\\ntext\\nShakespeare\\ntext\\nMacbeth\\nattribute\\nnumber=\"I\"\\nelement\\nscene\\nattribute\\nnumber=\"vii\"\\nelement\\nverse\\nelement\\ntitle\\ntext\\nWill I with ...\\ntext\\nMacbeth’s castle\\n◮Figure 10.2\\nThe XML document in Figure 10.1 as a simpliﬁed DOM object.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.1\\nBasic XML concepts\\n199\\n//article\\n[.//yr = 2001 or .//yr = 2002]\\n//section\\n[about(.,summer holidays)]\\nholidays\\nsummer\\nsection\\narticle\\n◮Figure 10.3\\nAn XML query in NEXI format and its partial representation as a tree.\\ncan process an XML document by starting at the root element and then de-\\nscending down the tree from parents to children.\\nXPath is a standard for enumerating paths in an XML document collection.\\nXPATH\\nWe will also refer to paths as XML contexts or simply contexts in this chapter.\\nXML CONTEXT\\nOnly a small subset of XPath is needed for our purposes. The XPath expres-\\nsion node selects all nodes of that name. Successive elements of a path are\\nseparated by slashes, so act/scene selects all scene elements whose par-\\nent is an act element. Double slashes indicate that an arbitrary number of\\nelements can intervene on a path: play//scene selects all scene elements\\noccurring in a play element. In Figure 10.2 this set consists of a single scene el-\\nement, which is accessible via the path play, act, scene from the top. An initial\\nslash starts the path at the root element. /play/title selects the play’s ti-\\ntle in Figure 10.1, /play//title selects a set with two members (the play’s\\ntitle and the scene’s title), and /scene/title selects no elements. For no-\\ntational convenience, we allow the ﬁnal element of a path to be a vocabulary\\nterm and separate it from the element path by the symbol #, even though this\\ndoes not conform to the XPath standard. For example, title#\"Macbeth\"\\nselects all titles containing the term Macbeth.\\nWe also need the concept of schema in this chapter. A schema puts con-\\nSCHEMA\\nstraints on the structure of allowable XML documents for a particular ap-\\nplication. A schema for Shakespeare’s plays may stipulate that scenes can\\nonly occur as children of acts and that only acts and scenes have the num-\\nber attribute. Two standards for schemas for XML documents are XML DTD\\nXML DTD\\n(document type deﬁnition) and XML Schema. Users can only write structured\\nXML SCHEMA\\nqueries for an XML retrieval system if they have some minimal knowledge\\nabout the schema of the collection.\\nroot node and text is not embedded in text nodes. See http://www.w3.org/DOM/.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n200\\n10\\nXML retrieval\\nM’s castle\\ntitle\\nWill I . . .\\nverse\\nscene\\nJulius Caesar\\ntitle\\nbook\\nGallic war\\ntitle\\nJulius Caesar\\nauthor\\nbook\\nd1\\nq1\\nq2\\n◮Figure 10.4\\nTree representation of XML documents and queries.\\nA common format for XML queries is NEXI (Narrowed Extended XPath\\nNEXI\\nI). We give an example in Figure 10.3. We display the query on four lines for\\ntypographical convenience, but it is intended to be read as one unit without\\nline breaks. In particular, //section is embedded under //article.\\nThe query in Figure 10.3 speciﬁes a search for sections about the sum-\\nmer holidays that are part of articles from 2001 or 2002. As in XPath dou-\\nble slashes indicate that an arbitrary number of elements can intervene on\\na path.\\nThe dot in a clause in square brackets refers to the element the\\nclause modiﬁes. The clause [.//yr = 2001 or .//yr = 2002] mod-\\niﬁes //article. Thus, the dot refers to //article in this case. Similarly,\\nthe dot in [about(., summer holidays)] refers to the section that the\\nclause modiﬁes.\\nThe two yr conditions are relational attribute constraints. Only articles\\nwhose yr attribute is 2001 or 2002 (or that contain an element whose yr\\nattribute is 2001 or 2002) are to be considered. The about clause is a ranking\\nconstraint: Sections that occur in the right type of article are to be ranked\\naccording to how relevant they are to the topic summer holidays.\\nWe usually handle relational attribute constraints by preﬁltering or post-\\nﬁltering: We simply exclude all elements from the result set that do not meet\\nthe relational attribute constraints. In this chapter, we will not address how\\nto do this efﬁciently and instead focus on the core information retrieval prob-\\nlem in XML retrieval, namely how to rank documents according to the rele-\\nvance criteria expressed in the about conditions of the NEXI query.\\nIf we discard relational attributes, we can represent documents as trees\\nwith only one type of node: element nodes. In other words, we remove\\nall attribute nodes from the XML document, such as the number attribute in\\nFigure 10.1. Figure 10.4 shows a subtree of the document in Figure 10.1 as an\\nelement-node tree (labeled d1).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n201\\nWe can represent queries as trees in the same way. This is a query-by-\\nexample approach to query language design because users pose queries by\\ncreating objects that satisfy the same formal description as documents. In\\nFigure 10.4, q1 is a search for books whose titles score highly for the keywords\\nJulius Caesar. q2 is a search for books whose author elements score highly for\\nJulius Caesar and whose title elements score highly for Gallic war.3\\n10.2\\nChallenges in XML retrieval\\nIn this section, we discuss a number of challenges that make structured re-\\ntrieval more difﬁcult than unstructured retrieval. Recall from page 195 the\\nbasic setting we assume in structured retrieval: the collection consists of\\nstructured documents and queries are either structured (as in Figure 10.3)\\nor unstructured (e.g., summer holidays).\\nThe ﬁrst challenge in structured retrieval is that users want us to return\\nparts of documents (i.e., XML elements), not entire documents as IR systems\\nusually do in unstructured retrieval. If we query Shakespeare’s plays for\\nMacbeth’s castle, should we return the scene, the act or the entire play in Fig-\\nure 10.2? In this case, the user is probably looking for the scene. On the other\\nhand, an otherwise unspeciﬁed search for Macbeth should return the play of\\nthis name, not a subunit.\\nOne criterion for selecting the most appropriate part of a document is the\\nstructured document retrieval principle:\\nSTRUCTURED\\nDOCUMENT RETRIEVAL\\nPRINCIPLE\\nStructured document retrieval principle. A system should always re-\\ntrieve the most speciﬁc part of a document answering the query.\\nThis principle motivates a retrieval strategy that returns the smallest unit\\nthat contains the information sought, but does not go below this level. How-\\never, it can be hard to implement this principle algorithmically. Consider the\\nquery title#\"Macbeth\" applied to Figure 10.2. The title of the tragedy,\\nMacbeth, and the title of Act I, Scene vii, Macbeth’s castle, are both good hits\\nbecause they contain the matching term Macbeth. But in this case, the title of\\nthe tragedy, the higher node, is preferred. Deciding which level of the tree is\\nright for answering a query is difﬁcult.\\nParallel to the issue of which parts of a document to return to the user is\\nthe issue of which parts of a document to index. In Section 2.1.2 (page 20), we\\ndiscussed the need for a document unit or indexing unit in indexing and re-\\nINDEXING UNIT\\ntrieval. In unstructured retrieval, it is usually clear what the right document\\n3. To represent the semantics of NEXI queries fully we would also need to designate one node\\nin the tree as a “target node”, for example, the section in the tree in Figure 10.3. Without the\\ndesignation of a target node, the tree in Figure 10.3 is not a search for sections embedded in\\narticles (as speciﬁed by NEXI), but a search for articles that contain sections.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n202\\n10\\nXML retrieval\\n◮Figure 10.5\\nPartitioning an XML document into non-overlapping indexing units.\\nunit is: ﬁles on your desktop, email messages, web pages on the web etc. In\\nstructured retrieval, there are a number of different approaches to deﬁning\\nthe indexing unit.\\nOne approach is to group nodes into non-overlapping pseudodocuments\\nas shown in Figure 10.5. In the example, books, chapters and sections have\\nbeen designated to be indexing units, but without overlap. For example, the\\nleftmost dashed indexing unit contains only those parts of the tree domi-\\nnated by book that are not already part of other indexing units. The disad-\\nvantage of this approach is that pseudodocuments may not make sense to\\nthe user because they are not coherent units. For instance, the leftmost in-\\ndexing unit in Figure 10.5 merges three disparate elements, the class, author\\nand title elements.\\nWe can also use one of the largest elements as the indexing unit, for exam-\\nple, the book element in a collection of books or the play element for Shake-\\nspeare’s works. We can then postprocess search results to ﬁnd for each book\\nor play the subelement that is the best hit. For example, the query Macbeth’s\\ncastle may return the play Macbeth, which we can then postprocess to identify\\nact I, scene vii as the best-matching subelement. Unfortunately, this two-\\nstage retrieval process fails to return the best subelement for many queries\\nbecause the relevance of a whole book is often not a good predictor of the\\nrelevance of small subelements within it.\\nInstead of retrieving large units and identifying subelements (top down),\\nwe can also search all leaves, select the most relevant ones and then extend\\nthem to larger units in postprocessing (bottom up). For the query Macbeth’s\\ncastle in Figure 10.1, we would retrieve the title Macbeth’s castle in the ﬁrst\\npass and then decide in a postprocessing step whether to return the title, the\\nscene, the act or the play. This approach has a similar problem as the last one:\\nThe relevance of a leaf element is often not a good predictor of the relevance\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n203\\nof elements it is contained in.\\nThe least restrictive approach is to index all elements. This is also prob-\\nlematic. Many XML elements are not meaningful search results, e.g., typo-\\ngraphical elements like <b>definitely</b> or an ISBN number which\\ncannot be interpreted without context. Also, indexing all elements means\\nthat search results will be highly redundant. For the query Macbeth’s castle\\nand the document in Figure 10.1, we would return all of the play, act, scene\\nand title elements on the path between the root node and Macbeth’s castle.\\nThe leaf node would then occur four times in the result set, once directly and\\nthree times as part of other elements. We call elements that are contained\\nwithin each other nested. Returning redundant nested elements in a list of\\nNESTED ELEMENTS\\nreturned hits is not very user-friendly.\\nBecause of the redundancy caused by nested elements it is common to re-\\nstrict the set of elements that are eligible to be returned. Restriction strategies\\ninclude:\\n• discard all small elements\\n• discard all element types that users do not look at (this requires a working\\nXML retrieval system that logs this information)\\n• discard all element types that assessors generally do not judge to be rele-\\nvant (if relevance assessments are available)\\n• only keep element types that a system designer or librarian has deemed\\nto be useful search results\\nIn most of these approaches, result sets will still contain nested elements.\\nThus, we may want to remove some elements in a postprocessing step to re-\\nduce redundancy. Alternatively, we can collapse several nested elements in\\nthe results list and use highlighting of query terms to draw the user’s atten-\\ntion to the relevant passages. If query terms are highlighted, then scanning a\\nmedium-sized element (e.g., a section) takes little more time than scanning a\\nsmall subelement (e.g., a paragraph). Thus, if the section and the paragraph\\nboth occur in the results list, it is sufﬁcient to show the section. An additional\\nadvantage of this approach is that the paragraph is presented together with\\nits context (i.e., the embedding section). This context may be helpful in in-\\nterpreting the paragraph (e.g., the source of the information reported) even\\nif the paragraph on its own satisﬁes the query.\\nIf the user knows the schema of the collection and is able to specify the\\ndesired type of element, then the problem of redundancy is alleviated as few\\nnested elements have the same type. But as we discussed in the introduction,\\nusers often don’t know what the name of an element in the collection is (Is the\\nVatican a country or a city?) or they may not know how to compose structured\\nqueries at all.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n204\\n10\\nXML retrieval\\nGates\\nbook\\nGates\\nauthor\\nbook\\nGates\\ncreator\\nbook\\nGates\\nlastname\\nBill\\nﬁrstname\\nauthor\\nbook\\nq3\\nq4\\nd2\\nd3\\n◮Figure 10.6\\nSchema heterogeneity: intervening nodes and mismatched names.\\nA challenge in XML retrieval related to nesting is that we may need to\\ndistinguish different contexts of a term when we compute term statistics for\\nranking, in particular inverse document frequency (idf) statistics as deﬁned\\nin Section 6.2.1 (page 117). For example, the term Gates under the node author\\nis unrelated to an occurrence under a content node like section if used to refer\\nto the plural of gate. It makes little sense to compute a single document\\nfrequency for Gates in this example.\\nOne solution is to compute idf for XML-context/term pairs, e.g., to com-\\npute different idf weights for author#\"Gates\" and section#\"Gates\".\\nUnfortunately, this scheme will run into sparse data problems – that is, many\\nXML-context pairs occur too rarely to reliably estimate df (see Section 13.2,\\npage 260, for a discussion of sparseness). A compromise is only to con-\\nsider the parent node x of the term and not the rest of the path from the\\nroot to x to distinguish contexts. There are still conﬂations of contexts that\\nare harmful in this scheme. For instance, we do not distinguish names of\\nauthors and names of corporations if both have the parent node name. But\\nmost important distinctions, like the example contrast author#\"Gates\" vs.\\nsection#\"Gates\", will be respected.\\nIn many cases, several different XML schemas occur in a collection since\\nthe XML documents in an IR application often come from more than one\\nsource. This phenomenon is called schema heterogeneity or schema diversity\\nSCHEMA\\nHETEROGENEITY\\nand presents yet another challenge. As illustrated in Figure 10.6 comparable\\nelements may have different names: creator in d2 vs. author in d3. In other\\ncases, the structural organization of the schemas may be different: Author\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n205\\nnames are direct descendants of the node author in q3, but there are the in-\\ntervening nodes ﬁrstname and lastname in d3. If we employ strict matching\\nof trees, then q3 will retrieve neither d2 nor d3 although both documents are\\nrelevant. Some form of approximate matching of element names in combina-\\ntion with semi-automatic matching of different document structures can help\\nhere. Human editing of correspondences of elements in different schemas\\nwill usually do better than automatic methods.\\nSchema heterogeneity is one reason for query-document mismatches like\\nq3/d2 and q3/d3. Another reason is that users often are not familiar with the\\nelement names and the structure of the schemas of collections they search\\nas mentioned. This poses a challenge for interface design in XML retrieval.\\nIdeally, the user interface should expose the tree structure of the collection\\nand allow users to specify the elements they are querying. If we take this\\napproach, then designing the query interface in structured retrieval is more\\ncomplex than a search box for keyword queries in unstructured retrieval.\\nWe can also support the user by interpreting all parent-child relationships\\nin queries as descendant relationships with any number of intervening nodes\\nallowed. We call such queries extended queries. The tree in Figure 10.3 and q4\\nEXTENDED QUERY\\nin Figure 10.6 are examples of extended queries. We show edges that are\\ninterpreted as descendant relationships as dashed arrows. In q4, a dashed\\narrow connects book and Gates. As a pseudo-XPath notation for q4, we adopt\\nbook//#\"Gates\": a book that somewhere in its structure contains the word\\nGates where the path from the book node to Gates can be arbitrarily long.\\nThe pseudo-XPath notation for the extended query that in addition speciﬁes\\nthat Gates occurs in a section of the book is book//section//#\"Gates\".\\nIt is convenient for users to be able to issue such extended queries without\\nhaving to specify the exact structural conﬁguration in which a query term\\nshould occur – either because they do not care about the exact conﬁguration\\nor because they do not know enough about the schema of the collection to be\\nable to specify it.\\nIn Figure 10.7, the user is looking for a chapter entitled FFT (q5). Sup-\\npose there is no such chapter in the collection, but that there are references to\\nbooks on FFT (d4). A reference to a book on FFT is not exactly what the user\\nis looking for, but it is better than returning nothing. Extended queries do not\\nhelp here. The extended query q6 also returns nothing. This is a case where\\nwe may want to interpret the structural constraints speciﬁed in the query as\\nhints as opposed to as strict conditions. As we will discuss in Section 10.4,\\nusers prefer a relaxed interpretation of structural constraints: Elements that\\ndo not meet structural constraints perfectly should be ranked lower, but they\\nshould not be omitted from search results.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n206\\n10\\nXML retrieval\\nFFT\\ntitle\\nchapter\\nFFT\\ntitle\\nchapter\\nFFT\\ntitle\\nencryption\\ntitle\\nreferences\\nchapter\\nbook\\nq5\\nq6\\nd4\\n◮Figure 10.7\\nA structural mismatch between two queries and a document.\\n10.3\\nA vector space model for XML retrieval\\nIn this section, we present a simple vector space model for XML retrieval.\\nIt is not intended to be a complete description of a state-of-the-art system.\\nInstead, we want to give the reader a ﬂavor of how documents can be repre-\\nsented and retrieved in XML retrieval.\\nTo take account of structure in retrieval in Figure 10.4, we want a book\\nentitled Julius Caesar to be a match for q1 and no match (or a lower weighted\\nmatch) for q2. In unstructured retrieval, there would be a single dimension\\nof the vector space for Caesar. In XML retrieval, we must separate the title\\nword Caesar from the author name Caesar. One way of doing this is to have\\neach dimension of the vector space encode a word together with its position\\nwithin the XML tree.\\nFigure 10.8 illustrates this representation. We ﬁrst take each text node\\n(which in our setup is always a leaf) and break it into multiple nodes, one for\\neach word. So the leaf node Bill Gates is split into two leaves Bill and Gates.\\nNext we deﬁne the dimensions of the vector space to be lexicalized subtrees\\nof documents – subtrees that contain at least one vocabulary term. A sub-\\nset of these possible lexicalized subtrees is shown in the ﬁgure, but there are\\nothers – e.g., the subtree corresponding to the whole document with the leaf\\nnode Gates removed. We can now represent queries and documents as vec-\\ntors in this space of lexicalized subtrees and compute matches between them.\\nThis means that we can use the vector space formalism from Chapter 6 for\\nXML retrieval. The main difference is that the dimensions of vector space\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.3\\nA vector space model for XML retrieval\\n207\\n◮Figure 10.8\\nA mapping of an XML document (left) to a set of lexicalized subtrees\\n(right).\\nin unstructured retrieval are vocabulary terms whereas they are lexicalized\\nsubtrees in XML retrieval.\\nThere is a tradeoff between the dimensionality of the space and accuracy\\nof query results. If we trivially restrict dimensions to vocabulary terms, then\\nwe have a standard vector space retrieval system that will retrieve many\\ndocuments that do not match the structure of the query (e.g., Gates in the\\ntitle as opposed to the author element). If we create a separate dimension\\nfor each lexicalized subtree occurring in the collection, the dimensionality of\\nthe space becomes too large. A compromise is to index all paths that end\\nin a single vocabulary term, in other words, all XML-context/term pairs.\\nWe call such an XML-context/term pair a structural term and denote it by\\nSTRUCTURAL TERM\\n⟨c, t⟩: a pair of XML-context c and vocabulary term t. The document in\\nFigure 10.8 has nine structural terms. Seven are shown (e.g., \"Bill\" and\\nAuthor#\"Bill\") and two are not shown: /Book/Author#\"Bill\" and\\n/Book/Author#\"Gates\". The tree with the leaves Bill and Gates is a lexical-\\nized subtree that is not a structural term. We use the previously introduced\\npseudo-XPath notation for structural terms.\\nAs we discussed in the last section users are bad at remembering details\\nabout the schema and at constructing queries that comply with the schema.\\nWe will therefore interpret all queries as extended queries – that is, there can\\nbe an arbitrary number of intervening nodes in the document for any parent-\\nchild node pair in the query. For example, we interpret q5 in Figure 10.7 as\\nq6.\\nBut we still prefer documents that match the query structure closely by\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n208\\n10\\nXML retrieval\\ninserting fewer additional nodes. We ensure that retrieval results respect this\\npreference by computing a weight for each match. A simple measure of the\\nsimilarity of a path cq in a query and a path cd in a document is the following\\ncontext resemblance function CR:\\nCONTEXT\\nRESEMBLANCE\\nCR(cq, cd) =\\n(\\n1+|cq|\\n1+|cd|\\nif cq matches cd\\n0\\nif cq does not match cd\\n(10.1)\\nwhere |cq| and |cd| are the number of nodes in the query path and document\\npath, respectively, and cq matches cd iff we can transform cq into cd by in-\\nserting additional nodes. Two examples from Figure 10.6 are CR(cq4, cd2) =\\n3/4 = 0.75 and CR(cq4, cd3) = 3/5 = 0.6 where cq4, cd2 and cd3 are the rele-\\nvant paths from top to leaf node in q4, d2 and d3, respectively. The value of\\nCR(cq, cd) is 1.0 if q and d are identical.\\nThe ﬁnal score for a document is computed as a variant of the cosine mea-\\nsure (Equation (6.10), page 121), which we call SIMNOMERGE for reasons\\nthat will become clear shortly. SIMNOMERGE is deﬁned as follows:\\nSIMNOMERGE(q, d) = ∑\\nck∈B ∑\\ncl∈B\\nCR(ck, cl) ∑\\nt∈V\\nweight(q, t, ck)\\nweight(d, t, cl)\\nq\\n∑c∈B,t∈V weight2(d, t, c)\\n(10.2)\\nwhere V is the vocabulary of non-structural terms; B is the set of all XML con-\\ntexts; and weight(q, t, c) and weight(d, t, c) are the weights of term t in XML\\ncontext c in query q and document d, respectively. We compute the weights\\nusing one of the weightings from Chapter 6, such as idft · wft,d. The inverse\\ndocument frequency idft depends on which elements we use to compute\\ndft as discussed in Section 10.2. The similarity measure SIMNOMERGE(q, d)\\nis not a true cosine measure since its value can be larger than 1.0 (Exer-\\ncise 10.11). We divide by\\nq\\n∑c∈B,t∈V weight2(d, t, c) to normalize for doc-\\nument length (Section 6.3.1, page 121). We have omitted query length nor-\\nmalization to simplify the formula. It has no effect on ranking since, for\\na given query, the normalizer\\nq\\n∑c∈B,t∈V weight2(q, t, c) is the same for all\\ndocuments.\\nThe algorithm for computing SIMNOMERGE for all documents in the col-\\nlection is shown in Figure 10.9. The array normalizer in Figure 10.9 contains\\nq\\n∑c∈B,t∈V weight2(d, t, c) from Equation (10.2) for each document.\\nWe give an example of how SIMNOMERGE computes query-document\\nsimilarities in Figure 10.10. ⟨c1, t⟩is one of the structural terms in the query.\\nWe successively retrieve all postings lists for structural terms ⟨c′, t⟩with the\\nsame vocabulary term t. Three example postings lists are shown. For the\\nﬁrst one, we have CR(c1, c1) = 1.0 since the two contexts are identical. The\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.3\\nA vector space model for XML retrieval\\n209\\nSCOREDOCUMENTSWITHSIMNOMERGE(q, B, V, N, normalizer)\\n1\\nfor n ←1 to N\\n2\\ndo score[n] ←0\\n3\\nfor each ⟨cq, t⟩∈q\\n4\\ndo wq ←WEIGHT(q, t, cq)\\n5\\nfor each c ∈B\\n6\\ndo if CR(cq, c) > 0\\n7\\nthen postings ←GETPOSTINGS(⟨c, t⟩)\\n8\\nfor each posting ∈postings\\n9\\ndo x ←CR(cq, c) ∗wq ∗weight(posting)\\n10\\nscore[docID(posting)] += x\\n11\\nfor n ←1 to N\\n12\\ndo score[n] ←score[n]/normalizer[n]\\n13\\nreturn score\\n◮Figure 10.9\\nThe algorithm for scoring documents with SIMNOMERGE.\\nquery\\n⟨c1, t⟩\\nCR(c1, c1)=1.0\\nCR(c1, c2)=0\\nCR(c1, c3)=0.63\\ninverted index\\n⟨c1, t⟩\\n−→\\n⟨d1, 0.5⟩\\n⟨d4, 0.1⟩\\n⟨d9, 0.2⟩\\n...\\n⟨c2, t⟩\\n−→\\n⟨d2, 0.25⟩\\n⟨d3, 0.1⟩\\n⟨d12, 0.9⟩\\n...\\n⟨c3, t⟩\\n−→\\n⟨d3, 0.7⟩\\n⟨d6, 0.8⟩\\n⟨d9, 0.6⟩\\n...\\n◮Figure 10.10\\nScoring of a query with one structural term in SIMNOMERGE.\\nnext context has no context resemblance with c1: CR(c1, c2) = 0 and the cor-\\nresponding postings list is ignored. The context match of c1 with c3 is 0.63>0\\nand it will be processed. In this example, the highest ranking document is d9\\nwith a similarity of 1.0 × 0.2 + 0.63 × 0.6 = 0.578. To simplify the ﬁgure, the\\nquery weight of ⟨c1, t⟩is assumed to be 1.0.\\nThe query-document similarity function in Figure 10.9 is called SIMNOMERGE\\nbecause different XML contexts are kept separate for the purpose of weight-\\ning. An alternative similarity function is SIMMERGE which relaxes the match-\\ning conditions of query and document further in the following three ways.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n210\\n10\\nXML retrieval\\n• We collect the statistics used for computing weight(q, t, c) and weight(d, t, c)\\nfrom all contexts that have a non-zero resemblance to c (as opposed to just\\nfrom c as in SIMNOMERGE). For instance, for computing the document\\nfrequency of the structural term atl#\"recognition\", we also count\\noccurrences of recognition in XML contexts fm/atl, article//atl etc.\\n• We modify Equation (10.2) by merging all structural terms in the docu-\\nment that have a non-zero context resemblance to a given query struc-\\ntural term. For example, the contexts /play/act/scene/title and\\n/play/title in the document will be merged when matching against\\nthe query term /play/title#\"Macbeth\".\\n• The context resemblance function is further relaxed: Contexts have a non-\\nzero resemblance in many cases where the deﬁnition of CR in Equation (10.1)\\nreturns 0.\\nSee the references in Section 10.6 for details.\\nThese three changes alleviate the problem of sparse term statistics dis-\\ncussed in Section 10.2 and increase the robustness of the matching function\\nagainst poorly posed structural queries. The evaluation of SIMNOMERGE\\nand SIMMERGE in the next section shows that the relaxed matching condi-\\ntions of SIMMERGE increase the effectiveness of XML retrieval.\\n?\\nExercise 10.1\\nConsider computing df for a structural term as the number of times that the structural\\nterm occurs under a particular parent node. Assume the following: the structural\\nterm ⟨c, t⟩= author#\"Herbert\" occurs once as the child of the node squib; there are\\n10 squib nodes in the collection; ⟨c, t⟩occurs 1000 times as the child of article; there are\\n1,000,000 article nodes in the collection. The idf weight of ⟨c, t⟩then is log2 10/1 ≈3.3\\nwhen occurring as the child of squib and log2 1,000,000/1000 ≈10.0 when occurring\\nas the child of article. (i) Explain why this is not an appropriate weighting for ⟨c, t⟩.\\nWhy should ⟨c, t⟩not receive a weight that is three times higher in articles than in\\nsquibs? (ii) Suggest a better way of computing idf.\\nExercise 10.2\\nWrite down all the structural terms occurring in the XML document in Figure 10.8.\\nExercise 10.3\\nHow many structural terms does the document in Figure 10.1 yield?\\n10.4\\nEvaluation of XML retrieval\\nThe premier venue for research on XML retrieval is the INEX (INitiative for\\nINEX\\nthe Evaluation of XML retrieval) program, a collaborative effort that has pro-\\nduced reference collections, sets of queries, and relevance judgments.\\nA\\nyearly INEX meeting is held to present and discuss research results. The\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.4\\nEvaluation of XML retrieval\\n211\\n12,107\\nnumber of documents\\n494 MB\\nsize\\n1995–2002\\ntime of publication of articles\\n1,532\\naverage number of XML nodes per document\\n6.9\\naverage depth of a node\\n30\\nnumber of CAS topics\\n30\\nnumber of CO topics\\n◮Table 10.2\\nINEX 2002 collection statistics.\\nIEEE Transac-\\ntion on Pat-\\ntern Analysis\\njournal title\\nActivity\\nrecognition\\narticle title\\nThis work fo-\\ncuses on . . .\\nparagraph\\nIntroduction\\ntitle\\nfront matter\\nsection\\nbody\\narticle\\n◮Figure 10.11\\nSimpliﬁed schema of the documents in the INEX collection.\\nINEX 2002 collection consisted of about 12,000 articles from IEEE journals.\\nWe give collection statistics in Table 10.2 and show part of the schema of\\nthe collection in Figure 10.11. The IEEE journal collection was expanded in\\n2005. Since 2006 INEX uses the much larger English Wikipedia as a test col-\\nlection. The relevance of documents is judged by human assessors using the\\nmethodology introduced in Section 8.1 (page 152), appropriately modiﬁed\\nfor structured documents as we will discuss shortly.\\nTwo types of information needs or topics in INEX are content-only or CO\\ntopics and content-and-structure (CAS) topics. CO topics are regular key-\\nCO TOPICS\\nword queries as in unstructured information retrieval. CAS topics have struc-\\nCAS TOPICS\\ntural constraints in addition to keywords. We already encountered an exam-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n212\\n10\\nXML retrieval\\nple of a CAS topic in Figure 10.3. The keywords in this case are summer and\\nholidays and the structural constraints specify that the keywords occur in a\\nsection that in turn is part of an article and that this article has an embedded\\nyear attribute with value 2001 or 2002.\\nSince CAS queries have both structural and content criteria, relevance as-\\nsessments are more complicated than in unstructured retrieval. INEX 2002\\ndeﬁned component coverage and topical relevance as orthogonal dimen-\\nsions of relevance. The component coverage dimension evaluates whether the\\nCOMPONENT\\nCOVERAGE\\nelement retrieved is “structurally” correct, i.e., neither too low nor too high\\nin the tree. We distinguish four cases:\\n• Exact coverage (E). The information sought is the main topic of the com-\\nponent and the component is a meaningful unit of information.\\n• Too small (S). The information sought is the main topic of the component,\\nbut the component is not a meaningful (self-contained) unit of informa-\\ntion.\\n• Too large (L). The information sought is present in the component, but is\\nnot the main topic.\\n• No coverage (N). The information sought is not a topic of the component.\\nThe topical relevance dimension also has four levels: highly relevant (3),\\nTOPICAL RELEVANCE\\nfairly relevant (2), marginally relevant (1) and nonrelevant (0). Components\\nare judged on both dimensions and the judgments are then combined into\\na digit-letter code. 2S is a fairly relevant component that is too small and\\n3E is a highly relevant component that has exact coverage. In theory, there\\nare 16 combinations of coverage and relevance, but many cannot occur. For\\nexample, a nonrelevant component cannot have exact coverage, so the com-\\nbination 3N is not possible.\\nThe relevance-coverage combinations are quantized as follows:\\nQ(rel, cov) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1.00\\nif\\n(rel, cov) = 3E\\n0.75\\nif\\n(rel, cov) ∈{2E, 3L}\\n0.50\\nif\\n(rel, cov) ∈{1E, 2L, 2S}\\n0.25\\nif\\n(rel, cov) ∈{1S, 1L}\\n0.00\\nif\\n(rel, cov) = 0N\\nThis evaluation scheme takes account of the fact that binary relevance judg-\\nments, which are standard in unstructured information retrieval (Section 8.5.1,\\npage 166), are not appropriate for XML retrieval. A 2S component provides\\nincomplete information and may be difﬁcult to interpret without more con-\\ntext, but it does answer the query partially. The quantization function Q\\ndoes not impose a binary choice relevant/nonrelevant and instead allows us\\nto grade the component as partially relevant.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.4\\nEvaluation of XML retrieval\\n213\\nalgorithm\\naverage precision\\nSIMNOMERGE\\n0.242\\nSIMMERGE\\n0.271\\n◮Table 10.3\\nINEX 2002 results of the vector space model in Section 10.3 for content-\\nand-structure (CAS) queries and the quantization function Q.\\nThe number of relevant components in a retrieved set A of components\\ncan then be computed as:\\n#(relevant items retrieved) = ∑\\nc∈A\\nQ(rel(c), cov(c))\\nAs an approximation, the standard deﬁnitions of precision, recall and F from\\nChapter 8 can be applied to this modiﬁed deﬁnition of relevant items re-\\ntrieved, with some subtleties because we sum graded as opposed to binary\\nrelevance assessments. See the references on focused retrieval in Section 10.6\\nfor further discussion.\\nOne ﬂaw of measuring relevance this way is that overlap is not accounted\\nfor. We discussed the concept of marginal relevance in the context of un-\\nstructured retrieval in Section 8.5.1 (page 166). This problem is worse in\\nXML retrieval because of the problem of multiple nested elements occur-\\nring in a search result as we discussed on page 203. Much of the recent focus\\nat INEX has been on developing algorithms and evaluation measures that\\nreturn non-redundant results lists and evaluate them properly. See the refer-\\nences in Section 10.6.\\nTable 10.3 shows two INEX 2002 runs of the vector space system we de-\\nscribed in Section 10.3. The better run is the SIMMERGE run, which incor-\\nporates few structural constraints and mostly relies on keyword matching.\\nSIMMERGE’s median average precision (where the median is with respect to\\naverage precision numbers over topics) is only 0.147. Effectiveness in XML\\nretrieval is often lower than in unstructured retrieval since XML retrieval is\\nharder. Instead of just ﬁnding a document, we have to ﬁnd the subpart of a\\ndocument that is most relevant to the query. Also, XML retrieval effective-\\nness – when evaluated as described here – can be lower than unstructured\\nretrieval effectiveness on a standard evaluation because graded judgments\\nlower measured performance. Consider a system that returns a document\\nwith graded relevance 0.6 and binary relevance 1 at the top of the retrieved\\nlist. Then, interpolated precision at 0.00 recall (cf. page 158) is 1.0 on a binary\\nevaluation, but can be as low as 0.6 on a graded evaluation.\\nTable 10.3 gives us a sense of the typical performance of XML retrieval,\\nbut it does not compare structured with unstructured retrieval. Table 10.4\\ndirectly shows the effect of using structure in retrieval. The results are for a\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n214\\n10\\nXML retrieval\\ncontent only\\nfull structure\\nimprovement\\nprecision at 5\\n0.2000\\n0.3265\\n63.3%\\nprecision at 10\\n0.1820\\n0.2531\\n39.1%\\nprecision at 20\\n0.1700\\n0.1796\\n5.6%\\nprecision at 30\\n0.1527\\n0.1531\\n0.3%\\n◮Table 10.4\\nA comparison of content-only and full-structure search in INEX\\n2003/2004.\\nlanguage-model-based system (cf. Chapter 12) that is evaluated on a subset\\nof CAS topics from INEX 2003 and 2004. The evaluation metric is precision\\nat k as deﬁned in Chapter 8 (page 161). The discretization function used for\\nthe evaluation maps highly relevant elements (roughly corresponding to the\\n3E elements deﬁned for Q) to 1 and all other elements to 0. The content-\\nonly system treats queries and documents as unstructured bags of words.\\nThe full-structure model ranks elements that satisfy structural constraints\\nhigher than elements that do not. For instance, for the query in Figure 10.3\\nan element that contains the phrase summer holidays in a section will be rated\\nhigher than one that contains it in an abstract.\\nThe table shows that structure helps increase precision at the top of the\\nresults list. There is a large increase of precision at k = 5 and at k = 10. There\\nis almost no improvement at k = 30. These results demonstrate the beneﬁts\\nof structured retrieval. Structured retrieval imposes additional constraints on\\nwhat to return and documents that pass the structural ﬁlter are more likely\\nto be relevant. Recall may suffer because some relevant documents will be\\nﬁltered out, but for precision-oriented tasks structured retrieval is superior.\\n10.5\\nText-centric vs. data-centric XML retrieval\\nIn the type of structured retrieval we cover in this chapter, XML structure\\nserves as a framework within which we match the text of the query with the\\ntext of the XML documents. This exempliﬁes a system that is optimized for\\ntext-centric XML. While both text and structure are important, we give higher\\nTEXT-CENTRIC XML\\npriority to text. We do this by adapting unstructured retrieval methods to\\nhandling additional structural constraints. The premise of our approach is\\nthat XML document retrieval is characterized by (i) long text ﬁelds (e.g., sec-\\ntions of a document), (ii) inexact matching, and (iii) relevance-ranked results.\\nRelational databases do not deal well with this use case.\\nIn contrast, data-centric XML mainly encodes numerical and non-text attribute-\\nDATA-CENTRIC XML\\nvalue data.\\nWhen querying data-centric XML, we want to impose exact\\nmatch conditions in most cases. This puts the emphasis on the structural\\naspects of XML documents and queries. An example is:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.5\\nText-centric vs. data-centric XML retrieval\\n215\\nFind employees whose salary is the same this month as it was 12 months\\nago.\\nThis query requires no ranking. It is purely structural and an exact matching\\nof the salaries in the two time periods is probably sufﬁcient to meet the user’s\\ninformation need.\\nText-centric approaches are appropriate for data that are essentially text\\ndocuments, marked up as XML to capture document structure. This is be-\\ncoming a de facto standard for publishing text databases since most text\\ndocuments have some form of interesting structure – paragraphs, sections,\\nfootnotes etc. Examples include assembly manuals, issues of journals, Shake-\\nspeare’s collected works and newswire articles.\\nData-centric approaches are commonly used for data collections with com-\\nplex structures that mainly contain non-text data. A text-centric retrieval\\nengine will have a hard time with proteomic data in bioinformatics or with\\nthe representation of a city map that (together with street names and other\\ntextual descriptions) forms a navigational database.\\nTwo other types of queries that are difﬁcult to handle in a text-centric struc-\\ntured retrieval model are joins and ordering constraints. The query for em-\\nployees with unchanged salary requires a join. The following query imposes\\nan ordering constraint:\\nRetrieve the chapter of the book Introduction to algorithms that follows\\nthe chapter Binomial heaps.\\nThis query relies on the ordering of elements in XML – in this case the order-\\ning of chapter elements underneath the book node. There are powerful query\\nlanguages for XML that can handle numerical attributes, joins and ordering\\nconstraints. The best known of these is XQuery, a language proposed for\\nstandardization by the W3C. It is designed to be broadly applicable in all ar-\\neas where XML is used. Due to its complexity, it is challenging to implement\\nan XQuery-based ranked retrieval system with the performance characteris-\\ntics that users have come to expect in information retrieval. This is currently\\none of the most active areas of research in XML retrieval.\\nRelational databases are better equipped to handle many structural con-\\nstraints, particularly joins (but ordering is also difﬁcult in a database frame-\\nwork – the tuples of a relation in the relational calculus are not ordered). For\\nthis reason, most data-centric XML retrieval systems are extensions of rela-\\ntional databases (see the references in Section 10.6). If text ﬁelds are short,\\nexact matching meets user needs and retrieval results in form of unordered\\nsets are acceptable, then using a relational database for XML retrieval is ap-\\npropriate.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n216\\n10\\nXML retrieval\\n10.6\\nReferences and further reading\\nThere are many good introductions to XML, including (Harold and Means\\n2004). Table 10.1 is inspired by a similar table in (van Rijsbergen 1979). Sec-\\ntion 10.4 follows the overview of INEX 2002 by Gövert and Kazai (2003),\\npublished in the proceedings of the meeting (Fuhr et al. 2003a). The pro-\\nceedings of the four following INEX meetings were published as Fuhr et al.\\n(2003b), Fuhr et al. (2005), Fuhr et al. (2006), and Fuhr et al. (2007). An up-\\ntodate overview article is Fuhr and Lalmas (2007). The results in Table 10.4\\nare from (Kamps et al. 2006). Chu-Carroll et al. (2006) also present evidence\\nthat XML queries increase precision compared with unstructured queries.\\nInstead of coverage and relevance, INEX now evaluates on the related but\\ndifferent dimensions of exhaustivity and speciﬁcity (Lalmas and Tombros\\n2007). Trotman et al. (2006) relate the tasks investigated at INEX to real world\\nuses of structured retrieval such as structured book search on internet book-\\nstore sites.\\nThe structured document retrieval principle is due to Chiaramella et al.\\n(1996). Figure 10.5 is from (Fuhr and Großjohann 2004). Rahm and Bernstein\\n(2001) give a survey of automatic schema matching that is applicable to XML.\\nThe vector-space based XML retrieval method in Section 10.3 is essentially\\nIBM Haifa’s JuruXML system as presented by Mass et al. (2003) and Carmel\\net al. (2003). Schlieder and Meuss (2002) and Grabs and Schek (2002) describe\\nsimilar approaches. Carmel et al. (2003) represent queries as XML fragments.\\nXML FRAGMENT\\nThe trees that represent XML queries in this chapter are all XML fragments,\\nbut XML fragments also permit the operators +, −and phrase on content\\nnodes.\\nWe chose to present the vector space model for XML retrieval because it\\nis simple and a natural extension of the unstructured vector space model\\nin Chapter 6. But many other unstructured retrieval methods have been\\napplied to XML retrieval with at least as much success as the vector space\\nmodel. These methods include language models (cf. Chapter 12, e.g., Kamps\\net al. (2004), List et al. (2005), Ogilvie and Callan (2005)), systems that use\\na relational database as a backend (Mihajlovi´c et al. 2005, Theobald et al.\\n2005; 2008), probabilistic weighting (Lu et al. 2007), and fusion (Larson 2005).\\nThere is currently no consensus as to what the best approach to XML retrieval\\nis.\\nMost early work on XML retrieval accomplished relevance ranking by fo-\\ncusing on individual terms, including their structural contexts, in query and\\ndocument. As in unstructured information retrieval, there is a trend in more\\nrecent work to model relevance ranking as combining evidence from dis-\\nparate measurements about the query, the document and their match. The\\ncombination function can be tuned manually (Arvola et al. 2005, Sigurbjörns-\\nson et al. 2004) or trained using machine learning methods (Vittaut and Gal-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n10.7\\nExercises\\n217\\nlinari (2006), cf. Section 15.4.1, page 341).\\nAn active area of XML retrieval research is focused retrieval (Trotman et al.\\nFOCUSED RETRIEVAL\\n2007), which aims to avoid returning nested elements that share one or more\\ncommon subelements (cf. discussion in Section 10.2, page 203). There is ev-\\nidence that users dislike redundancy caused by nested elements (Betsi et al.\\n2006). Focused retrieval requires evaluation measures that penalize redun-\\ndant results lists (Kazai and Lalmas 2006, Lalmas et al. 2007). Trotman and\\nGeva (2006) argue that XML retrieval is a form of passage retrieval. In passage\\nPASSAGE RETRIEVAL\\nretrieval (Salton et al. 1993, Hearst and Plaunt 1993, Zobel et al. 1995, Hearst\\n1997, Kaszkiel and Zobel 1997), the retrieval system returns short passages\\ninstead of documents in response to a user query. While element bound-\\naries in XML documents are cues for identifying good segment boundaries\\nbetween passages, the most relevant passage often does not coincide with an\\nXML element.\\nIn the last several years, the query format at INEX has been the NEXI stan-\\ndard proposed by Trotman and Sigurbjörnsson (2004). Figure 10.3 is from\\ntheir paper. O’Keefe and Trotman (2004) give evidence that users cannot reli-\\nably distinguish the child and descendant axes. This justiﬁes only permitting\\ndescendant axes in NEXI (and XML fragments). These structural constraints\\nwere only treated as “hints” in recent INEXes. Assessors can judge an ele-\\nment highly relevant even though it violates one of the structural constraints\\nspeciﬁed in a NEXI query.\\nAn alternative to structured query languages like NEXI is a more sophisti-\\ncated user interface for query formulation (Tannier and Geva 2005, van Zwol\\net al. 2006, Woodley and Geva 2006).\\nA broad overview of XML retrieval that covers database as well as IR ap-\\nproaches is given by Amer-Yahia and Lalmas (2006) and an extensive refer-\\nence list on the topic can be found in (Amer-Yahia et al. 2005). Chapter 6\\nof Grossman and Frieder (2004) is a good introduction to structured text re-\\ntrieval from a database perspective. The proposed standard for XQuery is\\navailable at http://www.w3.org/TR/xquery/ including an extension for full-text\\nqueries (Amer-Yahia et al. 2006): http://www.w3.org/TR/xquery-full-text/. Work\\nthat has looked at combining the relational database and the unstructured\\ninformation retrieval approaches includes (Fuhr and Rölleke 1997), (Navarro\\nand Baeza-Yates 1997), (Cohen 1998), and (Chaudhuri et al. 2006).\\n10.7\\nExercises\\n?\\nExercise 10.4\\nFind a reasonably sized XML document collection (or a collection using a markup lan-\\nguage different from XML like HTML) on the web and download it. Jon Bosak’s XML\\nedition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or\\nthe ﬁrst 10,000 documents of the Wikipedia are good choices. Create three CAS topics\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n218\\n10\\nXML retrieval\\nof the type shown in Figure 10.3 that you would expect to do better than analogous\\nCO topics. Explain why an XML retrieval system would be able to exploit the XML\\nstructure of the documents to achieve better retrieval results on the topics than an\\nunstructured retrieval system.\\nExercise 10.5\\nFor the collection and the topics in Exercise 10.4, (i) are there pairs of elements e1 and\\ne2, with e2 a subelement of e1 such that both answer one of the topics? Find one case\\neach where (ii) e1 (iii) e2 is the better answer to the query.\\nExercise 10.6\\nImplement the (i) SIMMERGE (ii) SIMNOMERGE algorithm in Section 10.3 and run it\\nfor the collection and the topics in Exercise 10.4. (iii) Evaluate the results by assigning\\nbinary relevance judgments to the ﬁrst ﬁve documents of the three retrieved lists for\\neach algorithm. Which algorithm performs better?\\nExercise 10.7\\nAre all of the elements in Exercise 10.4 appropriate to be returned as hits to a user or\\nare there elements (as in the example <b>definitely</b> on page 203) that you\\nwould exclude?\\nExercise 10.8\\nWe discussed the tradeoff between accuracy of results and dimensionality of the vec-\\ntor space on page 207. Give an example of an information need that we can answer\\ncorrectly if we index all lexicalized subtrees, but cannot answer if we only index struc-\\ntural terms.\\nExercise 10.9\\nIf we index all structural terms, what is the size of the index as a function of text size?\\nExercise 10.10\\nIf we index all lexicalized subtrees, what is the size of the index as a function of text\\nsize?\\nExercise 10.11\\nGive an example of a query-document pair for which SIMNOMERGE(q, d) is larger\\nthan 1.0.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n219\\n11\\nProbabilistic information\\nretrieval\\nDuring the discussion of relevance feedback in Section 9.1.2, we observed\\nthat if we have some known relevant and nonrelevant documents, then we\\ncan straightforwardly start to estimate the probability of a term t appearing\\nin a relevant document P(t|R = 1), and that this could be the basis of a\\nclassiﬁer that decides whether documents are relevant or not. In this chapter,\\nwe more systematically introduce this probabilistic approach to IR, which\\nprovides a different formal basis for a retrieval model and results in different\\ntechniques for setting term weights.\\nUsers start with information needs, which they translate into query repre-\\nsentations. Similarly, there are documents, which are converted into document\\nrepresentations (the latter differing at least by how text is tokenized, but per-\\nhaps containing fundamentally less information, as when a non-positional\\nindex is used). Based on these two representations, a system tries to de-\\ntermine how well documents satisfy information needs. In the Boolean or\\nvector space models of IR, matching is done in a formally deﬁned but seman-\\ntically imprecise calculus of index terms. Given only a query, an IR system\\nhas an uncertain understanding of the information need. Given the query\\nand document representations, a system has an uncertain guess of whether\\na document has content relevant to the information need. Probability theory\\nprovides a principled foundation for such reasoning under uncertainty. This\\nchapter provides one answer as to how to exploit this foundation to estimate\\nhow likely it is that a document is relevant to an information need.\\nThere is more than one possible retrieval model which has a probabilistic\\nbasis. Here, we will introduce probability theory and the Probability Rank-\\ning Principle (Sections 11.1–11.2), and then concentrate on the Binary Inde-\\npendence Model (Section 11.3), which is the original and still most inﬂuential\\nprobabilistic retrieval model. Finally, we will introduce related but extended\\nmethods which use term counts, including the empirically successful Okapi\\nBM25 weighting scheme, and Bayesian Network models for IR (Section 11.4).\\nIn Chapter 12, we then present the alternative probabilistic language model-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n220\\n11\\nProbabilistic information retrieval\\ning approach to IR, which has been developed with considerable success in\\nrecent years.\\n11.1\\nReview of basic probability theory\\nWe hope that the reader has seen a little basic probability theory previously.\\nWe will give a very quick review; some references for further reading appear\\nat the end of the chapter. A variable A represents an event (a subset of the\\nspace of possible outcomes). Equivalently, we can represent the subset via a\\nrandom variable, which is a function from outcomes to real numbers; the sub-\\nRANDOM VARIABLE\\nset is the domain over which the random variable A has a particular value.\\nOften we will not know with certainty whether an event is true in the world.\\nWe can ask the probability of the event 0 ≤P(A) ≤1. For two events A and\\nB, the joint event of both events occurring is described by the joint probabil-\\nity P(A, B). The conditional probability P(A|B) expresses the probability of\\nevent A given that event B occurred. The fundamental relationship between\\njoint and conditional probabilities is given by the chain rule:\\nCHAIN RULE\\nP(A, B) = P(A ∩B) = P(A|B)P(B) = P(B|A)P(A)\\n(11.1)\\nWithout making any assumptions, the probability of a joint event equals the\\nprobability of one of the events multiplied by the probability of the other\\nevent conditioned on knowing the ﬁrst event happened.\\nWriting P(A) for the complement of an event, we similarly have:\\nP(A, B) = P(B|A)P(A)\\n(11.2)\\nProbability theory also has a partition rule, which says that if an event B can\\nPARTITION RULE\\nbe divided into an exhaustive set of disjoint subcases, then the probability of\\nB is the sum of the probabilities of the subcases. A special case of this rule\\ngives that:\\nP(B) = P(A, B) + P(A, B)\\n(11.3)\\nFrom these we can derive Bayes’ Rule for inverting conditional probabili-\\nBAYES’ RULE\\nties:\\nP(A|B) = P(B|A)P(A)\\nP(B)\\n=\\n\"\\nP(B|A)\\n∑X∈{A,A} P(B|X)P(X)\\n#\\nP(A)\\n(11.4)\\nThis equation can also be thought of as a way of updating probabilities. We\\nstart off with an initial estimate of how likely the event A is when we do\\nnot have any other information; this is the prior probability P(A). Bayes’ rule\\nPRIOR PROBABILITY\\nlets us derive a posterior probability P(A|B) after having seen the evidence B,\\nPOSTERIOR\\nPROBABILITY\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.2\\nThe Probability Ranking Principle\\n221\\nbased on the likelihood of B occurring in the two cases that A does or does not\\nhold.1\\nFinally, it is often useful to talk about the odds of an event, which provide\\nODDS\\na kind of multiplier for how probabilities change:\\nOdds:\\nO(A) = P(A)\\nP(A) =\\nP(A)\\n1 −P(A)\\n(11.5)\\n11.2\\nThe Probability Ranking Principle\\n11.2.1\\nThe 1/0 loss case\\nWe assume a ranked retrieval setup as in Section 6.3, where there is a collec-\\ntion of documents, the user issues a query, and an ordered list of documents\\nis returned. We also assume a binary notion of relevance as in Chapter 8. For\\na query q and a document d in the collection, let Rd,q be an indicator random\\nvariable that says whether d is relevant with respect to a given query q. That\\nis, it takes on a value of 1 when the document is relevant and 0 otherwise. In\\ncontext we will often write just R for Rd,q.\\nUsing a probabilistic model, the obvious order in which to present doc-\\numents to the user is to rank documents by their estimated probability of\\nrelevance with respect to the information need: P(R = 1|d, q). This is the ba-\\nsis of the Probability Ranking Principle (PRP) (van Rijsbergen 1979, 113–114):\\nPROBABILITY\\nRANKING PRINCIPLE\\n“If a reference retrieval system’s response to each request is a ranking\\nof the documents in the collection in order of decreasing probability\\nof relevance to the user who submitted the request, where the prob-\\nabilities are estimated as accurately as possible on the basis of what-\\never data have been made available to the system for this purpose, the\\noverall effectiveness of the system to its user will be the best that is\\nobtainable on the basis of those data.”\\nIn the simplest case of the PRP, there are no retrieval costs or other utility\\nconcerns that would differentially weight actions or errors. You lose a point\\nfor either returning a nonrelevant document or failing to return a relevant\\ndocument (such a binary situation where you are evaluated on your accuracy\\nis called 1/0 loss). The goal is to return the best possible results as the top k\\n1/0 LOSS\\ndocuments, for any value of k the user chooses to examine. The PRP then\\nsays to simply rank all documents in decreasing order of P(R = 1|d, q). If\\na set of retrieval results is to be returned, rather than an ordering, the Bayes\\nBAYES OPTIMAL\\nDECISION RULE\\n1. The term likelihood is just a synonym of probability. It is the probability of an event or data\\naccording to a model. The term is usually used when people are thinking of holding the data\\nﬁxed, while varying the model.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n222\\n11\\nProbabilistic information retrieval\\nOptimal Decision Rule, the decision which minimizes the risk of loss, is to\\nsimply return documents that are more likely relevant than nonrelevant:\\nd is relevant iff P(R = 1|d, q) > P(R = 0|d, q)\\n(11.6)\\nTheorem 11.1. The PRP is optimal, in the sense that it minimizes the expected loss\\n(also known as the Bayes risk) under 1/0 loss.\\nBAYES RISK\\nThe proof can be found in Ripley (1996). However, it requires that all proba-\\nbilities are known correctly. This is never the case in practice. Nevertheless,\\nthe PRP still provides a very useful foundation for developing models of IR.\\n11.2.2\\nThe PRP with retrieval costs\\nSuppose, instead, that we assume a model of retrieval costs. Let C1 be the\\ncost of not retrieving a relevant document and C0 the cost of retrieval of a\\nnonrelevant document. Then the Probability Ranking Principle says that if\\nfor a speciﬁc document d and for all documents d′ not yet retrieved\\nC0 · P(R = 0|d) −C1 · P(R = 1|d) ≤C0 · P(R = 0|d′) −C1 · P(R = 1|d′)\\n(11.7)\\nthen d is the next document to be retrieved. Such a model gives a formal\\nframework where we can model differential costs of false positives and false\\nnegatives and even system performance issues at the modeling stage, rather\\nthan simply at the evaluation stage, as we did in Section 8.6 (page 168). How-\\never, we will not further consider loss/utility models in this chapter.\\n11.3\\nThe Binary Independence Model\\nThe Binary Independence Model (BIM) we present in this section is the model\\nBINARY\\nINDEPENDENCE\\nMODEL\\nthat has traditionally been used with the PRP. It introduces some simple as-\\nsumptions, which make estimating the probability function P(R|d, q) prac-\\ntical. Here, “binary” is equivalent to Boolean: documents and queries are\\nboth represented as binary term incidence vectors. That is, a document d\\nis represented by the vector ⃗x = (x1, . . . , xM) where xt = 1 if term t is\\npresent in document d and xt = 0 if t is not present in d. With this rep-\\nresentation, many possible documents have the same vector representation.\\nSimilarly, we represent q by the incidence vector ⃗q (the distinction between\\nq and ⃗q is less central since commonly q is in the form of a set of words).\\n“Independence” means that terms are modeled as occurring in documents\\nindependently. The model recognizes no association between terms. This\\nassumption is far from correct, but it nevertheless often gives satisfactory\\nresults in practice; it is the “naive” assumption of Naive Bayes models, dis-\\ncussed further in Section 13.4 (page 265). Indeed, the Binary Independence\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n223\\nModel is exactly the same as the multivariate Bernoulli Naive Bayes model\\npresented in Section 13.3 (page 263). In a sense this assumption is equivalent\\nto an assumption of the vector space model, where each term is a dimension\\nthat is orthogonal to all other terms.\\nWe will ﬁrst present a model which assumes that the user has a single\\nstep information need. As discussed in Chapter 9, seeing a range of results\\nmight let the user reﬁne their information need. Fortunately, as mentioned\\nthere, it is straightforward to extend the Binary Independence Model so as to\\nprovide a framework for relevance feedback, and we present this model in\\nSection 11.3.4.\\nTo make a probabilistic retrieval strategy precise, we need to estimate how\\nterms in documents contribute to relevance, speciﬁcally, we wish to know\\nhow term frequency, document frequency, document length, and other statis-\\ntics that we can compute inﬂuence judgments about document relevance,\\nand how they can be reasonably combined to estimate the probability of doc-\\nument relevance. We then order documents by decreasing estimated proba-\\nbility of relevance.\\nWe assume here that the relevance of each document is independent of the\\nrelevance of other documents. As we noted in Section 8.5.1 (page 166), this\\nis incorrect: the assumption is especially harmful in practice if it allows a\\nsystem to return duplicate or near duplicate documents. Under the BIM, we\\nmodel the probability P(R|d, q) that a document is relevant via the probabil-\\nity in terms of term incidence vectors P(R|⃗x,⃗q). Then, using Bayes rule, we\\nhave:\\nP(R = 1|⃗x,⃗q)\\n=\\nP(⃗x|R = 1,⃗q)P(R = 1|⃗q)\\nP(⃗x|⃗q)\\n(11.8)\\nP(R = 0|⃗x,⃗q)\\n=\\nP(⃗x|R = 0,⃗q)P(R = 0|⃗q)\\nP(⃗x|⃗q)\\nHere, P(⃗x|R = 1,⃗q) and P(⃗x|R = 0,⃗q) are the probability that if a relevant or\\nnonrelevant, respectively, document is retrieved, then that document’s rep-\\nresentation is ⃗x. You should think of this quantity as deﬁned with respect to\\na space of possible documents in a domain. How do we compute all these\\nprobabilities? We never know the exact probabilities, and so we have to use\\nestimates: Statistics about the actual document collection are used to estimate\\nthese probabilities. P(R = 1|⃗q) and P(R = 0|⃗q) indicate the prior probability\\nof retrieving a relevant or nonrelevant document respectively for a query ⃗q.\\nAgain, if we knew the percentage of relevant documents in the collection,\\nthen we could use this number to estimate P(R = 1|⃗q) and P(R = 0|⃗q). Since\\na document is either relevant or nonrelevant to a query, we must have that:\\nP(R = 1|⃗x,⃗q) + P(R = 0|⃗x,⃗q) = 1\\n(11.9)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n224\\n11\\nProbabilistic information retrieval\\n11.3.1\\nDeriving a ranking function for query terms\\nGiven a query q, we wish to order returned documents by descending P(R =\\n1|d, q). Under the BIM, this is modeled as ordering by P(R = 1|⃗x,⃗q). Rather\\nthan estimating this probability directly, because we are interested only in the\\nranking of documents, we work with some other quantities which are easier\\nto compute and which give the same ordering of documents. In particular,\\nwe can rank documents by their odds of relevance (as the odds of relevance\\nis monotonic with the probability of relevance). This makes things easier,\\nbecause we can ignore the common denominator in (11.8), giving:\\nO(R|⃗x,⃗q) = P(R = 1|⃗x,⃗q)\\nP(R = 0|⃗x,⃗q) =\\nP(R=1|⃗q)P(⃗x|R=1,⃗q)\\nP(⃗x|⃗q)\\nP(R=0|⃗q)P(⃗x|R=0,⃗q)\\nP(⃗x|⃗q)\\n= P(R = 1|⃗q)\\nP(R = 0|⃗q) · P(⃗x|R = 1,⃗q)\\nP(⃗x|R = 0,⃗q)\\n(11.10)\\nThe left term in the rightmost expression of Equation (11.10) is a constant for\\na given query. Since we are only ranking documents, there is thus no need\\nfor us to estimate it. The right-hand term does, however, require estimation,\\nand this initially appears to be difﬁcult: How can we accurately estimate the\\nprobability of an entire term incidence vector occurring? It is at this point that\\nwe make the Naive Bayes conditional independence assumption that the presence\\nNAIVE BAYES\\nASSUMPTION\\nor absence of a word in a document is independent of the presence or absence\\nof any other word (given the query):\\nP(⃗x|R = 1,⃗q)\\nP(⃗x|R = 0,⃗q) =\\nM\\n∏\\nt=1\\nP(xt|R = 1,⃗q)\\nP(xt|R = 0,⃗q)\\n(11.11)\\nSo:\\nO(R|⃗x,⃗q) = O(R|⃗q) ·\\nM\\n∏\\nt=1\\nP(xt|R = 1,⃗q)\\nP(xt|R = 0,⃗q)\\n(11.12)\\nSince each xt is either 0 or 1, we can separate the terms to give:\\nO(R|⃗x,⃗q) = O(R|⃗q) · ∏\\nt:xt=1\\nP(xt = 1|R = 1,⃗q)\\nP(xt = 1|R = 0,⃗q) · ∏\\nt:xt=0\\nP(xt = 0|R = 1,⃗q)\\nP(xt = 0|R = 0,⃗q)\\n(11.13)\\nHenceforth, let pt = P(xt = 1|R = 1,⃗q) be the probability of a term appear-\\ning in a document relevant to the query, and ut = P(xt = 1|R = 0,⃗q) be the\\nprobability of a term appearing in a nonrelevant document. These quantities\\ncan be visualized in the following contingency table where the columns add\\nto 1:\\n(11.14)\\ndocument\\nrelevant (R = 1)\\nnonrelevant (R = 0)\\nTerm present\\nxt = 1\\npt\\nut\\nTerm absent\\nxt = 0\\n1 −pt\\n1 −ut\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n225\\nLet us make an additional simplifying assumption that terms not occur-\\nring in the query are equally likely to occur in relevant and nonrelevant doc-\\numents: that is, if qt = 0 then pt = ut. (This assumption can be changed,\\nas when doing relevance feedback in Section 11.3.4.) Then we need only\\nconsider terms in the products that appear in the query, and so,\\nO(R|⃗q,⃗x) = O(R|⃗q) ·\\n∏\\nt:xt=qt=1\\npt\\nut ·\\n∏\\nt:xt=0,qt=1\\n1 −pt\\n1 −ut\\n(11.15)\\nThe left product is over query terms found in the document and the right\\nproduct is over query terms not found in the document.\\nWe can manipulate this expression by including the query terms found in\\nthe document into the right product, but simultaneously dividing through\\nby them in the left product, so the value is unchanged. Then we have:\\nO(R|⃗q,⃗x) = O(R|⃗q) ·\\n∏\\nt:xt=qt=1\\npt(1 −ut)\\nut(1 −pt) · ∏\\nt:qt=1\\n1 −pt\\n1 −ut\\n(11.16)\\nThe left product is still over query terms found in the document, but the right\\nproduct is now over all query terms. That means that this right product is a\\nconstant for a particular query, just like the odds O(R|⃗q). So the only quantity\\nthat needs to be estimated to rank documents for relevance to a query is the\\nleft product. We can equally rank documents by the logarithm of this term,\\nsince log is a monotonic function. The resulting quantity used for ranking is\\ncalled the Retrieval Status Value (RSV) in this model:\\nRETRIEVAL STATUS\\nVALUE\\nRSVd = log\\n∏\\nt:xt=qt=1\\npt(1 −ut)\\nut(1 −pt) =\\n∑\\nt:xt=qt=1\\nlog pt(1 −ut)\\nut(1 −pt)\\n(11.17)\\nSo everything comes down to computing the RSV. Deﬁne ct:\\nct = log pt(1 −ut)\\nut(1 −pt) = log\\npt\\n(1 −pt) + log 1 −ut\\nut\\n(11.18)\\nThe ct terms are log odds ratios for the terms in the query. We have the\\nodds of the term appearing if the document is relevant (pt/(1 −pt)) and the\\nodds of the term appearing if the document is nonrelevant (ut/(1 −ut)). The\\nodds ratio is the ratio of two such odds, and then we ﬁnally take the log of that\\nODDS RATIO\\nquantity. The value will be 0 if a term has equal odds of appearing in relevant\\nand nonrelevant documents, and positive if it is more likely to appear in\\nrelevant documents. The ct quantities function as term weights in the model,\\nand the document score for a query is RSVd = ∑xt=qt=1 ct. Operationally, we\\nsum them in accumulators for query terms appearing in documents, just as\\nfor the vector space model calculations discussed in Section 7.1 (page 135).\\nWe now turn to how we estimate these ct quantities for a particular collection\\nand query.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n226\\n11\\nProbabilistic information retrieval\\n11.3.2\\nProbability estimates in theory\\nFor each term t, what would these ct numbers look like for the whole collec-\\ntion? (11.19) gives a contingency table of counts of documents in the collec-\\ntion, where dft is the number of documents that contain term t:\\n(11.19)\\ndocuments\\nrelevant\\nnonrelevant\\nTotal\\nTerm present\\nxt = 1\\ns\\ndft −s\\ndft\\nTerm absent\\nxt = 0\\nS −s\\n(N −dft) −(S −s)\\nN −dft\\nTotal\\nS\\nN −S\\nN\\nUsing this, pt = s/S and ut = (dft −s)/(N −S) and\\nct = K(N, dft, S, s) = log\\ns/(S −s)\\n(dft −s)/((N −dft) −(S −s))\\n(11.20)\\nTo avoid the possibility of zeroes (such as if every or no relevant document\\nhas a particular term) it is fairly standard to add 1\\n2 to each of the quantities\\nin the center 4 terms of (11.19), and then to adjust the marginal counts (the\\ntotals) accordingly (so, the bottom right cell totals N + 2). Then we have:\\nˆct = K(N, dft, S, s) = log\\n(s + 1\\n2)/(S −s + 1\\n2)\\n(dft −s + 1\\n2)/(N −dft −S + s + 1\\n2)\\n(11.21)\\nAdding 1\\n2 in this way is a simple form of smoothing. For trials with cat-\\negorical outcomes (such as noting the presence or absence of a term), one\\nway to estimate the probability of an event from data is simply to count the\\nnumber of times an event occurred divided by the total number of trials.\\nThis is referred to as the relative frequency of the event. Estimating the prob-\\nRELATIVE FREQUENCY\\nability as the relative frequency is the maximum likelihood estimate (or MLE),\\nMAXIMUM LIKELIHOOD\\nESTIMATE\\nMLE\\nbecause this value makes the observed data maximally likely. However, if\\nwe simply use the MLE, then the probability given to events we happened to\\nsee is usually too high, whereas other events may be completely unseen and\\ngiving them as a probability estimate their relative frequency of 0 is both an\\nunderestimate, and normally breaks our models, since anything multiplied\\nby 0 is 0. Simultaneously decreasing the estimated probability of seen events\\nand increasing the probability of unseen events is referred to as smoothing.\\nSMOOTHING\\nOne simple way of smoothing is to add a number α to each of the observed\\ncounts. These pseudocounts correspond to the use of a uniform distribution\\nPSEUDOCOUNTS\\nover the vocabulary as a Bayesian prior, following Equation (11.4). We ini-\\nBAYESIAN PRIOR\\ntially assume a uniform distribution over events, where the size of α denotes\\nthe strength of our belief in uniformity, and we then update the probability\\nbased on observed events. Since our belief in uniformity is weak, we use\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n227\\nα = 1\\n2. This is a form of maximum a posteriori (MAP) estimation, where we\\nMAXIMUM A\\nPOSTERIORI\\nMAP\\nchoose the most likely point value for probabilities based on the prior and\\nthe observed evidence, following Equation (11.4). We will further discuss\\nmethods of smoothing estimated counts to give probability models in Sec-\\ntion 12.2.2 (page 243); the simple method of adding 1\\n2 to each observed count\\nwill do for now.\\n11.3.3\\nProbability estimates in practice\\nUnder the assumption that relevant documents are a very small percentage\\nof the collection, it is plausible to approximate statistics for nonrelevant doc-\\numents by statistics from the whole collection. Under this assumption, ut\\n(the probability of term occurrence in nonrelevant documents for a query) is\\ndft/N and\\nlog[(1 −ut)/ut] = log[(N −dft)/dft] ≈log N/dft\\n(11.22)\\nIn other words, we can provide a theoretical justiﬁcation for the most fre-\\nquently used form of idf weighting, which we saw in Section 6.2.1.\\nThe approximation technique in Equation (11.22) cannot easily be extended\\nto relevant documents. The quantity pt can be estimated in various ways:\\n1. We can use the frequency of term occurrence in known relevant docu-\\nments (if we know some). This is the basis of probabilistic approaches to\\nrelevance feedback weighting in a feedback loop, discussed in the next\\nsubsection.\\n2. Croft and Harper (1979) proposed using a constant in their combination\\nmatch model. For instance, we might assume that pt is constant over all\\nterms xt in the query and that pt = 0.5. This means that each term has\\neven odds of appearing in a relevant document, and so the pt and (1 −pt)\\nfactors cancel out in the expression for RSV. Such an estimate is weak, but\\ndoesn’t disagree violently with our hopes for the search terms appearing\\nin many but not all relevant documents. Combining this method with our\\nearlier approximation for ut, the document ranking is determined simply\\nby which query terms occur in documents scaled by their idf weighting.\\nFor short documents (titles or abstracts) in situations in which iterative\\nsearching is undesirable, using this weighting term alone can be quite\\nsatisfactory, although in many other circumstances we would like to do\\nbetter.\\n3. Greiff (1998) argues that the constant estimate of pt in the Croft and Harper\\n(1979) model is theoretically problematic and not observed empirically: as\\nmight be expected, pt is shown to rise with dft. Based on his data analysis,\\na plausible proposal would be to use the estimate pt = 1\\n3 + 2\\n3dft/N.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n228\\n11\\nProbabilistic information retrieval\\nIterative methods of estimation, which combine some of the above ideas,\\nare discussed in the next subsection.\\n11.3.4\\nProbabilistic approaches to relevance feedback\\nWe can use (pseudo-)relevance feedback, perhaps in an iterative process of\\nestimation, to get a more accurate estimate of pt. The probabilistic approach\\nto relevance feedback works as follows:\\n1. Guess initial estimates of pt and ut. This can be done using the probability\\nestimates of the previous section. For instance, we can assume that pt is\\nconstant over all xt in the query, in particular, perhaps taking pt = 1\\n2.\\n2. Use the current estimates of pt and ut to determine a best guess at the set\\nof relevant documents R = {d : Rd,q = 1}. Use this model to retrieve a set\\nof candidate relevant documents, which we present to the user.\\n3. We interact with the user to reﬁne the model of R. We do this by learn-\\ning from the user relevance judgments for some subset of documents V.\\nBased on relevance judgments, V is partitioned into two subsets: VR =\\n{d ∈V, Rd,q = 1} ⊂R and VNR = {d ∈V, Rd,q = 0}, which is disjoint\\nfrom R.\\n4. We reestimate pt and ut on the basis of known relevant and nonrelevant\\ndocuments. If the sets VR and VNR are large enough, we may be able\\nto estimate these quantities directly from these documents as maximum\\nlikelihood estimates:\\npt = |VRt|/|VR|\\n(11.23)\\n(where VRt is the set of documents in VR containing xt). In practice,\\nwe usually need to smooth these estimates. We can do this by adding\\n1\\n2 to both the count |VRt| and to the number of relevant documents not\\ncontaining the term, giving:\\npt = |VRt| + 1\\n2\\n|VR| + 1\\n(11.24)\\nHowever, the set of documents judged by the user (V) is usually very\\nsmall, and so the resulting statistical estimate is quite unreliable (noisy),\\neven if the estimate is smoothed. So it is often better to combine the new\\ninformation with the original guess in a process of Bayesian updating. In\\nthis case we have:\\np(k+1)\\nt\\n= |VRt| + κp(k)\\nt\\n|VR| + κ\\n(11.25)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n229\\nHere p(k)\\nt\\nis the kth estimate for pt in an iterative updating process and\\nis used as a Bayesian prior in the next iteration with a weighting of κ.\\nRelating this equation back to Equation (11.4) requires a bit more proba-\\nbility theory than we have presented here (we need to use a beta distribu-\\ntion prior, conjugate to the Bernoulli random variable Xt). But the form\\nof the resulting equation is quite straightforward: rather than uniformly\\ndistributing pseudocounts, we now distribute a total of κ pseudocounts\\naccording to the previous estimate, which acts as the prior distribution.\\nIn the absence of other evidence (and assuming that the user is perhaps\\nindicating roughly 5 relevant or nonrelevant documents) then a value\\nof around κ = 5 is perhaps appropriate. That is, the prior is strongly\\nweighted so that the estimate does not change too much from the evi-\\ndence provided by a very small number of documents.\\n5. Repeat the above process from step 2, generating a succession of approxi-\\nmations to R and hence pt, until the user is satisﬁed.\\nIt is also straightforward to derive a pseudo-relevance feedback version of\\nthis algorithm, where we simply pretend that VR = V. More brieﬂy:\\n1. Assume initial estimates for pt and ut as above.\\n2. Determine a guess for the size of the relevant document set. If unsure, a\\nconservative (too small) guess is likely to be best. This motivates use of a\\nﬁxed size set V of highest ranked documents.\\n3. Improve our guesses for pt and ut. We choose from the methods of Equa-\\ntions (11.23) and (11.25) for re-estimating pt, except now based on the set\\nV instead of VR. If we let Vt be the subset of documents in V containing\\nxt and use add 1\\n2 smoothing, we get:\\npt = |Vt| + 1\\n2\\n|V| + 1\\n(11.26)\\nand if we assume that documents that are not retrieved are nonrelevant\\nthen we can update our ut estimates as:\\nut = dft −|Vt| + 1\\n2\\nN −|V| + 1\\n(11.27)\\n4. Go to step 2 until the ranking of the returned results converges.\\nOnce we have a real estimate for pt then the ct weights used in the RSV\\nvalue look almost like a tf-idf value. For instance, using Equation (11.18),\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n230\\n11\\nProbabilistic information retrieval\\nEquation (11.22), and Equation (11.26), we have:\\nct = log\\n\\x14\\npt\\n1 −pt · 1 −ut\\nut\\n\\x15\\n≈log\\n\"\\n|Vt| + 1\\n2\\n|V| −|Vt| + 1 · N\\ndft\\n#\\n(11.28)\\nBut things aren’t quite the same: pt/(1 −pt) measures the (estimated) pro-\\nportion of relevant documents that the term t occurs in, not term frequency.\\nMoreover, if we apply log identities:\\nct = log\\n|Vt| + 1\\n2\\n|V| −|Vt| + 1 + log N\\ndft\\n(11.29)\\nwe see that we are now adding the two log scaled components rather than\\nmultiplying them.\\n?\\nExercise 11.1\\nWork through the derivation of Equation (11.20) from Equations (11.18) and (11.19).\\nExercise 11.2\\nWhat are the differences between standard vector space tf-idf weighting and the BIM\\nprobabilistic retrieval model (in the case where no document relevance information\\nis available)?\\nExercise 11.3\\n[⋆⋆]\\nLet Xt be a random variable indicating whether the term t appears in a document.\\nSuppose we have |R| relevant documents in the document collection and that Xt = 1\\nin s of the documents. Take the observed data to be just these observations of Xt for\\neach document in R. Show that the MLE for the parameter pt = P(Xt = 1|R = 1,⃗q),\\nthat is, the value for pt which maximizes the probability of the observed data, is\\npt = s/|R|.\\nExercise 11.4\\nDescribe the differences between vector space relevance feedback and probabilistic\\nrelevance feedback.\\n11.4\\nAn appraisal and some extensions\\n11.4.1\\nAn appraisal of probabilistic models\\nProbabilistic methods are one of the oldest formal models in IR. Already\\nin the 1970s they were held out as an opportunity to place IR on a ﬁrmer\\ntheoretical footing, and with the resurgence of probabilistic methods in com-\\nputational linguistics in the 1990s, that hope has returned, and probabilis-\\ntic methods are again one of the currently hottest topics in IR. Traditionally,\\nprobabilistic IR has had neat ideas but the methods have never won on per-\\nformance. Getting reasonable approximations of the needed probabilities for\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.4\\nAn appraisal and some extensions\\n231\\na probabilistic IR model is possible, but it requires some major assumptions.\\nIn the BIM these are:\\n• a Boolean representation of documents/queries/relevance\\n• term independence\\n• terms not in the query don’t affect the outcome\\n• document relevance values are independent\\nIt is perhaps the severity of the modeling assumptions that makes achieving\\ngood performance difﬁcult. A general problem seems to be that probabilistic\\nmodels either require partial relevance information or else only allow for\\nderiving apparently inferior term weighting models.\\nThings started to change in the 1990s when the BM25 weighting scheme,\\nwhich we discuss in the next section, showed very good performance, and\\nstarted to be adopted as a term weighting scheme by many groups. The\\ndifference between “vector space” and “probabilistic” IR systems is not that\\ngreat: in either case, you build an information retrieval scheme in the exact\\nsame way that we discussed in Chapter 7. For a probabilistic IR system, it’s\\njust that, at the end, you score queries not by cosine similarity and tf-idf in\\na vector space, but by a slightly different formula motivated by probability\\ntheory. Indeed, sometimes people have changed an existing vector-space\\nIR system into an effectively probabilistic system simply by adopted term\\nweighting formulas from probabilistic models. In this section, we brieﬂy\\npresent three extensions of the traditional probabilistic model, and in the next\\nchapter, we look at the somewhat different probabilistic language modeling\\napproach to IR.\\n11.4.2\\nTree-structured dependencies between terms\\nSome of the assumptions of the BIM can be removed. For example, we can\\nremove the assumption that terms are independent. This assumption is very\\nfar from true in practice. A case that particularly violates this assumption is\\nterm pairs like Hong and Kong, which are strongly dependent. But dependen-\\ncies can occur in various complex conﬁgurations, such as between the set of\\nterms New, York, England, City, Stock, Exchange, and University. van Rijsbergen\\n(1979) proposed a simple, plausible model which allowed a tree structure of\\nterm dependencies, as in Figure 11.1. In this model each term can be directly\\ndependent on only one other term, giving a tree structure of dependencies.\\nWhen it was invented in the 1970s, estimation problems held back the practi-\\ncal success of this model, but the idea was reinvented as the Tree Augmented\\nNaive Bayes model by Friedman and Goldszmidt (1996), who used it with\\nsome success on various machine learning data sets.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n232\\n11\\nProbabilistic information retrieval\\nx1\\nx2\\nx3\\nx4\\nx5\\nx6\\nx7\\n◮Figure 11.1\\nA tree of dependencies between terms. In this graphical model rep-\\nresentation, a term xi is directly dependent on a term xk if there is an arrow xk →xi.\\n11.4.3\\nOkapi BM25: a non-binary model\\nThe BIM was originally designed for short catalog records and abstracts of\\nfairly consistent length, and it works reasonably in these contexts, but for\\nmodern full-text search collections, it seems clear that a model should pay\\nattention to term frequency and document length, as in Chapter 6. The BM25\\nBM25 WEIGHTS\\nweighting scheme, often called Okapi weighting, after the system in which it was\\nOKAPI WEIGHTING\\nﬁrst implemented, was developed as a way of building a probabilistic model\\nsensitive to these quantities while not introducing too many additional pa-\\nrameters into the model (Spärck Jones et al. 2000). We will not develop the\\nfull theory behind the model here, but just present a series of forms that\\nbuild up to the standard form now used for document scoring. The simplest\\nscore for document d is just idf weighting of the query terms present, as in\\nEquation (11.22):\\nRSVd = ∑\\nt∈q\\nlog N\\ndft\\n(11.30)\\nSometimes, an alternative version of idf is used. If we start with the formula\\nin Equation (11.21) but in the absence of relevance feedback information we\\nestimate that S = s = 0, then we get an alternative idf formulation as follows:\\nRSVd = ∑\\nt∈q\\nlog N −dft + 1\\n2\\ndft + 1\\n2\\n(11.31)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.4\\nAn appraisal and some extensions\\n233\\nThis variant behaves slightly strangely: if a term occurs in over half the doc-\\numents in the collection then this model gives a negative term weight, which\\nis presumably undesirable. But, assuming the use of a stop list, this normally\\ndoesn’t happen, and the value for each summand can be given a ﬂoor of 0.\\nWe can improve on Equation (11.30) by factoring in the frequency of each\\nterm and document length:\\nRSVd = ∑\\nt∈q\\nlog\\n\\x14 N\\ndft\\n\\x15\\n·\\n(k1 + 1)tftd\\nk1((1 −b) + b × (Ld/Lave)) + tftd\\n(11.32)\\nHere, tftd is the frequency of term t in document d, and Ld and Lave are the\\nlength of document d and the average document length for the whole col-\\nlection. The variable k1 is a positive tuning parameter that calibrates the\\ndocument term frequency scaling. A k1 value of 0 corresponds to a binary\\nmodel (no term frequency), and a large value corresponds to using raw term\\nfrequency. b is another tuning parameter (0 ≤b ≤1) which determines\\nthe scaling by document length: b = 1 corresponds to fully scaling the term\\nweight by the document length, while b = 0 corresponds to no length nor-\\nmalization.\\nIf the query is long, then we might also use similar weighting for query\\nterms. This is appropriate if the queries are paragraph long information\\nneeds, but unnecessary for short queries.\\nRSVd = ∑\\nt∈q\\n\\x14\\nlog N\\ndft\\n\\x15\\n·\\n(k1 + 1)tftd\\nk1((1 −b) + b × (Ld/Lave)) + tftd\\n· (k3 + 1)tftq\\nk3 + tftq\\n(11.33)\\nwith tftq being the frequency of term t in the query q, and k3 being another\\npositive tuning parameter that this time calibrates term frequency scaling\\nof the query. In the equation presented, there is no length normalization of\\nqueries (it is as if b = 0 here). Length normalization of the query is unnec-\\nessary because retrieval is being done with respect to a single ﬁxed query.\\nThe tuning parameters of these formulas should ideally be set to optimize\\nperformance on a development test collection (see page 153). That is, we\\ncan search for values of these parameters that maximize performance on a\\nseparate development test collection (either manually or with optimization\\nmethods such as grid search or something more advanced), and then use\\nthese parameters on the actual test collection. In the absence of such opti-\\nmization, experiments have shown reasonable values are to set k1 and k3 to\\na value between 1.2 and 2 and b = 0.75.\\nIf we have relevance judgments available, then we can use the full form of\\n(11.21) in place of the approximation log(N/dft) introduced in (11.22):\\nRSVd\\n= ∑\\nt∈q\\nlog\\n\"\"\\n(|VRt| + 1\\n2)/(|VNRt| + 1\\n2)\\n(dft −|VRt| + 1\\n2)/(N −dft −|VR| + |VRt| + 1\\n2)\\n#\\n(11.34)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n234\\n11\\nProbabilistic information retrieval\\n×\\n(k1 + 1)tftd\\nk1((1 −b) + b(Ld/Lave)) + tftd\\n× (k3 + 1)tftq\\nk3 + tftq\\n\\x15\\nHere, VRt, NVRt, and VR are used as in Section 11.3.4. The ﬁrst part of the\\nexpression reﬂects relevance feedback (or just idf weighting if no relevance\\ninformation is available), the second implements document term frequency\\nand document length scaling, and the third considers term frequency in the\\nquery.\\nRather than just providing a term weighting method for terms in a user’s\\nquery, relevance feedback can also involve augmenting the query (automat-\\nically or with manual review) with some (say, 10–20) of the top terms in the\\nknown-relevant documents as ordered by the relevance factor ˆct from Equa-\\ntion (11.21), and the above formula can then be used with such an augmented\\nquery vector⃗q.\\nThe BM25 term weighting formulas have been used quite widely and quite\\nsuccessfully across a range of collections and search tasks. Especially in the\\nTREC evaluations, they performed well and were widely adopted by many\\ngroups. See Spärck Jones et al. (2000) for extensive motivation and discussion\\nof experimental results.\\n11.4.4\\nBayesian network approaches to IR\\nTurtle and Croft (1989; 1991) introduced into information retrieval the use\\nof Bayesian networks (Jensen and Jensen 2001), a form of probabilistic graph-\\nBAYESIAN NETWORKS\\nical model. We skip the details because fully introducing the formalism of\\nBayesian networks would require much too much space, but conceptually,\\nBayesian networks use directed graphs to show probabilistic dependencies\\nbetween variables, as in Figure 11.1, and have led to the development of so-\\nphisticated algorithms for propagating inﬂuence so as to allow learning and\\ninference with arbitrary knowledge within arbitrary directed acyclic graphs.\\nTurtle and Croft used a sophisticated network to better model the complex\\ndependencies between a document and a user’s information need.\\nThe model decomposes into two parts: a document collection network and\\na query network. The document collection network is large, but can be pre-\\ncomputed: it maps from documents to terms to concepts. The concepts are\\na thesaurus-based expansion of the terms appearing in the document. The\\nquery network is relatively small but a new network needs to be built each\\ntime a query comes in, and then attached to the document network. The\\nquery network maps from query terms, to query subexpressions (built us-\\ning probabilistic or “noisy” versions of AND and OR operators), to the user’s\\ninformation need.\\nThe result is a ﬂexible probabilistic network which can generalize vari-\\nous simpler Boolean and probabilistic models. Indeed, this is the primary\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n11.5\\nReferences and further reading\\n235\\ncase of a statistical ranked retrieval model that naturally supports structured\\nquery operators. The system allowed efﬁcient large-scale retrieval, and was\\nthe basis of the InQuery text retrieval system, built at the University of Mas-\\nsachusetts. This system performed very well in TREC evaluations and for a\\ntime was sold commercially. On the other hand, the model still used various\\napproximations and independence assumptions to make parameter estima-\\ntion and computation possible. There has not been much follow-on work\\nalong these lines, but we would note that this model was actually built very\\nearly on in the modern era of using Bayesian networks, and there have been\\nmany subsequent developments in the theory, and the time is perhaps right\\nfor a new generation of Bayesian network-based information retrieval sys-\\ntems.\\n11.5\\nReferences and further reading\\nLonger introductions to probability theory can be found in most introduc-\\ntory probability and statistics books, such as (Grinstead and Snell 1997, Rice\\n2006, Ross 2006). An introduction to Bayesian utility theory can be found in\\n(Ripley 1996).\\nThe probabilistic approach to IR originated in the UK in the 1950s. The\\nﬁrst major presentation of a probabilistic model is Maron and Kuhns (1960).\\nRobertson and Jones (1976) introduce the main foundations of the BIM and\\nvan Rijsbergen (1979) presents in detail the classic BIM probabilistic model.\\nThe idea of the PRP is variously attributed to S. E. Robertson, M. E. Maron\\nand W. S. Cooper (the term “Probabilistic Ordering Principle” is used in\\nRobertson and Jones (1976), but PRP dominates in later work). Fuhr (1992)\\nis a more recent presentation of probabilistic IR, which includes coverage of\\nother approaches such as probabilistic logics and Bayesian networks. Crestani\\net al. (1998) is another survey.Spärck Jones et al. (2000) is the deﬁnitive pre-\\nsentation of probabilistic IR experiments by the “London school”, and Robert-\\nson (2005) presents a retrospective on the group’s participation in TREC eval-\\nuations, including detailed discussion of the Okapi BM25 scoring function\\nand its development. Robertson et al. (2004) extend BM25 to the case of mul-\\ntiple weighted ﬁelds.\\nThe open-source Indri search engine, which is distributed with the Lemur\\ntoolkit (http://www.lemurproject.org/) merges ideas from Bayesian inference net-\\nworks and statistical language modeling approaches (see Chapter 12), in par-\\nticular preserving the former’s support for structured query operators.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n237\\n12\\nLanguage models for information\\nretrieval\\nA common suggestion to users for coming up with good queries is to think\\nof words that would likely appear in a relevant document, and to use those\\nwords as the query. The language modeling approach to IR directly models\\nthat idea: a document is a good match to a query if the document model\\nis likely to generate the query, which will in turn happen if the document\\ncontains the query words often. This approach thus provides a different real-\\nization of some of the basic ideas for document ranking which we saw in Sec-\\ntion 6.2 (page 117). Instead of overtly modeling the probability P(R = 1|q, d)\\nof relevance of a document d to a query q, as in the traditional probabilis-\\ntic approach to IR (Chapter 11), the basic language modeling approach in-\\nstead builds a probabilistic language model Md from each document d, and\\nranks documents based on the probability of the model generating the query:\\nP(q|Md).\\nIn this chapter, we ﬁrst introduce the concept of language models (Sec-\\ntion 12.1) and then describe the basic and most commonly used language\\nmodeling approach to IR, the Query Likelihood Model (Section 12.2). Af-\\nter some comparisons between the language modeling approach and other\\napproaches to IR (Section 12.3), we ﬁnish by brieﬂy describing various ex-\\ntensions to the language modeling approach (Section 12.4).\\n12.1\\nLanguage models\\n12.1.1\\nFinite automata and language models\\nWhat do we mean by a document model generating a query? A traditional\\ngenerative model of a language, of the kind familiar from formal language\\nGENERATIVE MODEL\\ntheory, can be used either to recognize or to generate strings. For example,\\nthe ﬁnite automaton shown in Figure 12.1 can generate strings that include\\nthe examples shown. The full set of strings that can be generated is called\\nthe language of the automaton.1\\nLANGUAGE\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n238\\n12\\nLanguage models for information retrieval\\nI\\nwish\\nI wish\\nI wish I wish\\nI wish I wish I wish\\nI wish I wish I wish I wish I wish I wish\\n...\\nCANNOT GENERATE: wish I wish\\n◮Figure 12.1\\nA simple ﬁnite automaton and some of the strings in the language it\\ngenerates. →shows the start state of the automaton and a double circle indicates a\\n(possible) ﬁnishing state.\\nq1\\nP(STOP|q1) = 0.2\\nthe\\n0.2\\na\\n0.1\\nfrog\\n0.01\\ntoad\\n0.01\\nsaid\\n0.03\\nlikes\\n0.02\\nthat\\n0.04\\n...\\n...\\n◮Figure 12.2\\nA one-state ﬁnite automaton that acts as a unigram language model.\\nWe show a partial speciﬁcation of the state emission probabilities.\\nIf instead each node has a probability distribution over generating differ-\\nent terms, we have a language model. The notion of a language model is\\ninherently probabilistic. A language model is a function that puts a probability\\nLANGUAGE MODEL\\nmeasure over strings drawn from some vocabulary. That is, for a language\\nmodel M over an alphabet Σ:\\n∑\\ns∈Σ∗\\nP(s) = 1\\n(12.1)\\nOne simple kind of language model is equivalent to a probabilistic ﬁnite\\nautomaton consisting of just a single node with a single probability distri-\\nbution over producing different terms, so that ∑t∈V P(t) = 1, as shown\\nin Figure 12.2. After generating each word, we decide whether to stop or\\nto loop around and then produce another word, and so the model also re-\\nquires a probability of stopping in the ﬁnishing state. Such a model places a\\nprobability distribution over any sequence of words. By construction, it also\\nprovides a model for generating text according to its distribution.\\n1. Finite automata can have outputs attached to either their states or their arcs; we use states\\nhere, because that maps directly on to the way probabilistic automata are usually formalized.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.1\\nLanguage models\\n239\\nModel M1\\nModel M2\\nthe\\n0.2\\nthe\\n0.15\\na\\n0.1\\na\\n0.12\\nfrog\\n0.01\\nfrog\\n0.0002\\ntoad\\n0.01\\ntoad\\n0.0001\\nsaid\\n0.03\\nsaid\\n0.03\\nlikes\\n0.02\\nlikes\\n0.04\\nthat\\n0.04\\nthat\\n0.04\\ndog\\n0.005\\ndog\\n0.01\\ncat\\n0.003\\ncat\\n0.015\\nmonkey\\n0.001\\nmonkey\\n0.002\\n...\\n...\\n...\\n...\\n◮Figure 12.3\\nPartial speciﬁcation of two unigram language models.\\n\\x0f\\nExample 12.1:\\nTo ﬁnd the probability of a word sequence, we just multiply the\\nprobabilities which the model gives to each word in the sequence, together with the\\nprobability of continuing or stopping after producing each word. For example,\\nP(frog said that toad likes frog)\\n=\\n(0.01 × 0.03 × 0.04 × 0.01 × 0.02 × 0.01)\\n(12.2)\\n×(0.8 × 0.8 × 0.8 × 0.8 × 0.8 × 0.8 × 0.2)\\n≈\\n0.000000000001573\\nAs you can see, the probability of a particular string/document, is usually a very\\nsmall number! Here we stopped after generating frog the second time. The ﬁrst line of\\nnumbers are the term emission probabilities, and the second line gives the probabil-\\nity of continuing or stopping after generating each word. An explicit stop probability\\nis needed for a ﬁnite automaton to be a well-formed language model according to\\nEquation (12.1). Nevertheless, most of the time, we will omit to include STOP and\\n(1 −STOP) probabilities (as do most other authors). To compare two models for a\\ndata set, we can calculate their likelihood ratio, which results from simply dividing the\\nLIKELIHOOD RATIO\\nprobability of the data according to one model by the probability of the data accord-\\ning to the other model. Providing that the stop probability is ﬁxed, its inclusion will\\nnot alter the likelihood ratio that results from comparing the likelihood of two lan-\\nguage models generating a string. Hence, it will not alter the ranking of documents.2\\nNevertheless, formally, the numbers will no longer truly be probabilities, but only\\nproportional to probabilities. See Exercise 12.4.\\n\\x0f\\nExample 12.2:\\nSuppose, now, that we have two language models M1 and M2,\\nshown partially in Figure 12.3. Each gives a probability estimate to a sequence of\\n2. In the IR context that we are leading up to, taking the stop probability to be ﬁxed across\\nmodels seems reasonable. This is because we are generating queries, and the length distribution\\nof queries is ﬁxed and independent of the document from which we are generating the language\\nmodel.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n240\\n12\\nLanguage models for information retrieval\\nterms, as already illustrated in Example 12.1. The language model that gives the\\nhigher probability to the sequence of terms is more likely to have generated the term\\nsequence. This time, we will omit STOP probabilities from our calculations. For the\\nsequence shown, we get:\\n(12.3)\\ns\\nfrog\\nsaid\\nthat\\ntoad\\nlikes\\nthat\\ndog\\nM1\\n0.01\\n0.03\\n0.04\\n0.01\\n0.02\\n0.04\\n0.005\\nM2\\n0.0002\\n0.03\\n0.04\\n0.0001\\n0.04\\n0.04\\n0.01\\nP(s|M1) = 0.00000000000048\\nP(s|M2) = 0.000000000000000384\\nand we see that P(s|M1) > P(s|M2). We present the formulas here in terms of prod-\\nucts of probabilities, but, as is common in probabilistic applications, in practice it is\\nusually best to work with sums of log probabilities (cf. page 258).\\n12.1.2\\nTypes of language models\\nHow do we build probabilities over sequences of terms? We can always\\nuse the chain rule from Equation (11.1) to decompose the probability of a\\nsequence of events into the probability of each successive event conditioned\\non earlier events:\\nP(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t1t2)P(t4|t1t2t3)\\n(12.4)\\nThe simplest form of language model simply throws away all conditioning\\ncontext, and estimates each term independently. Such a model is called a\\nunigram language model:\\nUNIGRAM LANGUAGE\\nMODEL\\nPuni(t1t2t3t4) = P(t1)P(t2)P(t3)P(t4)\\n(12.5)\\nThere are many more complex kinds of language models, such as bigram\\nBIGRAM LANGUAGE\\nMODEL\\nlanguage models, which condition on the previous term,\\nPbi(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t2)P(t4|t3)\\n(12.6)\\nand even more complex grammar-based language models such as proba-\\nbilistic context-free grammars. Such models are vital for tasks like speech\\nrecognition, spelling correction, and machine translation, where you need\\nthe probability of a term conditioned on surrounding context. However,\\nmost language-modeling work in IR has used unigram language models.\\nIR is not the place where you most immediately need complex language\\nmodels, since IR does not directly depend on the structure of sentences to\\nthe extent that other tasks like speech recognition do. Unigram models are\\noften sufﬁcient to judge the topic of a text. Moreover, as we shall see, IR lan-\\nguage models are frequently estimated from a single document and so it is\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.1\\nLanguage models\\n241\\nquestionable whether there is enough training data to do more. Losses from\\ndata sparseness (see the discussion on page 260) tend to outweigh any gains\\nfrom richer models. This is an example of the bias-variance tradeoff (cf. Sec-\\ntion 14.6, page 308): With limited training data, a more constrained model\\ntends to perform better. In addition, unigram models are more efﬁcient to\\nestimate and apply than higher-order models. Nevertheless, the importance\\nof phrase and proximity queries in IR in general suggests that future work\\nshould make use of more sophisticated language models, and some has be-\\ngun to (see Section 12.5, page 252). Indeed, making this move parallels the\\nmodel of van Rijsbergen in Chapter 11 (page 231).\\n12.1.3\\nMultinomial distributions over words\\nUnder the unigram language model the order of words is irrelevant, and so\\nsuch models are often called “bag of words” models, as discussed in Chap-\\nter 6 (page 117). Even though there is no conditioning on preceding context,\\nthis model nevertheless still gives the probability of a particular ordering of\\nterms. However, any other ordering of this bag of terms will have the same\\nprobability. So, really, we have a multinomial distribution over words. So long\\nMULTINOMIAL\\nDISTRIBUTION\\nas we stick to unigram models, the language model name and motivation\\ncould be viewed as historical rather than necessary. We could instead just\\nrefer to the model as a multinomial model. From this perspective, the equa-\\ntions presented above do not present the multinomial probability of a bag of\\nwords, since they do not sum over all possible orderings of those words, as\\nis done by the multinomial coefﬁcient (the ﬁrst term on the right-hand side)\\nin the standard presentation of a multinomial model:\\nP(d) =\\nLd!\\ntft1,d!tft2,d! · · · tftM,d! P(t1)tft1,dP(t2)tft2,d · · · P(tM)tftM,d\\n(12.7)\\nHere, Ld = ∑1≤i≤M tfti,d is the length of document d, M is the size of the term\\nvocabulary, and the products are now over the terms in the vocabulary, not\\nthe positions in the document. However, just as with STOP probabilities, in\\npractice we can also leave out the multinomial coefﬁcient in our calculations,\\nsince, for a particular bag of words, it will be a constant, and so it has no effect\\non the likelihood ratio of two different models generating a particular bag of\\nwords. Multinomial distributions also appear in Section 13.2 (page 258).\\nThe fundamental problem in designing language models is that we do not\\nknow what exactly we should use as the model Md. However, we do gener-\\nally have a sample of text that is representative of that model. This problem\\nmakes a lot of sense in the original, primary uses of language models. For ex-\\nample, in speech recognition, we have a training sample of (spoken) text. But\\nwe have to expect that, in the future, users will use different words and in\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n242\\n12\\nLanguage models for information retrieval\\ndifferent sequences, which we have never observed before, and so the model\\nhas to generalize beyond the observed data to allow unknown words and se-\\nquences. This interpretation is not so clear in the IR case, where a document\\nis ﬁnite and usually ﬁxed. The strategy we adopt in IR is as follows. We\\npretend that the document d is only a representative sample of text drawn\\nfrom a model distribution, treating it like a ﬁne-grained topic. We then esti-\\nmate a language model from this sample, and use that model to calculate the\\nprobability of observing any word sequence, and, ﬁnally, we rank documents\\naccording to their probability of generating the query.\\n?\\nExercise 12.1\\n[⋆]\\nIncluding stop probabilities in the calculation, what will the sum of the probability\\nestimates of all strings in the language of length 1 be? Assume that you generate a\\nword and then decide whether to stop or not (i.e., the null string is not part of the\\nlanguage).\\nExercise 12.2\\n[⋆]\\nIf the stop probability is omitted from calculations, what will the sum of the scores\\nassigned to strings in the language of length 1 be?\\nExercise 12.3\\n[⋆]\\nWhat is the likelihood ratio of the document according to M1 and M2 in Exam-\\nple 12.2?\\nExercise 12.4\\n[⋆]\\nNo explicit STOP probability appeared in Example 12.2. Assuming that the STOP\\nprobability of each model is 0.1, does this change the likelihood ratio of a document\\naccording to the two models?\\nExercise 12.5\\n[⋆⋆]\\nHow might a language model be used in a spelling correction system? In particular,\\nconsider the case of context-sensitive spelling correction, and correcting incorrect us-\\nages of words, such as their in Are you their? (See Section 3.5 (page 65) for pointers to\\nsome literature on this topic.)\\n12.2\\nThe query likelihood model\\n12.2.1\\nUsing query likelihood language models in IR\\nLanguage modeling is a quite general formal approach to IR, with many vari-\\nant realizations. The original and basic method for using language models\\nin IR is the query likelihood model. In it, we construct from each document d\\nQUERY LIKELIHOOD\\nMODEL\\nin the collection a language model Md. Our goal is to rank documents by\\nP(d|q), where the probability of a document is interpreted as the likelihood\\nthat it is relevant to the query. Using Bayes rule (as introduced in Section 11.1,\\npage 220), we have:\\nP(d|q) = P(q|d)P(d)/P(q)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n243\\nP(q) is the same for all documents, and so can be ignored. The prior prob-\\nability of a document P(d) is often treated as uniform across all d and so it\\ncan also be ignored, but we could implement a genuine prior which could in-\\nclude criteria like authority, length, genre, newness, and number of previous\\npeople who have read the document. But, given these simpliﬁcations, we\\nreturn results ranked by simply P(q|d), the probability of the query q under\\nthe language model derived from d. The Language Modeling approach thus\\nattempts to model the query generation process: Documents are ranked by\\nthe probability that a query would be observed as a random sample from the\\nrespective document model.\\nThe most common way to do this is using the multinomial unigram lan-\\nguage model, which is equivalent to a multinomial Naive Bayes model (page 263),\\nwhere the documents are the classes, each treated in the estimation as a sep-\\narate “language”. Under this model, we have that:\\nP(q|Md) = Kq ∏\\nt∈V\\nP(t|Md)tft,d\\n(12.8)\\nwhere, again Kq = Ld!/(tft1,d!tft2,d! · · · tftM,d!) is the multinomial coefﬁcient\\nfor the query q, which we will henceforth ignore, since it is a constant for a\\nparticular query.\\nFor retrieval based on a language model (henceforth LM), we treat the\\ngeneration of queries as a random process. The approach is to\\n1. Infer a LM for each document.\\n2. Estimate P(q|Mdi), the probability of generating the query according to\\neach of these document models.\\n3. Rank the documents according to these probabilities.\\nThe intuition of the basic model is that the user has a prototype document in\\nmind, and generates a query based on words that appear in this document.\\nOften, users have a reasonable idea of terms that are likely to occur in doc-\\numents of interest and they will choose query terms that distinguish these\\ndocuments from others in the collection.3 Collection statistics are an integral\\npart of the language model, rather than being used heuristically as in many\\nother approaches.\\n12.2.2\\nEstimating the query generation probability\\nIn this section we describe how to estimate P(q|Md). The probability of pro-\\nducing the query given the LM Md of document d using maximum likelihood\\n3.\\nOf course, in other cases, they do not. The answer to this within the language modeling\\napproach is translation language models, as brieﬂy discussed in Section 12.4.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n244\\n12\\nLanguage models for information retrieval\\nestimation (MLE) and the unigram assumption is:\\nˆP(q|Md) = ∏\\nt∈q\\nˆPmle(t|Md) = ∏\\nt∈q\\ntft,d\\nLd\\n(12.9)\\nwhere Md is the language model of document d, tft,d is the (raw) term fre-\\nquency of term t in document d, and Ld is the number of tokens in docu-\\nment d. That is, we just count up how often each word occurred, and divide\\nthrough by the total number of words in the document d. This is the same\\nmethod of calculating an MLE as we saw in Section 11.3.2 (page 226), but\\nnow using a multinomial over word counts.\\nThe classic problem with using language models is one of estimation (the\\nˆ symbol on the P’s is used above to stress that the model is estimated):\\nterms appear very sparsely in documents. In particular, some words will\\nnot have appeared in the document at all, but are possible words for the in-\\nformation need, which the user may have used in the query. If we estimate\\nˆP(t|Md) = 0 for a term missing from a document d, then we get a strict\\nconjunctive semantics: documents will only give a query non-zero probabil-\\nity if all of the query terms appear in the document. Zero probabilities are\\nclearly a problem in other uses of language models, such as when predicting\\nthe next word in a speech recognition application, because many words will\\nbe sparsely represented in the training data. It may seem rather less clear\\nwhether this is problematic in an IR application. This could be thought of\\nas a human-computer interface issue: vector space systems have generally\\npreferred more lenient matching, though recent web search developments\\nhave tended more in the direction of doing searches with such conjunctive\\nsemantics. Regardless of the approach here, there is a more general prob-\\nlem of estimation: occurring words are also badly estimated; in particular,\\nthe probability of words occurring once in the document is normally over-\\nestimated, since their one occurrence was partly by chance. The answer to\\nthis (as we saw in Section 11.3.2, page 226) is smoothing. But as people have\\ncome to understand the LM approach better, it has become apparent that the\\nrole of smoothing in this model is not only to avoid zero probabilities. The\\nsmoothing of terms actually implements major parts of the term weighting\\ncomponent (Exercise 12.8). It is not just that an unsmoothed model has con-\\njunctive semantics; an unsmoothed model works badly because it lacks parts\\nof the term weighting component.\\nThus, we need to smooth probabilities in our document language mod-\\nels: to discount non-zero probabilities and to give some probability mass to\\nunseen words. There’s a wide space of approaches to smoothing probabil-\\nity distributions to deal with this problem. In Section 11.3.2 (page 226), we\\nalready discussed adding a number (1, 1/2, or a small α) to the observed\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n245\\ncounts and renormalizing to give a probability distribution.4\\nIn this sec-\\ntion we will mention a couple of other smoothing methods, which involve\\ncombining observed counts with a more general reference probability distri-\\nbution. The general approach is that a non-occurring term should be possi-\\nble in a query, but its probability should be somewhat close to but no more\\nlikely than would be expected by chance from the whole collection. That is,\\nif tft,d = 0 then\\nˆP(t|Md) ≤cft/T\\nwhere cft is the raw count of the term in the collection, and T is the raw size\\n(number of tokens) of the entire collection. A simple idea that works well in\\npractice is to use a mixture between a document-speciﬁc multinomial distri-\\nbution and a multinomial distribution estimated from the entire collection:\\nˆP(t|d) = λ ˆPmle(t|Md) + (1 −λ) ˆPmle(t|Mc)\\n(12.10)\\nwhere 0 < λ < 1 and Mc is a language model built from the entire doc-\\nument collection. This mixes the probability from the document with the\\ngeneral collection frequency of the word. Such a model is referred to as a\\nlinear interpolation language model.5 Correctly setting λ is important to the\\nLINEAR\\nINTERPOLATION\\ngood performance of this model.\\nAn alternative is to use a language model built from the whole collection\\nas a prior distribution in a Bayesian updating process (rather than a uniform\\nBAYESIAN SMOOTHING\\ndistribution, as we saw in Section 11.3.2). We then get the following equation:\\nˆP(t|d) = tft,d + α ˆP(t|Mc)\\nLd + α\\n(12.11)\\nBoth of these smoothing methods have been shown to perform well in IR\\nexperiments; we will stick with the linear interpolation smoothing method\\nfor the rest of this section. While different in detail, they are both conceptu-\\nally similar: in both cases the probability estimate for a word present in the\\ndocument combines a discounted MLE and a fraction of the estimate of its\\nprevalence in the whole collection, while for words not present in a docu-\\nment, the estimate is just a fraction of the estimate of the prevalence of the\\nword in the whole collection.\\nThe role of smoothing in LMs for IR is not simply or principally to avoid es-\\ntimation problems. This was not clear when the models were ﬁrst proposed,\\nbut it is now understood that smoothing is essential to the good properties\\n4. In the context of probability theory, (re)normalization refers to summing numbers that cover\\nan event space and dividing them through by their sum, so that the result is a probability distri-\\nbution which sums to 1. This is distinct from both the concept of term normalization in Chapter 2\\nand the concept of length normalization in Chapter 6, which is done with a L2 norm.\\n5. It is also referred to as Jelinek-Mercer smoothing.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n246\\n12\\nLanguage models for information retrieval\\nof the models. The reason for this is explored in Exercise 12.8. The extent\\nof smoothing in these two models is controlled by the λ and α parameters: a\\nsmall value of λ or a large value of α means more smoothing. This parameter\\ncan be tuned to optimize performance using a line search (or, for the linear\\ninterpolation model, by other methods, such as the expectation maximimiza-\\ntion algorithm; see Section 16.5, page 368). The value need not be a constant.\\nOne approach is to make the value a function of the query size. This is useful\\nbecause a small amount of smoothing (a “conjunctive-like” search) is more\\nsuitable for short queries, while a lot of smoothing is more suitable for long\\nqueries.\\nTo summarize, the retrieval ranking for a query q under the basic LM for\\nIR we have been considering is given by:\\nP(d|q) ∝P(d)∏\\nt∈q\\n((1 −λ)P(t|Mc) + λP(t|Md))\\n(12.12)\\nThis equation captures the probability that the document that the user had\\nin mind was in fact d.\\n\\x0f\\nExample 12.3:\\nSuppose the document collection contains two documents:\\n•\\nd1: Xyzzy reports a proﬁt but revenue is down\\n•\\nd2: Quorus narrows quarter loss but revenue decreases further\\nThe model will be MLE unigram models from the documents and collection, mixed\\nwith λ = 1/2.\\nSuppose the query is revenue down. Then:\\nP(q|d1)\\n=\\n[(1/8 + 2/16)/2] × [(1/8 + 1/16)/2]\\n(12.13)\\n=\\n1/8 × 3/32 = 3/256\\nP(q|d2)\\n=\\n[(1/8 + 2/16)/2] × [(0/8 + 1/16)/2]\\n=\\n1/8 × 1/32 = 1/256\\nSo, the ranking is d1 > d2.\\n12.2.3\\nPonte and Croft’s Experiments\\nPonte and Croft (1998) present the ﬁrst experiments on the language model-\\ning approach to information retrieval. Their basic approach is the model that\\nwe have presented until now. However, we have presented an approach\\nwhere the language model is a mixture of two multinomials, much as in\\n(Miller et al. 1999, Hiemstra 2000) rather than Ponte and Croft’s multivari-\\nate Bernoulli model. The use of multinomials has been standard in most\\nsubsequent work in the LM approach and experimental results in IR, as\\nwell as evidence from text classiﬁcation which we consider in Section 13.3\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n247\\nPrecision\\nRec.\\ntf-idf\\nLM\\n%chg\\n0.0\\n0.7439\\n0.7590\\n+2.0\\n0.1\\n0.4521\\n0.4910\\n+8.6\\n0.2\\n0.3514\\n0.4045\\n+15.1\\n*\\n0.3\\n0.2761\\n0.3342\\n+21.0\\n*\\n0.4\\n0.2093\\n0.2572\\n+22.9\\n*\\n0.5\\n0.1558\\n0.2061\\n+32.3\\n*\\n0.6\\n0.1024\\n0.1405\\n+37.1\\n*\\n0.7\\n0.0451\\n0.0760\\n+68.7\\n*\\n0.8\\n0.0160\\n0.0432\\n+169.6\\n*\\n0.9\\n0.0033\\n0.0063\\n+89.3\\n1.0\\n0.0028\\n0.0050\\n+76.9\\nAve\\n0.1868\\n0.2233\\n+19.55\\n*\\n◮Figure 12.4\\nResults of a comparison of tf-idf with language modeling (LM) term\\nweighting by Ponte and Croft (1998). The version of tf-idf from the INQUERY IR sys-\\ntem includes length normalization of tf. The table gives an evaluation according to\\n11-point average precision with signiﬁcance marked with a * according to a Wilcoxon\\nsigned rank test. The language modeling approach always does better in these exper-\\niments, but note that where the approach shows signiﬁcant gains is at higher levels\\nof recall.\\n(page 263), suggests that it is superior. Ponte and Croft argued strongly for\\nthe effectiveness of the term weights that come from the language modeling\\napproach over traditional tf-idf weights. We present a subset of their results\\nin Figure 12.4 where they compare tf-idf to language modeling by evaluating\\nTREC topics 202–250 over TREC disks 2 and 3. The queries are sentence-\\nlength natural language queries. The language modeling approach yields\\nsigniﬁcantly better results than their baseline tf-idf based term weighting ap-\\nproach. And indeed the gains shown here have been extended in subsequent\\nwork.\\n?\\nExercise 12.6\\n[⋆]\\nConsider making a language model from the following training text:\\nthe martian has landed on the latin pop sensation ricky martin\\na. Under a MLE-estimated unigram probability model, what are P(the) and P(martian)?\\nb. Under a MLE-estimated bigram model, what are P(sensation|pop) and P(pop|the)?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n248\\n12\\nLanguage models for information retrieval\\nExercise 12.7\\n[⋆⋆]\\nSuppose we have a collection that consists of the 4 documents given in the below\\ntable.\\ndocID\\nDocument text\\n1\\nclick go the shears boys click click click\\n2\\nclick click\\n3\\nmetal here\\n4\\nmetal shears click here\\nBuild a query likelihood language model for this document collection. Assume a\\nmixture model between the documents and the collection, with both weighted at 0.5.\\nMaximum likelihood estimation (mle) is used to estimate both as unigram models.\\nWork out the model probabilities of the queries click, shears, and hence click shears for\\neach document, and use those probabilities to rank the documents returned by each\\nquery. Fill in these probabilities in the below table:\\nQuery\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nclick\\nshears\\nclick shears\\nWhat is the ﬁnal ranking of the documents for the query click shears?\\nExercise 12.8\\n[⋆⋆]\\nUsing the calculations in Exercise 12.7 as inspiration or as examples where appro-\\npriate, write one sentence each describing the treatment that the model in Equa-\\ntion (12.10) gives to each of the following quantities. Include whether it is present\\nin the model or not and whether the effect is raw or scaled.\\na. Term frequency in a document\\nb. Collection frequency of a term\\nc. Document frequency of a term\\nd. Length normalization of a term\\nExercise 12.9\\n[⋆⋆]\\nIn the mixture model approach to the query likelihood model (Equation (12.12)), the\\nprobability estimate of a term is based on the term frequency of a word in a document,\\nand the collection frequency of the word. Doing this certainly guarantees that each\\nterm of a query (in the vocabulary) has a non-zero chance of being generated by each\\ndocument. But it has a more subtle but important effect of implementing a form of\\nterm weighting, related to what we saw in Chapter 6. Explain how this works. In\\nparticular, include in your answer a concrete numeric example showing this term\\nweighting at work.\\n12.3\\nLanguage modeling versus other approaches in IR\\nThe language modeling approach provides a novel way of looking at the\\nproblem of text retrieval, which links it with a lot of recent work in speech\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.3\\nLanguage modeling versus other approaches in IR\\n249\\nand language processing. As Ponte and Croft (1998) emphasize, the language\\nmodeling approach to IR provides a different approach to scoring matches\\nbetween queries and documents, and the hope is that the probabilistic lan-\\nguage modeling foundation improves the weights that are used, and hence\\nthe performance of the model. The major issue is estimation of the docu-\\nment model, such as choices of how to smooth it effectively. The model\\nhas achieved very good retrieval results. Compared to other probabilistic\\napproaches, such as the BIM from Chapter 11, the main difference initially\\nappears to be that the LM approach does away with explicitly modeling rel-\\nevance (whereas this is the central variable evaluated in the BIM approach).\\nBut this may not be the correct way to think about things, as some of the\\npapers in Section 12.5 further discuss. The LM approach assumes that docu-\\nments and expressions of information needs are objects of the same type, and\\nassesses their match by importing the tools and methods of language mod-\\neling from speech and natural language processing. The resulting model is\\nmathematically precise, conceptually simple, computationally tractable, and\\nintuitively appealing. This seems similar to the situation with XML retrieval\\n(Chapter 10): there the approaches that assume queries and documents are\\nobjects of the same type are also among the most successful.\\nOn the other hand, like all IR models, you can also raise objections to the\\nmodel. The assumption of equivalence between document and information\\nneed representation is unrealistic. Current LM approaches use very simple\\nmodels of language, usually unigram models. Without an explicit notion of\\nrelevance, relevance feedback is difﬁcult to integrate into the model, as are\\nuser preferences. It also seems necessary to move beyond a unigram model\\nto accommodate notions of phrase or passage matching or Boolean retrieval\\noperators. Subsequent work in the LM approach has looked at addressing\\nsome of these concerns, including putting relevance back into the model and\\nallowing a language mismatch between the query language and the docu-\\nment language.\\nThe model has signiﬁcant relations to traditional tf-idf models. Term fre-\\nquency is directly represented in tf-idf models, and much recent work has\\nrecognized the importance of document length normalization. The effect of\\ndoing a mixture of document generation probability with collection gener-\\nation probability is a little like idf: terms rare in the general collection but\\ncommon in some documents will have a greater inﬂuence on the ranking of\\ndocuments. In most concrete realizations, the models share treating terms as\\nif they were independent. On the other hand, the intuitions are probabilistic\\nrather than geometric, the mathematical models are more principled rather\\nthan heuristic, and the details of how statistics like term frequency and doc-\\nument length are used differ. If you are concerned mainly with performance\\nnumbers, recent work has shown the LM approach to be very effective in re-\\ntrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n250\\n12\\nLanguage models for information retrieval\\nQuery\\nQuery model\\nP(t|Query)\\nDocument\\nDoc. model\\nP(t|Document)\\n(a)\\n(b)\\n(c)\\n◮Figure 12.5\\nThree ways of developing the language modeling approach: (a) query\\nlikelihood, (b) document likelihood, and (c) model comparison.\\nperhaps still insufﬁcient evidence that its performance so greatly exceeds that\\nof a well-tuned traditional vector space retrieval system as to justify chang-\\ning an existing implementation.\\n12.4\\nExtended language modeling approaches\\nIn this section we brieﬂy mention some of the work that extends the basic\\nlanguage modeling approach.\\nThere are other ways to think of using the language modeling idea in IR\\nsettings, and many of them have been tried in subsequent work. Rather than\\nlooking at the probability of a document language model Md generating the\\nquery, you can look at the probability of a query language model Mq gener-\\nating the document. The main reason that doing things in this direction and\\ncreating a document likelihood model is less appealing is that there is much less\\nDOCUMENT\\nLIKELIHOOD MODEL\\ntext available to estimate a language model based on the query text, and so\\nthe model will be worse estimated, and will have to depend more on being\\nsmoothed with some other language model. On the other hand, it is easy to\\nsee how to incorporate relevance feedback into such a model: you can ex-\\npand the query with terms taken from relevant documents in the usual way\\nand hence update the language model Mq (Zhai and Lafferty 2001a). Indeed,\\nwith appropriate modeling choices, this approach leads to the BIM model of\\nChapter 11. The relevance model of Lavrenko and Croft (2001) is an instance\\nof a document likelihood model, which incorporates pseudo-relevance feed-\\nback into a language modeling approach. It achieves very strong empirical\\nresults.\\nRather than directly generating in either direction, we can make a lan-\\nguage model from both the document and query, and then ask how different\\nthese two language models are from each other. Lafferty and Zhai (2001) lay\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n12.4\\nExtended language modeling approaches\\n251\\nout these three ways of thinking about the problem, which we show in Fig-\\nure 12.5, and develop a general risk minimization approach for document\\nretrieval. For instance, one way to model the risk of returning a document d\\nas relevant to a query q is to use the Kullback-Leibler (KL) divergence between\\nKULLBACK-LEIBLER\\nDIVERGENCE\\ntheir respective language models:\\nR(d; q) = KL(Md∥Mq) = ∑\\nt∈V\\nP(t|Mq) log P(t|Mq)\\nP(t|Md)\\n(12.14)\\nKL divergence is an asymmetric divergence measure originating in informa-\\ntion theory, which measures how bad the probability distribution Mq is at\\nmodeling Md (Cover and Thomas 1991, Manning and Schütze 1999). Laf-\\nferty and Zhai (2001) present results suggesting that a model comparison\\napproach outperforms both query-likelihood and document-likelihood ap-\\nproaches. One disadvantage of using KL divergence as a ranking function\\nis that scores are not comparable across queries. This does not matter for ad\\nhoc retrieval, but is important in other applications such as topic tracking.\\nKraaij and Spitters (2003) suggest an alternative proposal which models sim-\\nilarity as a normalized log-likelihood ratio (or, equivalently, as a difference\\nbetween cross-entropies).\\nBasic LMs do not address issues of alternate expression, that is, synonymy,\\nor any deviation in use of language between queries and documents. Berger\\nand Lafferty (1999) introduce translation models to bridge this query-document\\ngap. A translation model lets you generate query words not in a document by\\nTRANSLATION MODEL\\ntranslation to alternate terms with similar meaning. This also provides a ba-\\nsis for performing cross-language IR. We assume that the translation model\\ncan be represented by a conditional probability distribution T(·|·) between\\nvocabulary terms. The form of the translation query generation model is\\nthen:\\nP(q|Md) = ∏\\nt∈q ∑\\nv∈V\\nP(v|Md)T(t|v)\\n(12.15)\\nThe term P(v|Md) is the basic document language model, and the term T(t|v)\\nperforms translation. This model is clearly more computationally intensive\\nand we need to build a translation model. The translation model is usually\\nbuilt using separate resources (such as a traditional thesaurus or bilingual\\ndictionary or a statistical machine translation system’s translation diction-\\nary), but can be built using the document collection if there are pieces of\\ntext that naturally paraphrase or summarize other pieces of text. Candi-\\ndate examples are documents and their titles or abstracts, or documents and\\nanchor-text pointing to them in a hypertext environment.\\nBuilding extended LM approaches remains an active area of research. In\\ngeneral, translation models, relevance feedback models, and model compar-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n252\\n12\\nLanguage models for information retrieval\\nison approaches have all been demonstrated to improve performance over\\nthe basic query likelihood LM.\\n12.5\\nReferences and further reading\\nFor more details on the basic concepts of probabilistic language models and\\ntechniques for smoothing, see either Manning and Schütze (1999, Chapter 6)\\nor Jurafsky and Martin (2008, Chapter 4).\\nThe important initial papers that originated the language modeling ap-\\nproach to IR are: (Ponte and Croft 1998, Hiemstra 1998, Berger and Lafferty\\n1999, Miller et al. 1999). Other relevant papers can be found in the next sev-\\neral years of SIGIR proceedings. (Croft and Lafferty 2003) contains a col-\\nlection of papers from a workshop on language modeling approaches and\\nHiemstra and Kraaij (2005) review one prominent thread of work on using\\nlanguage modeling approaches for TREC tasks. Zhai and Lafferty (2001b)\\nclarify the role of smoothing in LMs for IR and present detailed empirical\\ncomparisons of different smoothing methods. Zaragoza et al. (2003) advo-\\ncate using full Bayesian predictive distributions rather than MAP point es-\\ntimates, but while they outperform Bayesian smoothing, they fail to outper-\\nform a linear interpolation. Zhai and Lafferty (2002) argue that a two-stage\\nsmoothing model with ﬁrst Bayesian smoothing followed by linear interpo-\\nlation gives a good model of the task, and performs better and more stably\\nthan a single form of smoothing. A nice feature of the LM approach is that it\\nprovides a convenient and principled way to put various kinds of prior infor-\\nmation into the model; Kraaij et al. (2002) demonstrate this by showing the\\nvalue of link information as a prior in improving web entry page retrieval\\nperformance. As brieﬂy discussed in Chapter 16 (page 353), Liu and Croft\\n(2004) show some gains by smoothing a document LM with estimates from\\na cluster of similar documents; Tao et al. (2006) report larger gains by doing\\ndocument-similarity based smoothing.\\nHiemstra and Kraaij (2005) present TREC results showing a LM approach\\nbeating use of BM25 weights.\\nRecent work has achieved some gains by\\ngoing beyond the unigram model, providing the higher order models are\\nsmoothed with lower order models (Gao et al. 2004, Cao et al. 2005), though\\nthe gains to date remain modest. Spärck Jones (2004) presents a critical view-\\npoint on the rationale for the language modeling approach, but Lafferty and\\nZhai (2003) argue that a uniﬁed account can be given of the probabilistic\\nsemantics underlying both the language modeling approach presented in\\nthis chapter and the classical probabilistic information retrieval approach of\\nChapter 11. The Lemur Toolkit (http://www.lemurproject.org/) provides a ﬂexi-\\nble open source framework for investigating language modeling approaches\\nto IR.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n253\\n13\\nText classiﬁcation and Naive\\nBayes\\nThus far, this book has mainly discussed the process of ad hoc retrieval, where\\nusers have transient information needs that they try to address by posing\\none or more queries to a search engine. However, many users have ongoing\\ninformation needs. For example, you might need to track developments in\\nmulticore computer chips. One way of doing this is to issue the query multi-\\ncore AND computer AND chip against an index of recent newswire articles each\\nmorning. In this and the following two chapters we examine the question:\\nHow can this repetitive task be automated? To this end, many systems sup-\\nport standing queries. A standing query is like any other query except that it\\nSTANDING QUERY\\nis periodically executed on a collection to which new documents are incre-\\nmentally added over time.\\nIf your standing query is just multicore AND computer AND chip, you will tend\\nto miss many relevant new articles which use other terms such as multicore\\nprocessors. To achieve good recall, standing queries thus have to be reﬁned\\nover time and can gradually become quite complex. In this example, using a\\nBoolean search engine with stemming, you might end up with a query like\\n(multicore OR multi-core) AND (chip OR processor OR microprocessor).\\nTo capture the generality and scope of the problem space to which stand-\\ning queries belong, we now introduce the general notion of a classiﬁcation\\nCLASSIFICATION\\nproblem. Given a set of classes, we seek to determine which class(es) a given\\nobject belongs to. In the example, the standing query serves to divide new\\nnewswire articles into the two classes: documents about multicore computer chips\\nand documents not about multicore computer chips. We refer to this as two-class\\nclassiﬁcation. Classiﬁcation using standing queries is also called routing or\\nROUTING\\nﬁlteringand will be discussed further in Section 15.3.1 (page 335).\\nFILTERING\\nA class need not be as narrowly focused as the standing query multicore\\ncomputer chips. Often, a class is a more general subject area like China or coffee.\\nSuch more general classes are usually referred to as topics, and the classiﬁca-\\ntion task is then called text classiﬁcation, text categorization, topic classiﬁcation,\\nTEXT CLASSIFICATION\\nor topic spotting. An example for China appears in Figure 13.1. Standing\\nqueries and topics differ in their degree of speciﬁcity, but the methods for\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n254\\n13\\nText classiﬁcation and Naive Bayes\\nsolving routing, ﬁltering, and text classiﬁcation are essentially the same. We\\ntherefore include routing and ﬁltering under the rubric of text classiﬁcation\\nin this and the following chapters.\\nThe notion of classiﬁcation is very general and has many applications within\\nand beyond information retrieval (IR). For instance, in computer vision, a\\nclassiﬁer may be used to divide images into classes such as landscape, por-\\ntrait, and neither. We focus here on examples from information retrieval such\\nas:\\n• Several of the preprocessing steps necessary for indexing as discussed in\\nChapter 2: detecting a document’s encoding (ASCII, Unicode UTF-8 etc;\\npage 20); word segmentation (Is the white space between two letters a\\nword boundary or not? page 24 ) ; truecasing (page 30); and identifying\\nthe language of a document (page 46).\\n• The automatic detection of spam pages (which then are not included in\\nthe search engine index).\\n• The automatic detection of sexually explicit content (which is included in\\nsearch results only if the user turns an option such as SafeSearch off).\\n• Sentiment detection or the automatic classiﬁcation of a movie or product\\nSENTIMENT DETECTION\\nreview as positive or negative. An example application is a user search-\\ning for negative reviews before buying a camera to make sure it has no\\nundesirable features or quality problems.\\n• Personal email sorting. A user may have folders like talk announcements,\\nEMAIL SORTING\\nelectronic bills, email from family and friends, and so on, and may want a\\nclassiﬁer to classify each incoming email and automatically move it to the\\nappropriate folder. It is easier to ﬁnd messages in sorted folders than in\\na very large inbox. The most common case of this application is a spam\\nfolder that holds all suspected spam messages.\\n• Topic-speciﬁc or vertical search. Vertical search engines restrict searches to\\nVERTICAL SEARCH\\nENGINE\\na particular topic. For example, the query computer science on a vertical\\nsearch engine for the topic China will return a list of Chinese computer\\nscience departments with higher precision and recall than the query com-\\nputer science China on a general purpose search engine. This is because the\\nvertical search engine does not include web pages in its index that contain\\nthe term china in a different sense (e.g., referring to a hard white ceramic),\\nbut does include relevant pages even if they do not explicitly mention the\\nterm China.\\n• Finally, the ranking function in ad hoc information retrieval can also be\\nbased on a document classiﬁer as we will explain in Section 15.4 (page 341).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n255\\nThis list shows the general importance of classiﬁcation in IR. Most retrieval\\nsystems today contain multiple components that use some form of classiﬁer.\\nThe classiﬁcation task we will use as an example in this book is text classiﬁ-\\ncation.\\nA computer is not essential for classiﬁcation. Many classiﬁcation tasks\\nhave traditionally been solved manually. Books in a library are assigned\\nLibrary of Congress categories by a librarian. But manual classiﬁcation is\\nexpensive to scale. The multicore computer chips example illustrates one al-\\nternative approach: classiﬁcation by the use of standing queries – which can\\nbe thought of as rules – most commonly written by hand. As in our exam-\\nRULES IN TEXT\\nCLASSIFICATION\\nple (multicore OR multi-core) AND (chip OR processor OR microprocessor), rules are\\nsometimes equivalent to Boolean expressions.\\nA rule captures a certain combination of keywords that indicates a class.\\nHand-coded rules have good scaling properties, but creating and maintain-\\ning them over time is labor intensive. A technically skilled person (e.g., a\\ndomain expert who is good at writing regular expressions) can create rule\\nsets that will rival or exceed the accuracy of the automatically generated clas-\\nsiﬁers we will discuss shortly; however, it can be hard to ﬁnd someone with\\nthis specialized skill.\\nApart from manual classiﬁcation and hand-crafted rules, there is a third\\napproach to text classiﬁcation, namely, machine learning-based text classiﬁ-\\ncation. It is the approach that we focus on in the next several chapters. In\\nmachine learning, the set of rules or, more generally, the decision criterion of\\nthe text classiﬁer, is learned automatically from training data. This approach\\nis also called statistical text classiﬁcation if the learning method is statistical.\\nSTATISTICAL TEXT\\nCLASSIFICATION\\nIn statistical text classiﬁcation, we require a number of good example docu-\\nments (or training documents) for each class. The need for manual classiﬁ-\\ncation is not eliminated because the training documents come from a person\\nwho has labeled them – where labeling refers to the process of annotating\\nLABELING\\neach document with its class. But labeling is arguably an easier task than\\nwriting rules. Almost anybody can look at a document and decide whether\\nor not it is related to China. Sometimes such labeling is already implicitly\\npart of an existing workﬂow. For instance, you may go through the news\\narticles returned by a standing query each morning and give relevance feed-\\nback (cf. Chapter 9) by moving the relevant articles to a special folder like\\nmulticore-processors.\\nWe begin this chapter with a general introduction to the text classiﬁcation\\nproblem including a formal deﬁnition (Section 13.1); we then cover Naive\\nBayes, a particularly simple and effective classiﬁcation method (Sections 13.2–\\n13.4). All of the classiﬁcation algorithms we study represent documents in\\nhigh-dimensional spaces. To improve the efﬁciency of these algorithms, it\\nis generally desirable to reduce the dimensionality of these spaces; to this\\nend, a technique known as feature selection is commonly applied in text clas-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n256\\n13\\nText classiﬁcation and Naive Bayes\\nsiﬁcation as discussed in Section 13.5. Section 13.6 covers evaluation of text\\nclassiﬁcation. In the following chapters, Chapters 14 and 15, we look at two\\nother families of classiﬁcation methods, vector space classiﬁers and support\\nvector machines.\\n13.1\\nThe text classiﬁcation problem\\nIn text classiﬁcation, we are given a description d ∈X of a document, where\\nX is the document space; and a ﬁxed set of classes C = {c1, c2, . . . , cJ}. Classes\\nDOCUMENT SPACE\\nCLASS\\nare also called categories or labels. Typically, the document space X is some\\ntype of high-dimensional space, and the classes are human deﬁned for the\\nneeds of an application, as in the examples China and documents that talk\\nabout multicore computer chips above. We are given a training set D of labeled\\nTRAINING SET\\ndocuments ⟨d, c⟩,where ⟨d, c⟩∈X × C. For example:\\n⟨d, c⟩= ⟨Beijing joins the World Trade Organization, China⟩\\nfor the one-sentence document Beijing joins the World Trade Organization and\\nthe class (or label) China.\\nUsing a learning method or learning algorithm, we then wish to learn a clas-\\nLEARNING METHOD\\nsiﬁer or classiﬁcation function γ that maps documents to classes:\\nCLASSIFIER\\nγ : X →C\\n(13.1)\\nThis type of learning is called supervised learning because a supervisor (the\\nSUPERVISED LEARNING\\nhuman who deﬁnes the classes and labels training documents) serves as a\\nteacher directing the learning process. We denote the supervised learning\\nmethod by Γ and write Γ(D) = γ. The learning method Γ takes the training\\nset D as input and returns the learned classiﬁcation function γ.\\nMost names for learning methods Γ are also used for classiﬁers γ. We\\ntalk about the Naive Bayes (NB) learning method Γ when we say that “Naive\\nBayes is robust,” meaning that it can be applied to many different learning\\nproblems and is unlikely to produce classiﬁers that fail catastrophically. But\\nwhen we say that “Naive Bayes had an error rate of 20%,” we are describing\\nan experiment in which a particular NB classiﬁer γ (which was produced by\\nthe NB learning method) had a 20% error rate in an application.\\nFigure 13.1 shows an example of text classiﬁcation from the Reuters-RCV1\\ncollection, introduced in Section 4.2, page 69. There are six classes (UK, China,\\n..., sports), each with three training documents. We show a few mnemonic\\nwords for each document’s content. The training set provides some typical\\nexamples for each class, so that we can learn the classiﬁcation function γ.\\nOnce we have learned γ, we can apply it to the test set (or test data), for ex-\\nTEST SET\\nample, the new document ﬁrst private Chinese airline whose class is unknown.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.1\\nThe text classiﬁcation problem\\n257\\nclasses:\\ntraining\\nset:\\ntest\\nset:\\nregions\\nindustries\\nsubject areas\\nγ(d′) =China\\nﬁrst\\nprivate\\nChinese\\nairline\\nUK\\nChina\\npoultry\\ncoffee\\nelections\\nsports\\nLondon\\ncongestion\\nBig Ben\\nParliament\\nthe Queen\\nWindsor\\nBeijing\\nOlympics\\nGreat Wall\\ntourism\\ncommunist\\nMao\\nchicken\\nfeed\\nducks\\npate\\nturkey\\nbird ﬂu\\nbeans\\nroasting\\nrobusta\\narabica\\nharvest\\nKenya\\nvotes\\nrecount\\nrun-off\\nseat\\ncampaign\\nTV ads\\nbaseball\\ndiamond\\nsoccer\\nforward\\ncaptain\\nteam\\nd′\\n◮Figure 13.1\\nClasses, training set, and test set in text classiﬁcation .\\nIn Figure 13.1, the classiﬁcation function assigns the new document to class\\nγ(d) = China, which is the correct assignment.\\nThe classes in text classiﬁcation often have some interesting structure such\\nas the hierarchy in Figure 13.1. There are two instances each of region cate-\\ngories, industry categories, and subject area categories. A hierarchy can be\\nan important aid in solving a classiﬁcation problem; see Section 15.3.2 for\\nfurther discussion. Until then, we will make the assumption in the text clas-\\nsiﬁcation chapters that the classes form a set with no subset relationships\\nbetween them.\\nDeﬁnition (13.1) stipulates that a document is a member of exactly one\\nclass. This is not the most appropriate model for the hierarchy in Figure 13.1.\\nFor instance, a document about the 2008 Olympics should be a member of\\ntwo classes: the China class and the sports class. This type of classiﬁcation\\nproblem is referred to as an any-of problem and we will return to it in Sec-\\ntion 14.5 (page 306). For the time being, we only consider one-of problems\\nwhere a document is a member of exactly one class.\\nOur goal in text classiﬁcation is high accuracy on test data or new data – for\\nexample, the newswire articles that we will encounter tomorrow morning\\nin the multicore chip example. It is easy to achieve high accuracy on the\\ntraining set (e.g., we can simply memorize the labels). But high accuracy on\\nthe training set in general does not mean that the classiﬁer will work well on\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n258\\n13\\nText classiﬁcation and Naive Bayes\\nnew data in an application. When we use the training set to learn a classiﬁer\\nfor test data, we make the assumption that training data and test data are\\nsimilar or from the same distribution. We defer a precise deﬁnition of this\\nnotion to Section 14.6 (page 308).\\n13.2\\nNaive Bayes text classiﬁcation\\nThe ﬁrst supervised learning method we introduce is the multinomial Naive\\nMULTINOMIAL NAIVE\\nBAYES\\nBayes or multinomial NB model, a probabilistic learning method. The proba-\\nbility of a document d being in class c is computed as\\nP(c|d) ∝P(c) ∏\\n1≤k≤nd\\nP(tk|c)\\n(13.2)\\nwhere P(tk|c) is the conditional probability of term tk occurring in a docu-\\nment of class c.1 We interpret P(tk|c) as a measure of how much evidence\\ntk contributes that c is the correct class. P(c) is the prior probability of a\\ndocument occurring in class c. If a document’s terms do not provide clear\\nevidence for one class versus another, we choose the one that has a higher\\nprior probability. ⟨t1, t2, . . . , tnd⟩are the tokens in d that are part of the vocab-\\nulary we use for classiﬁcation and nd is the number of such tokens in d. For\\nexample, ⟨t1, t2, . . . , tnd⟩for the one-sentence document Beijing and Taipei join\\nthe WTO might be ⟨Beijing, Taipei, join, WTO⟩, with nd = 4, if we treat the terms\\nand and the as stop words.\\nIn text classiﬁcation, our goal is to ﬁnd the best class for the document. The\\nbest class in NB classiﬁcation is the most likely or maximum a posteriori (MAP)\\nMAXIMUM A\\nPOSTERIORI CLASS\\nclass cmap:\\ncmap = arg max\\nc∈C\\nˆP(c|d) = arg max\\nc∈C\\nˆP(c) ∏\\n1≤k≤nd\\nˆP(tk|c).\\n(13.3)\\nWe write ˆP for P because we do not know the true values of the parameters\\nP(c) and P(tk|c), but estimate them from the training set as we will see in a\\nmoment.\\nIn Equation (13.3), many conditional probabilities are multiplied, one for\\neach position 1 ≤k ≤nd. This can result in a ﬂoating point underﬂow.\\nIt is therefore better to perform the computation by adding logarithms of\\nprobabilities instead of multiplying probabilities. The class with the highest\\nlog probability score is still the most probable; log(xy) = log(x) + log(y)\\nand the logarithm function is monotonic. Hence, the maximization that is\\n1. We will explain in the next section why P(c|d) is proportional to (∝), not equal to the quantity\\non the right.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.2\\nNaive Bayes text classiﬁcation\\n259\\nactually done in most implementations of NB is:\\ncmap = arg max\\nc∈C\\n[log ˆP(c) + ∑\\n1≤k≤nd\\nlog ˆP(tk|c)].\\n(13.4)\\nEquation (13.4) has a simple interpretation. Each conditional parameter\\nlog ˆP(tk|c) is a weight that indicates how good an indicator tk is for c. Sim-\\nilarly, the prior log ˆP(c) is a weight that indicates the relative frequency of\\nc. More frequent classes are more likely to be the correct class than infre-\\nquent classes. The sum of log prior and term weights is then a measure of\\nhow much evidence there is for the document being in the class, and Equa-\\ntion (13.4) selects the class for which we have the most evidence.\\nWe will initially work with this intuitive interpretation of the multinomial\\nNB model and defer a formal derivation to Section 13.4.\\nHow do we estimate the parameters ˆP(c) and ˆP(tk|c)? We ﬁrst try the\\nmaximum likelihood estimate (MLE; Section 11.3.2, page 226), which is sim-\\nply the relative frequency and corresponds to the most likely value of each\\nparameter given the training data. For the priors this estimate is:\\nˆP(c) = Nc\\nN ,\\n(13.5)\\nwhere Nc is the number of documents in class c and N is the total number of\\ndocuments.\\nWe estimate the conditional probability ˆP(t|c) as the relative frequency of\\nterm t in documents belonging to class c:\\nˆP(t|c) =\\nTct\\n∑t′∈V Tct′ ,\\n(13.6)\\nwhere Tct is the number of occurrences of t in training documents from class\\nc, including multiple occurrences of a term in a document. We have made the\\npositional independence assumption here, which we will discuss in more detail\\nin the next section: Tct is a count of occurrences in all positions k in the doc-\\numents in the training set. Thus, we do not compute different estimates for\\ndifferent positions and, for example, if a word occurs twice in a document,\\nin positions k1 and k2, then ˆP(tk1|c) = ˆP(tk2|c).\\nThe problem with the MLE estimate is that it is zero for a term–class combi-\\nnation that did not occur in the training data. If the term WTO in the training\\ndata only occurred in China documents, then the MLE estimates for the other\\nclasses, for example UK, will be zero:\\nˆP(WTO|UK) = 0.\\nNow, the one-sentence document Britain is a member of the WTO will get a\\nconditional probability of zero for UK because we are multiplying the condi-\\ntional probabilities for all terms in Equation (13.2). Clearly, the model should\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n260\\n13\\nText classiﬁcation and Naive Bayes\\nTRAINMULTINOMIALNB(C, D)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nN ←COUNTDOCS(D)\\n3\\nfor each c ∈C\\n4\\ndo Nc ←COUNTDOCSINCLASS(D, c)\\n5\\nprior[c] ←Nc/N\\n6\\ntextc ←CONCATENATETEXTOFALLDOCSINCLASS(D, c)\\n7\\nfor each t ∈V\\n8\\ndo Tct ←COUNTTOKENSOFTERM(textc, t)\\n9\\nfor each t ∈V\\n10\\ndo condprob[t][c] ←\\nTct+1\\n∑t′(Tct′+1)\\n11\\nreturn V, prior, condprob\\nAPPLYMULTINOMIALNB(C, V, prior, condprob, d)\\n1\\nW ←EXTRACTTOKENSFROMDOC(V, d)\\n2\\nfor each c ∈C\\n3\\ndo score[c] ←log prior[c]\\n4\\nfor each t ∈W\\n5\\ndo score[c] += log condprob[t][c]\\n6\\nreturn arg maxc∈C score[c]\\n◮Figure 13.2\\nNaive Bayes algorithm (multinomial model): Training and testing.\\nassign a high probability to the UK class because the term Britain occurs. The\\nproblem is that the zero probability for WTO cannot be “conditioned away,”\\nno matter how strong the evidence for the class UK from other features. The\\nestimate is 0 because of sparseness: The training data are never large enough\\nSPARSENESS\\nto represent the frequency of rare events adequately, for example, the fre-\\nquency of WTO occurring in UK documents.\\nTo eliminate zeros, we use add-one or Laplace smoothing, which simply adds\\nADD-ONE SMOOTHING\\none to each count (cf. Section 11.3.2):\\nˆP(t|c) =\\nTct + 1\\n∑t′∈V(Tct′ + 1) =\\nTct + 1\\n(∑t′∈V Tct′) + B,\\n(13.7)\\nwhere B = |V| is the number of terms in the vocabulary. Add-one smoothing\\ncan be interpreted as a uniform prior (each term occurs once for each class)\\nthat is then updated as evidence from the training data comes in. Note that\\nthis is a prior probability for the occurrence of a term as opposed to the prior\\nprobability of a class which we estimate in Equation (13.5) on the document\\nlevel.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.2\\nNaive Bayes text classiﬁcation\\n261\\n◮Table 13.1\\nData for parameter estimation examples.\\ndocID\\nwords in document\\nin c = China?\\ntraining set\\n1\\nChinese Beijing Chinese\\nyes\\n2\\nChinese Chinese Shanghai\\nyes\\n3\\nChinese Macao\\nyes\\n4\\nTokyo Japan Chinese\\nno\\ntest set\\n5\\nChinese Chinese Chinese Tokyo Japan\\n?\\n◮Table 13.2\\nTraining and test times for NB.\\nmode\\ntime complexity\\ntraining\\nΘ(|D|Lave + |C||V|)\\ntesting\\nΘ(La + |C|Ma) = Θ(|C|Ma)\\nWe have now introduced all the elements we need for training and apply-\\ning an NB classiﬁer. The complete algorithm is described in Figure 13.2.\\n\\x0f\\nExample 13.1:\\nFor the example in Table 13.1, the multinomial parameters we\\nneed to classify the test document are the priors ˆP(c) = 3/4 and ˆP(c) = 1/4 and the\\nfollowing conditional probabilities:\\nˆP(Chinese|c)\\n=\\n(5 + 1)/(8 + 6) = 6/14 = 3/7\\nˆP(Tokyo|c) = ˆP(Japan|c)\\n=\\n(0 + 1)/(8 + 6) = 1/14\\nˆP(Chinese|c)\\n=\\n(1 + 1)/(3 + 6) = 2/9\\nˆP(Tokyo|c) = ˆP(Japan|c)\\n=\\n(1 + 1)/(3 + 6) = 2/9\\nThe denominators are (8 + 6) and (3 + 6) because the lengths of textc and textc are 8\\nand 3, respectively, and because the constant B in Equation (13.7) is 6 as the vocabu-\\nlary consists of six terms.\\nWe then get:\\nˆP(c|d5)\\n∝\\n3/4 · (3/7)3 · 1/14 · 1/14 ≈0.0003.\\nˆP(c|d5)\\n∝\\n1/4 · (2/9)3 · 2/9 · 2/9 ≈0.0001.\\nThus, the classiﬁer assigns the test document to c = China. The reason for this clas-\\nsiﬁcation decision is that the three occurrences of the positive indicator Chinese in d5\\noutweigh the occurrences of the two negative indicators Japan and Tokyo.\\nWhat is the time complexity of NB? The complexity of computing the pa-\\nrameters is Θ(|C||V|) because the set of parameters consists of |C||V| con-\\nditional probabilities and |C| priors. The preprocessing necessary for com-\\nputing the parameters (extracting the vocabulary, counting terms, etc.) can\\nbe done in one pass through the training data. The time complexity of this\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n262\\n13\\nText classiﬁcation and Naive Bayes\\ncomponent is therefore Θ(|D|Lave), where |D| is the number of documents\\nand Lave is the average length of a document.\\nWe use Θ(|D|Lave) as a notation for Θ(T) here, where T is the length of the\\ntraining collection. This is nonstandard; Θ(.) is not deﬁned for an average.\\nWe prefer expressing the time complexity in terms of D and Lave because\\nthese are the primary statistics used to characterize training collections.\\nThe time complexity of APPLYMULTINOMIALNB in Figure 13.2 is Θ(|C|La).\\nLa and Ma are the numbers of tokens and types, respectively, in the test doc-\\nument. APPLYMULTINOMIALNB can be modiﬁed to be Θ(La + |C|Ma) (Ex-\\nercise 13.8). Finally, assuming that the length of test documents is bounded,\\nΘ(La + |C|Ma) = Θ(|C|Ma) because La < b|C|Ma for a ﬁxed constant b.2\\nTable 13.2 summarizes the time complexities. In general, we have |C||V| <\\n|D|Lave, so both training and testing complexity are linear in the time it takes\\nto scan the data. Because we have to look at the data at least once, NB can be\\nsaid to have optimal time complexity. Its efﬁciency is one reason why NB is\\na popular text classiﬁcation method.\\n13.2.1\\nRelation to multinomial unigram language model\\nThe multinomial NB model is formally identical to the multinomial unigram\\nlanguage model (Section 12.2.1, page 242). In particular, Equation (13.2) is\\na special case of Equation (12.12) from page 243, which we repeat here for\\nλ = 1:\\nP(d|q) ∝P(d)∏\\nt∈q\\nP(t|Md).\\n(13.8)\\nThe document d in text classiﬁcation (Equation (13.2)) takes the role of the\\nquery in language modeling (Equation (13.8)) and the classes c in text clas-\\nsiﬁcation take the role of the documents d in language modeling. We used\\nEquation (13.8) to rank documents according to the probability that they are\\nrelevant to the query q. In NB classiﬁcation, we are usually only interested\\nin the top-ranked class.\\nWe also used MLE estimates in Section 12.2.2 (page 243) and encountered\\nthe problem of zero estimates owing to sparse data (page 244); but instead\\nof add-one smoothing, we used a mixture of two distributions to address the\\nproblem there. Add-one smoothing is closely related to add- 1\\n2 smoothing in\\nSection 11.3.4 (page 228).\\n?\\nExercise 13.1\\nWhy is |C||V| < |D|Lave in Table 13.2 expected to hold for most text collections?\\n2. Our assumption here is that the length of test documents is bounded. La would exceed\\nb|C|Ma for extremely long test documents.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.3\\nThe Bernoulli model\\n263\\nTRAINBERNOULLINB(C, D)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nN ←COUNTDOCS(D)\\n3\\nfor each c ∈C\\n4\\ndo Nc ←COUNTDOCSINCLASS(D, c)\\n5\\nprior[c] ←Nc/N\\n6\\nfor each t ∈V\\n7\\ndo Nct ←COUNTDOCSINCLASSCONTAININGTERM(D, c, t)\\n8\\ncondprob[t][c] ←(Nct + 1)/(Nc + 2)\\n9\\nreturn V, prior, condprob\\nAPPLYBERNOULLINB(C, V, prior, condprob, d)\\n1\\nVd ←EXTRACTTERMSFROMDOC(V, d)\\n2\\nfor each c ∈C\\n3\\ndo score[c] ←log prior[c]\\n4\\nfor each t ∈V\\n5\\ndo if t ∈Vd\\n6\\nthen score[c] += log condprob[t][c]\\n7\\nelse score[c] += log(1 −condprob[t][c])\\n8\\nreturn arg maxc∈C score[c]\\n◮Figure 13.3\\nNB algorithm (Bernoulli model): Training and testing. The add-one\\nsmoothing in Line 8 (top) is in analogy to Equation (13.7) with B = 2.\\n13.3\\nThe Bernoulli model\\nThere are two different ways we can set up an NB classiﬁer. The model we in-\\ntroduced in the previous section is the multinomial model. It generates one\\nterm from the vocabulary in each position of the document, where we as-\\nsume a generative model that will be discussed in more detail in Section 13.4\\n(see also page 237).\\nAn alternative to the multinomial model is the multivariate Bernoulli model\\nor Bernoulli model. It is equivalent to the binary independence model of Sec-\\nBERNOULLI MODEL\\ntion 11.3 (page 222), which generates an indicator for each term of the vo-\\ncabulary, either 1 indicating presence of the term in the document or 0 indi-\\ncating absence. Figure 13.3 presents training and testing algorithms for the\\nBernoulli model. The Bernoulli model has the same time complexity as the\\nmultinomial model.\\nThe different generation models imply different estimation strategies and\\ndifferent classiﬁcation rules. The Bernoulli model estimates ˆP(t|c) as the frac-\\ntion of documents of class c that contain term t (Figure 13.3, TRAINBERNOULLI-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n264\\n13\\nText classiﬁcation and Naive Bayes\\nNB, line 8). In contrast, the multinomial model estimates ˆP(t|c) as the frac-\\ntion of tokens or fraction of positions in documents of class c that contain term\\nt (Equation (13.7)). When classifying a test document, the Bernoulli model\\nuses binary occurrence information, ignoring the number of occurrences,\\nwhereas the multinomial model keeps track of multiple occurrences. As a\\nresult, the Bernoulli model typically makes many mistakes when classifying\\nlong documents. For example, it may assign an entire book to the class China\\nbecause of a single occurrence of the term China.\\nThe models also differ in how nonoccurring terms are used in classiﬁca-\\ntion. They do not affect the classiﬁcation decision in the multinomial model;\\nbut in the Bernoulli model the probability of nonoccurrence is factored in\\nwhen computing P(c|d) (Figure 13.3, APPLYBERNOULLINB, Line 7). This is\\nbecause only the Bernoulli NB model models absence of terms explicitly.\\n\\x0f\\nExample 13.2:\\nApplying the Bernoulli model to the example in Table 13.1, we\\nhave the same estimates for the priors as before:\\nˆP(c) = 3/4, ˆP(c) = 1/4. The\\nconditional probabilities are:\\nˆP(Chinese|c)\\n=\\n(3 + 1)/(3 + 2) = 4/5\\nˆP(Japan|c) = ˆP(Tokyo|c)\\n=\\n(0 + 1)/(3 + 2) = 1/5\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c)\\n=\\n(1 + 1)/(3 + 2) = 2/5\\nˆP(Chinese|c)\\n=\\n(1 + 1)/(1 + 2) = 2/3\\nˆP(Japan|c) = ˆP(Tokyo|c)\\n=\\n(1 + 1)/(1 + 2) = 2/3\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c)\\n=\\n(0 + 1)/(1 + 2) = 1/3\\nThe denominators are (3 + 2) and (1 + 2) because there are three documents in c\\nand one document in c and because the constant B in Equation (13.7) is 2 – there are\\ntwo cases to consider for each term, occurrence and nonoccurrence.\\nThe scores of the test document for the two classes are\\nˆP(c|d5)\\n∝\\nˆP(c) · ˆP(Chinese|c) · ˆP(Japan|c) · ˆP(Tokyo|c)\\n· (1 −ˆP(Beijing|c)) · (1 −ˆP(Shanghai|c)) · (1 −ˆP(Macao|c))\\n=\\n3/4 · 4/5 · 1/5 · 1/5 · (1−2/5) · (1−2/5) · (1−2/5)\\n≈\\n0.005\\nand, analogously,\\nˆP(c|d5)\\n∝\\n1/4 · 2/3 · 2/3 · 2/3 · (1−1/3) · (1−1/3) · (1−1/3)\\n≈\\n0.022\\nThus, the classiﬁer assigns the test document to c = not-China. When looking only\\nat binary occurrence and not at term frequency, Japan and Tokyo are indicators for c\\n(2/3 > 1/5) and the conditional probabilities of Chinese for c and c are not different\\nenough (4/5 vs. 2/3) to affect the classiﬁcation decision.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n265\\n13.4\\nProperties of Naive Bayes\\nTo gain a better understanding of the two models and the assumptions they\\nmake, let us go back and examine how we derived their classiﬁcation rules in\\nChapters 11 and 12. We decide class membership of a document by assigning\\nit to the class with the maximum a posteriori probability (cf. Section 11.3.2,\\npage 226), which we compute as follows:\\ncmap\\n=\\narg max\\nc∈C\\nP(c|d)\\n=\\narg max\\nc∈C\\nP(d|c)P(c)\\nP(d)\\n(13.9)\\n=\\narg max\\nc∈C\\nP(d|c)P(c),\\n(13.10)\\nwhere Bayes’ rule (Equation (11.4), page 220) is applied in (13.9) and we drop\\nthe denominator in the last step because P(d) is the same for all classes and\\ndoes not affect the argmax.\\nWe can interpret Equation (13.10) as a description of the generative process\\nwe assume in Bayesian text classiﬁcation. To generate a document, we ﬁrst\\nchoose class c with probability P(c) (top nodes in Figures 13.4 and 13.5). The\\ntwo models differ in the formalization of the second step, the generation of\\nthe document given the class, corresponding to the conditional distribution\\nP(d|c):\\nMultinomial\\nP(d|c)\\n=\\nP(⟨t1, . . . , tk, . . . , tnd⟩|c)\\n(13.11)\\nBernoulli\\nP(d|c)\\n=\\nP(⟨e1, . . . , ei, . . . , eM⟩|c),\\n(13.12)\\nwhere ⟨t1, . . . , tnd⟩is the sequence of terms as it occurs in d (minus terms\\nthat were excluded from the vocabulary) and ⟨e1, . . . , ei, . . . , eM⟩is a binary\\nvector of dimensionality M that indicates for each term whether it occurs in\\nd or not.\\nIt should now be clearer why we introduced the document space X in\\nEquation (13.1) when we deﬁned the classiﬁcation problem. A critical step\\nin solving a text classiﬁcation problem is to choose the document represen-\\ntation. ⟨t1, . . . , tnd⟩and ⟨e1, . . . , eM⟩are two different document representa-\\ntions. In the ﬁrst case, X is the set of all term sequences (or, more precisely,\\nsequences of term tokens). In the second case, X is {0, 1}M.\\nWe cannot use Equations (13.11) and (13.12) for text classiﬁcation directly.\\nFor the Bernoulli model, we would have to estimate 2M|C| different param-\\neters, one for each possible combination of M values ei and a class. The\\nnumber of parameters in the multinomial case has the same order of magni-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n266\\n13\\nText classiﬁcation and Naive Bayes\\nC=China\\nX1=Beijing\\nX2=and\\nX3=Taipei\\nX4=join\\nX5=WTO\\n◮Figure 13.4\\nThe multinomial NB model.\\ntude.3 This being a very large quantity, estimating these parameters reliably\\nis infeasible.\\nTo reduce the number of parameters, we make the Naive Bayes conditional\\nCONDITIONAL\\nINDEPENDENCE\\nASSUMPTION\\nindependence assumption. We assume that attribute values are independent of\\neach other given the class:\\nMultinomial\\nP(d|c)\\n=\\nP(⟨t1, . . . , tnd⟩|c) = ∏\\n1≤k≤nd\\nP(Xk = tk|c)\\n(13.13)\\nBernoulli\\nP(d|c)\\n=\\nP(⟨e1, . . . , eM⟩|c) = ∏\\n1≤i≤M\\nP(Ui = ei|c).\\n(13.14)\\nWe have introduced two random variables here to make the two different\\ngenerative models explicit. Xk is the random variable for position k in the\\nRANDOM VARIABLE X\\ndocument and takes as values terms from the vocabulary. P(Xk = t|c) is the\\nprobability that in a document of class c the term t will occur in position k. Ui\\nRANDOM VARIABLE U\\nis the random variable for vocabulary term i and takes as values 0 (absence)\\nand 1 (presence). ˆP(Ui = 1|c) is the probability that in a document of class c\\nthe term ti will occur – in any position and possibly multiple times.\\nWe illustrate the conditional independence assumption in Figures 13.4 and 13.5.\\nThe class China generates values for each of the ﬁve term attributes (multi-\\nnomial) or six binary attributes (Bernoulli) with a certain probability, inde-\\npendent of the values of the other attributes. The fact that a document in the\\nclass China contains the term Taipei does not make it more likely or less likely\\nthat it also contains Beijing.\\nIn reality, the conditional independence assumption does not hold for text\\ndata. Terms are conditionally dependent on each other. But as we will dis-\\ncuss shortly, NB models perform well despite the conditional independence\\nassumption.\\n3. In fact, if the length of documents is not bounded, the number of parameters in the multino-\\nmial case is inﬁnite.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n267\\nUAlaska=0\\nUBeijing=1\\nUIndia=0\\nUjoin=1\\nUTaipei=1\\nUWTO=1\\nC=China\\n◮Figure 13.5\\nThe Bernoulli NB model.\\nEven when assuming conditional independence, we still have too many\\nparameters for the multinomial model if we assume a different probability\\ndistribution for each position k in the document. The position of a term in a\\ndocument by itself does not carry information about the class. Although\\nthere is a difference between China sues France and France sues China, the\\noccurrence of China in position 1 versus position 3 of the document is not\\nuseful in NB classiﬁcation because we look at each term separately. The con-\\nditional independence assumption commits us to this way of processing the\\nevidence.\\nAlso, if we assumed different term distributions for each position k, we\\nwould have to estimate a different set of parameters for each k. The probabil-\\nity of bean appearing as the ﬁrst term of a coffee document could be different\\nfrom it appearing as the second term, and so on. This again causes problems\\nin estimation owing to data sparseness.\\nFor these reasons, we make a second independence assumption for the\\nmultinomial model, positional independence: The conditional probabilities for\\nPOSITIONAL\\nINDEPENDENCE\\na term are the same independent of position in the document.\\nP(Xk1 = t|c) = P(Xk2 = t|c)\\nfor all positions k1, k2, terms t and classes c. Thus, we have a single dis-\\ntribution of terms that is valid for all positions ki and we can use X as its\\nsymbol.4 Positional independence is equivalent to adopting the bag of words\\nmodel, which we introduced in the context of ad hoc retrieval in Chapter 6\\n(page 117).\\nWith conditional and positional independence assumptions, we only need\\nto estimate Θ(M|C|) parameters P(tk|c) (multinomial model) or P(ei|c) (Bernoulli\\n4. Our terminology is nonstandard. The random variable X is a categorical variable, not a multi-\\nnomial variable, and the corresponding NB model should perhaps be called a sequence model. We\\nhave chosen to present this sequence model and the multinomial model in Section 13.4.1 as the\\nsame model because they are computationally identical.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n268\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.3\\nMultinomial versus Bernoulli model.\\nmultinomial model\\nBernoulli model\\nevent model\\ngeneration of token\\ngeneration of document\\nrandom variable(s)\\nX = t iff t occurs at given pos\\nUt = 1 iff t occurs in doc\\ndocument representation\\nd = ⟨t1, . . . , tk, . . . , tnd⟩, tk ∈V\\nd = ⟨e1, . . . , ei, . . . , eM⟩,\\nei ∈{0, 1}\\nparameter estimation\\nˆP(X = t|c)\\nˆP(Ui = e|c)\\ndecision rule: maximize\\nˆP(c) ∏1≤k≤nd ˆP(X = tk|c)\\nˆP(c) ∏ti∈V ˆP(Ui = ei|c)\\nmultiple occurrences\\ntaken into account\\nignored\\nlength of docs\\ncan handle longer docs\\nworks best for short docs\\n# features\\ncan handle more\\nworks best with fewer\\nestimate for term the\\nˆP(X = the|c) ≈0.05\\nˆP(Uthe = 1|c) ≈1.0\\nmodel), one for each term–class combination, rather than a number that is\\nat least exponential in M, the size of the vocabulary.\\nThe independence\\nassumptions reduce the number of parameters to be estimated by several\\norders of magnitude.\\nTo summarize, we generate a document in the multinomial model (Fig-\\nure 13.4) by ﬁrst picking a class C = c with P(c) where C is a random variable\\nRANDOM VARIABLE C\\ntaking values from C as values. Next we generate term tk in position k with\\nP(Xk = tk|c) for each of the nd positions of the document. The Xk all have\\nthe same distribution over terms for a given c. In the example in Figure 13.4,\\nwe show the generation of ⟨t1, t2, t3, t4, t5⟩= ⟨Beijing, and, Taipei, join, WTO⟩,\\ncorresponding to the one-sentence document Beijing and Taipei join WTO.\\nFor a completely speciﬁed document generation model, we would also\\nhave to deﬁne a distribution P(nd|c) over lengths. Without it, the multino-\\nmial model is a token generation model rather than a document generation\\nmodel.\\nWe generate a document in the Bernoulli model (Figure 13.5) by ﬁrst pick-\\ning a class C = c with P(c) and then generating a binary indicator ei for each\\nterm ti of the vocabulary (1 ≤i ≤M). In the example in Figure 13.5, we\\nshow the generation of ⟨e1, e2, e3, e4, e5, e6⟩= ⟨0, 1, 0, 1, 1, 1⟩, corresponding,\\nagain, to the one-sentence document Beijing and Taipei join WTO where we\\nhave assumed that and is a stop word.\\nWe compare the two models in Table 13.3, including estimation equations\\nand decision rules.\\nNaive Bayes is so called because the independence assumptions we have\\njust made are indeed very naive for a model of natural language. The condi-\\ntional independence assumption states that features are independent of each\\nother given the class. This is hardly ever true for terms in documents. In\\nmany cases, the opposite is true. The pairs hong and kong or london and en-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n269\\n◮Table 13.4\\nCorrect estimation implies accurate prediction, but accurate predic-\\ntion does not imply correct estimation.\\nc1\\nc2\\nclass selected\\ntrue probability P(c|d)\\n0.6\\n0.4\\nc1\\nˆP(c) ∏1≤k≤nd ˆP(tk|c) (Equation (13.13))\\n0.00099\\n0.00001\\nNB estimate ˆP(c|d)\\n0.99\\n0.01\\nc1\\nglish in Figure 13.7 are examples of highly dependent terms. In addition, the\\nmultinomial model makes an assumption of positional independence. The\\nBernoulli model ignores positions in documents altogether because it only\\ncares about absence or presence. This bag-of-words model discards all in-\\nformation that is communicated by the order of words in natural language\\nsentences. How can NB be a good text classiﬁer when its model of natural\\nlanguage is so oversimpliﬁed?\\nThe answer is that even though the probability estimates of NB are of low\\nquality, its classiﬁcation decisions are surprisingly good. Consider a document\\nd with true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Ta-\\nble 13.4. Assume that d contains many terms that are positive indicators for\\nc1 and many terms that are negative indicators for c2. Thus, when using the\\nmultinomial model in Equation (13.13), ˆP(c1) ∏1≤k≤nd ˆP(tk|c1) will be much\\nlarger than ˆP(c2) ∏1≤k≤nd ˆP(tk|c2) (0.00099 vs. 0.00001 in the table). After di-\\nvision by 0.001 to get well-formed probabilities for P(c|d), we end up with\\none estimate that is close to 1.0 and one that is close to 0.0. This is common:\\nThe winning class in NB classiﬁcation usually has a much larger probabil-\\nity than the other classes and the estimates diverge very signiﬁcantly from\\nthe true probabilities. But the classiﬁcation decision is based on which class\\ngets the highest score. It does not matter how accurate the estimates are. De-\\nspite the bad estimates, NB estimates a higher probability for c1 and therefore\\nassigns d to the correct class in Table 13.4. Correct estimation implies accurate\\nprediction, but accurate prediction does not imply correct estimation. NB classiﬁers\\nestimate badly, but often classify well.\\nEven if it is not the method with the highest accuracy for text, NB has many\\nvirtues that make it a strong contender for text classiﬁcation. It excels if there\\nare many equally important features that jointly contribute to the classiﬁ-\\ncation decision. It is also somewhat robust to noise features (as deﬁned in\\nthe next section) and concept drift – the gradual change over time of the con-\\nCONCEPT DRIFT\\ncept underlying a class like US president from Bill Clinton to George W. Bush\\n(see Section 13.7). Classiﬁers like kNN (Section 14.3, page 297) can be care-\\nfully tuned to idiosyncratic properties of a particular time period. This will\\nthen hurt them when documents in the following time period have slightly\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n270\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.5\\nA set of documents for which the NB independence assumptions are\\nproblematic.\\n(1)\\nHe moved from London, Ontario, to London, England.\\n(2)\\nHe moved from London, England, to London, Ontario.\\n(3)\\nHe moved from England to London, Ontario.\\ndifferent properties.\\nThe Bernoulli model is particularly robust with respect to concept drift.\\nWe will see in Figure 13.8 that it can have decent performance when using\\nfewer than a dozen terms. The most important indicators for a class are less\\nlikely to change. Thus, a model that only relies on these features is more\\nlikely to maintain a certain level of accuracy in concept drift.\\nNB’s main strength is its efﬁciency: Training and classiﬁcation can be ac-\\ncomplished with one pass over the data. Because it combines efﬁciency with\\ngood accuracy it is often used as a baseline in text classiﬁcation research.\\nIt is often the method of choice if (i) squeezing out a few extra percentage\\npoints of accuracy is not worth the trouble in a text classiﬁcation application,\\n(ii) a very large amount of training data is available and there is more to be\\ngained from training on a lot of data than using a better classiﬁer on a smaller\\ntraining set, or (iii) if its robustness to concept drift can be exploited.\\nIn this book, we discuss NB as a classiﬁer for text. The independence as-\\nsumptions do not hold for text. However, it can be shown that NB is an\\noptimal classiﬁer (in the sense of minimal error rate on new data) for data\\nOPTIMAL CLASSIFIER\\nwhere the independence assumptions do hold.\\n13.4.1\\nA variant of the multinomial model\\nAn alternative formalization of the multinomial model represents each doc-\\nument d as an M-dimensional vector of counts ⟨tft1,d, . . . , tftM,d⟩where tfti,d\\nis the term frequency of ti in d. P(d|c) is then computed as follows (cf. Equa-\\ntion (12.8), page 243);\\nP(d|c) = P(⟨tft1,d, . . . , tftM,d⟩|c) ∝∏\\n1≤i≤M\\nP(X = ti|c)tfti,d\\n(13.15)\\nNote that we have omitted the multinomial factor. See Equation (12.8) (page 243).\\nEquation (13.15) is equivalent to the sequence model in Equation (13.2) as\\nP(X = ti|c)tfti,d = 1 for terms that do not occur in d (tfti,d = 0) and a term\\nthat occurs tfti,d ≥1 times will contribute tfti,d factors both in Equation (13.2)\\nand in Equation (13.15).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n271\\nSELECTFEATURES(D, c, k)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nL ←[]\\n3\\nfor each t ∈V\\n4\\ndo A(t, c) ←COMPUTEFEATUREUTILITY(D, t, c)\\n5\\nAPPEND(L, ⟨A(t, c), t⟩)\\n6\\nreturn FEATURESWITHLARGESTVALUES(L, k)\\n◮Figure 13.6\\nBasic feature selection algorithm for selecting the k best features.\\n?\\nExercise 13.2\\n[⋆]\\nWhich of the documents in Table 13.5 have identical and different bag of words rep-\\nresentations for (i) the Bernoulli model (ii) the multinomial model? If there are differ-\\nences, describe them.\\nExercise 13.3\\nThe rationale for the positional independence assumption is that there is no useful\\ninformation in the fact that a term occurs in position k of a document. Find exceptions.\\nConsider formulaic documents with a ﬁxed document structure.\\nExercise 13.4\\nTable 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the\\ndifference.\\n13.5\\nFeature selection\\nFeature selection is the process of selecting a subset of the terms occurring\\nFEATURE SELECTION\\nin the training set and using only this subset as features in text classiﬁca-\\ntion. Feature selection serves two main purposes. First, it makes training\\nand applying a classiﬁer more efﬁcient by decreasing the size of the effective\\nvocabulary. This is of particular importance for classiﬁers that, unlike NB,\\nare expensive to train. Second, feature selection often increases classiﬁca-\\ntion accuracy by eliminating noise features. A noise feature is one that, when\\nNOISE FEATURE\\nadded to the document representation, increases the classiﬁcation error on\\nnew data. Suppose a rare term, say arachnocentric, has no information about\\na class, say China, but all instances of arachnocentric happen to occur in China\\ndocuments in our training set. Then the learning method might produce a\\nclassiﬁer that misassigns test documents containing arachnocentric to China.\\nSuch an incorrect generalization from an accidental property of the training\\nset is called overﬁtting.\\nOVERFITTING\\nWe can view feature selection as a method for replacing a complex clas-\\nsiﬁer (using all features) with a simpler one (using a subset of the features).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n272\\n13\\nText classiﬁcation and Naive Bayes\\nIt may appear counterintuitive at ﬁrst that a seemingly weaker classiﬁer is\\nadvantageous in statistical text classiﬁcation, but when discussing the bias-\\nvariance tradeoff in Section 14.6 (page 308), we will see that weaker models\\nare often preferable when limited training data are available.\\nThe basic feature selection algorithm is shown in Figure 13.6. For a given\\nclass c, we compute a utility measure A(t, c) for each term of the vocabulary\\nand select the k terms that have the highest values of A(t, c). All other terms\\nare discarded and not used in classiﬁcation. We will introduce three different\\nutility measures in this section: mutual information, A(t, c) = I(Ut; Cc); the\\nχ2 test, A(t, c) = X2(t, c); and frequency, A(t, c) = N(t, c).\\nOf the two NB models, the Bernoulli model is particularly sensitive to\\nnoise features. A Bernoulli NB classiﬁer requires some form of feature se-\\nlection or else its accuracy will be low.\\nThis section mainly addresses feature selection for two-class classiﬁcation\\ntasks like China versus not-China. Section 13.5.5 brieﬂy discusses optimiza-\\ntions for systems with more than two classes.\\n13.5.1\\nMutual information\\nA common feature selection method is to compute A(t, c) as the expected\\nmutual information (MI) of term t and class c.5 MI measures how much in-\\nMUTUAL INFORMATION\\nformation the presence/absence of a term contributes to making the correct\\nclassiﬁcation decision on c. Formally:\\nI(U; C)\\n=\\n∑\\net∈{1,0} ∑\\nec∈{1,0}\\nP(U = et, C = ec) log2\\nP(U = et, C = ec)\\nP(U = et)P(C = ec),\\n(13.16)\\nwhere U is a random variable that takes values et = 1 (the document contains\\nterm t) and et = 0 (the document does not contain t), as deﬁned on page 266,\\nand C is a random variable that takes values ec = 1 (the document is in class\\nc) and ec = 0 (the document is not in class c). We write Ut and Cc if it is not\\nclear from context which term t and class c we are referring to.\\nForMLEs of the probabilities, Equation (13.16) is equivalent to Equation (13.17):\\nI(U; C)\\n=\\nN11\\nN log2\\nNN11\\nN1.N.1\\n+ N01\\nN log2\\nNN01\\nN0.N.1\\n(13.17)\\n+ N10\\nN log2\\nNN10\\nN1.N.0\\n+ N00\\nN log2\\nNN00\\nN0.N.0\\nwhere the Ns are counts of documents that have the values of et and ec that\\nare indicated by the two subscripts. For example, N10 is the number of doc-\\n5. Take care not to confuse expected mutual information with pointwise mutual information,\\nwhich is deﬁned as log N11/E11 where N11 and E11 are deﬁned as in Equation (13.18). The\\ntwo measures have different properties. See Section 13.7.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n273\\numents that contain t (et = 1) and are not in c (ec = 0). N1. = N10 + N11 is\\nthe number of documents that contain t (et = 1) and we count documents\\nindependent of class membership (ec ∈{0, 1}). N = N00 + N01 + N10 + N11\\nis the total number of documents. An example of one of the MLE estimates\\nthat transform Equation (13.16) into Equation (13.17) is P(U = 1, C = 1) =\\nN11/N.\\n\\x0f\\nExample 13.3:\\nConsider the class poultry and the term export in Reuters-RCV1.\\nThe counts of the number of documents with the four possible combinations of indi-\\ncator values are as follows:\\nec = epoultry = 1\\nec = epoultry = 0\\net = eexport = 1\\nN11 = 49\\nN10 = 27,652\\net = eexport = 0\\nN01 = 141\\nN00 = 774,106\\nAfter plugging these values into Equation (13.17) we get:\\nI(U; C)\\n=\\n49\\n801,948 log2\\n801,948 · 49\\n(49+27,652)(49+141)\\n+\\n141\\n801,948 log2\\n801,948 · 141\\n(141+774,106)(49+141)\\n+ 27,652\\n801,948 log2\\n801,948 · 27,652\\n(49+27,652)(27,652+774,106)\\n+ 774,106\\n801,948 log2\\n801,948 · 774,106\\n(141+774,106)(27,652+774,106)\\n≈\\n0.0001105\\nTo select k terms t1, . . . , tk for a given class, we use the feature selection al-\\ngorithm in Figure 13.6: We compute the utility measure as A(t, c) = I(Ut, Cc)\\nand select the k terms with the largest values.\\nMutual information measures how much information – in the information-\\ntheoretic sense – a term contains about the class. If a term’s distribution is\\nthe same in the class as it is in the collection as a whole, then I(U; C) =\\n0. MI reaches its maximum value if the term is a perfect indicator for class\\nmembership, that is, if the term is present in a document if and only if the\\ndocument is in the class.\\nFigure 13.7 shows terms with high mutual information scores for the six\\nclasses in Figure 13.1.6 The selected terms (e.g., london, uk, british for the class\\nUK) are of obvious utility for making classiﬁcation decisions for their respec-\\ntive classes. At the bottom of the list for UK we ﬁnd terms like peripherals\\nand tonight (not shown in the ﬁgure) that are clearly not helpful in deciding\\n6. Feature scores were computed on the ﬁrst 100,000 documents, except for poultry, a rare class,\\nfor which 800,000 documents were used. We have omitted numbers and other special words\\nfrom the top ten lists.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n274\\n13\\nText classiﬁcation and Naive Bayes\\nUK\\nlondon\\n0.1925\\nuk\\n0.0755\\nbritish\\n0.0596\\nstg\\n0.0555\\nbritain\\n0.0469\\nplc\\n0.0357\\nengland\\n0.0238\\npence\\n0.0212\\npounds\\n0.0149\\nenglish\\n0.0126\\nChina\\nchina\\n0.0997\\nchinese\\n0.0523\\nbeijing\\n0.0444\\nyuan\\n0.0344\\nshanghai\\n0.0292\\nhong\\n0.0198\\nkong\\n0.0195\\nxinhua\\n0.0155\\nprovince\\n0.0117\\ntaiwan\\n0.0108\\npoultry\\npoultry\\n0.0013\\nmeat\\n0.0008\\nchicken\\n0.0006\\nagriculture\\n0.0005\\navian\\n0.0004\\nbroiler\\n0.0003\\nveterinary\\n0.0003\\nbirds\\n0.0003\\ninspection\\n0.0003\\npathogenic\\n0.0003\\ncoffee\\ncoffee\\n0.0111\\nbags\\n0.0042\\ngrowers\\n0.0025\\nkg\\n0.0019\\ncolombia\\n0.0018\\nbrazil\\n0.0016\\nexport\\n0.0014\\nexporters\\n0.0013\\nexports\\n0.0013\\ncrop\\n0.0012\\nelections\\nelection\\n0.0519\\nelections\\n0.0342\\npolls\\n0.0339\\nvoters\\n0.0315\\nparty\\n0.0303\\nvote\\n0.0299\\npoll\\n0.0225\\ncandidate\\n0.0202\\ncampaign\\n0.0202\\ndemocratic\\n0.0198\\nsports\\nsoccer\\n0.0681\\ncup\\n0.0515\\nmatch\\n0.0441\\nmatches\\n0.0408\\nplayed\\n0.0388\\nleague\\n0.0386\\nbeat\\n0.0301\\ngame\\n0.0299\\ngames\\n0.0284\\nteam\\n0.0264\\n◮Figure 13.7\\nFeatures with high mutual information scores for six Reuters-RCV1\\nclasses.\\nwhether the document is in the class. As you might expect, keeping the in-\\nformative terms and eliminating the non-informative ones tends to reduce\\nnoise and improve the classiﬁer’s accuracy.\\nSuch an accuracy increase can be observed in Figure 13.8, which shows\\nF1 as a function of vocabulary size after feature selection for Reuters-RCV1.7\\nComparing F1 at 132,776 features (corresponding to selection of all features)\\nand at 10–100 features, we see that MI feature selection increases F1 by about\\n0.1 for the multinomial model and by more than 0.2 for the Bernoulli model.\\nFor the Bernoulli model, F1 peaks early, at ten features selected. At that point,\\nthe Bernoulli model is better than the multinomial model. When basing a\\nclassiﬁcation decision on only a few features, it is more robust to consider bi-\\nnary occurrence only. For the multinomial model (MI feature selection), the\\npeak occurs later, at 100 features, and its effectiveness recovers somewhat at\\n7. We trained the classiﬁers on the ﬁrst 100,000 documents and computed F1 on the next 100,000.\\nThe graphs are averages over ﬁve classes.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n275\\n#\\n# #\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n1\\n10\\n100\\n1000\\n10000\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nnumber of features selected\\nF1 measure\\no\\no o oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nx\\nx\\nx x\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n#\\no\\nx\\nb\\nmultinomial, MI\\nmultinomial, chisquare\\nmultinomial, frequency\\nbinomial, MI\\n◮Figure 13.8\\nEffect of feature set size on accuracy for multinomial and Bernoulli\\nmodels.\\nthe end when we use all features. The reason is that the multinomial takes\\nthe number of occurrences into account in parameter estimation and clas-\\nsiﬁcation and therefore better exploits a larger number of features than the\\nBernoulli model. Regardless of the differences between the two methods,\\nusing a carefully selected subset of the features results in better effectiveness\\nthan using all features.\\n13.5.2\\nχ2 Feature selection\\nAnother popular feature selection method is χ2. In statistics, the χ2 test is\\nχ2 FEATURE SELECTION\\napplied to test the independence of two events, where two events A and B are\\ndeﬁned to be independent if P(AB) = P(A)P(B) or, equivalently, P(A|B) =\\nINDEPENDENCE\\nP(A) and P(B|A) = P(B). In feature selection, the two events are occurrence\\nof the term and occurrence of the class. We then rank terms with respect to\\nthe following quantity:\\nX2(D, t, c) =\\n∑\\net∈{0,1} ∑\\nec∈{0,1}\\n(Netec −Eetec)2\\nEetec\\n(13.18)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n276\\n13\\nText classiﬁcation and Naive Bayes\\nwhere et and ec are deﬁned as in Equation (13.16). N is the observed frequency\\nin D and E the expected frequency. For example, E11 is the expected frequency\\nof t and c occurring together in a document assuming that term and class are\\nindependent.\\n\\x0f\\nExample 13.4:\\nWe ﬁrst compute E11 for the data in Example 13.3:\\nE11\\n=\\nN × P(t) × P(c) = N × N11 + N10\\nN\\n× N11 + N01\\nN\\n=\\nN × 49 + 141\\nN\\n× 49 + 27652\\nN\\n≈6.6\\nwhere N is the total number of documents as before.\\nWe compute the other Eetec in the same way:\\nepoultry = 1\\nepoultry = 0\\neexport = 1\\nN11 = 49\\nE11 ≈6.6\\nN10 = 27,652\\nE10 ≈27,694.4\\neexport = 0\\nN01 = 141\\nE01 ≈183.4\\nN00 = 774,106\\nE00 ≈774,063.6\\nPlugging these values into Equation (13.18), we get a X2 value of 284:\\nX2(D, t, c) =\\n∑\\net∈{0,1}\\n∑\\nec∈{0,1}\\n(Netec −Eetec)2\\nEetec\\n≈284\\nX2 is a measure of how much expected counts E and observed counts N\\ndeviate from each other. A high value of X2 indicates that the hypothesis of\\nindependence, which implies that expected and observed counts are similar,\\nis incorrect. In our example, X2 ≈284 > 10.83. Based on Table 13.6, we\\ncan reject the hypothesis that poultry and export are independent with only a\\n0.001 chance of being wrong.8 Equivalently, we say that the outcome X2 ≈\\n284 > 10.83 is statistically signiﬁcant at the 0.001 level. If the two events are\\nSTATISTICAL\\nSIGNIFICANCE\\ndependent, then the occurrence of the term makes the occurrence of the class\\nmore likely (or less likely), so it should be helpful as a feature. This is the\\nrationale of χ2 feature selection.\\nAn arithmetically simpler way of computing X2 is the following:\\nX2(D, t, c) =\\n(N11 + N10 + N01 + N00) × (N11N00 −N10N01)2\\n(N11 + N01) × (N11 + N10) × (N10 + N00) × (N01 + N00)\\n(13.19)\\nThis is equivalent to Equation (13.18) (Exercise 13.14).\\n8. We can make this inference because, if the two events are independent, then X2 ∼χ2, where\\nχ2 is the χ2 distribution. See, for example, Rice (2006).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n277\\n◮Table 13.6\\nCritical values of the χ2 distribution with one degree of freedom. For\\nexample, if the two events are independent, then P(X2 > 6.63) < 0.01. So for X2 >\\n6.63 the assumption of independence can be rejected with 99% conﬁdence.\\np\\nχ2 critical value\\n0.1\\n2.71\\n0.05\\n3.84\\n0.01\\n6.63\\n0.005\\n7.88\\n0.001\\n10.83\\n$\\nAssessing χ2 as a feature selection method\\nFrom a statistical point of view, χ2 feature selection is problematic. For a\\ntest with one degree of freedom, the so-called Yates correction should be\\nused (see Section 13.7), which makes it harder to reach statistical signiﬁcance.\\nAlso, whenever a statistical test is used multiple times, then the probability\\nof getting at least one error increases. If 1,000 hypotheses are rejected, each\\nwith 0.05 error probability, then 0.05 × 1000 = 50 calls of the test will be\\nwrong on average. However, in text classiﬁcation it rarely matters whether a\\nfew additional terms are added to the feature set or removed from it. Rather,\\nthe relative importance of features is important. As long as χ2 feature selec-\\ntion only ranks features with respect to their usefulness and is not used to\\nmake statements about statistical dependence or independence of variables,\\nwe need not be overly concerned that it does not adhere strictly to statistical\\ntheory.\\n13.5.3\\nFrequency-based feature selection\\nA third feature selection method is frequency-based feature selection, that is,\\nselecting the terms that are most common in the class. Frequency can be\\neither deﬁned as document frequency (the number of documents in the class\\nc that contain the term t) or as collection frequency (the number of tokens of\\nt that occur in documents in c). Document frequency is more appropriate for\\nthe Bernoulli model, collection frequency for the multinomial model.\\nFrequency-based feature selection selects some frequent terms that have\\nno speciﬁc information about the class, for example, the days of the week\\n(Monday, Tuesday, ...), which are frequent across classes in newswire text.\\nWhen many thousands of features are selected, then frequency-based fea-\\nture selection often does well. Thus, if somewhat suboptimal accuracy is\\nacceptable, then frequency-based feature selection can be a good alternative\\nto more complex methods. However, Figure 13.8 is a case where frequency-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n278\\n13\\nText classiﬁcation and Naive Bayes\\nbased feature selection performs a lot worse than MI and χ2 and should not\\nbe used.\\n13.5.4\\nFeature selection for multiple classiﬁers\\nIn an operational system with a large number of classiﬁers, it is desirable\\nto select a single set of features instead of a different one for each classiﬁer.\\nOne way of doing this is to compute the X2 statistic for an n × 2 table where\\nthe columns are occurrence and nonoccurrence of the term and each row\\ncorresponds to one of the classes. We can then select the k terms with the\\nhighest X2 statistic as before.\\nMore commonly, feature selection statistics are ﬁrst computed separately\\nfor each class on the two-class classiﬁcation task c versus c and then com-\\nbined. One combination method computes a single ﬁgure of merit for each\\nfeature, for example, by averaging the values A(t, c) for feature t, and then\\nselects the k features with highest ﬁgures of merit. Another frequently used\\ncombination method selects the top k/n features for each of n classiﬁers and\\nthen combines these n sets into one global feature set.\\nClassiﬁcation accuracy often decreases when selecting k common features\\nfor a system with n classiﬁers as opposed to n different sets of size k. But even\\nif it does, the gain in efﬁciency owing to a common document representation\\nmay be worth the loss in accuracy.\\n13.5.5\\nComparison of feature selection methods\\nMutual information and χ2 represent rather different feature selection meth-\\nods. The independence of term t and class c can sometimes be rejected with\\nhigh conﬁdence even if t carries little information about membership of a\\ndocument in c. This is particularly true for rare terms. If a term occurs once\\nin a large collection and that one occurrence is in the poultry class, then this\\nis statistically signiﬁcant. But a single occurrence is not very informative\\naccording to the information-theoretic deﬁnition of information.\\nBecause\\nits criterion is signiﬁcance, χ2 selects more rare terms (which are often less\\nreliable indicators) than mutual information. But the selection criterion of\\nmutual information also does not necessarily select the terms that maximize\\nclassiﬁcation accuracy.\\nDespite the differences between the two methods, the classiﬁcation accu-\\nracy of feature sets selected with χ2 and MI does not seem to differ systemat-\\nically. In most text classiﬁcation problems, there are a few strong indicators\\nand many weak indicators. As long as all strong indicators and a large num-\\nber of weak indicators are selected, accuracy is expected to be good. Both\\nmethods do this.\\nFigure 13.8 compares MI and χ2 feature selection for the multinomial model.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n279\\nPeak effectiveness is virtually the same for both methods. χ2 reaches this\\npeak later, at 300 features, probably because the rare, but highly signiﬁcant\\nfeatures it selects initially do not cover all documents in the class. However,\\nfeatures selected later (in the range of 100–300)are of better quality than those\\nselected by MI.\\nAll three methods – MI, χ2 and frequency based – are greedy methods.\\nGREEDY FEATURE\\nSELECTION\\nThey may select features that contribute no incremental information over\\npreviously selected features. In Figure 13.7, kong is selected as the seventh\\nterm even though it is highly correlated with previously selected hong and\\ntherefore redundant. Although such redundancy can negatively impact ac-\\ncuracy, non-greedy methods (see Section 13.7 for references) are rarely used\\nin text classiﬁcation due to their computational cost.\\n?\\nExercise 13.5\\nConsider the following frequencies for the class coffee for four terms in the ﬁrst 100,000\\ndocuments of Reuters-RCV1:\\nterm\\nN00\\nN01\\nN10\\nN11\\nbrazil\\n98,012\\n102\\n1835\\n51\\ncouncil\\n96,322\\n133\\n3525\\n20\\nproducers\\n98,524\\n119\\n1118\\n34\\nroasted\\n99,824\\n143\\n23\\n10\\nSelect two of these four terms based on (i) χ2, (ii) mutual information, (iii) frequency.\\n13.6\\nEvaluation of text classiﬁcation\\n] Historically, the classic Reuters-21578 collection was the main benchmark\\nfor text classiﬁcation evaluation. This is a collection of 21,578 newswire ar-\\nticles, originally collected and labeled by Carnegie Group, Inc. and Reuters,\\nLtd. in the course of developing the CONSTRUE text classiﬁcation system.\\nIt is much smaller than and predates the Reuters-RCV1 collection discussed\\nin Chapter 4 (page 69). The articles are assigned classes from a set of 118\\ntopic categories. A document may be assigned several classes or none, but\\nthe commonest case is single assignment (documents with at least one class\\nreceived an average of 1.24 classes). The standard approach to this any-of\\nproblem (Chapter 14, page 306) is to learn 118 two-class classiﬁers, one for\\neach class, where the two-class classiﬁer for class c is the classiﬁer for the two\\nTWO-CLASS CLASSIFIER\\nclasses c and its complement c.\\nFor each of these classiﬁers, we can measure recall, precision, and accu-\\nracy. In recent work, people almost invariably use the ModApte split, which\\nMODAPTE SPLIT\\nincludes only documents that were viewed and assessed by a human indexer,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n280\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.7\\nThe ten largest classes in the Reuters-21578 collection with number of\\ndocuments in training and test sets.\\nclass\\n# train\\n# testclass\\n# train\\n# test\\nearn\\n2877\\n1087 trade\\n369\\n119\\nacquisitions\\n1650\\n179 interest\\n347\\n131\\nmoney-fx\\n538\\n179 ship\\n197\\n89\\ngrain\\n433\\n149 wheat\\n212\\n71\\ncrude\\n389\\n189 corn\\n182\\n56\\nand comprises 9,603 training documents and 3,299 test documents. The dis-\\ntribution of documents in classes is very uneven, and some work evaluates\\nsystems on only documents in the ten largest classes. They are listed in Ta-\\nble 13.7. A typical document with topics is shown in Figure 13.9.\\nIn Section 13.1, we stated as our goal in text classiﬁcation the minimization\\nof classiﬁcation error on test data. Classiﬁcation error is 1.0 minus classiﬁca-\\ntion accuracy, the proportion of correct decisions, a measure we introduced\\nin Section 8.3 (page 155). This measure is appropriate if the percentage of\\ndocuments in the class is high, perhaps 10% to 20% and higher. But as we\\ndiscussed in Section 8.3, accuracy is not a good measure for “small” classes\\nbecause always saying no, a strategy that defeats the purpose of building a\\nclassiﬁer, will achieve high accuracy. The always-no classiﬁer is 99% accurate\\nfor a class with relative frequency 1%. For small classes, precision, recall and\\nF1 are better measures.\\nWe will use effectiveness as a generic term for measures that evaluate the\\nEFFECTIVENESS\\nquality of classiﬁcation decisions, including precision, recall, F1, and accu-\\nracy. Performance refers to the computational efﬁciency of classiﬁcation and\\nPERFORMANCE\\nEFFICIENCY\\nIR systems in this book. However, many researchers mean effectiveness, not\\nefﬁciency of text classiﬁcation when they use the term performance.\\nWhen we process a collection with several two-class classiﬁers (such as\\nReuters-21578 with its 118 classes), we often want to compute a single ag-\\ngregate measure that combines the measures for individual classiﬁers. There\\nare two methods for doing this. Macroaveraging computes a simple aver-\\nMACROAVERAGING\\nage over classes. Microaveraging pools per-document decisions across classes,\\nMICROAVERAGING\\nand then computes an effectiveness measure on the pooled contingency ta-\\nble. Table 13.8 gives an example.\\nThe differences between the two methods can be large. Macroaveraging\\ngives equal weight to each class, whereas microaveraging gives equal weight\\nto each per-document classiﬁcation decision. Because the F1 measure ignores\\ntrue negatives and its magnitude is mostly determined by the number of\\ntrue positives, large classes dominate small classes in microaveraging. In the\\nexample, microaveraged precision (0.83) is much closer to the precision of\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n281\\n<REUTERS TOPICS=’’YES’’ LEWISSPLIT=’’TRAIN’’\\nCGISPLIT=’’TRAINING-SET’’ OLDID=’’12981’’ NEWID=’’798’’>\\n<DATE> 2-MAR-1987 16:51:43.42</DATE>\\n<TOPICS><D>livestock</D><D>hog</D></TOPICS>\\n<TITLE>AMERICAN PORK CONGRESS KICKS OFF TOMORROW</TITLE>\\n<DATELINE> CHICAGO, March 2 - </DATELINE><BODY>The American Pork\\nCongress kicks off tomorrow, March 3, in Indianapolis with 160\\nof the nations pork producers from 44 member states determining\\nindustry positions on a number of issues, according to the\\nNational Pork Producers Council, NPPC.\\nDelegates to the three day Congress will be considering 26\\nresolutions concerning various issues, including the future\\ndirection of farm policy and the tax law as it applies to the\\nagriculture sector. The delegates will also debate whether to\\nendorse concepts of a national PRV (pseudorabies virus) control\\nand eradication program, the NPPC said. A large\\ntrade show, in conjunction with the congress, will feature\\nthe latest in technology in all areas of the industry, the NPPC\\nadded. Reuter\\n\\\\&\\\\#3;</BODY></TEXT></REUTERS>\\n◮Figure 13.9\\nA sample document from the Reuters-21578 collection.\\nc2 (0.9) than to the precision of c1 (0.5) because c2 is ﬁve times larger than\\nc1. Microaveraged results are therefore really a measure of effectiveness on\\nthe large classes in a test collection. To get a sense of effectiveness on small\\nclasses, you should compute macroaveraged results.\\nIn one-of classiﬁcation (Section 14.5, page 306), microaveraged F1 is the\\nsame as accuracy (Exercise 13.6).\\nTable 13.9 gives microaveraged and macroaveraged effectiveness of Naive\\nBayes for the ModApte split of Reuters-21578. To give a sense of the relative\\neffectiveness of NB, we compare it with linear SVMs (rightmost column; see\\nChapter 15), one of the most effective classiﬁers, but also one that is more\\nexpensive to train than NB. NB has a microaveraged F1 of 80%, which is\\n9% less than the SVM (89%), a 10% relative decrease (row “micro-avg-L (90\\nclasses)”). So there is a surprisingly small effectiveness penalty for its sim-\\nplicity and efﬁciency. However, on small classes, some of which only have on\\nthe order of ten positive examples in the training set, NB does much worse.\\nIts macroaveraged F1 is 13% below the SVM, a 22% relative decrease (row\\n“macro-avg (90 classes)”).\\nThe table also compares NB with the other classiﬁers we cover in this book:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n282\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.8\\nMacro- and microaveraging. “Truth” is the true class and “call” the\\ndecision of the classiﬁer. In this example, macroaveraged precision is [10/(10 + 10) +\\n90/(10 + 90)]/2 = (0.5 + 0.9)/2 = 0.7. Microaveraged precision is 100/(100 + 20) ≈\\n0.83.\\nclass 1\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n10\\n10\\ncall:\\nno\\n10\\n970\\nclass 2\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n90\\n10\\ncall:\\nno\\n10\\n890\\npooled table\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n100\\n20\\ncall:\\nno\\n20\\n1860\\n◮Table 13.9\\nText classiﬁcation effectiveness numbers on Reuters-21578 for F1 (in\\npercent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais\\net al. (1998) (b: NB, Rocchio, trees, SVM).\\n(a)\\nNB\\nRocchio\\nkNN\\nSVM\\nmicro-avg-L (90 classes)\\n80\\n85\\n86\\n89\\nmacro-avg (90 classes)\\n47\\n59\\n60\\n60\\n(b)\\nNB\\nRocchio\\nkNN\\ntrees\\nSVM\\nearn\\n96\\n93\\n97\\n98\\n98\\nacq\\n88\\n65\\n92\\n90\\n94\\nmoney-fx\\n57\\n47\\n78\\n66\\n75\\ngrain\\n79\\n68\\n82\\n85\\n95\\ncrude\\n80\\n70\\n86\\n85\\n89\\ntrade\\n64\\n65\\n77\\n73\\n76\\ninterest\\n65\\n63\\n74\\n67\\n78\\nship\\n85\\n49\\n79\\n74\\n86\\nwheat\\n70\\n69\\n77\\n93\\n92\\ncorn\\n65\\n48\\n78\\n92\\n90\\nmicro-avg (top 10)\\n82\\n65\\n82\\n88\\n92\\nmicro-avg-D (118 classes)\\n75\\n62\\nn/a\\nn/a\\n87\\nRocchio and kNN. In addition, we give numbers for decision trees, an impor-\\nDECISION TREES\\ntant classiﬁcation method we do not cover. The bottom part of the table\\nshows that there is considerable variation from class to class. For instance,\\nNB beats kNN on ship, but is much worse on money-fx.\\nComparing parts (a) and (b) of the table, one is struck by the degree to\\nwhich the cited papers’ results differ. This is partly due to the fact that the\\nnumbers in (b) are break-even scores (cf. page 161) averaged over 118 classes,\\nwhereas the numbers in (a) are true F1 scores (computed without any know-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n283\\nledge of the test set) averaged over ninety classes. This is unfortunately typ-\\nical of what happens when comparing different results in text classiﬁcation:\\nThere are often differences in the experimental setup or the evaluation that\\ncomplicate the interpretation of the results.\\nThese and other results have shown that the average effectiveness of NB\\nis uncompetitive with classiﬁers like SVMs when trained and tested on inde-\\npendent and identically distributed (i.i.d.) data, that is, uniform data with all the\\ngood properties of statistical sampling. However, these differences may of-\\nten be invisible or even reverse themselves when working in the real world\\nwhere, usually, the training sample is drawn from a subset of the data to\\nwhich the classiﬁer will be applied, the nature of the data drifts over time\\nrather than being stationary (the problem of concept drift we mentioned on\\npage 269), and there may well be errors in the data (among other problems).\\nMany practitioners have had the experience of being unable to build a fancy\\nclassiﬁer for a certain problem that consistently performs better than NB.\\nOur conclusion from the results in Table 13.9 is that, although most re-\\nsearchers believe that an SVM is better than kNN and kNN better than NB,\\nthe ranking of classiﬁers ultimately depends on the class, the document col-\\nlection, and the experimental setup. In text classiﬁcation, there is always\\nmore to know than simply which machine learning algorithm was used, as\\nwe further discuss in Section 15.3 (page 334).\\nWhen performing evaluations like the one in Table 13.9, it is important to\\nmaintain a strict separation between the training set and the test set. We can\\neasily make correct classiﬁcation decisions on the test set by using informa-\\ntion we have gleaned from the test set, such as the fact that a particular term\\nis a good predictor in the test set (even though this is not the case in the train-\\ning set). A more subtle example of using knowledge about the test set is to\\ntry a large number of values of a parameter (e.g., the number of selected fea-\\ntures) and select the value that is best for the test set. As a rule, accuracy on\\nnew data – the type of data we will encounter when we use the classiﬁer in\\nan application – will be much lower than accuracy on a test set that the clas-\\nsiﬁer has been tuned for. We discussed the same problem in ad hoc retrieval\\nin Section 8.1 (page 153).\\nIn a clean statistical text classiﬁcation experiment, you should never run\\nany program on or even look at the test set while developing a text classiﬁca-\\ntion system. Instead, set aside a development set for testing while you develop\\nDEVELOPMENT SET\\nyour method. When such a set serves the primary purpose of ﬁnding a good\\nvalue for a parameter, for example, the number of selected features, then it\\nis also called held-out data. Train the classiﬁer on the rest of the training set\\nHELD-OUT DATA\\nwith different parameter values, and then select the value that gives best re-\\nsults on the held-out part of the training set. Ideally, at the very end, when\\nall parameters have been set and the method is fully speciﬁed, you run one\\nﬁnal experiment on the test set and publish the results. Because no informa-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n284\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.10\\nData for parameter estimation exercise.\\ndocID\\nwords in document\\nin c = China?\\ntraining set\\n1\\nTaipei Taiwan\\nyes\\n2\\nMacao Taiwan Shanghai\\nyes\\n3\\nJapan Sapporo\\nno\\n4\\nSapporo Osaka Taiwan\\nno\\ntest set\\n5\\nTaiwan Taiwan Sapporo\\n?\\ntion about the test set was used in developing the classiﬁer, the results of this\\nexperiment should be indicative of actual performance in practice.\\nThis ideal often cannot be met; researchers tend to evaluate several sys-\\ntems on the same test set over a period of several years. But it is neverthe-\\nless highly important to not look at the test data and to run systems on it as\\nsparingly as possible. Beginners often violate this rule, and their results lose\\nvalidity because they have implicitly tuned their system to the test data sim-\\nply by running many variant systems and keeping the tweaks to the system\\nthat worked best on the test set.\\n?\\nExercise 13.6\\n[⋆⋆]\\nAssume a situation where every document in the test collection has been assigned\\nexactly one class, and that a classiﬁer also assigns exactly one class to each document.\\nThis setup is called one-of classiﬁcation (Section 14.5, page 306). Show that in one-of\\nclassiﬁcation (i) the total number of false positive decisions equals the total number\\nof false negative decisions and (ii) microaveraged F1 and accuracy are identical.\\nExercise 13.7\\nThe class priors in Figure 13.2 are computed as the fraction of documents in the class\\nas opposed to the fraction of tokens in the class. Why?\\nExercise 13.8\\nThe function APPLYMULTINOMIALNB in Figure 13.2 has time complexity Θ(La +\\n|C|La). How would you modify the function so that its time complexity is Θ(La +\\n|C|Ma)?\\nExercise 13.9\\nBased on the data in Table 13.10, (i) estimate a multinomial Naive Bayes classiﬁer, (ii)\\napply the classiﬁer to the test document, (iii) estimate a Bernoulli NB classiﬁer, (iv)\\napply the classiﬁer to the test document. You need not estimate parameters that you\\ndon’t need for classifying the test document.\\nExercise 13.10\\nYour task is to classify words as English or not English. Words are generated by a\\nsource with the following distribution:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n285\\nevent\\nword\\nEnglish?\\nprobability\\n1\\nozb\\nno\\n4/9\\n2\\nuzu\\nno\\n4/9\\n3\\nzoo\\nyes\\n1/18\\n4\\nbun\\nyes\\n1/18\\n(i) Compute the parameters (priors and conditionals) of a multinomial NB classi-\\nﬁer that uses the letters b, n, o, u, and z as features. Assume a training set that\\nreﬂects the probability distribution of the source perfectly. Make the same indepen-\\ndence assumptions that are usually made for a multinomial classiﬁer that uses terms\\nas features for text classiﬁcation. Compute parameters using smoothing, in which\\ncomputed-zero probabilities are smoothed into probability 0.01, and computed-nonzero\\nprobabilities are untouched. (This simplistic smoothing may cause P(A) + P(A) > 1.\\nSolutions are not required to correct this.) (ii) How does the classiﬁer classify the\\nword zoo? (iii) Classify the word zoo using a multinomial classiﬁer as in part (i), but\\ndo not make the assumption of positional independence. That is, estimate separate\\nparameters for each position in a word. You only need to compute the parameters\\nyou need for classifying zoo.\\nExercise 13.11\\nWhat are the values of I(Ut; Cc) and X2(D, t, c) if term and class are completely inde-\\npendent? What are the values if they are completely dependent?\\nExercise 13.12\\nThe feature selection method in Equation (13.16) is most appropriate for the Bernoulli\\nmodel. Why? How could one modify it for the multinomial model?\\nExercise 13.13\\nFeatures can also be selected according toinformation gain (IG), which is deﬁned as:\\nINFORMATION GAIN\\nIG(D, t, c) = H(pD) −\\n∑\\nx∈{Dt+,Dt−}\\n|x|\\n|D| H(px)\\nwhere H is entropy, D is the training set, and Dt+, and Dt−are the subset of D with\\nterm t, and the subset of D without term t, respectively. pA is the class distribution\\nin (sub)collection A, e.g., pA(c) = 0.25, pA(c) = 0.75 if a quarter of the documents in\\nA are in class c.\\nShow that mutual information and information gain are equivalent.\\nExercise 13.14\\nShow that the two X2 formulas (Equations (13.18) and (13.19)) are equivalent.\\nExercise 13.15\\nIn the χ2 example on page 276 we have |N11 −E11| = |N10 −E10| = |N01 −E01| =\\n|N00 −E00|. Show that this holds in general.\\nExercise 13.16\\nχ2 and mutual information do not distinguish between positively and negatively cor-\\nrelated features. Because most good text classiﬁcation features are positively corre-\\nlated (i.e., they occur more often in c than in c), one may want to explicitly rule out\\nthe selection of negative indicators. How would you do this?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n286\\n13\\nText classiﬁcation and Naive Bayes\\n13.7\\nReferences and further reading\\nGeneral introductions to statistical classiﬁcation and machine learning can be\\nfound in (Hastie et al. 2001), (Mitchell 1997), and (Duda et al. 2000), including\\nmany important methods (e.g., decision trees and boosting) that we do not\\ncover. A comprehensive review of text classiﬁcation methods and results is\\n(Sebastiani 2002). Manning and Schütze (1999, Chapter 16) give an accessible\\nintroduction to text classiﬁcation with coverage of decision trees, perceptrons\\nand maximum entropy models. More information on the superlinear time\\ncomplexity of learning methods that are more accurate than Naive Bayes can\\nbe found in (Perkins et al. 2003) and (Joachims 2006a).\\nMaron and Kuhns (1960) described one of the ﬁrst NB text classiﬁers. Lewis\\n(1998) focuses on the history of NB classiﬁcation. Bernoulli and multinomial\\nmodels and their accuracy for different collections are discussed by McCal-\\nlum and Nigam (1998). Eyheramendy et al. (2003) present additional NB\\nmodels. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu\\n(2001) analyze why NB performs well although its probability estimates are\\npoor. The ﬁrst paper also discusses NB’s optimality when the independence\\nassumptions are true of the data. Pavlov et al. (2004) propose a modiﬁed\\ndocument representation that partially addresses the inappropriateness of\\nthe independence assumptions. Bennett (2000) attributes the tendency of NB\\nprobability estimates to be close to either 0 or 1 to the effect of document\\nlength. Ng and Jordan (2001) show that NB is sometimes (although rarely)\\nsuperior to discriminative methods because it more quickly reaches its opti-\\nmal error rate. The basic NB model presented in this chapter can be tuned for\\nbetter effectiveness (Rennie et al. 2003;Kołcz and Yih 2007). The problem of\\nconcept drift and other reasons why state-of-the-art classiﬁers do not always\\nexcel in practice are discussed by Forman (2006) and Hand (2006).\\nEarly uses of mutual information and χ2 for feature selection in text clas-\\nsiﬁcation are Lewis and Ringuette (1994) and Schütze et al. (1995), respec-\\ntively. Yang and Pedersen (1997) review feature selection methods and their\\nimpact on classiﬁcation effectiveness. They ﬁnd that pointwise mutual infor-\\nPOINTWISE MUTUAL\\nINFORMATION\\nmation is not competitive with other methods. Yang and Pedersen refer to\\nexpected mutual information (Equation (13.16)) as information gain (see Ex-\\nercise 13.13, page 285). (Snedecor and Cochran 1989) is a good reference for\\nthe χ2 test in statistics, including the Yates’ correction for continuity for 2 × 2\\ntables. Dunning (1993) discusses problems of the χ2 test when counts are\\nsmall. Nongreedy feature selection techniques are described by Hastie et al.\\n(2001). Cohen (1995) discusses the pitfalls of using multiple signiﬁcance tests\\nand methods to avoid them. Forman (2004) evaluates different methods for\\nfeature selection for multiple classiﬁers.\\nDavid D. Lewis deﬁnes the ModApte split at www.daviddlewis.com/resources/testcollections/reuters215\\nbased on Apté et al. (1994). Lewis (1995) describes utility measures for the\\nUTILITY MEASURE\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n13.7\\nReferences and further reading\\n287\\nevaluation of text classiﬁcation systems. Yang and Liu (1999) employ signif-\\nicance tests in the evaluation of text classiﬁcation methods.\\nLewis et al. (2004) ﬁnd that SVMs (Chapter 15) perform better on Reuters-\\nRCV1 than kNN and Rocchio (Chapter 14).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n289\\n14\\nVector space classiﬁcation\\nThe document representation in Naive Bayes is a sequence of terms or a bi-\\nnary vector ⟨e1, . . . , e|V|⟩∈{0, 1}|V|. In this chapter we adopt a different\\nrepresentation for text classiﬁcation, the vector space model, developed in\\nChapter 6. It represents each document as a vector with one real-valued com-\\nponent, usually a tf-idf weight, for each term. Thus, the document space X,\\nthe domain of the classiﬁcation function γ, is R|V|. This chapter introduces a\\nnumber of classiﬁcation methods that operate on real-valued vectors.\\nThe basic hypothesis in using the vector space model for classiﬁcation is\\nthe contiguity hypothesis.\\nCONTIGUITY\\nHYPOTHESIS\\nContiguity hypothesis. Documents in the same class form a contigu-\\nous region and regions of different classes do not overlap.\\nThere are many classiﬁcation tasks, in particular the type of text classiﬁcation\\nthat we encountered in Chapter 13, where classes can be distinguished by\\nword patterns. For example, documents in the class China tend to have high\\nvalues on dimensions like Chinese, Beijing, and Mao whereas documents in the\\nclass UK tend to have high values for London, British and Queen. Documents\\nof the two classes therefore form distinct contiguous regions as shown in\\nFigure 14.1 and we can draw boundaries that separate them and classify new\\ndocuments. How exactly this is done is the topic of this chapter.\\nWhether or not a set of documents is mapped into a contiguous region de-\\npends on the particular choices we make for the document representation:\\ntype of weighting, stop list etc. To see that the document representation is\\ncrucial, consider the two classes written by a group vs. written by a single per-\\nson. Frequent occurrence of the ﬁrst person pronoun I is evidence for the\\nsingle-person class. But that information is likely deleted from the document\\nrepresentation if we use a stop list. If the document representation chosen\\nis unfavorable, the contiguity hypothesis will not hold and successful vector\\nspace classiﬁcation is not possible.\\nThe same considerations that led us to prefer weighted representations, in\\nparticular length-normalized tf-idf representations, in Chapters 6 and 7 also\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n290\\n14\\nVector space classiﬁcation\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\nChina\\nKenya\\nUK\\n⋆\\n◮Figure 14.1\\nVector space classiﬁcation into three classes.\\napply here. For example, a term with 5 occurrences in a document should get\\na higher weight than a term with one occurrence, but a weight 5 times larger\\nwould give too much emphasis to the term. Unweighted and unnormalized\\ncounts should not be used in vector space classiﬁcation.\\nWe introduce two vector space classiﬁcation methods in this chapter, Roc-\\nchio and kNN. Rocchio classiﬁcation (Section 14.2) divides the vector space\\ninto regions centered on centroids or prototypes, one for each class, computed\\nPROTOTYPE\\nas the center of mass of all documents in the class. Rocchio classiﬁcation is\\nsimple and efﬁcient, but inaccurate if classes are not approximately spheres\\nwith similar radii.\\nkNN or k nearest neighbor classiﬁcation (Section 14.3) assigns the majority\\nclass of the k nearest neighbors to a test document. kNN requires no explicit\\ntraining and can use the unprocessed training set directly in classiﬁcation.\\nIt is less efﬁcient than other classiﬁcation methods in classifying documents.\\nIf the training set is large, then kNN can handle non-spherical and other\\ncomplex classes better than Rocchio.\\nA large number of text classiﬁers can be viewed as linear classiﬁers – clas-\\nsiﬁers that classify based on a simple linear combination of the features (Sec-\\ntion 14.4). Such classiﬁers partition the space of features into regions sepa-\\nrated by linear decision hyperplanes, in a manner to be detailed below. Because\\nof the bias-variance tradeoff (Section 14.6) more complex nonlinear models\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.1\\nDocument representations and measures of relatedness in vector spaces\\n291\\ndtrue\\ndprojected\\nx1\\nx2 x3 x4\\nx5\\nx′\\n1\\nx′\\n2 x′\\n3 x′\\n4\\nx′\\n5\\nx′\\n1\\nx′\\n2\\nx′\\n3\\nx′\\n4\\nx′\\n5\\n◮Figure 14.2\\nProjections of small areas of the unit sphere preserve distances. Left:\\nA projection of the 2D semicircle to 1D. For the points x1, x2, x3, x4, x5 at x coordinates\\n−0.9, −0.2, 0, 0.2, 0.9 the distance |x2x3| ≈0.201 only differs by 0.5% from |x′\\n2x′\\n3| =\\n0.2; but |x1x3|/|x′\\n1x′\\n3| = dtrue/dprojected ≈1.06/0.9 ≈1.18 is an example of a large\\ndistortion (18%) when projecting a large area. Right: The corresponding projection of\\nthe 3D hemisphere to 2D.\\nare not systematically better than linear models. Nonlinear models have\\nmore parameters to ﬁt on a limited amount of training data and are more\\nlikely to make mistakes for small and noisy data sets.\\nWhen applying two-class classiﬁers to problems with more than two classes,\\nthere are one-of tasks – a document must be assigned to exactly one of several\\nmutually exclusive classes – and any-of tasks – a document can be assigned to\\nany number of classes as we will explain in Section 14.5. Two-class classiﬁers\\nsolve any-of problems and can be combined to solve one-of problems.\\n14.1\\nDocument representations and measures of relatedness in vec-\\ntor spaces\\nAs in Chapter 6, we represent documents as vectors in R|V| in this chapter.\\nTo illustrate properties of document vectors in vector classiﬁcation, we will\\nrender these vectors as points in a plane as in the example in Figure 14.1.\\nIn reality, document vectors are length-normalized unit vectors that point\\nto the surface of a hypersphere. We can view the 2D planes in our ﬁgures\\nas projections onto a plane of the surface of a (hyper-)sphere as shown in\\nFigure 14.2. Distances on the surface of the sphere and on the projection\\nplane are approximately the same as long as we restrict ourselves to small\\nareas of the surface and choose an appropriate projection (Exercise 14.1).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n292\\n14\\nVector space classiﬁcation\\nDecisions of many vector space classiﬁers are based on a notion of dis-\\ntance, e.g., when computing the nearest neighbors in kNN classiﬁcation.\\nWe will use Euclidean distance in this chapter as the underlying distance\\nmeasure. We observed earlier (Exercise 6.18, page 131) that there is a direct\\ncorrespondence between cosine similarity and Euclidean distance for length-\\nnormalized vectors. In vector space classiﬁcation, it rarely matters whether\\nthe relatedness of two documents is expressed in terms of similarity or dis-\\ntance.\\nHowever, in addition to documents, centroids or averages of vectors also\\nplay an important role in vector space classiﬁcation. Centroids are not length-\\nnormalized. For unnormalized vectors, dot product, cosine similarity and\\nEuclidean distance all have different behavior in general (Exercise 14.6). We\\nwill be mostly concerned with small local regions when computing the sim-\\nilarity between a document and a centroid, and the smaller the region the\\nmore similar the behavior of the three measures is.\\n?\\nExercise 14.1\\nFor small areas, distances on the surface of the hypersphere are approximated well\\nby distances on its projection (Figure 14.2) because α ≈sin α for small angles. For\\nwhat size angle is the distortion α/ sin(α) (i) 1.01, (ii) 1.05 and (iii) 1.1?\\n14.2\\nRocchio classiﬁcation\\nFigure 14.1 shows three classes, China, UK and Kenya, in a two-dimensional\\n(2D) space. Documents are shown as circles, diamonds and X’s. The bound-\\naries in the ﬁgure, which we call decision boundaries, are chosen to separate\\nDECISION BOUNDARY\\nthe three classes, but are otherwise arbitrary. To classify a new document,\\ndepicted as a star in the ﬁgure, we determine the region it occurs in and as-\\nsign it the class of that region – China in this case. Our task in vector space\\nclassiﬁcation is to devise algorithms that compute good boundaries where\\n“good” means high classiﬁcation accuracy on data unseen during training.\\nPerhaps the best-known way of computing good class boundaries is Roc-\\nROCCHIO\\nCLASSIFICATION\\nchio classiﬁcation, which uses centroids to deﬁne the boundaries. The centroid\\nCENTROID\\nof a class c is computed as the vector average or center of mass of its mem-\\nbers:\\n⃗µ(c) =\\n1\\n|Dc| ∑\\nd∈Dc\\n⃗v(d)\\n(14.1)\\nwhere Dc is the set of documents in D whose class is c: Dc = {d : ⟨d, c⟩∈D}.\\nWe denote the normalized vector of d by ⃗v(d) (Equation (6.11), page 122).\\nThree example centroids are shown as solid circles in Figure 14.3.\\nThe boundary between two classes in Rocchio classiﬁcation is the set of\\npoints with equal distance from the two centroids. For example, |a1| = |a2|,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.2\\nRocchio classiﬁcation\\n293\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\nChina\\nKenya\\nUK\\n⋆\\na1\\na2\\nb1\\nb2\\nc1\\nc2\\n◮Figure 14.3\\nRocchio classiﬁcation.\\n|b1| = |b2|, and |c1| = |c2| in the ﬁgure. This set of points is always a line.\\nThe generalization of a line in M-dimensional space is a hyperplane, which\\nwe deﬁne as the set of points ⃗x that satisfy:\\n⃗wT⃗x = b\\n(14.2)\\nwhere ⃗w is the M-dimensional normal vector1 of the hyperplane and b is a\\nNORMAL VECTOR\\nconstant. This deﬁnition of hyperplanes includes lines (any line in 2D can\\nbe deﬁned by w1x1 + w2x2 = b) and 2-dimensional planes (any plane in 3D\\ncan be deﬁned by w1x1 + w2x2 + w3x3 = b). A line divides a plane in two,\\na plane divides 3-dimensional space in two, and hyperplanes divide higher-\\ndimensional spaces in two.\\nThus, the boundaries of class regions in Rocchio classiﬁcation are hyper-\\nplanes. The classiﬁcation rule in Rocchio is to classify a point in accordance\\nwith the region it falls into. Equivalently, we determine the centroid⃗µ(c) that\\nthe point is closest to and then assign it to c. As an example, consider the star\\nin Figure 14.3. It is located in the China region of the space and Rocchio\\ntherefore assigns it to China. We show the Rocchio algorithm in pseudocode\\nin Figure 14.4.\\n1. Recall from basic linear algebra that ⃗v · ⃗w = ⃗vT⃗w, i.e., the dot product of ⃗v and ⃗w equals the\\nproduct by matrix multiplication of the transpose of ⃗v and ⃗w.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n294\\n14\\nVector space classiﬁcation\\nterm weights\\nvector\\nChinese\\nJapan\\nTokyo\\nMacao\\nBeijing\\nShanghai\\n⃗d1\\n0\\n0\\n0\\n0\\n1.0\\n0\\n⃗d2\\n0\\n0\\n0\\n0\\n0\\n1.0\\n⃗d3\\n0\\n0\\n0\\n1.0\\n0\\n0\\n⃗d4\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n⃗d5\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n⃗µc\\n0\\n0\\n0\\n0.33\\n0.33\\n0.33\\n⃗µc\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n◮Table 14.1\\nVectors and class centroids for the data in Table 13.1.\\n\\x0f\\nExample 14.1:\\nTable 14.1 shows the tf-idf vector representations of the ﬁve docu-\\nments in Table 13.1 (page 261), using the formula (1+ log10 tft,d) log10(4/dft) if tft,d >\\n0 (Equation (6.14), page 127). The two class centroids are µc = 1/3 · (⃗d1 + ⃗d2 + ⃗d3)\\nand µc = 1/1 · (⃗d4).\\nThe distances of the test document from the centroids are\\n|µc −⃗d5| ≈1.15 and |µc −⃗d5| = 0.0. Thus, Rocchio assigns d5 to c.\\nThe separating hyperplane in this case has the following parameters:\\n⃗w\\n≈\\n(0 −0.71 −0.71 1/3 1/3 1/3)T\\nb\\n=\\n−1/3\\nSee Exercise 14.15 for how to compute ⃗w and b. We can easily verify that this hy-\\nperplane separates the documents as desired: ⃗wT⃗d1 ≈0 · 0 + −0.71 · 0 + −0.71 · 0 +\\n1/3 · 0 + 1/3 · 1.0 + 1/3 · 0 = 1/3 > b (and, similarly, ⃗wT⃗di > b for i = 2 and i = 3)\\nand ⃗wT⃗d4 = −1 < b. Thus, documents in c are above the hyperplane (⃗wT⃗d > b) and\\ndocuments in c are below the hyperplane (⃗wT⃗d < b).\\nThe assignment criterion in Figure 14.4 is Euclidean distance (APPLYROC-\\nCHIO, line 1). An alternative is cosine similarity:\\nAssign d to class c = arg max\\nc′\\ncos(⃗µ(c′),⃗v(d))\\nAs discussed in Section 14.1, the two assignment criteria will sometimes\\nmake different classiﬁcation decisions. We present the Euclidean distance\\nvariant of Rocchio classiﬁcation here because it emphasizes Rocchio’s close\\ncorrespondence to K-means clustering (Section 16.4, page 360).\\nRocchio classiﬁcation is a form of Rocchio relevance feedback (Section 9.1.1,\\npage 178). The average of the relevant documents, corresponding to the most\\nimportant component of the Rocchio vector in relevance feedback (Equa-\\ntion (9.3), page 182), is the centroid of the “class” of relevant documents.\\nWe omit the query component of the Rocchio formula in Rocchio classiﬁca-\\ntion since there is no query in text classiﬁcation. Rocchio classiﬁcation can be\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.2\\nRocchio classiﬁcation\\n295\\nTRAINROCCHIO(C, D)\\n1\\nfor each cj ∈C\\n2\\ndo Dj ←{d : ⟨d, cj⟩∈D}\\n3\\n⃗µj ←\\n1\\n|Dj| ∑d∈Dj ⃗v(d)\\n4\\nreturn {⃗µ1, . . . ,⃗µJ}\\nAPPLYROCCHIO({⃗µ1, . . . ,⃗µJ}, d)\\n1\\nreturn arg minj |⃗µj −⃗v(d)|\\n◮Figure 14.4\\nRocchio classiﬁcation: Training and testing.\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nX\\nX\\nA\\nB\\no\\n◮Figure 14.5\\nThe multimodal class “a” consists of two different clusters (small\\nupper circles centered on X’s).\\nRocchio classiﬁcation will misclassify “o” as “a”\\nbecause it is closer to the centroid A of the “a” class than to the centroid B of the “b”\\nclass.\\napplied to J > 2 classes whereas Rocchio relevance feedback is designed to\\ndistinguish only two classes, relevant and nonrelevant.\\nIn addition to respecting contiguity, the classes in Rocchio classiﬁcation\\nmust be approximate spheres with similar radii. In Figure 14.3, the solid\\nsquare just below the boundary between UK and Kenya is a better ﬁt for the\\nclass UK since UK is more scattered than Kenya. But Rocchio assigns it to\\nKenya because it ignores details of the distribution of points in a class and\\nonly uses distance from the centroid for classiﬁcation.\\nThe assumption of sphericity also does not hold in Figure 14.5. We can-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n296\\n14\\nVector space classiﬁcation\\nmode\\ntime complexity\\ntraining\\nΘ(|D|Lave + |C||V|)\\ntesting\\nΘ(La + |C|Ma) = Θ(|C|Ma)\\n◮Table 14.2\\nTraining and test times for Rocchio classiﬁcation.\\nLave is the average\\nnumber of tokens per document. La and Ma are the numbers of tokens and types,\\nrespectively, in the test document. Computing Euclidean distance between the class\\ncentroids and a document is Θ(|C|Ma).\\nnot represent the “a” class well with a single prototype because it has two\\nclusters. Rocchio often misclassiﬁes this type of multimodal class. A text clas-\\nMULTIMODAL CLASS\\nsiﬁcation example for multimodality is a country like Burma, which changed\\nits name to Myanmar in 1989. The two clusters before and after the name\\nchange need not be close to each other in space. We also encountered the\\nproblem of multimodality in relevance feedback (Section 9.1.2, page 184).\\nTwo-class classiﬁcation is another case where classes are rarely distributed\\nlike spheres with similar radii. Most two-class classiﬁers distinguish between\\na class like China that occupies a small region of the space and its widely\\nscattered complement. Assuming equal radii will result in a large number\\nof false positives. Most two-class classiﬁcation problems therefore require a\\nmodiﬁed decision rule of the form:\\nAssign d to class c iff |⃗µ(c) −⃗v(d)| < |⃗µ(c) −⃗v(d)| −b\\nfor a positive constant b. As in Rocchio relevance feedback, the centroid of\\nthe negative documents is often not used at all, so that the decision criterion\\nsimpliﬁes to |⃗µ(c) −⃗v(d)| < b′ for a positive constant b′.\\nTable 14.2 gives the time complexity of Rocchio classiﬁcation.2 Adding all\\ndocuments to their respective (unnormalized) centroid is Θ(|D|Lave) (as op-\\nposed to Θ(|D||V|)) since we need only consider non-zero entries. Dividing\\neach vector sum by the size of its class to compute the centroid is Θ(|V|).\\nOverall, training time is linear in the size of the collection (cf. Exercise 13.1).\\nThus, Rocchio classiﬁcation and Naive Bayes have the same linear training\\ntime complexity.\\nIn the next section, we will introduce another vector space classiﬁcation\\nmethod, kNN, that deals better with classes that have non-spherical, discon-\\nnected or other irregular shapes.\\n?\\nExercise 14.2\\n[⋆]\\nShow that Rocchio classiﬁcation can assign a label to a document that is different from\\nits training set label.\\n2. We write Θ(|D|Lave) for Θ(T) and assume that the length of test documents is bounded as\\nwe did on page 262.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.3\\nk nearest neighbor\\n297\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋆\\n◮Figure 14.6\\nVoronoi tessellation and decision boundaries (double lines) in 1NN\\nclassiﬁcation. The three classes are: X, circle and diamond.\\n14.3\\nk nearest neighbor\\nUnlike Rocchio, k nearest neighbor or kNN classiﬁcation determines the deci-\\nk NEAREST NEIGHBOR\\nCLASSIFICATION\\nsion boundary locally. For 1NN we assign each document to the class of its\\nclosest neighbor. For kNN we assign each document to the majority class of\\nits k closest neighbors where k is a parameter. The rationale of kNN classiﬁ-\\ncation is that, based on the contiguity hypothesis, we expect a test document\\nd to have the same label as the training documents located in the local region\\nsurrounding d.\\nDecision boundaries in 1NN are concatenated segments of the Voronoi tes-\\nVORONOI\\nTESSELLATION\\nsellation as shown in Figure 14.6. The Voronoi tessellation of a set of objects\\ndecomposes space into Voronoi cells, where each object’s cell consists of all\\npoints that are closer to the object than to other objects. In our case, the ob-\\njects are documents. The Voronoi tessellation then partitions the plane into\\n|D| convex polygons, each containing its corresponding document (and no\\nother) as shown in Figure 14.6, where a convex polygon is a convex region in\\n2-dimensional space bounded by lines.\\nFor general k ∈N in kNN, consider the region in the space for which the\\nset of k nearest neighbors is the same. This again is a convex polygon and the\\nspace is partitioned into convex polygons, within each of which the set of k\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n298\\n14\\nVector space classiﬁcation\\nTRAIN-KNN(C, D)\\n1\\nD′ ←PREPROCESS(D)\\n2\\nk ←SELECT-K(C, D′)\\n3\\nreturn D′, k\\nAPPLY-KNN(C, D′, k, d)\\n1\\nSk ←COMPUTENEARESTNEIGHBORS(D′, k, d)\\n2\\nfor each cj ∈C\\n3\\ndo pj ←|Sk ∩cj|/k\\n4\\nreturn arg maxj pj\\n◮Figure 14.7\\nkNN training (with preprocessing) and testing. pj is an estimate for\\nP(cj|Sk) = P(cj|d). cj denotes the set of all documents in the class cj.\\nnearest neighbors is invariant (Exercise 14.11).3\\n1NN is not very robust. The classiﬁcation decision of each test document\\nrelies on the class of a single training document, which may be incorrectly\\nlabeled or atypical. kNN for k > 1 is more robust. It assigns documents to\\nthe majority class of their k closest neighbors, with ties broken randomly.\\nThere is a probabilistic version of this kNN classiﬁcation algorithm. We\\ncan estimate the probability of membership in class c as the proportion of the\\nk nearest neighbors in c. Figure 14.6 gives an example for k = 3. Probabil-\\nity estimates for class membership of the star are ˆP(circle class|star) = 1/3,\\nˆP(X class|star) = 2/3, and ˆP(diamond class|star) = 0. The 3nn estimate\\n( ˆP1(circle class|star) = 1/3) and the 1nn estimate ( ˆP1(circle class|star) = 1)\\ndiffer with 3nn preferring the X class and 1nn preferring the circle class .\\nThe parameter k in kNN is often chosen based on experience or knowledge\\nabout the classiﬁcation problem at hand. It is desirable for k to be odd to\\nmake ties less likely. k = 3 and k = 5 are common choices, but much larger\\nvalues between 50 and 100 are also used. An alternative way of setting the\\nparameter is to select the k that gives best results on a held-out portion of the\\ntraining set.\\nWe can also weight the “votes” of the k nearest neighbors by their cosine\\n3. The generalization of a polygon to higher dimensions is a polytope. A polytope is a region\\nin M-dimensional space bounded by (M −1)-dimensional hyperplanes. In M dimensions, the\\ndecision boundaries for kNN consist of segments of (M −1)-dimensional hyperplanes that form\\nthe Voronoi tessellation into convex polytopes for the training set of documents. The decision\\ncriterion of assigning a document to the majority class of its k nearest neighbors applies equally\\nto M = 2 (tessellation into polygons) and M > 2 (tessellation into polytopes).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.3\\nk nearest neighbor\\n299\\nkNN with preprocessing of training set\\ntraining\\nΘ(|D|Lave)\\ntesting\\nΘ(La + |D|MaveMa) = Θ(|D|MaveMa)\\nkNN without preprocessing of training set\\ntraining\\nΘ(1)\\ntesting\\nΘ(La + |D|LaveMa) = Θ(|D|LaveMa)\\n◮Table 14.3\\nTraining and test times for kNN classiﬁcation. Mave is the average size\\nof the vocabulary of documents in the collection.\\nsimilarity. In this scheme, a class’s score is computed as:\\nscore(c, d) =\\n∑\\nd′∈Sk(d)\\nIc(d′) cos(⃗v(d′),⃗v(d))\\nwhere Sk(d) is the set of d’s k nearest neighbors and Ic(d′) = 1 iff d′ is in class\\nc and 0 otherwise. We then assign the document to the class with the highest\\nscore. Weighting by similarities is often more accurate than simple voting.\\nFor example, if two classes have the same number of neighbors in the top k,\\nthe class with the more similar neighbors wins.\\nFigure 14.7 summarizes the kNN algorithm.\\n\\x0f\\nExample 14.2:\\nThe distances of the test document from the four training docu-\\nments in Table 14.1 are |⃗d1 −⃗d5| = |⃗d2 −⃗d5| = |⃗d3 −⃗d5| ≈1.41 and |⃗d4 −⃗d5| = 0.0.\\nd5’s nearest neighbor is therefore d4 and 1NN assigns d5 to d4’s class, c.\\n$\\n14.3.1\\nTime complexity and optimality of kNN\\nTable 14.3 gives the time complexity of kNN. kNN has properties that are\\nquite different from most other classiﬁcation algorithms. Training a kNN\\nclassiﬁer simply consists of determining k and preprocessing documents. In\\nfact, if we preselect a value for k and do not preprocess, then kNN requires\\nno training at all. In practice, we have to perform preprocessing steps like\\ntokenization. It makes more sense to preprocess training documents once\\nas part of the training phase rather than repeatedly every time we classify a\\nnew test document.\\nTest time is Θ(|D|MaveMa) for kNN. It is linear in the size of the training\\nset as we need to compute the distance of each training document from the\\ntest document. Test time is independent of the number of classes J. kNN\\ntherefore has a potential advantage for problems with large J.\\nIn kNN classiﬁcation, we do not perform any estimation of parameters as\\nwe do in Rocchio classiﬁcation (centroids) or in Naive Bayes (priors and con-\\nditional probabilities). kNN simply memorizes all examples in the training\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n300\\n14\\nVector space classiﬁcation\\nset and then compares the test document to them. For this reason, kNN is\\nalso called memory-based learning or instance-based learning. It is usually desir-\\nMEMORY-BASED\\nLEARNING\\nable to have as much training data as possible in machine learning. But in\\nkNN large training sets come with a severe efﬁciency penalty in classiﬁca-\\ntion.\\nCan kNN testing be made more efﬁcient than Θ(|D|MaveMa) or, ignoring\\nthe length of documents, more efﬁcient than Θ(|D|)? There are fast kNN\\nalgorithms for small dimensionality M (Exercise 14.12). There are also ap-\\nproximations for large M that give error bounds for speciﬁc efﬁciency gains\\n(see Section 14.7). These approximations have not been extensively tested\\nfor text classiﬁcation applications, so it is not clear whether they can achieve\\nmuch better efﬁciency than Θ(|D|) without a signiﬁcant loss of accuracy.\\nThe reader may have noticed the similarity between the problem of ﬁnding\\nnearest neighbors of a test document and ad hoc retrieval, where we search\\nfor the documents with the highest similarity to the query (Section 6.3.2,\\npage 123). In fact, the two problems are both k nearest neighbor problems\\nand only differ in the relative density of (the vector of) the test document\\nin kNN (10s or 100s of non-zero entries) versus the sparseness of (the vec-\\ntor of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries).\\nWe introduced the inverted index for efﬁcient ad hoc retrieval in Section 1.1\\n(page 6). Is the inverted index also the solution for efﬁcient kNN?\\nAn inverted index restricts a search to those documents that have at least\\none term in common with the query. Thus in the context of kNN, the in-\\nverted index will be efﬁcient if the test document has no term overlap with a\\nlarge number of training documents. Whether this is the case depends on the\\nclassiﬁcation problem. If documents are long and no stop list is used, then\\nless time will be saved. But with short documents and a large stop list, an\\ninverted index may well cut the average test time by a factor of 10 or more.\\nThe search time in an inverted index is a function of the length of the post-\\nings lists of the terms in the query. Postings lists grow sublinearly with the\\nlength of the collection since the vocabulary increases according to Heaps’\\nlaw – if the probability of occurrence of some terms increases, then the prob-\\nability of occurrence of others must decrease. However, most new terms are\\ninfrequent. We therefore take the complexity of inverted index search to be\\nΘ(T) (as discussed in Section 2.4.2, page 41) and, assuming average docu-\\nment length does not change over time, Θ(T) = Θ(|D|).\\nAs we will see in the next chapter, kNN’s effectiveness is close to that of the\\nmost accurate learning methods in text classiﬁcation (Table 15.2, page 334). A\\nmeasure of the quality of a learning method is its Bayes error rate, the average\\nBAYES ERROR RATE\\nerror rate of classiﬁers learned by it for a particular problem. kNN is not\\noptimal for problems with a non-zero Bayes error rate – that is, for problems\\nwhere even the best possible classiﬁer has a non-zero classiﬁcation error. The\\nerror of 1NN is asymptotically (as the training set increases) bounded by\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n301\\n◮Figure 14.8\\nThere are an inﬁnite number of hyperplanes that separate two linearly\\nseparable classes.\\ntwice the Bayes error rate. That is, if the optimal classiﬁer has an error rate\\nof x, then 1NN has an asymptotic error rate of less than 2x. This is due to the\\neffect of noise – we already saw one example of noise in the form of noisy\\nfeatures in Section 13.5 (page 271), but noise can also take other forms as we\\nwill discuss in the next section. Noise affects two components of kNN: the\\ntest document and the closest training document. The two sources of noise\\nare additive, so the overall error of 1NN is twice the optimal error rate. For\\nproblems with Bayes error rate 0, the error rate of 1NN will approach 0 as\\nthe size of the training set increases.\\n?\\nExercise 14.3\\nExplain why kNN handles multimodal classes better than Rocchio.\\n14.4\\nLinear versus nonlinear classiﬁers\\nIn this section, we show that the two learning methods Naive Bayes and\\nRocchio are instances of linear classiﬁers, the perhaps most important group\\nof text classiﬁers, and contrast them with nonlinear classiﬁers. To simplify\\nthe discussion, we will only consider two-class classiﬁers in this section and\\ndeﬁne a linear classiﬁer as a two-class classiﬁer that decides class membership\\nLINEAR CLASSIFIER\\nby comparing a linear combination of the features to a threshold.\\nIn two dimensions, a linear classiﬁer is a line. Five examples are shown\\nin Figure 14.8. These lines have the functional form w1x1 + w2x2 = b. The\\nclassiﬁcation rule of a linear classiﬁer is to assign a document to c if w1x1 +\\nw2x2 > b and to c if w1x1 + w2x2 ≤b. Here, (x1, x2)T is the two-dimensional\\nvector representation of the document and (w1, w2)T is the parameter vector\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n302\\n14\\nVector space classiﬁcation\\nAPPLYLINEARCLASSIFIER(⃗w, b,⃗x)\\n1\\nscore ←∑M\\ni=1 wixi\\n2\\nif score > b\\n3\\nthen return 1\\n4\\nelse return 0\\n◮Figure 14.9\\nLinear classiﬁcation algorithm.\\nthat deﬁnes (together with b) the decision boundary. An alternative geomet-\\nric interpretation of a linear classiﬁer is provided in Figure 15.7 (page 343).\\nWe can generalize this 2D linear classiﬁer to higher dimensions by deﬁning\\na hyperplane as we did in Equation (14.2), repeated here as Equation (14.3):\\n⃗wT⃗x = b\\n(14.3)\\nThe assignment criterion then is: assign to c if ⃗wT⃗x > b and to c if ⃗wT⃗x ≤b.\\nWe call a hyperplane that we use as a linear classiﬁer a decision hyperplane.\\nDECISION HYPERPLANE\\nThe corresponding algorithm for linear classiﬁcation in M dimensions is\\nshown in Figure 14.9. Linear classiﬁcation at ﬁrst seems trivial given the\\nsimplicity of this algorithm. However, the difﬁculty is in training the lin-\\near classiﬁer, that is, in determining the parameters ⃗w and b based on the\\ntraining set. In general, some learning methods compute much better param-\\neters than others where our criterion for evaluating the quality of a learning\\nmethod is the effectiveness of the learned linear classiﬁer on new data.\\nWe now show that Rocchio and Naive Bayes are linear classiﬁers. To see\\nthis for Rocchio, observe that a vector ⃗x is on the decision boundary if it has\\nequal distance to the two class centroids:\\n|⃗µ(c1) −⃗x| = |⃗µ(c2) −⃗x|\\n(14.4)\\nSome basic arithmetic shows that this corresponds to a linear classiﬁer with\\nnormal vector ⃗w = ⃗µ(c1) −⃗µ(c2) and b = 0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2) (Exer-\\ncise 14.15).\\nWe can derive the linearity of Naive Bayes from its decision rule, which\\nchooses the category c with the largest ˆP(c|d) (Figure 13.2, page 260) where:\\nˆP(c|d) ∝ˆP(c) ∏\\n1≤k≤nd\\nˆP(tk|c)\\nand nd is the number of tokens in the document that are part of the vocabu-\\nlary. Denoting the complement category as ¯c, we obtain for the log odds:\\nlog\\nˆP(c|d)\\nˆP(¯c|d) = log\\nˆP(c)\\nˆP(¯c) + ∑\\n1≤k≤nd\\nlog\\nˆP(tk|c)\\nˆP(tk|¯c)\\n(14.5)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n303\\nti\\nwi\\nd1i\\nd2i\\nti\\nwi\\nd1i\\nd2i\\nprime\\n0.70\\n0\\n1\\ndlrs\\n-0.71\\n1\\n1\\nrate\\n0.67\\n1\\n0\\nworld\\n-0.35\\n1\\n0\\ninterest\\n0.63\\n0\\n0\\nsees\\n-0.33\\n0\\n0\\nrates\\n0.60\\n0\\n0\\nyear\\n-0.25\\n0\\n0\\ndiscount\\n0.46\\n1\\n0\\ngroup\\n-0.24\\n0\\n0\\nbundesbank\\n0.43\\n0\\n0\\ndlr\\n-0.24\\n0\\n0\\n◮Table 14.4\\nA linear classiﬁer. The dimensions ti and parameters wi of a linear\\nclassiﬁer for the class interest (as in interest rate) in Reuters-21578. The threshold is\\nb = 0. Terms like dlr and world have negative weights because they are indicators for\\nthe competing class currency.\\nWe choose class c if the odds are greater than 1 or, equivalently, if the log\\nodds are greater than 0. It is easy to see that Equation (14.5) is an instance\\nof Equation (14.3) for wi = log[ ˆP(ti|c)/ ˆP(ti|¯c)], xi = number of occurrences\\nof ti in d, and b = −log[ ˆP(c)/ ˆP(¯c)]. Here, the index i, 1 ≤i ≤M, refers\\nto terms of the vocabulary (not to positions in d as k does; cf. Section 13.4.1,\\npage 270) and ⃗x and ⃗w are M-dimensional vectors. So in log space, Naive\\nBayes is a linear classiﬁer.\\n\\x0f\\nExample 14.3:\\nTable 14.4 deﬁnes a linear classiﬁer for the category interest in\\nReuters-21578 (see Section 13.6, page 279). We assign document ⃗d1 “rate discount\\ndlrs world” to interest since ⃗wT⃗d1 = 0.67 · 1 + 0.46 · 1 + (−0.71) · 1 + (−0.35) · 1 =\\n0.07 > 0 = b. We assign ⃗d2 “prime dlrs” to the complement class (not in interest) since\\n⃗wT⃗d2 = −0.01 ≤b. For simplicity, we assume a simple binary vector representation\\nin this example: 1 for occurring terms, 0 for non-occurring terms.\\nFigure 14.10 is a graphical example of a linear problem, which we deﬁne to\\nmean that the underlying distributions P(d|c) and P(d|c) of the two classes\\nare separated by a line. We call this separating line the class boundary. It is\\nCLASS BOUNDARY\\nthe “true” boundary of the two classes and we distinguish it from the deci-\\nsion boundary that the learning method computes to approximate the class\\nboundary.\\nAs is typical in text classiﬁcation, there are some noise documents in Fig-\\nNOISE DOCUMENT\\nure 14.10 (marked with arrows) that do not ﬁt well into the overall distri-\\nbution of the classes. In Section 13.5 (page 271), we deﬁned a noise feature\\nas a misleading feature that, when included in the document representation,\\non average increases the classiﬁcation error. Analogously, a noise document\\nis a document that, when included in the training set, misleads the learn-\\ning method and increases classiﬁcation error.\\nIntuitively, the underlying\\ndistribution partitions the representation space into areas with mostly ho-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n304\\n14\\nVector space classiﬁcation\\n◮Figure 14.10\\nA linear problem with noise. In this hypothetical web page classiﬁ-\\ncation scenario, Chinese-only web pages are solid circles and mixed Chinese-English\\nweb pages are squares. The two classes are separated by a linear class boundary\\n(dashed line, short dashes), except for three noise documents (marked with arrows).\\nmogeneous class assignments. A document that does not conform with the\\ndominant class in its area is a noise document.\\nNoise documents are one reason why training a linear classiﬁer is hard. If\\nwe pay too much attention to noise documents when choosing the decision\\nhyperplane of the classiﬁer, then it will be inaccurate on new data. More\\nfundamentally, it is usually difﬁcult to determine which documents are noise\\ndocuments and therefore potentially misleading.\\nIf there exists a hyperplane that perfectly separates the two classes, then\\nwe call the two classes linearly separable. In fact, if linear separability holds,\\nLINEAR SEPARABILITY\\nthen there is an inﬁnite number of linear separators (Exercise 14.4) as illus-\\ntrated by Figure 14.8, where the number of possible separating hyperplanes\\nis inﬁnite.\\nFigure 14.8 illustrates another challenge in training a linear classiﬁer. If we\\nare dealing with a linearly separable problem, then we need a criterion for\\nselecting among all decision hyperplanes that perfectly separate the training\\ndata. In general, some of these hyperplanes will do well on new data, some\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n305\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n◮Figure 14.11\\nA nonlinear problem.\\nwill not.\\nAn example of a nonlinear classiﬁer is kNN. The nonlinearity of kNN is\\nNONLINEAR\\nCLASSIFIER\\nintuitively clear when looking at examples like Figure 14.6. The decision\\nboundaries of kNN (the double lines in Figure 14.6) are locally linear seg-\\nments, but in general have a complex shape that is not equivalent to a line in\\n2D or a hyperplane in higher dimensions.\\nFigure 14.11 is another example of a nonlinear problem: there is no good\\nlinear separator between the distributions P(d|c) and P(d|c) because of the\\ncircular “enclave” in the upper left part of the graph. Linear classiﬁers mis-\\nclassify the enclave, whereas a nonlinear classiﬁer like kNN will be highly\\naccurate for this type of problem if the training set is large enough.\\nIf a problem is nonlinear and its class boundaries cannot be approximated\\nwell with linear hyperplanes, then nonlinear classiﬁers are often more accu-\\nrate than linear classiﬁers. If a problem is linear, it is best to use a simpler\\nlinear classiﬁer.\\n?\\nExercise 14.4\\nProve that the number of linear separators of two classes is either inﬁnite or zero.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n306\\n14\\nVector space classiﬁcation\\n14.5\\nClassiﬁcation with more than two classes\\nWe can extend two-class linear classiﬁers to J > 2 classes. The method to use\\ndepends on whether the classes are mutually exclusive or not.\\nClassiﬁcation for classes that are not mutually exclusive is called any-of,\\nANY-OF\\nCLASSIFICATION\\nmultilabel, or multivalue classiﬁcation. In this case, a document can belong to\\nseveral classes simultaneously, or to a single class, or to none of the classes.\\nA decision on one class leaves all options open for the others. It is some-\\ntimes said that the classes are independent of each other, but this is misleading\\nsince the classes are rarely statistically independent in the sense deﬁned on\\npage 275. In terms of the formal deﬁnition of the classiﬁcation problem in\\nEquation (13.1) (page 256), we learn J different classiﬁers γj in any-of classi-\\nﬁcation, each returning either cj or cj: γj(d) ∈{cj, cj}.\\nSolving an any-of classiﬁcation task with linear classiﬁers is straightfor-\\nward:\\n1. Build a classiﬁer for each class, where the training set consists of the set\\nof documents in the class (positive labels) and its complement (negative\\nlabels).\\n2. Given the test document, apply each classiﬁer separately. The decision of\\none classiﬁer has no inﬂuence on the decisions of the other classiﬁers.\\nThe second type of classiﬁcation with more than two classes is one-of clas-\\nONE-OF\\nCLASSIFICATION\\nsiﬁcation. Here, the classes are mutually exclusive. Each document must\\nbelong to exactly one of the classes. One-of classiﬁcation is also called multi-\\nnomial, polytomous4, multiclass, or single-label classiﬁcation. Formally, there is a\\nsingle classiﬁcation function γ in one-of classiﬁcation whose range is C, i.e.,\\nγ(d) ∈{c1, . . . , cJ}. kNN is a (nonlinear) one-of classiﬁer.\\nTrue one-of problems are less common in text classiﬁcation than any-of\\nproblems. With classes like UK, China, poultry, or coffee, a document can be\\nrelevant to many topics simultaneously – as when the prime minister of the\\nUK visits China to talk about the coffee and poultry trade.\\nNevertheless, we will often make a one-of assumption, as we did in Fig-\\nure 14.1, even if classes are not really mutually exclusive. For the classiﬁca-\\ntion problem of identifying the language of a document, the one-of assump-\\ntion is a good approximation as most text is written in only one language.\\nIn such cases, imposing a one-of constraint can increase the classiﬁer’s ef-\\nfectiveness because errors that are due to the fact that the any-of classiﬁers\\nassigned a document to either no class or more than one class are eliminated.\\nJ hyperplanes do not divide R|V| into J distinct regions as illustrated in\\nFigure 14.12. Thus, we must use a combination method when using two-\\nclass linear classiﬁers for one-of classiﬁcation. The simplest method is to\\n4. A synonym of polytomous is polychotomous.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.5\\nClassiﬁcation with more than two classes\\n307\\n?\\n◮Figure 14.12\\nJ hyperplanes do not divide space into J disjoint regions.\\nrank classes and then select the top-ranked class. Geometrically, the ranking\\ncan be with respect to the distances from the J linear separators. Documents\\nclose to a class’s separator are more likely to be misclassiﬁed, so the greater\\nthe distance from the separator, the more plausible it is that a positive clas-\\nsiﬁcation decision is correct. Alternatively, we can use a direct measure of\\nconﬁdence to rank classes, e.g., probability of class membership. We can\\nstate this algorithm for one-of classiﬁcation with linear classiﬁers as follows:\\n1. Build a classiﬁer for each class, where the training set consists of the set\\nof documents in the class (positive labels) and its complement (negative\\nlabels).\\n2. Given the test document, apply each classiﬁer separately.\\n3. Assign the document to the class with\\n• the maximum score,\\n• the maximum conﬁdence value,\\n• or the maximum probability.\\nAn important tool for analyzing the performance of a classiﬁer for J > 2\\nclasses is the confusion matrix. The confusion matrix shows for each pair of\\nCONFUSION MATRIX\\nclasses ⟨c1, c2⟩, how many documents from c1 were incorrectly assigned to c2.\\nIn Table 14.5, the classiﬁer manages to distinguish the three ﬁnancial classes\\nmoney-fx, trade, and interest from the three agricultural classes wheat, corn,\\nand grain, but makes many errors within these two groups. The confusion\\nmatrix can help pinpoint opportunities for improving the accuracy of the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n308\\n14\\nVector space classiﬁcation\\nassigned class\\nmoney-fx\\ntrade\\ninterest\\nwheat\\ncorn\\ngrain\\ntrue class\\nmoney-fx\\n95\\n0\\n10\\n0\\n0\\n0\\ntrade\\n1\\n1\\n90\\n0\\n1\\n0\\ninterest\\n13\\n0\\n0\\n0\\n0\\n0\\nwheat\\n0\\n0\\n1\\n34\\n3\\n7\\ncorn\\n1\\n0\\n2\\n13\\n26\\n5\\ngrain\\n0\\n0\\n2\\n14\\n5\\n10\\n◮Table 14.5\\nA confusion matrix for Reuters-21578. For example, 14 documents\\nfrom grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006).\\nsystem. For example, to address the second largest error in Table 14.5 (14 in\\nthe row grain), one could attempt to introduce features that distinguish wheat\\ndocuments from grain documents.\\n?\\nExercise 14.5\\nCreate a training set of 300 documents, 100 each from three different languages (e.g.,\\nEnglish, French, Spanish). Create a test set by the same procedure, but also add 100\\ndocuments from a fourth language. Train (i) a one-of classiﬁer (ii) an any-of classi-\\nﬁer on this training set and evaluate it on the test set. (iii) Are there any interesting\\ndifferences in how the two classiﬁers behave on this task?\\n$\\n14.6\\nThe bias-variance tradeoff\\nNonlinear classiﬁers are more powerful than linear classiﬁers.\\nFor some\\nproblems, there exists a nonlinear classiﬁer with zero classiﬁcation error, but\\nno such linear classiﬁer. Does that mean that we should always use nonlinear\\nclassiﬁers for optimal effectiveness in statistical text classiﬁcation?\\nTo answer this question, we introduce the bias-variance tradeoff in this sec-\\ntion, one of the most important concepts in machine learning. The tradeoff\\nhelps explain why there is no universally optimal learning method. Selecting\\nan appropriate learning method is therefore an unavoidable part of solving\\na text classiﬁcation problem.\\nThroughout this section, we use linear and nonlinear classiﬁers as proto-\\ntypical examples of “less powerful” and “more powerful” learning, respec-\\ntively. This is a simpliﬁcation for a number of reasons. First, many nonlinear\\nmodels subsume linear models as a special case. For instance, a nonlinear\\nlearning method like kNN will in some cases produce a linear classiﬁer. Sec-\\nond, there are nonlinear models that are less complex than linear models.\\nFor instance, a quadratic polynomial with two parameters is less powerful\\nthan a 10,000-dimensional linear classiﬁer. Third, the complexity of learn-\\ning is not really a property of the classiﬁer because there are many aspects\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n309\\nof learning (such as feature selection, cf. (Section 13.5, page 271), regulariza-\\ntion, and constraints such as margin maximization in Chapter 15) that make\\na learning method either more powerful or less powerful without affecting\\nthe type of classiﬁer that is the ﬁnal result of learning – regardless of whether\\nthat classiﬁer is linear or nonlinear. We refer the reader to the publications\\nlisted in Section 14.7 for a treatment of the bias-variance tradeoff that takes\\ninto account these complexities. In this section, linear and nonlinear classi-\\nﬁers will simply serve as proxies for weaker and stronger learning methods\\nin text classiﬁcation.\\nWe ﬁrst need to state our objective in text classiﬁcation more precisely. In\\nSection 13.1 (page 256), we said that we want to minimize classiﬁcation er-\\nror on the test set. The implicit assumption was that training documents\\nand test documents are generated according to the same underlying distri-\\nbution. We will denote this distribution P(⟨d, c⟩) where d is the document\\nand c its label or class. Figures 13.4 and 13.5 were examples of generative\\nmodels that decompose P(⟨d, c⟩) into the product of P(c) and P(d|c). Fig-\\nures 14.10 and 14.11 depict generative models for ⟨d, c⟩with d ∈R2 and\\nc ∈{square, solid circle}.\\nIn this section, instead of using the number of correctly classiﬁed test doc-\\numents (or, equivalently, the error rate on test documents) as evaluation\\nmeasure, we adopt an evaluation measure that addresses the inherent un-\\ncertainty of labeling. In many text classiﬁcation problems, a given document\\nrepresentation can arise from documents belonging to different classes. This\\nis because documents from different classes can be mapped to the same doc-\\nument representation. For example, the one-sentence documents China sues\\nFrance and France sues China are mapped to the same document representa-\\ntion d′ = {China, France, sues} in a bag of words model. But only the latter\\ndocument is relevant to the class c′ = legal actions brought by France (which\\nmight be deﬁned, for example, as a standing query by an international trade\\nlawyer).\\nTo simplify the calculations in this section, we do not count the number\\nof errors on the test set when evaluating a classiﬁer, but instead look at how\\nwell the classiﬁer estimates the conditional probability P(c|d) of a document\\nbeing in a class. In the above example, we might have P(c′|d′) = 0.5.\\nOur goal in text classiﬁcation then is to ﬁnd a classiﬁer γ such that, aver-\\naged over documents d, γ(d) is as close as possible to the true probability\\nP(c|d). We measure this using mean squared error:\\nMSE(γ) = Ed[γ(d) −P(c|d)]2\\n(14.6)\\nwhere Ed is the expectation with respect to P(d). The mean squared error\\nterm gives partial credit for decisions by γ that are close if not completely\\nright.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n310\\n14\\nVector space classiﬁcation\\nE[x −α]2\\n=\\nEx2 −2Exα + α2\\n(14.8)\\n=\\n(Ex)2 −2Exα + α2\\n+Ex2 −2(Ex)2 + (Ex)2\\n=\\n[Ex −α]2\\n+Ex2 −E2x(Ex) + E(Ex)2\\n=\\n[Ex −α]2 + E[x −Ex]2\\nEDEd[ΓD(d) −P(c|d)]2\\n=\\nEdED[ΓD(d) −P(c|d)]2\\n(14.9)\\n=\\nEd[ [EDΓD(d) −P(c|d)]2\\n+ED[ΓD(d) −EDΓD(d)]2 ]\\n◮Figure 14.13\\nArithmetic transformations for the bias-variance decomposition.\\nFor the derivation of Equation (14.9), we set α = P(c|d) and x = ΓD(d) in Equa-\\ntion (14.8).\\nWe deﬁne a classiﬁer γ to be optimal for a distribution P(⟨d, c⟩) if it mini-\\nOPTIMAL CLASSIFIER\\nmizes MSE(γ).\\nMinimizing MSE is a desideratum for classiﬁers. We also need a criterion\\nfor learning methods. Recall that we deﬁned a learning method Γ as a function\\nthat takes a labeled training set D as input and returns a classiﬁer γ.\\nFor learning methods, we adopt as our goal to ﬁnd a Γ that, averaged over\\ntraining sets, learns classiﬁers γ with minimal MSE. We can formalize this as\\nminimizing learning error:\\nLEARNING ERROR\\nlearning-error(Γ) = ED[MSE(Γ(D))]\\n(14.7)\\nwhere ED is the expectation over labeled training sets. To keep things simple,\\nwe can assume that training sets have a ﬁxed size – the distribution P(⟨d, c⟩)\\nthen deﬁnes a distribution P(D) over training sets.\\nWe can use learning error as a criterion for selecting a learning method in\\nstatistical text classiﬁcation. A learning method Γ is optimal for a distribution\\nOPTIMAL LEARNING\\nMETHOD\\nP(D) if it minimizes the learning error.\\nWriting ΓD for Γ(D) for better readability, we can transform Equation (14.7)\\nas follows:\\nlearning-error(Γ)\\n=\\nED[MSE(ΓD)]\\n=\\nEDEd[ΓD(d) −P(c|d)]2\\n(14.10)\\n=\\nEd[bias(Γ, d) + variance(Γ, d)]\\n(14.11)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n311\\nbias(Γ, d)\\n=\\n[P(c|d) −EDΓD(d)]2\\n(14.12)\\nvariance(Γ, d)\\n=\\nED[ΓD(d) −EDΓD(d)]2\\n(14.13)\\nwhere the equivalence between Equations (14.10) and (14.11) is shown in\\nEquation (14.9) in Figure 14.13. Note that d and D are independent of each\\nother. In general, for a random document d and a random training set D, D\\ndoes not contain a labeled instance of d.\\nBias is the squared difference between P(c|d), the true conditional prob-\\nBIAS\\nability of d being in c, and ΓD(d), the prediction of the learned classiﬁer,\\naveraged over training sets. Bias is large if the learning method produces\\nclassiﬁers that are consistently wrong. Bias is small if (i) the classiﬁers are\\nconsistently right or (ii) different training sets cause errors on different docu-\\nments or (iii) different training sets cause positive and negative errors on the\\nsame documents, but that average out to close to 0. If one of these three con-\\nditions holds, then EDΓD(d), the expectation over all training sets, is close to\\nP(c|d).\\nLinear methods like Rocchio and Naive Bayes have a high bias for non-\\nlinear problems because they can only model one type of class boundary, a\\nlinear hyperplane. If the generative model P(⟨d, c⟩) has a complex nonlinear\\nclass boundary, the bias term in Equation (14.11) will be high because a large\\nnumber of points will be consistently misclassiﬁed. For example, the circular\\nenclave in Figure 14.11 does not ﬁt a linear model and will be misclassiﬁed\\nconsistently by linear classiﬁers.\\nWe can think of bias as resulting from our domain knowledge (or lack\\nthereof) that we build into the classiﬁer. If we know that the true boundary\\nbetween the two classes is linear, then a learning method that produces linear\\nclassiﬁers is more likely to succeed than a nonlinear method. But if the true\\nclass boundary is not linear and we incorrectly bias the classiﬁer to be linear,\\nthen classiﬁcation accuracy will be low on average.\\nNonlinear methods like kNN have low bias. We can see in Figure 14.6 that\\nthe decision boundaries of kNN are variable – depending on the distribu-\\ntion of documents in the training set, learned decision boundaries can vary\\ngreatly. As a result, each document has a chance of being classiﬁed correctly\\nfor some training sets. The average prediction EDΓD(d) is therefore closer to\\nP(c|d) and bias is smaller than for a linear learning method.\\nVariance is the variation of the prediction of learned classiﬁers: the aver-\\nVARIANCE\\nage squared difference between ΓD(d) and its average EDΓD(d). Variance is\\nlarge if different training sets D give rise to very different classiﬁers ΓD. It is\\nsmall if the training set has a minor effect on the classiﬁcation decisions ΓD\\nmakes, be they correct or incorrect. Variance measures how inconsistent the\\ndecisions are, not whether they are correct or incorrect.\\nLinear learning methods have low variance because most randomly drawn\\ntraining sets produce similar decision hyperplanes. The decision lines pro-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n312\\n14\\nVector space classiﬁcation\\nduced by linear learning methods in Figures 14.10 and 14.11 will deviate\\nslightly from the main class boundaries, depending on the training set, but\\nthe class assignment for the vast majority of documents (with the exception\\nof those close to the main boundary) will not be affected. The circular enclave\\nin Figure 14.11 will be consistently misclassiﬁed.\\nNonlinear methods like kNN have high variance. It is apparent from Fig-\\nure 14.6 that kNN can model very complex boundaries between two classes.\\nIt is therefore sensitive to noise documents of the sort depicted in Figure 14.10.\\nAs a result the variance term in Equation (14.11) is large for kNN: Test doc-\\numents are sometimes misclassiﬁed – if they happen to be close to a noise\\ndocument in the training set – and sometimes correctly classiﬁed – if there\\nare no noise documents in the training set near them. This results in high\\nvariation from training set to training set.\\nHigh-variance learning methods are prone to overﬁtting the training data.\\nOVERFITTING\\nThe goal in classiﬁcation is to ﬁt the training data to the extent that we cap-\\nture true properties of the underlying distribution P(⟨d, c⟩). In overﬁtting,\\nthe learning method also learns from noise. Overﬁtting increases MSE and\\nfrequently is a problem for high-variance learning methods.\\nWe can also think of variance as the model complexity or, equivalently, mem-\\nMEMORY CAPACITY\\nory capacity of the learning method – how detailed a characterization of the\\ntraining set it can remember and then apply to new data. This capacity corre-\\nsponds to the number of independent parameters available to ﬁt the training\\nset. Each kNN neighborhood Sk makes an independent classiﬁcation deci-\\nsion. The parameter in this case is the estimate ˆP(c|Sk) from Figure 14.7.\\nThus, kNN’s capacity is only limited by the size of the training set. It can\\nmemorize arbitrarily large training sets. In contrast, the number of parame-\\nters of Rocchio is ﬁxed – J parameters per dimension, one for each centroid\\n– and independent of the size of the training set. The Rocchio classiﬁer (in\\nform of the centroids deﬁning it) cannot “remember” ﬁne-grained details of\\nthe distribution of the documents in the training set.\\nAccording to Equation (14.7), our goal in selecting a learning method is to\\nminimize learning error. The fundamental insight captured by Equation (14.11),\\nwhich we can succinctly state as: learning-error = bias + variance, is that the\\nlearning error has two components, bias and variance, which in general can-\\nnot be minimized simultaneously. When comparing two learning methods\\nΓ1 and Γ2, in most cases the comparison comes down to one method having\\nhigher bias and lower variance and the other lower bias and higher variance.\\nThe decision for one learning method vs. another is then not simply a mat-\\nter of selecting the one that reliably produces good classiﬁers across training\\nsets (small variance) or the one that can learn classiﬁcation problems with\\nvery difﬁcult decision boundaries (small bias). Instead, we have to weigh\\nthe respective merits of bias and variance in our application and choose ac-\\ncordingly. This tradeoff is called the bias-variance tradeoff.\\nBIAS-VARIANCE\\nTRADEOFF\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n313\\nFigure 14.10 provides an illustration, which is somewhat contrived, but\\nwill be useful as an example for the tradeoff. Some Chinese text contains\\nEnglish words written in the Roman alphabet like CPU, ONLINE, and GPS.\\nConsider the task of distinguishing Chinese-only web pages from mixed\\nChinese-English web pages. A search engine might offer Chinese users with-\\nout knowledge of English (but who understand loanwords like CPU) the op-\\ntion of ﬁltering out mixed pages. We use two features for this classiﬁcation\\ntask: number of Roman alphabet characters and number of Chinese char-\\nacters on the web page. As stated earlier, the distribution P(⟨d, c⟩) of the\\ngenerative model generates most mixed (respectively, Chinese) documents\\nabove (respectively, below) the short-dashed line, but there are a few noise\\ndocuments.\\nIn Figure 14.10, we see three classiﬁers:\\n• One-feature classiﬁer. Shown as a dotted horizontal line. This classiﬁer\\nuses only one feature, the number of Roman alphabet characters. Assum-\\ning a learning method that minimizes the number of misclassiﬁcations\\nin the training set, the position of the horizontal decision boundary is\\nnot greatly affected by differences in the training set (e.g., noise docu-\\nments). So a learning method producing this type of classiﬁer has low\\nvariance. But its bias is high since it will consistently misclassify squares\\nin the lower left corner and “solid circle” documents with more than 50\\nRoman characters.\\n• Linear classiﬁer. Shown as a dashed line with long dashes. Learning lin-\\near classiﬁers has less bias since only noise documents and possibly a few\\ndocuments close to the boundary between the two classes are misclassi-\\nﬁed. The variance is higher than for the one-feature classiﬁers, but still\\nsmall: The dashed line with long dashes deviates only slightly from the\\ntrue boundary between the two classes, and so will almost all linear de-\\ncision boundaries learned from training sets. Thus, very few documents\\n(documents close to the class boundary) will be inconsistently classiﬁed.\\n• “Fit-training-set-perfectly” classiﬁer. Shown as a solid line. Here, the\\nlearning method constructs a decision boundary that perfectly separates\\nthe classes in the training set. This method has the lowest bias because\\nthere is no document that is consistently misclassiﬁed – the classiﬁers\\nsometimes even get noise documents in the test set right. But the variance\\nof this learning method is high. Because noise documents can move the\\ndecision boundary arbitrarily, test documents close to noise documents\\nin the training set will be misclassiﬁed – something that a linear learning\\nmethod is unlikely to do.\\nIt is perhaps surprising that so many of the best-known text classiﬁcation\\nalgorithms are linear. Some of these methods, in particular linear SVMs, reg-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n314\\n14\\nVector space classiﬁcation\\nularized logistic regression and regularized linear regression, are among the\\nmost effective known methods. The bias-variance tradeoff provides insight\\ninto their success. Typical classes in text classiﬁcation are complex and seem\\nunlikely to be modeled well linearly. However, this intuition is misleading\\nfor the high-dimensional spaces that we typically encounter in text appli-\\ncations. With increased dimensionality, the likelihood of linear separability\\nincreases rapidly (Exercise 14.17). Thus, linear models in high-dimensional\\nspaces are quite powerful despite their linearity. Even more powerful nonlin-\\near learning methods can model decision boundaries that are more complex\\nthan a hyperplane, but they are also more sensitive to noise in the training\\ndata. Nonlinear learning methods sometimes perform better if the training\\nset is large, but by no means in all cases.\\n14.7\\nReferences and further reading\\nAs discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio\\n(1971). Joachims (1997) presents a probabilistic analysis of the method. Roc-\\nchio classiﬁcation was widely used as a classiﬁcation method in TREC in the\\n1990s (Buckley et al. 1994a;b, Voorhees and Harman 2005). Initially, it was\\nused as a form of routing. Routing merely ranks documents according to rel-\\nROUTING\\nevance to a class without assigning them. Early work on ﬁltering, a true clas-\\nFILTERING\\nsiﬁcation approach that makes an assignment decision on each document,\\nwas published by Ittner et al. (1995) and Schapire et al. (1998). The deﬁnition\\nof routing we use here should not be confused with another sense. Routing\\ncan also refer to the electronic distribution of documents to subscribers, the\\nso-called push model of document distribution. In a pull model, each transfer\\nPUSH MODEL\\nPULL MODEL\\nof a document to the user is initiated by the user – for example, by means\\nof search or by selecting it from a list of documents on a news aggregation\\nwebsite.\\nSome authors restrict the name Roccchio classiﬁcation to two-class problems\\nand use the terms cluster-based (Iwayama and Tokunaga 1995) and centroid-\\nCENTROID-BASED\\nCLASSIFICATION\\nbased classiﬁcation (Han and Karypis 2000, Tan and Cheng 2007) for Rocchio\\nclassiﬁcation with J > 2.\\nA more detailed treatment of kNN can be found in (Hastie et al. 2001), in-\\ncluding methods for tuning the parameter k. An example of an approximate\\nfast kNN algorithm is locality-based hashing (Andoni et al. 2006). Klein-\\nberg (1997) presents an approximate Θ((M log2 M)(M + log N)) kNN algo-\\nrithm (where M is the dimensionality of the space and N the number of data\\npoints), but at the cost of exponential storage requirements: Θ((N log M)2M).\\nIndyk (2004) surveys nearest neighbor methods in high-dimensional spaces.\\nEarly work on kNN in text classiﬁcation was motivated by the availability\\nof massively parallel hardware architectures (Creecy et al. 1992). Yang (1994)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.8\\nExercises\\n315\\nuses an inverted index to speed up kNN classiﬁcation. The optimality result\\nfor 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart\\n(1967).\\nThe effectiveness of Rocchio classiﬁcation and kNN is highly dependent\\non careful parameter tuning (in particular, the parameters b′ for Rocchio on\\npage 296 and k for kNN), feature engineering (Section 15.3, page 334) and\\nfeature selection (Section 13.5, page 271). Buckley and Salton (1995), Schapire\\net al. (1998), Yang and Kisiel (2003) and Moschitti (2003) address these issues\\nfor Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al.\\n(2000) compare feature selection methods for kNN.\\nThe bias-variance tradeoff was introduced by Geman et al. (1992). The\\nderivation in Section 14.6 is for MSE(γ), but the tradeoff applies to many\\nloss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995)\\nand Lewis et al. (1996) discuss linear classiﬁers for text and Hastie et al. (2001)\\nlinear classiﬁers in general. Readers interested in the algorithms mentioned,\\nbut not described in this chapter may wish to consult Bishop (2006) for neu-\\nral networks, Hastie et al. (2001) for linear and logistic regression, and Min-\\nsky and Papert (1988) for the perceptron algorithm. Anagnostopoulos et al.\\n(2006) show that an inverted index can be used for highly efﬁcient document\\nclassiﬁcation with any linear classiﬁer, provided that the classiﬁer is still ef-\\nfective when trained on a modest number of features via feature selection.\\nWe have only presented the simplest method for combining two-class clas-\\nsiﬁers into a one-of classiﬁer. Another important method is the use of error-\\ncorrecting codes, where a vector of decisions of different two-class classiﬁers\\nis constructed for each document. A test document’s decision vector is then\\n“corrected” based on the distribution of decision vectors in the training set,\\na procedure that incorporates information from all two-class classiﬁers and\\ntheir correlations into the ﬁnal classiﬁcation decision (Dietterich and Bakiri\\n1995). Ghamrawi and McCallum (2005) also exploit dependencies between\\nclasses in any-of classiﬁcation. Allwein et al. (2000) propose a general frame-\\nwork for combining two-class classiﬁers.\\n14.8\\nExercises\\n?\\nExercise 14.6\\nIn Figure 14.14, which of the three vectors⃗a,⃗b, and⃗c is (i) most similar to ⃗x according\\nto dot product similarity, (ii) most similar to ⃗x according to cosine similarity, (iii)\\nclosest to ⃗x according to Euclidean distance?\\nExercise 14.7\\nDownload Reuters-21578 and train and test Rocchio and kNN classiﬁers for the classes\\nacquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Use the\\nModApte split. You may want to use one of a number of software packages that im-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n316\\n14\\nVector space classiﬁcation\\n0 1 2 3 4 5 6 7 8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\na\\nx\\nb\\nc\\n◮Figure 14.14\\nExample for differences between Euclidean distance, dot product\\nsimilarity and cosine similarity. The vectors are ⃗a = (0.5 1.5)T, ⃗x = (2 2)T, ⃗b =\\n(4 4)T, and⃗c = (8 6)T.\\nplement Rocchio classiﬁcation and kNN classiﬁcation, for example, the Bow toolkit\\n(McCallum 1996).\\nExercise 14.8\\nDownload 20 Newgroups (page 154) and train and test Rocchio and kNN classiﬁers\\nfor its 20 classes.\\nExercise 14.9\\nShow that the decision boundaries in Rocchio classiﬁcation are, as in kNN, given by\\nthe Voronoi tessellation.\\nExercise 14.10\\n[⋆]\\nComputing the distance between a dense centroid and a sparse vector is Θ(M) for\\na naive implementation that iterates over all M dimensions. Based on the equality\\n∑(xi −µi)2 = 1.0 + ∑µ2\\ni −2 ∑xiµi and assuming that ∑µ2\\ni has been precomputed,\\nwrite down an algorithm that is Θ(Ma) instead, where Ma is the number of distinct\\nterms in the test document.\\nExercise 14.11\\n[⋆⋆⋆]\\nProve that the region of the plane consisting of all points with the same k nearest\\nneighbors is a convex polygon.\\nExercise 14.12\\nDesign an algorithm that performs an efﬁcient 1NN search in 1 dimension (where\\nefﬁciency is with respect to the number of documents N). What is the time complexity\\nof the algorithm?\\nExercise 14.13\\n[⋆⋆⋆]\\nDesign an algorithm that performs an efﬁcient 1NN search in 2 dimensions with at\\nmost polynomial (in N) preprocessing time.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n14.8\\nExercises\\n317\\nb\\nb\\n◮Figure 14.15\\nA simple non-separable set of points.\\nExercise 14.14\\n[⋆⋆⋆]\\nCan one design an exact efﬁcient algorithm for 1NN for very large M along the ideas\\nyou used to solve the last exercise?\\nExercise 14.15\\nShow that Equation (14.4) deﬁnes a hyperplane with ⃗w = ⃗µ(c1) −⃗µ(c2) and b =\\n0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2).\\nExercise 14.16\\nWe can easily construct non-separable data sets in high dimensions by embedding\\na non-separable set like the one shown in Figure 14.15. Consider embedding Fig-\\nure 14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a small\\ndistance in a random direction). Why would you expect the resulting conﬁguration\\nto be linearly separable? How likely is then a non-separable set of m ≪M points in\\nM-dimensional space?\\nExercise 14.17\\nAssuming two classes, show that the percentage of non-separable assignments of the\\nvertices of a hypercube decreases with dimensionality M for M > 1. For example,\\nfor M = 1 the proportion of non-separable assignments is 0, for M = 2, it is 2/16.\\nOne of the two non-separable cases for M = 2 is shown in Figure 14.15, the other is\\nits mirror image. Solve the exercise either analytically or by simulation.\\nExercise 14.18\\nAlthough we point out the similarities of Naive Bayes with linear vector space classi-\\nﬁers, it does not make sense to represent count vectors (the document representations\\nin NB) in a continuous vector space. There is however a formalization of NB that is\\nanalogous to Rocchio. Show that NB assigns a document to the class (represented as\\na parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4, page 251)\\nto the document (represented as a count vector as in Section 13.4.1 (page 270), nor-\\nmalized to sum to 1) is smallest.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n319\\n15\\nSupport vector machines and\\nmachine learning on documents\\nImproving classiﬁer effectiveness has been an area of intensive machine-\\nlearning research over the last two decades, and this work has led to a new\\ngeneration of state-of-the-art classiﬁers, such as support vector machines,\\nboosted decision trees, regularized logistic regression, neural networks, and\\nrandom forests. Many of these methods, including support vector machines\\n(SVMs), the main topic of this chapter, have been applied with success to\\ninformation retrieval problems, particularly text classiﬁcation. An SVM is a\\nkind of large-margin classiﬁer: it is a vector space based machine learning\\nmethod where the goal is to ﬁnd a decision boundary between two classes\\nthat is maximally far from any point in the training data (possibly discount-\\ning some points as outliers or noise).\\nWe will initially motivate and develop SVMs for the case of two-class data\\nsets that are separable by a linear classiﬁer (Section 15.1), and then extend the\\nmodel in Section 15.2 to non-separable data, multi-class problems, and non-\\nlinear models, and also present some additional discussion of SVM perfor-\\nmance. The chapter then moves to consider the practical deployment of text\\nclassiﬁers in Section 15.3: what sorts of classiﬁers are appropriate when, and\\nhow can you exploit domain-speciﬁc text features in classiﬁcation? Finally,\\nwe will consider how the machine learning technology that we have been\\nbuilding for text classiﬁcation can be applied back to the problem of learning\\nhow to rank documents in ad hoc retrieval (Section 15.4). While several ma-\\nchine learning methods have been applied to this task, use of SVMs has been\\nprominent. Support vector machines are not necessarily better than other\\nmachine learning methods (except perhaps in situations with little training\\ndata), but they perform at the state-of-the-art level and have much current\\ntheoretical and empirical appeal.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n320\\n15\\nSupport vector machines and machine learning on documents\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nSupport vectors\\nMaximum\\nmargin\\ndecision\\nhyperplane\\nMargin is\\nmaximized\\n◮Figure 15.1\\nThe support vectors are the 5 points right up against the margin of\\nthe classiﬁer.\\n15.1\\nSupport vector machines: The linearly separable case\\nFor two-class, separable training data sets, such as the one in Figure 14.8\\n(page 301), there are lots of possible linear separators. Intuitively, a decision\\nboundary drawn in the middle of the void between data items of the two\\nclasses seems better than one which approaches very close to examples of\\none or both classes. While some learning methods such as the perceptron\\nalgorithm (see references in Section 14.7, page 314) ﬁnd just any linear sepa-\\nrator, others, like Naive Bayes, search for the best linear separator according\\nto some criterion. The SVM in particular deﬁnes the criterion to be looking\\nfor a decision surface that is maximally far away from any data point. This\\ndistance from the decision surface to the closest data point determines the\\nmargin of the classiﬁer. This method of construction necessarily means that\\nMARGIN\\nthe decision function for an SVM is fully speciﬁed by a (usually small) sub-\\nset of the data which deﬁnes the position of the separator. These points are\\nreferred to as the support vectors (in a vector space, a point can be thought of\\nSUPPORT VECTOR\\nas a vector between the origin and that point). Figure 15.1 shows the margin\\nand support vectors for a sample problem. Other data points play no part in\\ndetermining the decision surface that is chosen.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n321\\n◮Figure 15.2\\nAn intuition for large-margin classiﬁcation. Insisting on a large mar-\\ngin reduces the capacity of the model: the range of angles at which the fat deci-\\nsion surface can be placed is smaller than for a decision hyperplane (cf. Figure 14.8,\\npage 301).\\nMaximizing the margin seems good because points near the decision sur-\\nface represent very uncertain classiﬁcation decisions: there is almost a 50%\\nchance of the classiﬁer deciding either way. A classiﬁer with a large margin\\nmakes no low certainty classiﬁcation decisions. This gives you a classiﬁca-\\ntion safety margin: a slight error in measurement or a slight document vari-\\nation will not cause a misclassiﬁcation. Another intuition motivating SVMs\\nis shown in Figure 15.2. By construction, an SVM classiﬁer insists on a large\\nmargin around the decision boundary. Compared to a decision hyperplane,\\nif you have to place a fat separator between classes, you have fewer choices\\nof where it can be put. As a result of this, the memory capacity of the model\\nhas been decreased, and hence we expect that its ability to correctly general-\\nize to test data is increased (cf. the discussion of the bias-variance tradeoff in\\nChapter 14, page 312).\\nLet us formalize an SVM with algebra. A decision hyperplane (page 302)\\ncan be deﬁned by an intercept term b and a decision hyperplane normal vec-\\ntor ⃗w which is perpendicular to the hyperplane. This vector is commonly\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n322\\n15\\nSupport vector machines and machine learning on documents\\nreferred to in the machine learning literature as the weight vector. To choose\\nWEIGHT VECTOR\\namong all the hyperplanes that are perpendicular to the normal vector, we\\nspecify the intercept term b. Because the hyperplane is perpendicular to the\\nnormal vector, all points ⃗x on the hyperplane satisfy ⃗wT⃗x = −b. Now sup-\\npose that we have a set of training data points D = {(⃗xi, yi)}, where each\\nmember is a pair of a point ⃗xi and a class label yi corresponding to it.1 For\\nSVMs, the two data classes are always named +1 and −1 (rather than 1 and\\n0), and the intercept term is always explicitly represented as b (rather than\\nbeing folded into the weight vector ⃗w by adding an extra always-on feature).\\nThe math works out much more cleanly if you do things this way, as we will\\nsee almost immediately in the deﬁnition of functional margin. The linear\\nclassiﬁer is then:\\nf(⃗x) = sign(⃗wT⃗x + b)\\n(15.1)\\nA value of −1 indicates one class, and a value of +1 the other class.\\nWe are conﬁdent in the classiﬁcation of a point if it is far away from the\\ndecision boundary. For a given data set and decision hyperplane, we deﬁne\\nthe functional margin of the ith example ⃗xi with respect to a hyperplane ⟨⃗w, b⟩\\nFUNCTIONAL MARGIN\\nas the quantity yi(⃗wT⃗xi + b). The functional margin of a data set with re-\\nspect to a decision surface is then twice the functional margin of any of the\\npoints in the data set with minimal functional margin (the factor of 2 comes\\nfrom measuring across the whole width of the margin, as in Figure 15.3).\\nHowever, there is a problem with using this deﬁnition as is: the value is un-\\nderconstrained, because we can always make the functional margin as big\\nas we wish by simply scaling up ⃗w and b. For example, if we replace ⃗w by\\n5⃗w and b by 5b then the functional margin yi(5⃗wT⃗xi + 5b) is ﬁve times as\\nlarge. This suggests that we need to place some constraint on the size of the\\n⃗w vector. To get a sense of how to do that, let us look at the actual geometry.\\nWhat is the Euclidean distance from a point ⃗x to the decision boundary? In\\nFigure 15.3, we denote by r this distance. We know that the shortest distance\\nbetween a point and a hyperplane is perpendicular to the plane, and hence,\\nparallel to ⃗w. A unit vector in this direction is ⃗w/|⃗w|. The dotted line in the\\ndiagram is then a translation of the vector r⃗w/|⃗w|. Let us label the point on\\nthe hyperplane closest to ⃗x as ⃗x′. Then:\\n⃗x′ = ⃗x −yr ⃗w\\n|⃗w|\\n(15.2)\\nwhere multiplying by y just changes the sign for the two cases of ⃗x being on\\neither side of the decision surface. Moreover,⃗x′ lies on the decision boundary\\n1. As discussed in Section 14.1 (page 291), we present the general case of points in a vector\\nspace, but if the points are length normalized document vectors, then all the action is taking\\nplace on the surface of a unit sphere, and the decision surface intersects the sphere’s surface.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n323\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nu\\nt\\nu\\nt\\nu\\nt⃗x\\n+\\n⃗x′\\nr\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nρ\\n⃗w\\n◮Figure 15.3\\nThe geometric margin of a point (r) and a decision boundary (ρ).\\nand so satisﬁes ⃗wT⃗x′ + b = 0. Hence:\\n⃗wT\\x00⃗x −yr ⃗w\\n|⃗w|\\n\\x01 + b = 0\\n(15.3)\\nSolving for r gives:2\\nr = y ⃗wT⃗x + b\\n|⃗w|\\n(15.4)\\nAgain, the points closest to the separating hyperplane are support vectors.\\nThe geometric margin of the classiﬁer is the maximum width of the band that\\nGEOMETRIC MARGIN\\ncan be drawn separating the support vectors of the two classes. That is, it is\\ntwice the minimum value over data points for r given in Equation (15.4), or,\\nequivalently, the maximal width of one of the fat separators shown in Fig-\\nure 15.2. The geometric margin is clearly invariant to scaling of parameters:\\nif we replace ⃗w by 5⃗w and b by 5b, then the geometric margin is the same, be-\\ncause it is inherently normalized by the length of ⃗w. This means that we can\\nimpose any scaling constraint we wish on ⃗w without affecting the geometric\\nmargin. Among other choices, we could use unit vectors, as in Chapter 6, by\\n2. Recall that |⃗w| =\\n√\\n⃗wT⃗w.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n324\\n15\\nSupport vector machines and machine learning on documents\\nrequiring that |⃗w| = 1. This would have the effect of making the geometric\\nmargin the same as the functional margin.\\nSince we can scale the functional margin as we please, for convenience in\\nsolving large SVMs, let us choose to require that the functional margin of all\\ndata points is at least 1 and that it is equal to 1 for at least one data vector.\\nThat is, for all items in the data:\\nyi(⃗wT⃗xi + b) ≥1\\n(15.5)\\nand there exist support vectors for which the inequality is an equality. Since\\neach example’s distance from the hyperplane is ri = yi(⃗wT⃗xi + b)/|⃗w|, the\\ngeometric margin is ρ = 2/|⃗w|. Our desire is still to maximize this geometric\\nmargin. That is, we want to ﬁnd ⃗w and b such that:\\n• ρ = 2/|⃗w| is maximized\\n• For all (⃗xi, yi) ∈D, yi(⃗wT⃗xi + b) ≥1\\nMaximizing 2/|⃗w| is the same as minimizing |⃗w|/2. This gives the ﬁnal stan-\\ndard formulation of an SVM as a minimization problem:\\n(15.6)\\nFind ⃗w and b such that:\\n•\\n1\\n2 ⃗wT⃗w is minimized, and\\n• for all {(⃗xi, yi)}, yi(⃗wT⃗xi + b) ≥1\\nWe are now optimizing a quadratic function subject to linear constraints.\\nQuadratic optimization problems are a standard, well-known class of mathe-\\nQUADRATIC\\nPROGRAMMING\\nmatical optimization problems, and many algorithms exist for solving them.\\nWe could in principle build our SVM using standard quadratic programming\\n(QP) libraries, but there has been much recent research in this area aiming to\\nexploit the structure of the kind of QP that emerges from an SVM. As a result,\\nthere are more intricate but much faster and more scalable libraries available\\nespecially for building SVMs, which almost everyone uses to build models.\\nWe will not present the details of such algorithms here.\\nHowever, it will be helpful to what follows to understand the shape of the\\nsolution of such an optimization problem. The solution involves construct-\\ning a dual problem where a Lagrange multiplier αi is associated with each\\nconstraint yi(⃗wT⃗xi + b) ≥1 in the primal problem:\\n(15.7)\\nFind α1, . . . αN such that ∑αi −1\\n2 ∑i ∑j αiαjyiyj⃗xi\\nT⃗xj is maximized, and\\n• ∑i αiyi = 0\\n• αi ≥0 for all 1 ≤i ≤N\\nThe solution is then of the form:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n325\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\nb\\nb\\nu\\nt\\n◮Figure 15.4\\nA tiny 3 data point training set for an SVM.\\n(15.8)\\n⃗w = ∑αiyi⃗xi\\nb = yk −⃗wT⃗xk for any ⃗xk such that αk ̸= 0\\nIn the solution, most of the αi are zero. Each non-zero αi indicates that the\\ncorresponding ⃗xi is a support vector. The classiﬁcation function is then:\\nf(⃗x) = sign(∑i αiyi⃗xiT⃗x + b)\\n(15.9)\\nBoth the term to be maximized in the dual problem and the classifying func-\\ntion involve a dot product between pairs of points (⃗x and ⃗xi or ⃗xi and ⃗xj), and\\nthat is the only way the data are used – we will return to the signiﬁcance of\\nthis later.\\nTo recap, we start with a training data set. The data set uniquely deﬁnes\\nthe best separating hyperplane, and we feed the data through a quadratic\\noptimization procedure to ﬁnd this plane. Given a new point ⃗x to classify,\\nthe classiﬁcation function f(⃗x) in either Equation (15.1) or Equation (15.9) is\\ncomputing the projection of the point onto the hyperplane normal. The sign\\nof this function determines the class to assign to the point. If the point is\\nwithin the margin of the classiﬁer (or another conﬁdence threshold t that we\\nmight have determined to minimize classiﬁcation mistakes) then the classi-\\nﬁer can return “don’t know” rather than one of the two classes. The value\\nof f(⃗x) may also be transformed into a probability of classiﬁcation; ﬁtting\\na sigmoid to transform the values is standard (Platt 2000). Also, since the\\nmargin is constant, if the model includes dimensions from various sources,\\ncareful rescaling of some dimensions may be required. However, this is not\\na problem if our documents (points) are on the unit hypersphere.\\n\\x0f\\nExample 15.1:\\nConsider building an SVM over the (very little) data set shown in\\nFigure 15.4. Working geometrically, for an example like this, the maximum margin\\nweight vector will be parallel to the shortest line connecting points of the two classes,\\nthat is, the line between (1, 1) and (2, 3), giving a weight vector of (1, 2). The opti-\\nmal decision surface is orthogonal to that line and intersects it at the halfway point.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n326\\n15\\nSupport vector machines and machine learning on documents\\nTherefore, it passes through (1.5, 2). So, the SVM decision boundary is:\\ny = x1 + 2x2 −5.5\\nWorking algebraically, with the standard constraint that sign(yi(⃗wT⃗xi + b)) ≥1,\\nwe seek to minimize |⃗w|. This happens when this constraint is satisﬁed with equality\\nby the two support vectors. Further we know that the solution is ⃗w = (a, 2a) for some\\na. So we have that:\\na + 2a + b\\n=\\n−1\\n2a + 6a + b\\n=\\n1\\nTherefore, a = 2/5 and b = −11/5. So the optimal hyperplane is given by ⃗w =\\n(2/5, 4/5) and b = −11/5.\\nThe margin ρ is 2/|⃗w| = 2/√\\n4/25 + 16/25 = 2/(2\\n√\\n5/5) =\\n√\\n5. This answer can\\nbe conﬁrmed geometrically by examining Figure 15.4.\\n?\\nExercise 15.1\\n[⋆]\\nWhat is the minimum number of support vectors that there can be for a data set\\n(which contains instances of each class)?\\nExercise 15.2\\n[⋆⋆]\\nThe basis of being able to use kernels in SVMs (see Section 15.2.3) is that the classiﬁca-\\ntion function can be written in the form of Equation (15.9) (where, for large problems,\\nmost αi are 0). Show explicitly how the classiﬁcation function could be written in this\\nform for the data set from Example 15.1. That is, write f as a function where the data\\npoints appear and the only variable is ⃗x.\\nExercise 15.3\\n[⋆⋆]\\nInstall an SVM package such as SVMlight (http://svmlight.joachims.org/), and build an\\nSVM for the data set discussed in Example 15.1. Conﬁrm that the program gives the\\nsame solution as the text. For SVMlight, or another package that accepts the same\\ntraining data format, the training ﬁle would be:\\n+1 1:2 2:3\\n−1 1:2 2:0\\n−1 1:1 2:1\\nThe training command for SVMlight is then:\\nsvm_learn -c 1 -a alphas.dat train.dat model.dat\\nThe -c 1 option is needed to turn off use of the slack variables that we discuss in\\nSection 15.2.1. Check that the norm of the weight vector agrees with what we found\\nin Example 15.1. Examine the ﬁle alphas.dat which contains the αi values, and check\\nthat they agree with your answers in Exercise 15.2.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n327\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n⃗xi\\nξi\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\n⃗xj\\nξj\\n◮Figure 15.5\\nLarge margin classiﬁcation with slack variables.\\n15.2\\nExtensions to the SVM model\\n15.2.1\\nSoft margin classiﬁcation\\nFor the very high dimensional problems common in text classiﬁcation, some-\\ntimes the data are linearly separable. But in the general case they are not, and\\neven if they are, we might prefer a solution that better separates the bulk of\\nthe data while ignoring a few weird noise documents.\\nIf the training set D is not linearly separable, the standard approach is to\\nallow the fat decision margin to make a few mistakes (some points – outliers\\nor noisy examples – are inside or on the wrong side of the margin). We then\\npay a cost for each misclassiﬁed example, which depends on how far it is\\nfrom meeting the margin requirement given in Equation (15.5). To imple-\\nment this, we introduce slack variables ξi. A non-zero value for ξi allows ⃗xi to\\nSLACK VARIABLES\\nnot meet the margin requirement at a cost proportional to the value of ξi. See\\nFigure 15.5.\\nThe formulation of the SVM optimization problem with slack variables is:\\n(15.10)\\nFind ⃗w, b, and ξi ≥0 such that:\\n•\\n1\\n2⃗wT⃗w + C ∑i ξi is minimized\\n• and for all {(⃗xi, yi)}, yi(⃗wT⃗xi + b) ≥1 −ξi\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n328\\n15\\nSupport vector machines and machine learning on documents\\nThe optimization problem is then trading off how fat it can make the margin\\nversus how many points have to be moved around to allow this margin.\\nThe margin can be less than 1 for a point ⃗xi by setting ξi > 0, but then one\\npays a penalty of Cξi in the minimization for having done that. The sum of\\nthe ξi gives an upper bound on the number of training errors. Soft-margin\\nSVMs minimize training error traded off against margin. The parameter C\\nis a regularization term, which provides a way to control overﬁtting: as C\\nREGULARIZATION\\nbecomes large, it is unattractive to not respect the data at the cost of reducing\\nthe geometric margin; when it is small, it is easy to account for some data\\npoints with the use of slack variables and to have a fat margin placed so it\\nmodels the bulk of the data.\\nThe dual problem for soft margin classiﬁcation becomes:\\n(15.11)\\nFind α1, . . . αN such that ∑αi −1\\n2 ∑i ∑j αiαjyiyj⃗xi\\nT⃗xj is maximized, and\\n• ∑i αiyi = 0\\n• 0 ≤αi ≤C for all 1 ≤i ≤N\\nNeither the slack variables ξi nor Lagrange multipliers for them appear in the\\ndual problem. All we are left with is the constant C bounding the possible\\nsize of the Lagrange multipliers for the support vector data points. As before,\\nthe ⃗xi with non-zero αi will be the support vectors. The solution of the dual\\nproblem is of the form:\\n(15.12)\\n⃗w = ∑αyi⃗xi\\nb = yk(1 −ξk) −⃗wT⃗xk for k = arg maxk αk\\nAgain ⃗w is not needed explicitly for classiﬁcation, which can be done in terms\\nof dot products with data points, as in Equation (15.9).\\nTypically, the support vectors will be a small proportion of the training\\ndata. However, if the problem is non-separable or with small margin, then\\nevery data point which is misclassiﬁed or within the margin will have a non-\\nzero αi. If this set of points becomes large, then, for the nonlinear case which\\nwe turn to in Section 15.2.3, this can be a major slowdown for using SVMs at\\ntest time.\\nThe complexity of training and testing with linear SVMs is shown in Ta-\\nble 15.1.3 The time for training an SVM is dominated by the time for solving\\nthe underlying QP, and so the theoretical and empirical complexity varies de-\\npending on the method used to solve it. The standard result for solving QPs\\nis that it takes time cubic in the size of the data set (Kozlov et al. 1979). All the\\nrecent work on SVM training has worked to reduce that complexity, often by\\n3. We write Θ(|D|Lave) for Θ(T) (page 262) and assume that the length of test documents is\\nbounded as we did on page 262.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n329\\nClassiﬁer\\nMode\\nMethod\\nTime complexity\\nNB\\ntraining\\nΘ(|D|Lave + |C||V|)\\nNB\\ntesting\\nΘ(|C|Ma)\\nRocchio\\ntraining\\nΘ(|D|Lave + |C||V|)\\nRocchio\\ntesting\\nΘ(|C|Ma)\\nkNN\\ntraining\\npreprocessing\\nΘ(|D|Lave)\\nkNN\\ntesting\\npreprocessing\\nΘ(|D|MaveMa)\\nkNN\\ntraining\\nno preprocessing\\nΘ(1)\\nkNN\\ntesting\\nno preprocessing\\nΘ(|D|LaveMa)\\nSVM\\ntraining\\nconventional\\nO(|C||D|3Mave);\\n≈O(|C||D|1.7Mave), empirically\\nSVM\\ntraining\\ncutting planes\\nO(|C||D|Mave)\\nSVM\\ntesting\\nO(|C|Ma)\\n◮Table 15.1\\nTraining and testing complexity of various classiﬁers including SVMs.\\nTraining is the time the learning method takes to learn a classiﬁer over D, while test-\\ning is the time it takes a classiﬁer to classify one document. For SVMs, multiclass\\nclassiﬁcation is assumed to be done by a set of |C| one-versus-rest classiﬁers. Lave is\\nthe average number of tokens per document, while Mave is the average vocabulary\\n(number of non-zero features) of a document. La and Ma are the numbers of tokens\\nand types, respectively, in the test document.\\nbeing satisﬁed with approximate solutions. Standardly, empirical complex-\\nity is about O(|D|1.7) (Joachims 2006a). Nevertheless, the super-linear train-\\ning time of traditional SVM algorithms makes them difﬁcult or impossible\\nto use on very large training data sets. Alternative traditional SVM solu-\\ntion algorithms which are linear in the number of training examples scale\\nbadly with a large number of features, which is another standard attribute\\nof text problems. However, a new training algorithm based on cutting plane\\ntechniques gives a promising answer to this issue by having running time\\nlinear in the number of training examples and the number of non-zero fea-\\ntures in examples (Joachims 2006a). Nevertheless, the actual speed of doing\\nquadratic optimization remains much slower than simply counting terms as\\nis done in a Naive Bayes model. Extending SVM algorithms to nonlinear\\nSVMs, as in the next section, standardly increases training complexity by a\\nfactor of |D| (since dot products between examples need to be calculated),\\nmaking them impractical. In practice it can often be cheaper to materialize\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n330\\n15\\nSupport vector machines and machine learning on documents\\nthe higher-order features and to train a linear SVM.4\\n15.2.2\\nMulticlass SVMs\\nSVMs are inherently two-class classiﬁers. The traditional way to do mul-\\nticlass classiﬁcation with SVMs is to use one of the methods discussed in\\nSection 14.5 (page 306). In particular, the most common technique in prac-\\ntice has been to build |C| one-versus-rest classiﬁers (commonly referred to as\\n“one-versus-all” or OVA classiﬁcation), and to choose the class which classi-\\nﬁes the test datum with greatest margin. Another strategy is to build a set\\nof one-versus-one classiﬁers, and to choose the class that is selected by the\\nmost classiﬁers. While this involves building |C|(|C| −1)/2 classiﬁers, the\\ntime for training classiﬁers may actually decrease, since the training data set\\nfor each classiﬁer is much smaller.\\nHowever, these are not very elegant approaches to solving multiclass prob-\\nlems. A better alternative is provided by the construction of multiclass SVMs,\\nwhere we build a two-class classiﬁer over a feature vector Φ(⃗x, y) derived\\nfrom the pair consisting of the input features and the class of the datum. At\\ntest time, the classiﬁer chooses the class y = arg maxy′ ⃗wTΦ(⃗x, y′). The mar-\\ngin during training is the gap between this value for the correct class and\\nfor the nearest other class, and so the quadratic program formulation will\\nrequire that ∀i ∀y ̸= yi ⃗wTΦ(⃗xi, yi) −⃗wTΦ(⃗xi, y) ≥1 −ξi. This general\\nmethod can be extended to give a multiclass formulation of various kinds of\\nlinear classiﬁers. It is also a simple instance of a generalization of classiﬁca-\\ntion where the classes are not just a set of independent, categorical labels, but\\nmay be arbitrary structured objects with relationships deﬁned between them.\\nIn the SVM world, such work comes under the label of structural SVMs. We\\nSTRUCTURAL SVMS\\nmention them again in Section 15.4.2.\\n15.2.3\\nNonlinear SVMs\\nWith what we have presented so far, data sets that are linearly separable (per-\\nhaps with a few exceptions or some noise) are well-handled. But what are\\nwe going to do if the data set just doesn’t allow classiﬁcation by a linear clas-\\nsiﬁer? Let us look at a one-dimensional case. The top data set in Figure 15.6\\nis straightforwardly classiﬁed by a linear classiﬁer but the middle data set is\\nnot. We instead need to be able to pick out an interval. One way to solve this\\nproblem is to map the data on to a higher dimensional space and then to use\\na linear classiﬁer in the higher dimensional space. For example, the bottom\\npart of the ﬁgure shows that a linear separator can easily classify the data\\n4. Materializing the features refers to directly calculating higher order and interaction terms\\nand then putting them into a linear model.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n331\\n◮Figure 15.6\\nProjecting data that is not linearly separable into a higher dimensional\\nspace can make it linearly separable.\\nif we use a quadratic function to map the data into two dimensions (a po-\\nlar coordinates projection would be another possibility). The general idea is\\nto map the original feature space to some higher-dimensional feature space\\nwhere the training set is separable. Of course, we would want to do so in\\nways that preserve relevant dimensions of relatedness between data points,\\nso that the resultant classiﬁer should still generalize well.\\nSVMs, and also a number of other linear classiﬁers, provide an easy and\\nefﬁcient way of doing this mapping to a higher dimensional space, which is\\nreferred to as “the kernel trick”. It’s not really a trick: it just exploits the math\\nKERNEL TRICK\\nthat we have seen. The SVM linear classiﬁer relies on a dot product between\\ndata point vectors. Let K(⃗xi,⃗xj) = ⃗xiT⃗xj. Then the classiﬁer we have seen so\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n332\\n15\\nSupport vector machines and machine learning on documents\\nfar is:\\nf(⃗x) = sign(∑\\ni\\nαiyiK(⃗xi,⃗x) + b)\\n(15.13)\\nNow suppose we decide to map every data point into a higher dimensional\\nspace via some transformation Φ:⃗x 7→φ(⃗x). Then the dot product becomes\\nφ(⃗xi)Tφ(⃗xj). If it turned out that this dot product (which is just a real num-\\nber) could be computed simply and efﬁciently in terms of the original data\\npoints, then we wouldn’t have to actually map from ⃗x 7→φ(⃗x). Rather, we\\ncould simply compute the quantity K(⃗xi,⃗xj) = φ(⃗xi)Tφ(⃗xj), and then use the\\nfunction’s value in Equation (15.13). A kernel function K is such a function\\nKERNEL FUNCTION\\nthat corresponds to a dot product in some expanded feature space.\\n\\x0f\\nExample 15.2: The quadratic kernel in two dimensions.\\nFor 2-dimensional\\nvectors ⃗u = (u1\\nu2), ⃗v = (v1\\nv2), consider K(⃗u,⃗v) = (1 + ⃗uT⃗v)2. We wish to\\nshow that this is a kernel, i.e., that K(⃗u,⃗v) = φ(⃗u)Tφ(⃗v) for some φ. Consider φ(⃗u) =\\n(1 u2\\n1\\n√\\n2u1u2 u2\\n2\\n√\\n2u1\\n√\\n2u2). Then:\\nK(⃗u,⃗v)\\n=\\n(1 +⃗uT⃗v)2\\n(15.14)\\n=\\n1 + u2\\n1v2\\n1 + 2u1v1u2v2 + u2\\n2v2\\n2 + 2u1v1 + 2u2v2\\n=\\n(1 u2\\n1\\n√\\n2u1u2 u2\\n2\\n√\\n2u1\\n√\\n2u2)T(1 v2\\n1\\n√\\n2v1v2 v2\\n2\\n√\\n2v1\\n√\\n2v2)\\n=\\nφ(⃗u)Tφ(⃗v)\\nIn the language of functional analysis, what kinds of functions are valid\\nkernel functions? Kernel functions are sometimes more precisely referred to\\nKERNEL\\nas Mercer kernels, because they must satisfy Mercer’s condition: for any g(⃗x)\\nMERCER KERNEL\\nsuch that R\\ng(⃗x)2d⃗x is ﬁnite, we must have that:\\nZ\\nK(⃗x,⃗z)g(⃗x)g(⃗z)d⃗xd⃗z ≥0 .\\n(15.15)\\nA kernel function K must be continuous, symmetric, and have a positive def-\\ninite gram matrix. Such a K means that there exists a mapping to a reproduc-\\ning kernel Hilbert space (a Hilbert space is a vector space closed under dot\\nproducts) such that the dot product there gives the same value as the function\\nK. If a kernel does not satisfy Mercer’s condition, then the corresponding QP\\nmay have no solution. If you would like to better understand these issues,\\nyou should consult the books on SVMs mentioned in Section 15.5. Other-\\nwise, you can content yourself with knowing that 90% of work with kernels\\nuses one of two straightforward families of functions of two vectors, which\\nwe deﬁne below, and which deﬁne valid kernels.\\nThe two commonly used families of kernels are polynomial kernels and\\nradial basis functions. Polynomial kernels are of the form K(⃗x,⃗z) = (1 +\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n333\\n⃗xT⃗z)d. The case of d = 1 is a linear kernel, which is what we had before the\\nstart of this section (the constant 1 just changing the threshold). The case of\\nd = 2 gives a quadratic kernel, and is very commonly used. We illustrated\\nthe quadratic kernel in Example 15.2.\\nThe most common form of radial basis function is a Gaussian distribution,\\ncalculated as:\\nK(⃗x,⃗z) = e−(⃗x−⃗z)2/(2σ2)\\n(15.16)\\nA radial basis function (rbf) is equivalent to mapping the data into an inﬁ-\\nnite dimensional Hilbert space, and so we cannot illustrate the radial basis\\nfunction concretely, as we did a quadratic kernel. Beyond these two families,\\nthere has been interesting work developing other kernels, some of which is\\npromising for text applications. In particular, there has been investigation of\\nstring kernels (see Section 15.5).\\nThe world of SVMs comes with its own language, which is rather different\\nfrom the language otherwise used in machine learning. The terminology\\ndoes have deep roots in mathematics, but it’s important not to be too awed\\nby that terminology. Really, we are talking about some quite simple things. A\\npolynomial kernel allows us to model feature conjunctions (up to the order of\\nthe polynomial). That is, if we want to be able to model occurrences of pairs\\nof words, which give distinctive information about topic classiﬁcation, not\\ngiven by the individual words alone, like perhaps operating AND system or\\nethnic AND cleansing, then we need to use a quadratic kernel. If occurrences\\nof triples of words give distinctive information, then we need to use a cubic\\nkernel. Simultaneously you also get the powers of the basic features – for\\nmost text applications, that probably isn’t useful, but just comes along with\\nthe math and hopefully doesn’t do harm. A radial basis function allows you\\nto have features that pick out circles (hyperspheres) – although the decision\\nboundaries become much more complex as multiple such features interact. A\\nstring kernel lets you have features that are character subsequences of terms.\\nAll of these are straightforward notions which have also been used in many\\nother places under different names.\\n15.2.4\\nExperimental results\\nWe presented results in Section 13.6 showing that an SVM is a very effec-\\ntive text classiﬁer. The results of Dumais et al. (1998) given in Table 13.9\\nshow SVMs clearly performing the best. This was one of several pieces of\\nwork from this time that established the strong reputation of SVMs for text\\nclassiﬁcation. Another pioneering work on scaling and evaluating SVMs\\nfor text classiﬁcation was (Joachims 1998). We present some of his results\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n334\\n15\\nSupport vector machines and machine learning on documents\\nRoc-\\nDec.\\nlinear SVM\\nrbf-SVM\\nNB\\nchio\\nTrees\\nkNN\\nC = 0.5\\nC = 1.0\\nσ ≈7\\nearn\\n96.0\\n96.1\\n96.1\\n97.8\\n98.0\\n98.2\\n98.1\\nacq\\n90.7\\n92.1\\n85.3\\n91.8\\n95.5\\n95.6\\n94.7\\nmoney-fx\\n59.6\\n67.6\\n69.4\\n75.4\\n78.8\\n78.5\\n74.3\\ngrain\\n69.8\\n79.5\\n89.1\\n82.6\\n91.9\\n93.1\\n93.4\\ncrude\\n81.2\\n81.5\\n75.5\\n85.8\\n89.4\\n89.4\\n88.7\\ntrade\\n52.2\\n77.4\\n59.2\\n77.9\\n79.2\\n79.2\\n76.6\\ninterest\\n57.6\\n72.5\\n49.1\\n76.7\\n75.6\\n74.8\\n69.1\\nship\\n80.9\\n83.1\\n80.9\\n79.8\\n87.4\\n86.5\\n85.8\\nwheat\\n63.4\\n79.4\\n85.5\\n72.9\\n86.6\\n86.8\\n82.4\\ncorn\\n45.2\\n62.2\\n87.7\\n71.4\\n87.5\\n87.8\\n84.6\\nmicroavg.\\n72.3\\n79.9\\n79.4\\n82.6\\n86.7\\n87.5\\n86.4\\n◮Table 15.2\\nSVM classiﬁer break-even F1 from (Joachims 2002a, p. 114).\\nResults\\nare shown for the 10 largest categories and for microaveraged performance over all\\n90 categories on the Reuters-21578 data set.\\nfrom (Joachims 2002a) in Table 15.2.5 Joachims used a large number of term\\nfeatures in contrast to Dumais et al. (1998), who used MI feature selection\\n(Section 13.5.1, page 272) to build classiﬁers with a much more limited num-\\nber of features. The success of the linear SVM mirrors the results discussed\\nin Section 14.6 (page 308) on other linear approaches like Naive Bayes. It\\nseems that working with simple term features can get one a long way. It is\\nagain noticeable the extent to which different papers’ results for the same ma-\\nchine learning methods differ. In particular, based on replications by other\\nresearchers, the Naive Bayes results of (Joachims 1998) appear too weak, and\\nthe results in Table 13.9 should be taken as representative.\\n15.3\\nIssues in the classiﬁcation of text documents\\nThere are lots of applications of text classiﬁcation in the commercial world;\\nemail spam ﬁltering is perhaps now the most ubiquitous. Jackson and Mou-\\nlinier (2002) write: “There is no question concerning the commercial value of\\nbeing able to classify documents automatically by content. There are myriad\\n5. These results are in terms of the break-even F1 (see Section 8.4). Many researchers disprefer\\nthis measure for text classiﬁcation evaluation, since its calculation may involve interpolation\\nrather than an actual parameter setting of the system and it is not clear why this value should\\nbe reported rather than maximal F1 or another point on the precision/recall curve motivated by\\nthe task at hand. While earlier results in (Joachims 1998) suggested notable gains on this task\\nfrom the use of higher order polynomial or rbf kernels, this was with hard-margin SVMs. With\\nsoft-margin SVMs, a simple linear SVM with the default C = 1 performs best.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n335\\npotential applications of such a capability for corporate Intranets, govern-\\nment departments, and Internet publishers.”\\nMost of our discussion of classiﬁcation has focused on introducing various\\nmachine learning methods rather than discussing particular features of text\\ndocuments relevant to classiﬁcation. This bias is appropriate for a textbook,\\nbut is misplaced for an application developer. It is frequently the case that\\ngreater performance gains can be achieved from exploiting domain-speciﬁc\\ntext features than from changing from one machine learning method to an-\\nother. Jackson and Moulinier (2002) suggest that “Understanding the data\\nis one of the keys to successful categorization, yet this is an area in which\\nmost categorization tool vendors are extremely weak. Many of the ‘one size\\nﬁts all’ tools on the market have not been tested on a wide range of content\\ntypes.” In this section we wish to step back a little and consider the applica-\\ntions of text classiﬁcation, the space of possible solutions, and the utility of\\napplication-speciﬁc heuristics.\\n15.3.1\\nChoosing what kind of classiﬁer to use\\nWhen confronted with a need to build a text classiﬁer, the ﬁrst question to\\nask is how much training data is there currently available? None? Very little?\\nQuite a lot? Or a huge amount, growing every day? Often one of the biggest\\npractical challenges in ﬁelding a machine learning classiﬁer in real applica-\\ntions is creating or obtaining enough training data. For many problems and\\nalgorithms, hundreds or thousands of examples from each class are required\\nto produce a high performance classiﬁer and many real world contexts in-\\nvolve large sets of categories. We will initially assume that the classiﬁer is\\nneeded as soon as possible; if a lot of time is available for implementation,\\nmuch of it might be spent on assembling data resources.\\nIf you have no labeled training data, and especially if there are existing\\nstaff knowledgeable about the domain of the data, then you should never\\nforget the solution of using hand-written rules. That is, you write standing\\nqueries, as we touched on at the beginning of Chapter 13. For example:\\nIF (wheat OR grain) AND NOT (whole OR bread) THEN c = grain\\nIn practice, rules get a lot bigger than this, and can be phrased using more\\nsophisticated query languages than just Boolean expressions, including the\\nuse of numeric scores. With careful crafting (that is, by humans tuning the\\nrules on development data), the accuracy of such rules can become very high.\\nJacobs and Rau (1990) report identifying articles about takeovers with 92%\\nprecision and 88.5% recall, and Hayes and Weinstein (1990) report 94% re-\\ncall and 84% precision over 675 categories on Reuters newswire documents.\\nNevertheless the amount of work to create such well-tuned rules is very\\nlarge. A reasonable estimate is 2 days per class, and extra time has to go\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n336\\n15\\nSupport vector machines and machine learning on documents\\ninto maintenance of rules, as the content of documents in classes drifts over\\ntime (cf. page 269).\\nIf you have fairly little data and you are going to train a supervised clas-\\nsiﬁer, then machine learning theory says you should stick to a classiﬁer with\\nhigh bias, as we discussed in Section 14.6 (page 308). For example, there\\nare theoretical and empirical results that Naive Bayes does well in such cir-\\ncumstances (Ng and Jordan 2001, Forman and Cohen 2004), although this\\neffect is not necessarily observed in practice with regularized models over\\ntextual data (Klein and Manning 2002). At any rate, a very low bias model\\nlike a nearest neighbor model is probably counterindicated. Regardless, the\\nquality of the model will be adversely affected by the limited training data.\\nHere, the theoretically interesting answer is to try to apply semi-supervised\\nSEMI-SUPERVISED\\nLEARNING\\ntraining methods. This includes methods such as bootstrapping or the EM\\nalgorithm, which we will introduce in Section 16.5 (page 368). In these meth-\\nods, the system gets some labeled documents, and a further large supply\\nof unlabeled documents over which it can attempt to learn. One of the big\\nadvantages of Naive Bayes is that it can be straightforwardly extended to\\nbe a semi-supervised learning algorithm, but for SVMs, there is also semi-\\nsupervised learning work which goes under the title of transductive SVMs.\\nTRANSDUCTIVE SVMS\\nSee the references for pointers.\\nOften, the practical answer is to work out how to get more labeled data as\\nquickly as you can. The best way to do this is to insert yourself into a process\\nwhere humans will be willing to label data for you as part of their natural\\ntasks. For example, in many cases humans will sort or route email for their\\nown purposes, and these actions give information about classes. The alter-\\nnative of getting human labelers expressly for the task of training classiﬁers\\nis often difﬁcult to organize, and the labeling is often of lower quality, be-\\ncause the labels are not embedded in a realistic task context. Rather than\\ngetting people to label all or a random sample of documents, there has also\\nbeen considerable research on active learning, where a system is built which\\nACTIVE LEARNING\\ndecides which documents a human should label. Usually these are the ones\\non which a classiﬁer is uncertain of the correct classiﬁcation. This can be ef-\\nfective in reducing annotation costs by a factor of 2–4, but has the problem\\nthat the good documents to label to train one type of classiﬁer often are not\\nthe good documents to label to train a different type of classiﬁer.\\nIf there is a reasonable amount of labeled data, then you are in the per-\\nfect position to use everything that we have presented about text classiﬁ-\\ncation. For instance, you may wish to use an SVM. However, if you are\\ndeploying a linear classiﬁer such as an SVM, you should probably design\\nan application that overlays a Boolean rule-based classiﬁer over the machine\\nlearning classiﬁer. Users frequently like to adjust things that do not come\\nout quite right, and if management gets on the phone and wants the classi-\\nﬁcation of a particular document ﬁxed right now, then this is much easier to\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n337\\ndo by hand-writing a rule than by working out how to adjust the weights\\nof an SVM without destroying the overall classiﬁcation accuracy. This is one\\nreason why machine learning models like decision trees which produce user-\\ninterpretable Boolean-like models retain considerable popularity.\\nIf a huge amount of data are available, then the choice of classiﬁer probably\\nhas little effect on your results and the best choice may be unclear (cf. Banko\\nand Brill 2001). It may be best to choose a classiﬁer based on the scalability\\nof training or even runtime efﬁciency. To get to this point, you need to have\\nhuge amounts of data. The general rule of thumb is that each doubling of\\nthe training data size produces a linear increase in classiﬁer performance,\\nbut with very large amounts of data, the improvement becomes sub-linear.\\n15.3.2\\nImproving classiﬁer performance\\nFor any particular application, there is usually signiﬁcant room for improv-\\ning classiﬁer effectiveness through exploiting features speciﬁc to the domain\\nor document collection. Often documents will contain zones which are espe-\\ncially useful for classiﬁcation. Often there will be particular subvocabularies\\nwhich demand special treatment for optimal classiﬁcation effectiveness.\\nLarge and difﬁcult category taxonomies\\nIf a text classiﬁcation problem consists of a small number of well-separated\\ncategories, then many classiﬁcation algorithms are likely to work well. But\\nmany real classiﬁcation problems consist of a very large number of often\\nvery similar categories. The reader might think of examples like web direc-\\ntories (the Yahoo! Directory or the Open Directory Project), library classi-\\nﬁcation schemes (Dewey Decimal or Library of Congress) or the classiﬁca-\\ntion schemes used in legal or medical applications. For instance, the Yahoo!\\nDirectory consists of over 200,000 categories in a deep hierarchy. Accurate\\nclassiﬁcation over large sets of closely related classes is inherently difﬁcult.\\nMost large sets of categories have a hierarchical structure, and attempting\\nto exploit the hierarchy by doing hierarchical classiﬁcation is a promising ap-\\nHIERARCHICAL\\nCLASSIFICATION\\nproach. However, at present the effectiveness gains from doing this rather\\nthan just working with the classes that are the leaves of the hierarchy re-\\nmain modest.6 But the technique can be very useful simply to improve the\\nscalability of building classiﬁers over large hierarchies. Another simple way\\nto improve the scalability of classiﬁers over large hierarchies is the use of\\naggressive feature selection. We provide references to some work on hierar-\\nchical classiﬁcation in Section 15.5.\\n6. Using the small hierarchy in Figure 13.1 (page 257) as an example, the leaf classes are ones\\nlike poultry and coffee, as opposed to higher-up classes like industries.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n338\\n15\\nSupport vector machines and machine learning on documents\\nA general result in machine learning is that you can always get a small\\nboost in classiﬁcation accuracy by combining multiple classiﬁers, provided\\nonly that the mistakes that they make are at least somewhat independent.\\nThere is now a large literature on techniques such as voting, bagging, and\\nboosting multiple classiﬁers. Again, there are some pointers in the refer-\\nences. Nevertheless, ultimately a hybrid automatic/manual solution may be\\nneeded to achieve sufﬁcient classiﬁcation accuracy. A common approach in\\nsuch situations is to run a classiﬁer ﬁrst, and to accept all its high conﬁdence\\ndecisions, but to put low conﬁdence decisions in a queue for manual review.\\nSuch a process also automatically leads to the production of new training\\ndata which can be used in future versions of the machine learning classiﬁer.\\nHowever, note that this is a case in point where the resulting training data is\\nclearly not randomly sampled from the space of documents.\\nFeatures for text\\nThe default in both ad hoc retrieval and text classiﬁcation is to use terms\\nas features. However, for text classiﬁcation, a great deal of mileage can be\\nachieved by designing additional features which are suited to a speciﬁc prob-\\nlem. Unlike the case of IR query languages, since these features are internal\\nto the classiﬁer, there is no problem of communicating these features to an\\nend user. This process is generally referred to as feature engineering. At pre-\\nFEATURE ENGINEERING\\nsent, feature engineering remains a human craft, rather than something done\\nby machine learning. Good feature engineering can often markedly improve\\nthe performance of a text classiﬁer. It is especially beneﬁcial in some of the\\nmost important applications of text classiﬁcation, like spam and porn ﬁlter-\\ning.\\nClassiﬁcation problems will often contain large numbers of terms which\\ncan be conveniently grouped, and which have a similar vote in text classi-\\nﬁcation problems. Typical examples might be year mentions or strings of\\nexclamation marks. Or they may be more specialized tokens like ISBNs or\\nchemical formulas. Often, using them directly in a classiﬁer would greatly in-\\ncrease the vocabulary without providing classiﬁcatory power beyond know-\\ning that, say, a chemical formula is present. In such cases, the number of\\nfeatures and feature sparseness can be reduced by matching such items with\\nregular expressions and converting them into distinguished tokens. Con-\\nsequently, effectiveness and classiﬁer speed are normally enhanced. Some-\\ntimes all numbers are converted into a single feature, but often some value\\ncan be had by distinguishing different kinds of numbers, such as four digit\\nnumbers (which are usually years) versus other cardinal numbers versus real\\nnumbers with a decimal point. Similar techniques can be applied to dates,\\nISBN numbers, sports game scores, and so on.\\nGoing in the other direction, it is often useful to increase the number of fea-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n339\\ntures by matching parts of words, and by matching selected multiword pat-\\nterns that are particularly discriminative. Parts of words are often matched\\nby character k-gram features. Such features can be particularly good at pro-\\nviding classiﬁcation clues for otherwise unknown words when the classiﬁer\\nis deployed. For instance, an unknown word ending in -rase is likely to be an\\nenzyme, even if it wasn’t seen in the training data. Good multiword patterns\\nare often found by looking for distinctively common word pairs (perhaps\\nusing a mutual information criterion between words, in a similar way to\\nits use in Section 13.5.1 (page 272) for feature selection) and then using fea-\\nture selection methods evaluated against classes. They are useful when the\\ncomponents of a compound would themselves be misleading as classiﬁca-\\ntion cues. For instance, this would be the case if the keyword ethnic was\\nmost indicative of the categories food and arts, the keyword cleansing was\\nmost indicative of the category home, but the collocation ethnic cleansing in-\\nstead indicates the category world news. Some text classiﬁers also make use\\nof features from named entity recognizers (cf. page 195).\\nDo techniques like stemming and lowercasing (Section 2.2, page 22) help\\nfor text classiﬁcation? As always, the ultimate test is empirical evaluations\\nconducted on an appropriate test collection. But it is nevertheless useful to\\nnote that such techniques have a more restricted chance of being useful for\\nclassiﬁcation. For IR, you often need to collapse forms of a word like oxy-\\ngenate and oxygenation, because the appearance of either in a document is a\\ngood clue that the document will be relevant to a query about oxygenation.\\nGiven copious training data, stemming necessarily delivers no value for text\\nclassiﬁcation. If several forms that stem together have a similar signal, the\\nparameters estimated for all of them will have similar weights. Techniques\\nlike stemming help only in compensating for data sparseness. This can be\\na useful role (as noted at the start of this section), but often different forms\\nof a word can convey signiﬁcantly different cues about the correct document\\nclassiﬁcation. Overly aggressive stemming can easily degrade classiﬁcation\\nperformance.\\nDocument zones in text classiﬁcation\\nAs already discussed in Section 6.1, documents usually have zones, such as\\nmail message headers like the subject and author, or the title and keywords\\nof a research article. Text classiﬁers can usually gain from making use of\\nthese zones during training and classiﬁcation.\\nUpweighting document zones.\\nIn text classiﬁcation problems, you can fre-\\nquently get a nice boost to effectiveness by differentially weighting contri-\\nbutions from different document zones. Often, upweighting title words is\\nparticularly effective (Cohen and Singer 1999, p. 163). As a rule of thumb,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n340\\n15\\nSupport vector machines and machine learning on documents\\nit is often effective to double the weight of title words in text classiﬁcation\\nproblems. You can also get value from upweighting words from pieces of\\ntext that are not so much clearly deﬁned zones, but where nevertheless evi-\\ndence from document structure or content suggests that they are important.\\nMurata et al. (2000) suggest that you can also get value (in an ad hoc retrieval\\ncontext) from upweighting the ﬁrst sentence of a (newswire) document.\\nSeparate feature spaces for document zones.\\nThere are two strategies that\\ncan be used for document zones. Above we upweighted words that appear\\nin certain zones. This means that we are using the same features (that is, pa-\\nrameters are “tied” across different zones), but we pay more attention to the\\nPARAMETER TYING\\noccurrence of terms in particular zones. An alternative strategy is to have a\\ncompletely separate set of features and corresponding parameters for words\\noccurring in different zones. This is in principle more powerful: a word\\ncould usually indicate the topic Middle East when in the title but Commodities\\nwhen in the body of a document. But, in practice, tying parameters is usu-\\nally more successful. Having separate feature sets means having two or more\\ntimes as many parameters, many of which will be much more sparsely seen\\nin the training data, and hence with worse estimates, whereas upweighting\\nhas no bad effects of this sort. Moreover, it is quite uncommon for words to\\nhave different preferences when appearing in different zones; it is mainly the\\nstrength of their vote that should be adjusted. Nevertheless, ultimately this\\nis a contingent result, depending on the nature and quantity of the training\\ndata.\\nConnections to text summarization.\\nIn Section 8.7, we mentioned the ﬁeld\\nof text summarization, and how most work in that ﬁeld has adopted the\\nlimited goal of extracting and assembling pieces of the original text that are\\njudged to be central based on features of sentences that consider the sen-\\ntence’s position and content. Much of this work can be used to suggest zones\\nthat may be distinctively useful for text classiﬁcation. For example Kołcz\\net al. (2000) consider a form of feature selection where you classify docu-\\nments based only on words in certain zones. Based on text summarization\\nresearch, they consider using (i) only the title, (ii) only the ﬁrst paragraph,\\n(iii) only the paragraph with the most title words or keywords, (iv) the ﬁrst\\ntwo paragraphs or the ﬁrst and last paragraph, or (v) all sentences with a\\nminimum number of title words or keywords. In general, these positional\\nfeature selection methods produced as good results as mutual information\\n(Section 13.5.1), and resulted in quite competitive classiﬁers. Ko et al. (2004)\\nalso took inspiration from text summarization research to upweight sen-\\ntences with either words from the title or words that are central to the doc-\\nument’s content, leading to classiﬁcation accuracy gains of almost 1%. This\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n341\\npresumably works because most such sentences are somehow more central\\nto the concerns of the document.\\n?\\nExercise 15.4\\n[⋆⋆]\\nSpam email often makes use of various cloaking techniques to try to get through. One\\nmethod is to pad or substitute characters so as to defeat word-based text classiﬁers.\\nFor example, you see terms like the following in spam email:\\nRep1icaRolex\\nbonmus\\nViiiaaaagra\\npi11z\\nPHARlbdMACY\\n[LEV]i[IT]l[RA]\\nse∧xual\\nClAfLlS\\nDiscuss how you could engineer features that would largely defeat this strategy.\\nExercise 15.5\\n[⋆⋆]\\nAnother strategy often used by purveyors of email spam is to follow the message\\nthey wish to send (such as buying a cheap stock or whatever) with a paragraph of\\ntext from another innocuous source (such as a news article). Why might this strategy\\nbe effective? How might it be addressed by a text classiﬁer?\\nExercise 15.6\\n[⋆]\\nWhat other kinds of features appear as if they would be useful in an email spam\\nclassiﬁer?\\n15.4\\nMachine learning methods in ad hoc information retrieval\\nRather than coming up with term and document weighting functions by\\nhand, as we primarily did in Chapter 6, we can view different sources of rele-\\nvance signal (cosine score, title match, etc.) as features in a learning problem.\\nA classiﬁer that has been fed examples of relevant and nonrelevant docu-\\nments for each of a set of queries can then ﬁgure out the relative weights\\nof these signals. If we conﬁgure the problem so that there are pairs of a\\ndocument and a query which are assigned a relevance judgment of relevant\\nor nonrelevant, then we can think of this problem too as a text classiﬁcation\\nproblem. Taking such a classiﬁcation approach is not necessarily best, and\\nwe present an alternative in Section 15.4.2. Nevertheless, given the material\\nwe have covered, the simplest place to start is to approach this problem as\\na classiﬁcation problem, by ordering the documents according to the conﬁ-\\ndence of a two-class classiﬁer in its relevance decision. And this move is not\\npurely pedagogical; exactly this approach is sometimes used in practice.\\n15.4.1\\nA simple example of machine-learned scoring\\nIn this section we generalize the methodology of Section 6.1.2 (page 113) to\\nmachine learning of the scoring function. In Section 6.1.2 we considered a\\ncase where we had to combine Boolean indicators of relevance; here we con-\\nsider more general factors to further develop the notion of machine-learned\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n342\\n15\\nSupport vector machines and machine learning on documents\\nExample\\nDocID\\nQuery\\nCosine score\\nω\\nJudgment\\nΦ1\\n37\\nlinux operating system\\n0.032\\n3\\nrelevant\\nΦ2\\n37\\npenguin logo\\n0.02\\n4\\nnonrelevant\\nΦ3\\n238\\noperating system\\n0.043\\n2\\nrelevant\\nΦ4\\n238\\nruntime environment\\n0.004\\n2\\nnonrelevant\\nΦ5\\n1741\\nkernel layer\\n0.022\\n3\\nrelevant\\nΦ6\\n2094\\ndevice driver\\n0.03\\n2\\nrelevant\\nΦ7\\n3191\\ndevice driver\\n0.027\\n5\\nnonrelevant\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n◮Table 15.3\\nTraining examples for machine-learned scoring.\\nrelevance. In particular, the factors we now consider go beyond Boolean\\nfunctions of query term presence in document zones, as in Section 6.1.2.\\nWe develop the ideas in a setting where the scoring function is a linear\\ncombination of two factors: (1) the vector space cosine similarity between\\nquery and document and (2) the minimum window width ω within which\\nthe query terms lie. As we noted in Section 7.2.2 (page 144), query term\\nproximity is often very indicative of a document being on topic, especially\\nwith longer documents and on the web. Among other things, this quantity\\ngives us an implementation of implicit phrases. Thus we have one factor that\\ndepends on the statistics of query terms in the document as a bag of words,\\nand another that depends on proximity weighting. We consider only two\\nfeatures in the development of the ideas because a two-feature exposition\\nremains simple enough to visualize. The technique can be generalized to\\nmany more features.\\nAs in Section 6.1.2, we are provided with a set of training examples, each\\nof which is a pair consisting of a query and a document, together with a\\nrelevance judgment for that document on that query that is either relevant or\\nnonrelevant. For each such example we can compute the vector space cosine\\nsimilarity, as well as the window width ω. The result is a training set as\\nshown in Table 15.3, which resembles Figure 6.5 (page 115) from Section 6.1.2.\\nHere, the two features (cosine score denoted α and window width ω) are\\nreal-valued predictors. If we once again quantify the judgment relevant as 1\\nand nonrelevant as 0, we seek a scoring function that combines the values of\\nthe features to generate a value that is (close to) 0 or 1. We wish this func-\\ntion to be in agreement with our set of training examples as far as possible.\\nWithout loss of generality, a linear classiﬁer will use a linear combination of\\nfeatures of the form\\nScore(d, q) = Score(α, ω) = aα + bω + c,\\n(15.17)\\nwith the coefﬁcients a, b, c to be learned from the training data. While it is\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n343\\n0\\n2\\n3\\n4\\n5\\n0\\n.\\n0\\n5\\n0\\n.\\n0\\n2\\n5\\nc\\no\\ns\\ni\\nn\\ne\\ns\\nc\\no\\nr\\ne\\n\\r\\nT\\ne\\nr\\nm\\np\\nr\\no\\nx\\ni\\nm\\ni\\nt\\ny\\n\\x18\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\n◮Figure 15.7\\nA collection of training examples. Each R denotes a training example\\nlabeled relevant, while each N is a training example labeled nonrelevant.\\npossible to formulate this as an error minimization problem as we did in\\nSection 6.1.2, it is instructive to visualize the geometry of Equation (15.17).\\nThe examples in Table 15.3 can be plotted on a two-dimensional plane with\\naxes corresponding to the cosine score α and the window width ω. This is\\ndepicted in Figure 15.7.\\nIn this setting, the function Score(α, ω) from Equation (15.17) represents\\na plane “hanging above” Figure 15.7. Ideally this plane (in the direction\\nperpendicular to the page containing Figure 15.7) assumes values close to\\n1 above the points marked R, and values close to 0 above the points marked\\nN. Since a plane is unlikely to assume only values close to 0 or 1 above the\\ntraining sample points, we make use of thresholding: given any query and\\ndocument for which we wish to determine relevance, we pick a value θ and\\nif Score(α, ω) > θ we declare the document to be relevant, else we declare\\nthe document to be nonrelevant. As we know from Figure 14.8 (page 301),\\nall points that satisfy Score(α, ω) = θ form a line (shown as a dashed line\\nin Figure 15.7) and we thus have a linear classiﬁer that separates relevant\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n344\\n15\\nSupport vector machines and machine learning on documents\\nfrom nonrelevant instances. Geometrically, we can ﬁnd the separating line\\nas follows. Consider the line passing through the plane Score(α, ω) whose\\nheight is θ above the page containing Figure 15.7. Project this line down onto\\nFigure 15.7; this will be the dashed line in Figure 15.7. Then, any subse-\\nquent query/document pair that falls below the dashed line in Figure 15.7 is\\ndeemed nonrelevant; above the dashed line, relevant.\\nThus, the problem of making a binary relevant/nonrelevant judgment given\\ntraining examples as above turns into one of learning the dashed line in Fig-\\nure 15.7 separating relevant training examples from the nonrelevant ones. Be-\\ning in the α-ω plane, this line can be written as a linear equation involving\\nα and ω, with two parameters (slope and intercept). The methods of lin-\\near classiﬁcation that we have already looked at in Chapters 13–15 provide\\nmethods for choosing this line. Provided we can build a sufﬁciently rich col-\\nlection of training samples, we can thus altogether avoid hand-tuning score\\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\\nto maintain a suitably representative set of training examples, whose rele-\\nvance assessments must be made by experts.\\n15.4.2\\nResult ranking by machine learning\\nThe above ideas can be readily generalized to functions of many more than\\ntwo variables. There are lots of other scores that are indicative of the rel-\\nevance of a document to a query, including static quality (PageRank-style\\nmeasures, discussed in Chapter 21), document age, zone contributions, doc-\\nument length, and so on. Providing that these measures can be calculated\\nfor a training document collection with relevance judgments, any number\\nof such measures can be used to train a machine learning classiﬁer. For in-\\nstance, we could train an SVM over binary relevance judgments, and order\\ndocuments based on their probability of relevance, which is monotonic with\\nthe documents’ signed distance from the decision boundary.\\nHowever, approaching IR result ranking like this is not necessarily the\\nright way to think about the problem.\\nStatisticians normally ﬁrst divide\\nproblems into classiﬁcation problems (where a categorical variable is pre-\\ndicted) versus regression problems (where a real number is predicted). In\\nREGRESSION\\nbetween is the specialized ﬁeld of ordinal regression where a ranking is pre-\\nORDINAL REGRESSION\\ndicted. Machine learning for ad hoc retrieval is most properly thought of as\\nan ordinal regression problem, where the goal is to rank a set of documents\\nfor a query, given training data of the same sort. This formulation gives\\nsome additional power, since documents can be evaluated relative to other\\ncandidate documents for the same query, rather than having to be mapped\\nto a global scale of goodness, while also weakening the problem space, since\\njust a ranking is required rather than an absolute measure of relevance. Is-\\nsues of ranking are especially germane in web search, where the ranking at\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n345\\nthe very top of the results list is exceedingly important, whereas decisions\\nof relevance of a document to a query may be much less important. Such\\nwork can and has been pursued using the structural SVM framework which\\nwe mentioned in Section 15.2.2, where the class being predicted is a ranking\\nof results for a query, but here we will present the slightly simpler ranking\\nSVM.\\nThe construction of a ranking SVM proceeds as follows. We begin with a\\nRANKING SVM\\nset of judged queries. For each training query q, we have a set of documents\\nreturned in response to the query, which have been totally ordered by a per-\\nson for relevance to the query. We construct a vector of features ψj = ψ(dj, q)\\nfor each document/query pair, using features such as those discussed in Sec-\\ntion 15.4.1, and many more. For two documents di and dj, we then form the\\nvector of feature differences:\\nΦ(di, dj, q) = ψ(di, q) −ψ(dj, q)\\n(15.18)\\nBy hypothesis, one of di and dj has been judged more relevant. If di is\\njudged more relevant than dj, denoted di ≺dj (di should precede dj in the\\nresults ordering), then we will assign the vector Φ(di, dj, q) the class yijq =\\n+1; otherwise −1. The goal then is to build a classiﬁer which will return\\n⃗wTΦ(di, dj, q) > 0\\niff\\ndi ≺dj\\n(15.19)\\nThis SVM learning task is formalized in a manner much like the other exam-\\nples that we saw before:\\n(15.20)\\nFind ⃗w, and ξi,j ≥0 such that:\\n•\\n1\\n2⃗wT⃗w + C ∑i,j ξi,j is minimized\\n• and for all {Φ(di, dj, q) : di ≺dj}, ⃗wTΦ(di, dj, q) ≥1 −ξi,j\\nWe can leave out yijq in the statement of the constraint, since we only need\\nto consider the constraint for document pairs ordered in one direction, since\\n≺is antisymmetric. These constraints are then solved, as before, to give\\na linear classiﬁer which can rank pairs of documents. This approach has\\nbeen used to build ranking functions which outperform standard hand-built\\nranking functions in IR evaluations on standard data sets; see the references\\nfor papers that present such results.\\nBoth of the methods that we have just looked at use a linear weighting\\nof document features that are indicators of relevance, as has most work in\\nthis area. It is therefore perhaps interesting to note that much of traditional\\nIR weighting involves nonlinear scaling of basic measurements (such as log-\\nweighting of term frequency, or idf). At the present time, machine learning is\\nvery good at producing optimal weights for features in a linear combination\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n346\\n15\\nSupport vector machines and machine learning on documents\\n(or other similar restricted model classes), but it is not good at coming up\\nwith good nonlinear scalings of basic measurements. This area remains the\\ndomain of human feature engineering.\\nThe idea of learning ranking functions has been around for a number of\\nyears, but it is only very recently that sufﬁcient machine learning knowledge,\\ntraining document collections, and computational power have come together\\nto make this method practical and exciting. It is thus too early to write some-\\nthing deﬁnitive on machine learning approaches to ranking in information\\nretrieval, but there is every reason to expect the use and importance of ma-\\nchine learned ranking approaches to grow over time. While skilled humans\\ncan do a very good job at deﬁning ranking functions by hand, hand tuning\\nis difﬁcult, and it has to be done again for each new document collection and\\nclass of users.\\n?\\nExercise 15.7\\nPlot the ﬁrst 7 rows of Table 15.3 in the α-ω plane to produce a ﬁgure like that in\\nFigure 15.7.\\nExercise 15.8\\nWrite down the equation of a line in the α-ω plane separating the Rs from the Ns.\\nExercise 15.9\\nGive a training example (consisting of values for α, ω and the relevance judgment)\\nthat when added to the training set makes it impossible to separate the R’s from the\\nN’s using a line in the α-ω plane.\\n15.5\\nReferences and further reading\\nThe somewhat quirky name support vector machine originates in the neu-\\nral networks literature, where learning algorithms were thought of as ar-\\nchitectures, and often referred to as “machines”. The distinctive element of\\nthis model is that the decision boundary to use is completely decided (“sup-\\nported”) by a few training data points, the support vectors.\\nFor a more detailed presentation of SVMs, a good, well-known article-\\nlength introduction is (Burges 1998). Chen et al. (2005) introduce the more\\nrecent ν-SVM, which provides an alternative parameterization for dealing\\nwith inseparable problems, whereby rather than specifying a penalty C, you\\nspecify a parameter ν which bounds the number of examples which can ap-\\npear on the wrong side of the decision surface. There are now also several\\nbooks dedicated to SVMs, large margin learning, and kernels: (Cristianini\\nand Shawe-Taylor 2000) and (Schölkopf and Smola 2001) are more math-\\nematically oriented, while (Shawe-Taylor and Cristianini 2004) aims to be\\nmore practical. For the foundations by their originator, see (Vapnik 1998).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n15.5\\nReferences and further reading\\n347\\nSome recent, more general books on statistical learning, such as (Hastie et al.\\n2001) also give thorough coverage of SVMs.\\nThe construction of multiclass SVMs is discussed in (Weston and Watkins\\n1999), (Crammer and Singer 2001), and (Tsochantaridis et al. 2005). The last\\nreference provides an introduction to the general framework of structural\\nSVMs.\\nThe kernel trick was ﬁrst presented in (Aizerman et al. 1964). For more\\nabout string kernels and other kernels for structured data, see (Lodhi et al.\\n2002) and (Gaertner et al. 2002). The Advances in Neural Information Pro-\\ncessing (NIPS) conferences have become the premier venue for theoretical\\nmachine learning work, such as on SVMs. Other venues such as SIGIR are\\nmuch stronger on experimental methodology and using text-speciﬁc features\\nto improve classiﬁer effectiveness.\\nA recent comparison of most current machine learning classiﬁers (though\\non problems rather different from typical text problems) can be found in\\n(Caruana and Niculescu-Mizil 2006). (Li and Yang 2003), discussed in Sec-\\ntion 13.6, is the most recent comparative evaluation of machine learning clas-\\nsiﬁers on text classiﬁcation. Older examinations of classiﬁers on text prob-\\nlems can be found in (Yang 1999, Yang and Liu 1999, Dumais et al. 1998).\\nJoachims (2002a) presents his work on SVMs applied to text problems in de-\\ntail. Zhang and Oles (2001) present an insightful comparison of Naive Bayes,\\nregularized logistic regression and SVM classiﬁers.\\nJoachims (1999) discusses methods of making SVM learning practical over\\nlarge text data sets. Joachims (2006a) improves on this work.\\nA number of approaches to hierarchical classiﬁcation have been developed\\nin order to deal with the common situation where the classes to be assigned\\nhave a natural hierarchical organization (Koller and Sahami 1997, McCal-\\nlum et al. 1998, Weigend et al. 1999, Dumais and Chen 2000). In a recent\\nlarge study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005)\\nconclude that hierarchical classiﬁcation noticeably if still modestly outper-\\nforms ﬂat classiﬁcation. Classiﬁer effectiveness remains limited by the very\\nsmall number of training documents for many classes. For a more general\\napproach that can be applied to modeling relations between classes, which\\nmay be arbitrary rather than simply the case of a hierarchy, see Tsochan-\\ntaridis et al. (2005).\\nMoschitti and Basili (2004) investigate the use of complex nominals, proper\\nnouns and word senses as features in text classiﬁcation.\\nDietterich (2002) overviews ensemble methods for classiﬁer combination,\\nwhile Schapire (2003) focuses particularly on boosting, which is applied to\\ntext classiﬁcation in (Schapire and Singer 2000).\\nChapelle et al. (2006) present an introduction to work in semi-supervised\\nmethods, including in particular chapters on using EM for semi-supervised\\ntext classiﬁcation (Nigam et al. 2006) and on transductive SVMs (Joachims\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n348\\n15\\nSupport vector machines and machine learning on documents\\n2006b). Sindhwani and Keerthi (2006) present a more efﬁcient implementa-\\ntion of a transductive SVM for large data sets.\\nTong and Koller (2001) explore active learning with SVMs for text classi-\\nﬁcation; Baldridge and Osborne (2004) point out that examples selected for\\nannotation with one classiﬁer in an active learning context may be no better\\nthan random examples when used with another classiﬁer.\\nMachine learning approaches to ranking for ad hoc retrieval were pio-\\nneered in (Wong et al. 1988), (Fuhr 1992), and (Gey 1994). But limited training\\ndata and poor machine learning techniques meant that these pieces of work\\nachieved only middling results, and hence they only had limited impact at\\nthe time.\\nTaylor et al. (2006) study using machine learning to tune the parameters\\nof the BM25 family of ranking functions (Section 11.4.3, page 232) so as to\\nmaximize NDCG (Section 8.4, page 163). Machine learning approaches to\\nordinal regression appear in (Herbrich et al. 2000) and (Burges et al. 2005),\\nand are applied to clickstream data in (Joachims 2002b). Cao et al. (2006)\\nstudy how to make this approach effective in IR, and Qin et al. (2007) suggest\\nan extension involving using multiple hyperplanes. Yue et al. (2007) study\\nhow to do ranking with a structural SVM approach, and in particular show\\nhow this construction can be effectively used to directly optimize for MAP\\n(Section 8.4, page 158), rather than using surrogate measures like accuracy or\\narea under the ROC curve. Geng et al. (2007) study feature selection for the\\nranking problem.\\nOther approaches to learning to rank have also been shown to be effective\\nfor web search, such as (Burges et al. 2005, Richardson et al. 2006).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n349\\n16\\nFlat clustering\\nClustering algorithms group a set of documents into subsets or clusters. The\\nCLUSTER\\nalgorithms’ goal is to create clusters that are coherent internally, but clearly\\ndifferent from each other. In other words, documents within a cluster should\\nbe as similar as possible; and documents in one cluster should be as dissimi-\\nlar as possible from documents in other clusters.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n◮Figure 16.1\\nAn example of a data set with a clear cluster structure.\\nClustering is the most common form of unsupervised learning. No super-\\nUNSUPERVISED\\nLEARNING\\nvision means that there is no human expert who has assigned documents\\nto classes. In clustering, it is the distribution and makeup of the data that\\nwill determine cluster membership. A simple example is Figure 16.1. It is\\nvisually clear that there are three distinct clusters of points. This chapter and\\nChapter 17 introduce algorithms that ﬁnd such clusters in an unsupervised\\nfashion.\\nThe difference between clustering and classiﬁcation may not seem great\\nat ﬁrst. After all, in both cases we have a partition of a set of documents\\ninto groups. But as we will see the two problems are fundamentally differ-\\nent. Classiﬁcation is a form of supervised learning (Chapter 13, page 256):\\nour goal is to replicate a categorical distinction that a human supervisor im-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n350\\n16\\nFlat clustering\\nposes on the data. In unsupervised learning, of which clustering is the most\\nimportant example, we have no such teacher to guide us.\\nThe key input to a clustering algorithm is the distance measure. In Fig-\\nure 16.1, the distance measure is distance in the 2D plane. This measure sug-\\ngests three different clusters in the ﬁgure. In document clustering, the dis-\\ntance measure is often also Euclidean distance. Different distance measures\\ngive rise to different clusterings. Thus, the distance measure is an important\\nmeans by which we can inﬂuence the outcome of clustering.\\nFlat clustering creates a ﬂat set of clusters without any explicit structure that\\nFLAT CLUSTERING\\nwould relate clusters to each other. Hierarchical clustering creates a hierarchy\\nof clusters and will be covered in Chapter 17. Chapter 17 also addresses the\\ndifﬁcult problem of labeling clusters automatically.\\nA second important distinction can be made between hard and soft cluster-\\ning algorithms. Hard clustering computes a hard assignment – each document\\nHARD CLUSTERING\\nis a member of exactly one cluster. The assignment of soft clustering algo-\\nSOFT CLUSTERING\\nrithms is soft – a document’s assignment is a distribution over all clusters.\\nIn a soft assignment, a document has fractional membership in several clus-\\nters. Latent semantic indexing, a form of dimensionality reduction, is a soft\\nclustering algorithm (Chapter 18, page 417).\\nThis chapter motivates the use of clustering in information retrieval by\\nintroducing a number of applications (Section 16.1), deﬁnes the problem\\nwe are trying to solve in clustering (Section 16.2) and discusses measures\\nfor evaluating cluster quality (Section 16.3). It then describes two ﬂat clus-\\ntering algorithms, K-means (Section 16.4), a hard clustering algorithm, and\\nthe Expectation-Maximization (or EM) algorithm (Section 16.5), a soft clus-\\ntering algorithm. K-means is perhaps the most widely used ﬂat clustering\\nalgorithm due to its simplicity and efﬁciency. The EM algorithm is a gen-\\neralization of K-means and can be applied to a large variety of document\\nrepresentations and distributions.\\n16.1\\nClustering in information retrieval\\nThe cluster hypothesis states the fundamental assumption we make when us-\\nCLUSTER HYPOTHESIS\\ning clustering in information retrieval.\\nCluster hypothesis. Documents in the same cluster behave similarly\\nwith respect to relevance to information needs.\\nThe hypothesis states that if there is a document from a cluster that is rele-\\nvant to a search request, then it is likely that other documents from the same\\ncluster are also relevant. This is because clustering puts together documents\\nthat share many terms. The cluster hypothesis essentially is the contiguity\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.1\\nClustering in information retrieval\\n351\\nApplication\\nWhat is\\nBeneﬁt\\nExample\\nclustered?\\nSearch result clustering\\nsearch\\nresults\\nmore effective information\\npresentation to user\\nFigure 16.2\\nScatter-Gather\\n(subsets\\nof)\\ncollection\\nalternative user interface:\\n“search without typing”\\nFigure 16.3\\nCollection clustering\\ncollection\\neffective information pre-\\nsentation for exploratory\\nbrowsing\\nMcKeown et al. (2002),\\nhttp://news.google.com\\nLanguage modeling\\ncollection\\nincreased precision and/or\\nrecall\\nLiu and Croft (2004)\\nCluster-based retrieval\\ncollection\\nhigher efﬁciency:\\nfaster\\nsearch\\nSalton (1971a)\\n◮Table 16.1\\nSome applications of clustering in information retrieval.\\nhypothesis in Chapter 14 (page 289). In both cases, we posit that similar\\ndocuments behave similarly with respect to relevance.\\nTable 16.1 shows some of the main applications of clustering in informa-\\ntion retrieval. They differ in the set of documents that they cluster – search\\nresults, collection or subsets of the collection – and the aspect of an informa-\\ntion retrieval system they try to improve – user experience, user interface,\\neffectiveness or efﬁciency of the search system. But they are all based on the\\nbasic assumption stated by the cluster hypothesis.\\nThe ﬁrst application mentioned in Table 16.1 is search result clustering where\\nSEARCH RESULT\\nCLUSTERING\\nby search results we mean the documents that were returned in response to\\na query. The default presentation of search results in information retrieval is\\na simple list. Users scan the list from top to bottom until they have found\\nthe information they are looking for. Instead, search result clustering clus-\\nters the search results, so that similar documents appear together. It is often\\neasier to scan a few coherent groups than many individual documents. This\\nis particularly useful if a search term has different word senses. The example\\nin Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the\\nanimal and an Apple operating system. The Clustered Results panel returned\\nby the Vivísimo search engine (http://vivisimo.com) can be a more effective user\\ninterface for understanding what is in the search results than a simple list of\\ndocuments.\\nA better user interface is also the goal of Scatter-Gather, the second ap-\\nSCATTER-GATHER\\nplication in Table 16.1. Scatter-Gather clusters the whole collection to get\\ngroups of documents that the user can select or gather. The selected groups\\nare merged and the resulting set is again clustered. This process is repeated\\nuntil a cluster of interest is found. An example is shown in Figure 16.3.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n352\\n16\\nFlat clustering\\n◮Figure 16.2\\nClustering of search results to improve recall.\\nNone of the top hits\\ncover the animal sense of jaguar, but users can easily access it by clicking on the cat\\ncluster in the Clustered Results panel on the left (third arrow from the top).\\nAutomatically generated clusters like those in Figure 16.3 are not as neatly\\norganized as a manually constructed hierarchical tree like the Open Direc-\\ntory at http://dmoz.org. Also, ﬁnding descriptive labels for clusters automati-\\ncally is a difﬁcult problem (Section 17.7, page 396). But cluster-based navi-\\ngation is an interesting alternative to keyword searching, the standard infor-\\nmation retrieval paradigm. This is especially true in scenarios where users\\nprefer browsing over searching because they are unsure about which search\\nterms to use.\\nAs an alternative to the user-mediated iterative clustering in Scatter-Gather,\\nwe can also compute a static hierarchical clustering of a collection that is\\nnot inﬂuenced by user interactions (“Collection clustering” in Table 16.1).\\nGoogle News and its precursor, the Columbia NewsBlaster system, are ex-\\namples of this approach. In the case of news, we need to frequently recom-\\npute the clustering to make sure that users can access the latest breaking\\nstories. Clustering is well suited for access to a collection of news stories\\nsince news reading is not really search, but rather a process of selecting a\\nsubset of stories about recent events.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.1\\nClustering in information retrieval\\n353\\n◮Figure 16.3\\nAn example of a user session in Scatter-Gather. A collection of New\\nYork Times news stories is clustered (“scattered”) into eight clusters (top row). The\\nuser manually gathers three of these into a smaller collection International Stories and\\nperforms another scattering operation. This process repeats until a small cluster with\\nrelevant documents is found (e.g., Trinidad).\\nThe fourth application of clustering exploits the cluster hypothesis directly\\nfor improving search results, based on a clustering of the entire collection.\\nWe use a standard inverted index to identify an initial set of documents that\\nmatch the query, but we then add other documents from the same clusters\\neven if they have low similarity to the query. For example, if the query is car\\nand several car documents are taken from a cluster of automobile documents,\\nthen we can add documents from this cluster that use terms other than car\\n(automobile, vehicle etc). This can increase recall since a group of documents\\nwith high mutual similarity is often relevant as a whole.\\nMore recently this idea has been used for language modeling. Equation (12.10),\\npage 245, showed that to avoid sparse data problems in the language mod-\\neling approach to IR, the model of document d can be interpolated with a\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n354\\n16\\nFlat clustering\\ncollection model. But the collection contains many documents with terms\\nuntypical of d. By replacing the collection model with a model derived from\\nd’s cluster, we get more accurate estimates of the occurrence probabilities of\\nterms in d.\\nClustering can also speed up search. As we saw in Section 6.3.2 (page 123)\\nsearch in the vector space model amounts to ﬁnding the nearest neighbors\\nto the query. The inverted index supports fast nearest-neighbor search for\\nthe standard IR setting. However, sometimes we may not be able to use an\\ninverted index efﬁciently, e.g., in latent semantic indexing (Chapter 18). In\\nsuch cases, we could compute the similarity of the query to every document,\\nbut this is slow. The cluster hypothesis offers an alternative: Find the clus-\\nters that are closest to the query and only consider documents from these\\nclusters. Within this much smaller set, we can compute similarities exhaus-\\ntively and rank documents in the usual way. Since there are many fewer\\nclusters than documents, ﬁnding the closest cluster is fast; and since the doc-\\numents matching a query are all similar to each other, they tend to be in\\nthe same clusters. While this algorithm is inexact, the expected decrease in\\nsearch quality is small. This is essentially the application of clustering that\\nwas covered in Section 7.1.6 (page 141).\\n?\\nExercise 16.1\\nDeﬁne two documents as similar if they have at least two proper names like Clinton\\nor Sarkozy in common. Give an example of an information need and two documents,\\nfor which the cluster hypothesis does not hold for this notion of similarity.\\nExercise 16.2\\nMake up a simple one-dimensional example (i.e. points on a line) with two clusters\\nwhere the inexactness of cluster-based retrieval shows up. In your example, retriev-\\ning clusters close to the query should do worse than direct nearest neighbor search.\\n16.2\\nProblem statement\\nWe can deﬁne the goal in hard ﬂat clustering as follows. Given (i) a set of\\ndocuments D = {d1, . . . , dN}, (ii) a desired number of clusters K, and (iii)\\nan objective function that evaluates the quality of a clustering, we want to\\nOBJECTIVE FUNCTION\\ncompute an assignment γ : D →{1, . . . , K} that minimizes (or, in other\\ncases, maximizes) the objective function. In most cases, we also demand that\\nγ is surjective, i.e., that none of the K clusters is empty.\\nThe objective function is often deﬁned in terms of similarity or distance\\nbetween documents. Below, we will see that the objective in K-means clus-\\ntering is to minimize the average distance between documents and their cen-\\ntroids or, equivalently, to maximize the similarity between documents and\\ntheir centroids. The discussion of similarity measures and distance metrics\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.2\\nProblem statement\\n355\\nin Chapter 14 (page 291) also applies to this chapter. As in Chapter 14, we use\\nboth similarity and distance to talk about relatedness between documents.\\nFor documents, the type of similarity we want is usually topic similarity\\nor high values on the same dimensions in the vector space model. For exam-\\nple, documents about China have high values on dimensions like Chinese,\\nBeijing, and Mao whereas documents about the UK tend to have high values\\nfor London, Britain and Queen. We approximate topic similarity with cosine\\nsimilarity or Euclidean distance in vector space (Chapter 6). If we intend to\\ncapture similarity of a type other than topic, for example, similarity of lan-\\nguage, then a different representation may be appropriate. When computing\\ntopic similarity, stop words can be safely ignored, but they are important\\ncues for separating clusters of English (in which the occurs frequently and la\\ninfrequently) and French documents (in which the occurs infrequently and la\\nfrequently).\\nA note on terminology.\\nAn alternative deﬁnition of hard clustering is that\\na document can be a full member of more than one cluster. Partitional clus-\\nPARTITIONAL\\nCLUSTERING\\ntering always refers to a clustering where each document belongs to exactly\\none cluster. (But in a partitional hierarchical clustering (Chapter 17) all mem-\\nbers of a cluster are of course also members of its parent.) On the deﬁnition\\nof hard clustering that permits multiple membership, the difference between\\nsoft clustering and hard clustering is that membership values in hard clus-\\ntering are either 0 or 1, whereas they can take on any non-negative value in\\nsoft clustering.\\nSome researchers distinguish between exhaustive clusterings that assign\\nEXHAUSTIVE\\neach document to a cluster and non-exhaustive clusterings, in which some\\ndocuments will be assigned to no cluster.\\nNon-exhaustive clusterings in\\nwhich each document is a member of either no cluster or one cluster are\\ncalled exclusive. We deﬁne clustering to be exhaustive in this book.\\nEXCLUSIVE\\n16.2.1\\nCardinality – the number of clusters\\nA difﬁcult issue in clustering is determining the number of clusters or cardi-\\nCARDINALITY\\nnality of a clustering, which we denote by K. Often K is nothing more than\\na good guess based on experience or domain knowledge. But for K-means,\\nwe will also introduce a heuristic method for choosing K and an attempt to\\nincorporate the selection of K into the objective function. Sometimes the ap-\\nplication puts constraints on the range of K. For example, the Scatter-Gather\\ninterface in Figure 16.3 could not display more than about K = 10 clusters\\nper layer because of the size and resolution of computer monitors in the early\\n1990s.\\nSince our goal is to optimize an objective function, clustering is essentially\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n356\\n16\\nFlat clustering\\na search problem. The brute force solution would be to enumerate all pos-\\nsible clusterings and pick the best. However, there are exponentially many\\npartitions, so this approach is not feasible.1 For this reason, most ﬂat clus-\\ntering algorithms reﬁne an initial partitioning iteratively. If the search starts\\nat an unfavorable initial point, we may miss the global optimum. Finding a\\ngood starting point is therefore another important problem we have to solve\\nin ﬂat clustering.\\n16.3\\nEvaluation of clustering\\nTypical objective functions in clustering formalize the goal of attaining high\\nintra-cluster similarity (documents within a cluster are similar) and low inter-\\ncluster similarity (documents from different clusters are dissimilar). This is\\nan internal criterion for the quality of a clustering. But good scores on an\\nINTERNAL CRITERION\\nOF QUALITY\\ninternal criterion do not necessarily translate into good effectiveness in an\\napplication. An alternative to internal criteria is direct evaluation in the ap-\\nplication of interest. For search result clustering, we may want to measure\\nthe time it takes users to ﬁnd an answer with different clustering algorithms.\\nThis is the most direct evaluation, but it is expensive, especially if large user\\nstudies are necessary.\\nAs a surrogate for user judgments, we can use a set of classes in an evalua-\\ntion benchmark or gold standard (see Section 8.5, page 164, and Section 13.6,\\npage 279). The gold standard is ideally produced by human judges with a\\ngood level of inter-judge agreement (see Chapter 8, page 152). We can then\\ncompute an external criterion that evaluates how well the clustering matches\\nEXTERNAL CRITERION\\nOF QUALITY\\nthe gold standard classes. For example, we may want to say that the opti-\\nmal clustering of the search results for jaguar in Figure 16.2 consists of three\\nclasses corresponding to the three senses car, animal, and operating system.\\nIn this type of evaluation, we only use the partition provided by the gold\\nstandard, not the class labels.\\nThis section introduces four external criteria of clustering quality. Purity is\\na simple and transparent evaluation measure. Normalized mutual information\\ncan be information-theoretically interpreted. The Rand index penalizes both\\nfalse positive and false negative decisions during clustering. The F measure\\nin addition supports differential weighting of these two types of errors.\\nTo compute purity, each cluster is assigned to the class which is most fre-\\nPURITY\\nquent in the cluster, and then the accuracy of this assignment is measured\\nby counting the number of correctly assigned documents and dividing by N.\\n1. An upper bound on the number of clusterings is KN/K!. The exact number of different\\npartitions of N documents into K clusters is the Stirling number of the second kind.\\nSee\\nhttp://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html or Comtet (1974).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.3\\nEvaluation of clustering\\n357\\nx\\no\\nx\\nx\\nx\\nx\\no\\nx\\no\\no ⋄\\no\\nx\\n⋄\\n⋄\\n⋄\\nx\\ncluster 1\\ncluster 2\\ncluster 3\\n◮Figure 16.4\\nPurity as an external evaluation criterion for cluster quality. Majority\\nclass and number of members of the majority class for the three clusters are: x, 5\\n(cluster 1); o, 4 (cluster 2); and ⋄, 3 (cluster 3). Purity is (1/17) × (5 + 4 + 3) ≈0.71.\\npurity\\nNMI\\nRI\\nF5\\nlower bound\\n0.0\\n0.0\\n0.0\\n0.0\\nmaximum\\n1\\n1\\n1\\n1\\nvalue for Figure 16.4\\n0.71\\n0.36\\n0.68\\n0.46\\n◮Table 16.2\\nThe four external evaluation measures applied to the clustering in\\nFigure 16.4.\\nFormally:\\npurity(Ω, C) = 1\\nN ∑\\nk\\nmax\\nj\\n|ωk ∩cj|\\n(16.1)\\nwhere Ω= {ω1, ω2, . . . , ωK} is the set of clusters and C = {c1, c2, . . . , cJ} is\\nthe set of classes. We interpret ωk as the set of documents in ωk and cj as the\\nset of documents in cj in Equation (16.1).\\nWe present an example of how to compute purity in Figure 16.4.2 Bad\\nclusterings have purity values close to 0, a perfect clustering has a purity of\\n1. Purity is compared with the other three measures discussed in this chapter\\nin Table 16.2.\\nHigh purity is easy to achieve when the number of clusters is large – in\\nparticular, purity is 1 if each document gets its own cluster. Thus, we cannot\\nuse purity to trade off the quality of the clustering against the number of\\nclusters.\\nA measure that allows us to make this tradeoff is normalized mutual infor-\\nNORMALIZED MUTUAL\\nINFORMATION\\n2. Recall our note of caution from Figure 14.2 (page 291) when looking at this and other 2D\\nﬁgures in this and the following chapter: these illustrations can be misleading because 2D pro-\\njections of length-normalized vectors distort similarities and distances between points.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n358\\n16\\nFlat clustering\\nmation or NMI:\\nNMI(Ω, C) =\\nI(Ω; C)\\n[H(Ω) + H(C)]/2\\n(16.2)\\nI is mutual information (cf. Chapter 13, page 272):\\nI(Ω; C)\\n= ∑\\nk ∑\\nj\\nP(ωk ∩cj) log P(ωk ∩cj)\\nP(ωk)P(cj)\\n(16.3)\\n= ∑\\nk ∑\\nj\\n|ωk ∩cj|\\nN\\nlog N|ωk ∩cj|\\n|ωk||cj|\\n(16.4)\\nwhere P(ωk), P(cj), and P(ωk ∩cj) are the probabilities of a document being\\nin cluster ωk, class cj, and in the intersection of ωk and cj, respectively. Equa-\\ntion (16.4) is equivalent to Equation (16.3) for maximum likelihood estimates\\nof the probabilities (i.e., the estimate of each probability is the corresponding\\nrelative frequency).\\nH is entropy as deﬁned in Chapter 5 (page 99):\\nH(Ω)\\n=\\n−∑\\nk\\nP(ωk) log P(ωk)\\n(16.5)\\n=\\n−∑\\nk\\n|ωk|\\nN log |ωk|\\nN\\n(16.6)\\nwhere, again, the second equation is based on maximum likelihood estimates\\nof the probabilities.\\nI(Ω; C) in Equation (16.3) measures the amount of information by which\\nour knowledge about the classes increases when we are told what the clusters\\nare. The minimum of I(Ω; C) is 0 if the clustering is random with respect to\\nclass membership. In that case, knowing that a document is in a particular\\ncluster does not give us any new information about what its class might be.\\nMaximum mutual information is reached for a clustering Ωexact that perfectly\\nrecreates the classes – but also if clusters in Ωexact are further subdivided into\\nsmaller clusters (Exercise 16.7). In particular, a clustering with K = N one-\\ndocument clusters has maximum MI. So MI has the same problem as purity:\\nit does not penalize large cardinalities and thus does not formalize our bias\\nthat, other things being equal, fewer clusters are better.\\nThe normalization by the denominator [H(Ω) + H(C)]/2 in Equation (16.2)\\nﬁxes this problem since entropy tends to increase with the number of clus-\\nters. For example, H(Ω) reaches its maximum log N for K = N, which en-\\nsures that NMI is low for K = N. Because NMI is normalized, we can use\\nit to compare clusterings with different numbers of clusters. The particular\\nform of the denominator is chosen because [H(Ω) + H(C)]/2 is a tight upper\\nbound on I(Ω; C) (Exercise 16.8). Thus, NMI is always a number between 0\\nand 1.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.3\\nEvaluation of clustering\\n359\\nAn alternative to this information-theoretic interpretation of clustering is\\nto view it as a series of decisions, one for each of the N(N −1)/2 pairs of\\ndocuments in the collection. We want to assign two documents to the same\\ncluster if and only if they are similar. A true positive (TP) decision assigns\\ntwo similar documents to the same cluster, a true negative (TN) decision as-\\nsigns two dissimilar documents to different clusters. There are two types\\nof errors we can commit. A false positive (FP) decision assigns two dissim-\\nilar documents to the same cluster. A false negative (FN) decision assigns\\ntwo similar documents to different clusters. The Rand index (RI) measures\\nRAND INDEX\\nRI\\nthe percentage of decisions that are correct. That is, it is simply accuracy\\n(Section 8.3, page 155).\\nRI =\\nTP + TN\\nTP + FP + FN + TN\\nAs an example, we compute RI for Figure 16.4. We ﬁrst compute TP + FP.\\nThe three clusters contain 6, 6, and 5 points, respectively, so the total number\\nof “positives” or pairs of documents that are in the same cluster is:\\nTP + FP =\\n\\x12\\n6\\n2\\n\\x13\\n+\\n\\x12\\n6\\n2\\n\\x13\\n+\\n\\x12\\n5\\n2\\n\\x13\\n= 40\\nOf these, the x pairs in cluster 1, the o pairs in cluster 2, the ⋄pairs in cluster 3,\\nand the x pair in cluster 3 are true positives:\\nTP =\\n\\x12\\n5\\n2\\n\\x13\\n+\\n\\x12\\n4\\n2\\n\\x13\\n+\\n\\x12\\n3\\n2\\n\\x13\\n+\\n\\x12\\n2\\n2\\n\\x13\\n= 20\\nThus, FP = 40 −20 = 20.\\nFN and TN are computed similarly, resulting in the following contingency\\ntable:\\nSame cluster\\nDifferent clusters\\nSame class\\nTP = 20\\nFN = 24\\nDifferent classes\\nFP = 20\\nTN = 72\\nRI is then (20 + 72)/(20 + 20 + 24 + 72) ≈0.68.\\nThe Rand index gives equal weight to false positives and false negatives.\\nSeparating similar documents is sometimes worse than putting pairs of dis-\\nsimilar documents in the same cluster. We can use the F measure (Section 8.3,\\nF MEASURE\\npage 154) to penalize false negatives more strongly than false positives by\\nselecting a value β > 1, thus giving more weight to recall.\\nP =\\nTP\\nTP + FP\\nR =\\nTP\\nTP + FN\\nFβ = (β2 + 1)PR\\nβ2P + R\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n360\\n16\\nFlat clustering\\nBased on the numbers in the contingency table, P = 20/40 = 0.5 and R =\\n20/44 ≈0.455. This gives us F1 ≈0.48 for β = 1 and F5 ≈0.456 for β = 5.\\nIn information retrieval, evaluating clustering with F has the advantage that\\nthe measure is already familiar to the research community.\\n?\\nExercise 16.3\\nReplace every point d in Figure 16.4 with two identical copies of d in the same class.\\n(i) Is it less difﬁcult, equally difﬁcult or more difﬁcult to cluster this set of 34 points\\nas opposed to the 17 points in Figure 16.4? (ii) Compute purity, NMI, RI, and F5 for\\nthe clustering with 34 points. Which measures increase and which stay the same after\\ndoubling the number of points? (iii) Given your assessment in (i) and the results in\\n(ii), which measures are best suited to compare the quality of the two clusterings?\\n16.4\\nK-means\\nK-means is the most important ﬂat clustering algorithm. Its objective is to\\nminimize the average squared Euclidean distance (Chapter 6, page 131) of\\ndocuments from their cluster centers where a cluster center is deﬁned as the\\nmean or centroid ⃗µ of the documents in a cluster ω:\\nCENTROID\\n⃗µ(ω) =\\n1\\n|ω| ∑\\n⃗x∈ω\\n⃗x\\nThe deﬁnition assumes that documents are represented as length-normalized\\nvectors in a real-valued space in the familiar way. We used centroids for Roc-\\nchio classiﬁcation in Chapter 14 (page 292). They play a similar role here.\\nThe ideal cluster in K-means is a sphere with the centroid as its center of\\ngravity. Ideally, the clusters should not overlap. Our desiderata for classes\\nin Rocchio classiﬁcation were the same. The difference is that we have no la-\\nbeled training set in clustering for which we know which documents should\\nbe in the same cluster.\\nA measure of how well the centroids represent the members of their clus-\\nters is the residual sum of squares or RSS, the squared distance of each vector\\nRESIDUAL SUM OF\\nSQUARES\\nfrom its centroid summed over all vectors:\\nRSSk = ∑\\n⃗x∈ωk\\n|⃗x −⃗µ(ωk)|2\\nRSS =\\nK\\n∑\\nk=1\\nRSSk\\n(16.7)\\nRSS is the objective function in K-means and our goal is to minimize it. Since\\nN is ﬁxed, minimizing RSS is equivalent to minimizing the average squared\\ndistance, a measure of how well centroids represent their documents.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n361\\nK-MEANS({⃗x1, . . . ,⃗xN}, K)\\n1\\n(⃗s1,⃗s2, . . . ,⃗sK) ←SELECTRANDOMSEEDS({⃗x1, . . . ,⃗xN}, K)\\n2\\nfor k ←1 to K\\n3\\ndo ⃗µk ←⃗sk\\n4\\nwhile stopping criterion has not been met\\n5\\ndo for k ←1 to K\\n6\\ndo ωk ←{}\\n7\\nfor n ←1 to N\\n8\\ndo j ←arg minj′ |⃗µj′ −⃗xn|\\n9\\nωj ←ωj ∪{⃗xn} (reassignment of vectors)\\n10\\nfor k ←1 to K\\n11\\ndo ⃗µk ←\\n1\\n|ωk| ∑⃗x∈ωk ⃗x (recomputation of centroids)\\n12\\nreturn {⃗µ1, . . . ,⃗µK}\\n◮Figure 16.5\\nThe K-means algorithm.\\nFor most IR applications, the vectors\\n⃗xn ∈RM should be length-normalized. Alternative methods of seed selection and\\ninitialization are discussed on page 364.\\nThe ﬁrst step of K-means is to select as initial cluster centers K randomly\\nselected documents, the seeds. The algorithm then moves the cluster centers\\nSEED\\naround in space in order to minimize RSS. As shown in Figure 16.5, this is\\ndone iteratively by repeating two steps until a stopping criterion is met: reas-\\nsigning documents to the cluster with the closest centroid; and recomputing\\neach centroid based on the current members of its cluster. Figure 16.6 shows\\nsnapshots from nine iterations of the K-means algorithm for a set of points.\\nThe “centroid” column of Table 17.2 (page 397) shows examples of centroids.\\nWe can apply one of the following termination conditions.\\n• A ﬁxed number of iterations I has been completed. This condition limits\\nthe runtime of the clustering algorithm, but in some cases the quality of\\nthe clustering will be poor because of an insufﬁcient number of iterations.\\n• Assignment of documents to clusters (the partitioning function γ) does\\nnot change between iterations. Except for cases with a bad local mini-\\nmum, this produces a good clustering, but runtimes may be unacceptably\\nlong.\\n• Centroids⃗µk do not change between iterations. This is equivalent to γ not\\nchanging (Exercise 16.5).\\n• Terminate when RSS falls below a threshold. This criterion ensures that\\nthe clustering is of a desired quality after termination. In practice, we\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n362\\n16\\nFlat clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n××\\nselection of seeds\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n××\\nassignment of documents (iter. 1)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+ +\\no\\no\\n+\\no\\n+\\n+\\n++\\n+\\n+\\n+\\n+ o\\n+\\n+\\no\\n+\\n+\\n+\\n+\\no\\no\\n+\\no\\n+\\n+\\no\\n+\\no\\n×\\n×\\n×\\n×\\nrecomputation/movement of ⃗µ’s (iter. 1)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+ +\\n+\\n+\\n+\\no\\n+\\n+\\n++\\n+\\no\\n+\\no o\\no\\no\\n+\\no\\n+\\no\\n+\\no\\n+\\no\\no\\no\\n+\\no\\n+\\no\\n×\\n×\\n⃗µ’s after convergence (iter. 9)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n..\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nmovement of ⃗µ’s in 9 iterations\\n◮Figure 16.6\\nA K-means example for K = 2 in R2.\\nThe position of the two cen-\\ntroids (⃗µ’s shown as X’s in the top four panels) converges after nine iterations.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n363\\nneed to combine it with a bound on the number of iterations to guarantee\\ntermination.\\n• Terminate when the decrease in RSS falls below a threshold θ. For small θ,\\nthis indicates that we are close to convergence. Again, we need to combine\\nit with a bound on the number of iterations to prevent very long runtimes.\\nWe now show that K-means converges by proving that RSS monotonically\\ndecreases in each iteration. We will use decrease in the meaning decrease or does\\nnot change in this section. First, RSS decreases in the reassignment step since\\neach vector is assigned to the closest centroid, so the distance it contributes\\nto RSS decreases. Second, it decreases in the recomputation step because the\\nnew centroid is the vector ⃗v for which RSSk reaches its minimum.\\nRSSk(⃗v)\\n=\\n∑\\n⃗x∈ωk\\n|⃗v −⃗x|2 = ∑\\n⃗x∈ωk\\nM\\n∑\\nm=1\\n(vm −xm)2\\n(16.8)\\n∂RSSk(⃗v)\\n∂vm\\n=\\n∑\\n⃗x∈ωk\\n2(vm −xm)\\n(16.9)\\nwhere xm and vm are the mth components of their respective vectors. Setting\\nthe partial derivative to zero, we get:\\nvm =\\n1\\n|ωk| ∑\\n⃗x∈ωk\\nxm\\n(16.10)\\nwhich is the componentwise deﬁnition of the centroid. Thus, we minimize\\nRSSk when the old centroid is replaced with the new centroid. RSS, the sum\\nof the RSSk, must then also decrease during recomputation.\\nSince there is only a ﬁnite set of possible clusterings, a monotonically de-\\ncreasing algorithm will eventually arrive at a (local) minimum. Take care,\\nhowever, to break ties consistently, e.g., by assigning a document to the clus-\\nter with the lowest index if there are several equidistant centroids. Other-\\nwise, the algorithm can cycle forever in a loop of clusterings that have the\\nsame cost.\\nWhile this proves the convergence of K-means, there is unfortunately no\\nguarantee that a global minimum in the objective function will be reached.\\nThis is a particular problem if a document set contains many outliers, doc-\\nOUTLIER\\numents that are far from any other documents and therefore do not ﬁt well\\ninto any cluster. Frequently, if an outlier is chosen as an initial seed, then no\\nother vector is assigned to it during subsequent iterations. Thus, we end up\\nwith a singleton cluster (a cluster with only one document) even though there\\nSINGLETON CLUSTER\\nis probably a clustering with lower RSS. Figure 16.7 shows an example of a\\nsuboptimal clustering resulting from a bad choice of initial seeds.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n364\\n16\\nFlat clustering\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\n×\\n×\\n×\\n×\\n×\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n◮Figure 16.7\\nThe outcome of clustering in K-means depends on the initial seeds.\\nFor seeds d2 and d5, K-means converges to {{d1, d2, d3}, {d4, d5, d6}}, a suboptimal\\nclustering. For seeds d2 and d3, it converges to {{d1, d2, d4, d5}, {d3, d6}}, the global\\noptimum for K = 2.\\nAnother type of suboptimal clustering that frequently occurs is one with\\nempty clusters (Exercise 16.11).\\nEffective heuristics for seed selection include (i) excluding outliers from\\nthe seed set; (ii) trying out multiple starting points and choosing the cluster-\\ning with lowest cost; and (iii) obtaining seeds from another method such as\\nhierarchical clustering. Since deterministic hierarchical clustering methods\\nare more predictable than K-means, a hierarchical clustering of a small ran-\\ndom sample of size iK (e.g., for i = 5 or i = 10) often provides good seeds\\n(see the description of the Buckshot algorithm, Chapter 17, page 399).\\nOther initialization methods compute seeds that are not selected from the\\nvectors to be clustered. A robust method that works well for a large variety\\nof document distributions is to select i (e.g., i = 10) random vectors for each\\ncluster and use their centroid as the seed for this cluster. See Section 16.6 for\\nmore sophisticated initializations.\\nWhat is the time complexity of K-means? Most of the time is spent on com-\\nputing vector distances. One such operation costs Θ(M). The reassignment\\nstep computes KN distances, so its overall complexity is Θ(KNM). In the\\nrecomputation step, each vector gets added to a centroid once, so the com-\\nplexity of this step is Θ(NM). For a ﬁxed number of iterations I, the overall\\ncomplexity is therefore Θ(IKNM). Thus, K-means is linear in all relevant\\nfactors: iterations, number of clusters, number of vectors and dimensionality\\nof the space. This means that K-means is more efﬁcient than the hierarchical\\nalgorithms in Chapter 17. We had to ﬁx the number of iterations I, which can\\nbe tricky in practice. But in most cases, K-means quickly reaches either com-\\nplete convergence or a clustering that is close to convergence. In the latter\\ncase, a few documents would switch membership if further iterations were\\ncomputed, but this has a small effect on the overall quality of the clustering.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n365\\nThere is one subtlety in the preceding argument. Even a linear algorithm\\ncan be quite slow if one of the arguments of Θ(. . .) is large, and M usually is\\nlarge. High dimensionality is not a problem for computing the distance be-\\ntween two documents. Their vectors are sparse, so that only a small fraction\\nof the theoretically possible M componentwise differences need to be com-\\nputed. Centroids, however, are dense since they pool all terms that occur in\\nany of the documents of their clusters. As a result, distance computations are\\ntime consuming in a naive implementation of K-means. However, there are\\nsimple and effective heuristics for making centroid-document similarities as\\nfast to compute as document-document similarities. Truncating centroids to\\nthe most signiﬁcant k terms (e.g., k = 1000) hardly decreases cluster quality\\nwhile achieving a signiﬁcant speedup of the reassignment step (see refer-\\nences in Section 16.6).\\nThe same efﬁciency problem is addressed by K-medoids, a variant of K-\\nK-MEDOIDS\\nmeans that computes medoids instead of centroids as cluster centers. We\\ndeﬁne the medoid of a cluster as the document vector that is closest to the\\nMEDOID\\ncentroid. Since medoids are sparse document vectors, distance computations\\nare fast.\\n$\\n16.4.1\\nCluster cardinality in K-means\\nWe stated in Section 16.2 that the number of clusters K is an input to most ﬂat\\nclustering algorithms. What do we do if we cannot come up with a plausible\\nguess for K?\\nA naive approach would be to select the optimal value of K according to\\nthe objective function, namely the value of K that minimizes RSS. Deﬁning\\nRSSmin(K) as the minimal RSS of all clusterings with K clusters, we observe\\nthat RSSmin(K) is a monotonically decreasing function in K (Exercise 16.13),\\nwhich reaches its minimum 0 for K = N where N is the number of doc-\\numents. We would end up with each document being in its own cluster.\\nClearly, this is not an optimal clustering.\\nA heuristic method that gets around this problem is to estimate RSSmin(K)\\nas follows. We ﬁrst perform i (e.g., i = 10) clusterings with K clusters (each\\nwith a different initialization) and compute the RSS of each. Then we take the\\nminimum of the i RSS values. We denote this minimum by d\\nRSSmin(K). Now\\nwe can inspect the values d\\nRSSmin(K) as K increases and ﬁnd the “knee” in the\\ncurve – the point where successive decreases in d\\nRSSmin become noticeably\\nsmaller. There are two such points in Figure 16.8, one at K = 4, where the\\ngradient ﬂattens slightly, and a clearer ﬂattening at K = 9. This is typical:\\nthere is seldom a single best number of clusters. We still need to employ an\\nexternal constraint to choose from a number of possible values of K (4 and 9\\nin this case).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n366\\n16\\nFlat clustering\\n2\\n4\\n6\\n8\\n10\\n1750\\n1800\\n1850\\n1900\\n1950\\nnumber of clusters\\nresidual sum of squares\\n◮Figure 16.8\\nEstimated minimal residual sum of squares as a function of the num-\\nber of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there\\nare two points where the d\\nRSSmin curve ﬂattens: at 4 clusters and at 9 clusters. The\\ndocuments were selected from the categories China, Germany, Russia and Sports, so\\nthe K = 4 clustering is closest to the Reuters classiﬁcation.\\nA second type of criterion for cluster cardinality imposes a penalty for each\\nnew cluster – where conceptually we start with a single cluster containing all\\ndocuments and then search for the optimal number of clusters K by succes-\\nsively incrementing K by one. To determine the cluster cardinality in this\\nway, we create a generalized objective function that combines two elements:\\ndistortion, a measure of how much documents deviate from the prototype of\\nDISTORTION\\ntheir clusters (e.g., RSS for K-means); and a measure of model complexity. We\\nMODEL COMPLEXITY\\ninterpret a clustering here as a model of the data. Model complexity in clus-\\ntering is usually the number of clusters or a function thereof. For K-means,\\nwe then get this selection criterion for K:\\nK = arg min\\nK\\n[RSSmin(K) + λK]\\n(16.11)\\nwhere λ is a weighting factor. A large value of λ favors solutions with few\\nclusters. For λ = 0, there is no penalty for more clusters and K = N is the\\nbest solution.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n367\\nThe obvious difﬁculty with Equation (16.11) is that we need to determine\\nλ. Unless this is easier than determining K directly, then we are back to\\nsquare one. In some cases, we can choose values of λ that have worked well\\nfor similar data sets in the past. For example, if we periodically cluster news\\nstories from a newswire, there is likely to be a ﬁxed value of λ that gives us\\nthe right K in each successive clustering. In this application, we would not\\nbe able to determine K based on past experience since K changes.\\nA theoretical justiﬁcation for Equation (16.11) is the Akaike Information Cri-\\nAKAIKE INFORMATION\\nCRITERION\\nterion or AIC, an information-theoretic measure that trades off distortion\\nagainst model complexity. The general form of AIC is:\\nAIC:\\nK = arg min\\nK\\n[−2L(K) + 2q(K)]\\n(16.12)\\nwhere −L(K), the negative maximum log-likelihood of the data for K clus-\\nters, is a measure of distortion and q(K), the number of parameters of a\\nmodel with K clusters, is a measure of model complexity. We will not at-\\ntempt to derive the AIC here, but it is easy to understand intuitively. The\\nﬁrst property of a good model of the data is that each data point is modeled\\nwell by the model. This is the goal of low distortion. But models should\\nalso be small (i.e., have low model complexity) since a model that merely\\ndescribes the data (and therefore has zero distortion) is worthless. AIC pro-\\nvides a theoretical justiﬁcation for one particular way of weighting these two\\nfactors, distortion and model complexity, when selecting a model.\\nFor K-means, the AIC can be stated as follows:\\nAIC:\\nK = arg min\\nK\\n[RSSmin(K) + 2MK]\\n(16.13)\\nEquation (16.13) is a special case of Equation (16.11) for λ = 2M.\\nTo derive Equation (16.13) from Equation (16.12) observe that q(K) = KM\\nin K-means since each element of the K centroids is a parameter that can be\\nvaried independently; and that L(K) = −(1/2)RSSmin(K) (modulo a con-\\nstant) if we view the model underlying K-means as a Gaussian mixture with\\nhard assignment, uniform cluster priors and identical spherical covariance\\nmatrices (see Exercise 16.19).\\nThe derivation of AIC is based on a number of assumptions, e.g., that the\\ndata are independent and identically distributed. These assumptions are\\nonly approximately true for data sets in information retrieval. As a conse-\\nquence, the AIC can rarely be applied without modiﬁcation in text clustering.\\nIn Figure 16.8, the dimensionality of the vector space is M ≈50,000. Thus,\\n2MK > 50,000 dominates the smaller RSS-based term (d\\nRSSmin(1) < 5000,\\nnot shown in the ﬁgure) and the minimum of the expression is reached for\\nK = 1. But as we know, K = 4 (corresponding to the four classes China,\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n368\\n16\\nFlat clustering\\nGermany, Russia and Sports) is a better choice than K = 1. In practice, Equa-\\ntion (16.11) is often more useful than Equation (16.13) – with the caveat that\\nwe need to come up with an estimate for λ.\\n?\\nExercise 16.4\\nWhy are documents that do not use the same term for the concept car likely to end\\nup in the same cluster in K-means clustering?\\nExercise 16.5\\nTwo of the possible termination conditions for K-means were (1) assignment does not\\nchange, (2) centroids do not change (page 361). Do these two conditions imply each\\nother?\\n$\\n16.5\\nModel-based clustering\\nIn this section, we describe a generalization of K-means, the EM algorithm.\\nIt can be applied to a larger variety of document representations and distri-\\nbutions than K-means.\\nIn K-means, we attempt to ﬁnd centroids that are good representatives. We\\ncan view the set of K centroids as a model that generates the data. Generating\\na document in this model consists of ﬁrst picking a centroid at random and\\nthen adding some noise. If the noise is normally distributed, this procedure\\nwill result in clusters of spherical shape. Model-based clustering assumes that\\nMODEL-BASED\\nCLUSTERING\\nthe data were generated by a model and tries to recover the original model\\nfrom the data. The model that we recover from the data then deﬁnes clusters\\nand an assignment of documents to clusters.\\nA commonly used criterion for estimating the model parameters is maxi-\\nmum likelihood. In K-means, the quantity exp(−RSS) is proportional to the\\nlikelihood that a particular model (i.e., a set of centroids) generated the data.\\nFor K-means, maximum likelihood and minimal RSS are equivalent criteria.\\nWe denote the model parameters by Θ. In K-means, Θ = {⃗µ1, . . . ,⃗µK}.\\nMore generally, the maximum likelihood criterion is to select the parame-\\nters Θ that maximize the log-likelihood of generating the data D:\\nΘ = arg max\\nΘ\\nL(D|Θ) = arg max\\nΘ\\nlog\\nN\\n∏\\nn=1\\nP(dn|Θ) = arg max\\nΘ\\nN\\n∑\\nn=1\\nlog P(dn|Θ)\\nL(D|Θ) is the objective function that measures the goodness of the cluster-\\ning. Given two clusterings with the same number of clusters, we prefer the\\none with higher L(D|Θ).\\nThis is the same approach we took in Chapter 12 (page 237) for language\\nmodeling and in Section 13.1 (page 265) for text classiﬁcation. In text clas-\\nsiﬁcation, we chose the class that maximizes the likelihood of generating a\\nparticular document. Here, we choose the clustering Θ that maximizes the\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.5\\nModel-based clustering\\n369\\nlikelihood of generating a given set of documents. Once we have Θ, we can\\ncompute an assignment probability P(d|ωk; Θ) for each document-cluster\\npair. This set of assignment probabilities deﬁnes a soft clustering.\\nAn example of a soft assignment is that a document about Chinese cars\\nmay have a fractional membership of 0.5 in each of the two clusters China\\nand automobiles, reﬂecting the fact that both topics are pertinent. A hard clus-\\ntering like K-means cannot model this simultaneous relevance to two topics.\\nModel-based clustering provides a framework for incorporating our know-\\nledge about a domain. K-means and the hierarchical algorithms in Chap-\\nter 17 make fairly rigid assumptions about the data. For example, clusters\\nin K-means are assumed to be spheres. Model-based clustering offers more\\nﬂexibility. The clustering model can be adapted to what we know about\\nthe underlying distribution of the data, be it Bernoulli (as in the example\\nin Table 16.3), Gaussian with non-spherical variance (another model that is\\nimportant in document clustering) or a member of a different family.\\nA commonly used algorithm for model-based clustering is the Expectation-\\nEXPECTATION-\\nMAXIMIZATION\\nALGORITHM\\nMaximization algorithm or EM algorithm. EM clustering is an iterative algo-\\nrithm that maximizes L(D|Θ). EM can be applied to many different types of\\nprobabilistic modeling. We will work with a mixture of multivariate Bernoulli\\ndistributions here, the distribution we know from Section 11.3 (page 222) and\\nSection 13.3 (page 263):\\nP(d|ωk; Θ) =\\n \\n∏\\ntm∈d\\nqmk\\n!  \\n∏\\ntm/∈d\\n(1 −qmk)\\n!\\n(16.14)\\nwhere Θ = {Θ1, . . . , ΘK}, Θk = (αk, q1k, . . . , qMk), and qmk = P(Um = 1|ωk)\\nare the parameters of the model.3 P(Um = 1|ωk) is the probability that a\\ndocument from cluster ωk contains term tm. The probability αk is the prior of\\ncluster ωk: the probability that a document d is in ωk if we have no informa-\\ntion about d.\\nThe mixture model then is:\\nP(d|Θ) =\\nK\\n∑\\nk=1\\nαk\\n \\n∏\\ntm∈d\\nqmk\\n!  \\n∏\\ntm/∈d\\n(1 −qmk)\\n!\\n(16.15)\\nIn this model, we generate a document by ﬁrst picking a cluster k with prob-\\nability αk and then generating the terms of the document according to the\\nparameters qmk. Recall that the document representation of the multivariate\\nBernoulli is a vector of M Boolean values (and not a real-valued vector).\\n3. Um is the random variable we deﬁned in Section 13.3 (page 266) for the Bernoulli Naive Bayes\\nmodel. It takes the values 1 (term tm is present in the document) and 0 (term tm is absent in the\\ndocument).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n370\\n16\\nFlat clustering\\nHow do we use EM to infer the parameters of the clustering from the data?\\nThat is, how do we choose parameters Θ that maximize L(D|Θ)? EM is simi-\\nlar to K-means in that it alternates between an expectation step, corresponding\\nEXPECTATION STEP\\nto reassignment, and a maximization step, corresponding to recomputation of\\nMAXIMIZATION STEP\\nthe parameters of the model. The parameters of K-means are the centroids,\\nthe parameters of the instance of EM in this section are the αk and qmk.\\nThe maximization step recomputes the conditional parameters qmk and the\\npriors αk as follows:\\nMaximization step:\\nqmk = ∑N\\nn=1 rnkI(tm ∈dn)\\n∑N\\nn=1 rnk\\nαk = ∑N\\nn=1 rnk\\nN\\n(16.16)\\nwhere I(tm ∈dn) = 1 if tm ∈dn and 0 otherwise and rnk is the soft as-\\nsignment of document dn to cluster k as computed in the preceding iteration.\\n(We’ll address the issue of initialization in a moment.) These are the max-\\nimum likelihood estimates for the parameters of the multivariate Bernoulli\\nfrom Table 13.3 (page 268) except that documents are assigned fractionally to\\nclusters here. These maximum likelihood estimates maximize the likelihood\\nof the data given the model.\\nThe expectation step computes the soft assignment of documents to clus-\\nters given the current parameters qmk and αk:\\nExpectation step :\\nrnk =\\nαk(∏tm∈dn qmk)(∏tm/∈dn(1 −qmk))\\n∑K\\nk=1 αk(∏tm∈dn qmk)(∏tm/∈dn(1 −qmk))\\n(16.17)\\nThis expectation step applies Equations (16.14) and (16.15) to computing the\\nlikelihood that ωk generated document dn. It is the classiﬁcation procedure\\nfor the multivariate Bernoulli in Table 13.3. Thus, the expectation step is\\nnothing else but Bernoulli Naive Bayes classiﬁcation (including normaliza-\\ntion, i.e. dividing by the denominator, to get a probability distribution over\\nclusters).\\nWe clustered a set of 11 documents into two clusters using EM in Ta-\\nble 16.3. After convergence in iteration 25, the ﬁrst 5 documents are assigned\\nto cluster 1 (ri,1 = 1.00) and the last 6 to cluster 2 (ri,1 = 0.00). Somewhat\\natypically, the ﬁnal assignment is a hard assignment here. EM usually con-\\nverges to a soft assignment.\\nIn iteration 25, the prior α1 for cluster 1 is\\n5/11 ≈0.45 because 5 of the 11 documents are in cluster 1. Some terms\\nare quickly associated with one cluster because the initial assignment can\\n“spread” to them unambiguously. For example, membership in cluster 2\\nspreads from document 7 to document 8 in the ﬁrst iteration because they\\nshare sugar (r8,1 = 0 in iteration 1).\\nFor parameters of terms occurring\\nin ambiguous contexts, convergence takes longer. Seed documents 6 and 7\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.5\\nModel-based clustering\\n371\\n(a)\\ndocID\\ndocument text\\ndocID\\ndocument text\\n1\\nhot chocolate cocoa beans\\n7\\nsweet sugar\\n2\\ncocoa ghana africa\\n8\\nsugar cane brazil\\n3\\nbeans harvest ghana\\n9\\nsweet sugar beet\\n4\\ncocoa butter\\n10\\nsweet cake icing\\n5\\nbutter trufﬂes\\n11\\ncake black forest\\n6\\nsweet chocolate\\n(b)\\nParameter\\nIteration of clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n15\\n25\\nα1\\n0.50\\n0.45\\n0.53\\n0.57\\n0.58\\n0.54\\n0.45\\nr1,1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\nr2,1\\n0.50\\n0.79\\n0.99\\n1.00\\n1.00\\n1.00\\n1.00\\nr3,1\\n0.50\\n0.84\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\nr4,1\\n0.50\\n0.75\\n0.94\\n1.00\\n1.00\\n1.00\\n1.00\\nr5,1\\n0.50\\n0.52\\n0.66\\n0.91\\n1.00\\n1.00\\n1.00\\nr6,1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n0.83\\n0.00\\nr7,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr8,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr9,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr10,1\\n0.50\\n0.40\\n0.14\\n0.01\\n0.00\\n0.00\\n0.00\\nr11,1\\n0.50\\n0.57\\n0.58\\n0.41\\n0.07\\n0.00\\n0.00\\nqafrica,1\\n0.000\\n0.100\\n0.134\\n0.158\\n0.158\\n0.169\\n0.200\\nqafrica,2\\n0.000\\n0.083\\n0.042\\n0.001\\n0.000\\n0.000\\n0.000\\nqbrazil,1\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\nqbrazil,2\\n0.000\\n0.167\\n0.195\\n0.213\\n0.214\\n0.196\\n0.167\\nqcocoa,1\\n0.000\\n0.400\\n0.432\\n0.465\\n0.474\\n0.508\\n0.600\\nqcocoa,2\\n0.000\\n0.167\\n0.090\\n0.014\\n0.001\\n0.000\\n0.000\\nqsugar,1\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\nqsugar,2\\n1.000\\n0.500\\n0.585\\n0.640\\n0.642\\n0.589\\n0.500\\nqsweet,1\\n1.000\\n0.300\\n0.238\\n0.180\\n0.159\\n0.153\\n0.000\\nqsweet,2\\n1.000\\n0.417\\n0.507\\n0.610\\n0.640\\n0.608\\n0.667\\n◮Table 16.3\\nThe EM clustering algorithm. The table shows a set of documents\\n(a) and parameter values for selected iterations during EM clustering (b). Parameters\\nshown are prior α1, soft assignment scores rn,1 (both omitted for cluster 2), and lexical\\nparameters qm,k for a few terms. The authors initially assigned document 6 to clus-\\nter 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For\\nsmoothing, the rnk in Equation (16.16) were replaced with rnk + ǫ where ǫ = 0.0001.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n372\\n16\\nFlat clustering\\nboth contain sweet. As a result, it takes 25 iterations for the term to be unam-\\nbiguously associated with cluster 2. (qsweet,1 = 0 in iteration 25.)\\nFinding good seeds is even more critical for EM than for K-means. EM is\\nprone to get stuck in local optima if the seeds are not chosen well. This is a\\ngeneral problem that also occurs in other applications of EM.4 Therefore, as\\nwith K-means, the initial assignment of documents to clusters is often com-\\nputed by a different algorithm. For example, a hard K-means clustering may\\nprovide the initial assignment, which EM can then “soften up.”\\n?\\nExercise 16.6\\nWe saw above that the time complexity of K-means is Θ(IKNM). What is the time\\ncomplexity of EM?\\n16.6\\nReferences and further reading\\nBerkhin (2006b) gives a general up-to-date survey of clustering methods with\\nspecial attention to scalability. The classic reference for clustering in pat-\\ntern recognition, covering both K-means and EM, is (Duda et al. 2000). Ras-\\nmussen (1992) introduces clustering from an information retrieval perspec-\\ntive. Anderberg (1973) provides a general introduction to clustering for ap-\\nplications. In addition to Euclidean distance and cosine similarity, Kullback-\\nLeibler divergence is often used in clustering as a measure of how (dis)similar\\ndocuments and clusters are (Xu and Croft 1999, Muresan and Harper 2004,\\nKurland and Lee 2004).\\nThe cluster hypothesis is due to Jardine and van Rijsbergen (1971) who\\nstate it as follows: Associations between documents convey information about the\\nrelevance of documents to requests. Salton (1971a; 1975), Croft (1978), Voorhees\\n(1985a), Can and Ozkarahan (1990), Cacheda et al. (2003), Can et al. (2004),\\nSingitham et al. (2004) and Altingövde et al. (2008) investigate the efﬁciency\\nand effectiveness of cluster-based retrieval.\\nWhile some of these studies\\nshow improvements in effectiveness, efﬁciency or both, there is no consensus\\nthat cluster-based retrieval works well consistently across scenarios. Cluster-\\nbased language modeling was pioneered by Liu and Croft (2004).\\nThere is good evidence that clustering of search results improves user ex-\\nperience and search result quality (Hearst and Pedersen 1996, Zamir and Et-\\nzioni 1999, Tombros et al. 2002, Käki 2005, Toda and Kataoka 2005), although\\nnot as much as search result structuring based on carefully edited category\\nhierarchies (Hearst 2006). The Scatter-Gather interface for browsing collec-\\ntions was presented by Cutting et al. (1992). A theoretical framework for an-\\n4. For example, this problem is common when EM is used to estimate parameters of hidden\\nMarkov models, probabilistic grammars, and machine translation models in natural language\\nprocessing (Manning and Schütze 1999).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.6\\nReferences and further reading\\n373\\nalyzing the properties of Scatter/Gather and other information seeking user\\ninterfaces is presented by Pirolli (2007). Schütze and Silverstein (1997) eval-\\nuate LSI (Chapter 18) and truncated representations of centroids for efﬁcient\\nK-means clustering.\\nThe Columbia NewsBlaster system (McKeown et al. 2002), a forerunner to\\nthe now much more famous and reﬁned Google News (http://news.google.com),\\nused hierarchical clustering (Chapter 17) to give two levels of news topic\\ngranularity. See Hatzivassiloglou et al. (2000) for details, and Chen and Lin\\n(2000) and Radev et al. (2001) for related systems.\\nOther applications of\\nclustering in information retrieval are duplicate detection (Yang and Callan\\n(2006), Section 19.6, page 438), novelty detection (see references in Section 17.9,\\npage 399) and metadata discovery on the semantic web (Alonso et al. 2006).\\nThe discussion of external evaluation measures is partially based on Strehl\\n(2002). Dom (2002) proposes a measure Q0 that is better motivated theoret-\\nically than NMI. Q0 is the number of bits needed to transmit class member-\\nships assuming cluster memberships are known. The Rand index is due to\\nRand (1971). Hubert and Arabie (1985) propose an adjusted Rand index that\\nADJUSTED RAND INDEX\\nranges between −1 and 1 and is 0 if there is only chance agreement between\\nclusters and classes (similar to κ in Chapter 8, page 165). Basu et al. (2004) ar-\\ngue that the three evaluation measures NMI, Rand index and F measure give\\nvery similar results. Stein et al. (2003) propose expected edge density as an in-\\nternal measure and give evidence that it is a good predictor of the quality of a\\nclustering. Kleinberg (2002) and Meil˘a (2005) present axiomatic frameworks\\nfor comparing clusterings.\\nAuthors that are often credited with the invention of the K-means algo-\\nrithm include Lloyd (1982) (ﬁrst distributed in 1957), Ball (1965), MacQueen\\n(1967), and Hartigan and Wong (1979). Arthur and Vassilvitskii (2006) in-\\nvestigate the worst-case complexity of K-means. Bradley and Fayyad (1998),\\nPelleg and Moore (1999) and Davidson and Satyanarayana (2003) investi-\\ngate the convergence properties of K-means empirically and how it depends\\non initial seed selection. Dhillon and Modha (2001) compare K-means clus-\\nters with SVD-based clusters (Chapter 18). The K-medoid algorithm was\\npresented by Kaufman and Rousseeuw (1990). The EM algorithm was orig-\\ninally introduced by Dempster et al. (1977). An in-depth treatment of EM is\\n(McLachlan and Krishnan 1996). See Section 18.5 (page 417) for publications\\non latent analysis, which can also be viewed as soft clustering.\\nAIC is due to Akaike (1974) (see also Burnham and Anderson (2002)). An\\nalternative to AIC is BIC, which can be motivated as a Bayesian model se-\\nlection procedure (Schwarz 1978). Fraley and Raftery (1998) show how to\\nchoose an optimal number of clusters based on BIC. An application of BIC to\\nK-means is (Pelleg and Moore 2000). Hamerly and Elkan (2003) propose an\\nalternative to BIC that performs better in their experiments. Another inﬂu-\\nential Bayesian approach for determining the number of clusters (simultane-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n374\\n16\\nFlat clustering\\nously with cluster assignment) is described by Cheeseman and Stutz (1996).\\nTwo methods for determining cardinality without external criteria are pre-\\nsented by Tibshirani et al. (2001).\\nWe only have space here for classical completely unsupervised clustering.\\nAn important current topic of research is how to use prior knowledge to\\nguide clustering (e.g., Ji and Xu (2006)) and how to incorporate interactive\\nfeedback during clustering (e.g., Huang and Mitchell (2006)). Fayyad et al.\\n(1998) propose an initialization for EM clustering. For algorithms that can\\ncluster very large data sets in one scan through the data see Bradley et al.\\n(1998).\\nThe applications in Table 16.1 all cluster documents. Other information re-\\ntrieval applications cluster words (e.g., Crouch 1988), contexts of words (e.g.,\\nSchütze and Pedersen 1995) or words and documents simultaneously (e.g.,\\nTishby and Slonim 2000, Dhillon 2001, Zha et al. 2001). Simultaneous clus-\\ntering of words and documents is an example of co-clustering or biclustering.\\nCO-CLUSTERING\\n16.7\\nExercises\\n?\\nExercise 16.7\\nLet Ωbe a clustering that exactly reproduces a class structure C and Ω′ a clustering\\nthat further subdivides some clusters in Ω. Show that I(Ω; C) = I(Ω′; C).\\nExercise 16.8\\nShow that I(Ω; C) ≤[H(Ω) + H(C)]/2.\\nExercise 16.9\\nMutual information is symmetric in the sense that its value does not change if the\\nroles of clusters and classes are switched: I(Ω; C) = I(C; Ω). Which of the other\\nthree evaluation measures are symmetric in this sense?\\nExercise 16.10\\nCompute RSS for the two clusterings in Figure 16.7.\\nExercise 16.11\\n(i) Give an example of a set of points and three initial centroids (which need not be\\nmembers of the set of points) for which 3-means converges to a clustering with an\\nempty cluster. (ii) Can a clustering with an empty cluster be the global optimum with\\nrespect to RSS?\\nExercise 16.12\\nDownload Reuters-21578.\\nDiscard documents that do not occur in one of the 10\\nclasses acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat.\\nDiscard documents that occur in two of these 10 classes. (i) Compute a K-means clus-\\ntering of this subset into 10 clusters. There are a number of software packages that\\nimplement K-means, such as WEKA (Witten and Frank 2005) and R (R Development\\nCore Team 2005). (ii) Compute purity, normalized mutual information, F1 and RI for\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n16.7\\nExercises\\n375\\nthe clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Ta-\\nble 14.5, page 308) for the 10 classes and 10 clusters. Identify classes that give rise to\\nfalse positives and false negatives.\\nExercise 16.13\\nProve that RSSmin(K) is monotonically decreasing in K.\\nExercise 16.14\\nThere is a soft version of K-means that computes the fractional membership of a doc-\\nument in a cluster as a monotonically decreasing function of the distance ∆from its\\ncentroid, e.g., as e−∆. Modify reassignment and recomputation steps of hard K-means\\nfor this soft version.\\nExercise 16.15\\nIn the last iteration in Table 16.3, document 6 is in cluster 2 even though it was the\\ninitial seed for cluster 1. Why does the document change membership?\\nExercise 16.16\\nThe values of the parameters qmk in iteration 25 in Table 16.3 are rounded. What are\\nthe exact values that EM will converge to?\\nExercise 16.17\\nPerform a K-means clustering for the documents in Table 16.3.\\nAfter how many\\niterations does K-means converge? Compare the result with the EM clustering in\\nTable 16.3 and discuss the differences.\\nExercise 16.18\\n[⋆⋆⋆]\\nModify the expectation and maximization steps of EM for a Gaussian mixture. The\\nmaximization step computes the maximum likelihood parameter estimates αk, ⃗µk,\\nand Σk for each of the clusters. The expectation step computes for each vector a soft\\nassignment to clusters (Gaussians) based on their current parameters. Write down\\nthe equations for Gaussian mixtures corresponding to Equations (16.16) and (16.17).\\nExercise 16.19\\n[⋆⋆⋆]\\nShow that K-means can be viewed as the limiting case of EM for Gaussian mixtures\\nif variance is very small and all covariances are 0.\\nExercise 16.20\\n[⋆⋆⋆]\\nThe within-point scatter of a clustering is deﬁned as ∑k\\n1\\n2 ∑⃗xi∈ωk ∑⃗xj∈ωk |⃗xi −⃗xj|2. Show\\nWITHIN-POINT\\nSCATTER\\nthat minimizing RSS and minimizing within-point scatter are equivalent.\\nExercise 16.21\\n[⋆⋆⋆]\\nDerive an AIC criterion for the multivariate Bernoulli mixture model from Equa-\\ntion (16.12).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n377\\n17\\nHierarchical clustering\\nFlat clustering is efﬁcient and conceptually simple, but as we saw in Chap-\\nter 16 it has a number of drawbacks. The algorithms introduced in Chap-\\nter 16 return a ﬂat unstructured set of clusters, require a prespeciﬁed num-\\nber of clusters as input and are nondeterministic. Hierarchical clustering (or\\nHIERARCHICAL\\nCLUSTERING\\nhierarchic clustering) outputs a hierarchy, a structure that is more informative\\nthan the unstructured set of clusters returned by ﬂat clustering.1 Hierarchical\\nclustering does not require us to prespecify the number of clusters and most\\nhierarchical algorithms that have been used in IR are deterministic. These ad-\\nvantages of hierarchical clustering come at the cost of lower efﬁciency. The\\nmost common hierarchical clustering algorithms have a complexity that is at\\nleast quadratic in the number of documents compared to the linear complex-\\nity of K-means and EM (cf. Section 16.4, page 364).\\nThis chapter ﬁrst introduces agglomerative hierarchical clustering (Section 17.1)\\nand presents four different agglomerative algorithms, in Sections 17.2–17.4,\\nwhich differ in the similarity measures they employ: single-link, complete-\\nlink, group-average, and centroid similarity. We then discuss the optimality\\nconditions of hierarchical clustering in Section 17.5. Section 17.6 introduces\\ntop-down (or divisive) hierarchical clustering. Section 17.7 looks at labeling\\nclusters automatically, a problem that must be solved whenever humans in-\\nteract with the output of clustering. We discuss implementation issues in\\nSection 17.8. Section 17.9 provides pointers to further reading, including ref-\\nerences to soft hierarchical clustering, which we do not cover in this book.\\nThere are few differences between the applications of ﬂat and hierarchi-\\ncal clustering in information retrieval. In particular, hierarchical clustering\\nis appropriate for any of the applications shown in Table 16.1 (page 351; see\\nalso Section 16.6, page 372). In fact, the example we gave for collection clus-\\ntering is hierarchical. In general, we select ﬂat clustering when efﬁciency\\nis important and hierarchical clustering when one of the potential problems\\n1. In this chapter, we only consider hierarchies that are binary trees like the one shown in Fig-\\nure 17.1 – but hierarchical clustering can be easily extended to other types of trees.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n378\\n17\\nHierarchical clustering\\nof ﬂat clustering (not enough structure, predetermined number of clusters,\\nnon-determinism) is a concern. In addition, many researchers believe that hi-\\nerarchical clustering produces better clusters than ﬂat clustering. However,\\nthere is no consensus on this issue (see references in Section 17.9).\\n17.1\\nHierarchical agglomerative clustering\\nHierarchical clustering algorithms are either top-down or bottom-up. Bottom-\\nup algorithms treat each document as a singleton cluster at the outset and\\nthen successively merge (or agglomerate) pairs of clusters until all clusters\\nhave been merged into a single cluster that contains all documents. Bottom-\\nup hierarchical clustering is therefore called hierarchical agglomerative cluster-\\nHIERARCHICAL\\nAGGLOMERATIVE\\nCLUSTERING\\ning or HAC. Top-down clustering requires a method for splitting a cluster.\\nHAC\\nIt proceeds by splitting clusters recursively until individual documents are\\nreached. See Section 17.6. HAC is more frequently used in IR than top-down\\nclustering and is the main subject of this chapter.\\nBefore looking at speciﬁc similarity measures used in HAC in Sections\\n17.2–17.4, we ﬁrst introduce a method for depicting hierarchical clusterings\\ngraphically, discuss a few key properties of HACs and present a simple algo-\\nrithm for computing an HAC.\\nAn HAC clustering is typically visualized as a dendrogram as shown in\\nDENDROGRAM\\nFigure 17.1. Each merge is represented by a horizontal line. The y-coordinate\\nof the horizontal line is the similarity of the two clusters that were merged,\\nwhere documents are viewed as singleton clusters. We call this similarity the\\ncombination similarity of the merged cluster. For example, the combination\\nCOMBINATION\\nSIMILARITY\\nsimilarity of the cluster consisting of Lloyd’s CEO questioned and Lloyd’s chief\\n/ U.S. grilling in Figure 17.1 is ≈0.56. We deﬁne the combination similarity\\nof a singleton cluster as its document’s self-similarity (which is 1.0 for cosine\\nsimilarity).\\nBy moving up from the bottom layer to the top node, a dendrogram al-\\nlows us to reconstruct the history of merges that resulted in the depicted\\nclustering. For example, we see that the two documents entitled War hero\\nColin Powell were merged ﬁrst in Figure 17.1 and that the last merge added\\nAg trade reform to a cluster consisting of the other 29 documents.\\nA fundamental assumption in HAC is that the merge operation is mono-\\nMONOTONICITY\\ntonic. Monotonic means that if s1, s2, . . . , sK−1 are the combination similarities\\nof the successive merges of an HAC, then s1 ≥s2 ≥. . . ≥sK−1 holds. A non-\\nmonotonic hierarchical clustering contains at least one inversion si < si+1\\nINVERSION\\nand contradicts the fundamental assumption that we chose the best merge\\navailable at each step. We will see an example of an inversion in Figure 17.12.\\nHierarchical clustering does not require a prespeciﬁed number of clusters.\\nHowever, in some applications we want a partition of disjoint clusters just as\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.1\\nHierarchical agglomerative clustering\\n379\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\nAg trade reform.\\nBack−to−school spending is up\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nViag stays positive\\nChrysler / Latin America\\nOhio Blue Cross\\nJapanese prime minister / Mexico\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nGerman unions split\\nWar hero Colin Powell\\nWar hero Colin Powell\\nOil prices slip\\nChains may raise prices\\nClinton signs law\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nMost active stocks\\nMexican markets\\nHog prices tumble\\nNYSE closing averages\\nBritish FTSE index\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\n◮Figure 17.1\\nA dendrogram of a single-link clustering of 30 documents from\\nReuters-RCV1. Two possible cuts of the dendrogram are shown: at 0.4 into 24 clusters\\nand at 0.1 into 12 clusters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n380\\n17\\nHierarchical clustering\\nin ﬂat clustering. In those cases, the hierarchy needs to be cut at some point.\\nA number of criteria can be used to determine the cutting point:\\n• Cut at a prespeciﬁed level of similarity. For example, we cut the dendro-\\ngram at 0.4 if we want clusters with a minimum combination similarity\\nof 0.4. In Figure 17.1, cutting the diagram at y = 0.4 yields 24 clusters\\n(grouping only documents with high similarity together) and cutting it at\\ny = 0.1 yields 12 clusters (one large ﬁnancial news cluster and 11 smaller\\nclusters).\\n• Cut the dendrogram where the gap between two successive combination\\nsimilarities is largest. Such large gaps arguably indicate “natural” clus-\\nterings. Adding one more cluster decreases the quality of the clustering\\nsigniﬁcantly, so cutting before this steep decrease occurs is desirable. This\\nstrategy is analogous to looking for the knee in the K-means graph in Fig-\\nure 16.8 (page 366).\\n• Apply Equation (16.11) (page 366):\\nK = arg min\\nK′\\n[RSS(K′) + λK′]\\nwhere K′ refers to the cut of the hierarchy that results in K′ clusters, RSS is\\nthe residual sum of squares and λ is a penalty for each additional cluster.\\nInstead of RSS, another measure of distortion can be used.\\n• As in ﬂat clustering, we can also prespecify the number of clusters K and\\nselect the cutting point that produces K clusters.\\nA simple, naive HAC algorithm is shown in Figure 17.2. We ﬁrst compute\\nthe N × N similarity matrix C. The algorithm then executes N −1 steps\\nof merging the currently most similar clusters. In each iteration, the two\\nmost similar clusters are merged and the rows and columns of the merged\\ncluster i in C are updated.2 The clustering is stored as a list of merges in\\nA. I indicates which clusters are still available to be merged. The function\\nSIM(i, m, j) computes the similarity of cluster j with the merge of clusters i\\nand m. For some HAC algorithms, SIM(i, m, j) is simply a function of C[j][i]\\nand C[j][m], for example, the maximum of these two values for single-link.\\nWe will now reﬁne this algorithm for the different similarity measures\\nof single-link and complete-link clustering (Section 17.2) and group-average\\nand centroid clustering (Sections 17.3 and 17.4). The merge criteria of these\\nfour variants of HAC are shown in Figure 17.3.\\n2. We assume that we use a deterministic method for breaking ties, such as always choose the\\nmerge that is the ﬁrst cluster with respect to a total ordering of the subsets of the document set\\nD.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.1\\nHierarchical agglomerative clustering\\n381\\nSIMPLEHAC(d1, . . . , dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i] ←SIM(dn, di)\\n4\\nI[n] ←1 (keeps track of active clusters)\\n5\\nA ←[] (assembles clustering as a sequence of merges)\\n6\\nfor k ←1 to N −1\\n7\\ndo ⟨i, m⟩←arg max{⟨i,m⟩:i̸=m∧I[i]=1∧I[m]=1} C[i][m]\\n8\\nA.APPEND(⟨i, m⟩) (store merge)\\n9\\nfor j ←1 to N\\n10\\ndo C[i][j] ←SIM(i, m, j)\\n11\\nC[j][i] ←SIM(i, m, j)\\n12\\nI[m] ←0 (deactivate cluster)\\n13\\nreturn A\\n◮Figure 17.2\\nA simple, but inefﬁcient HAC algorithm.\\nb\\nb\\nb\\nb\\n(a) single-link: maximum similarity\\nb\\nb\\nb\\nb\\n(b) complete-link: minimum similarity\\nb\\nb\\nb\\nb\\n(c) centroid: average inter-similarity\\nb\\nb\\nb\\nb\\n(d) group-average: average of all similarities\\n◮Figure 17.3\\nThe different notions of cluster similarity used by the four HAC al-\\ngorithms. An inter-similarity is a similarity between two documents from different\\nclusters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n382\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\nd5\\n×\\nd6\\n×\\nd7\\n×\\nd8\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\nd5\\n×\\nd6\\n×\\nd7\\n×\\nd8\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n◮Figure 17.4\\nA single-link (left) and complete-link (right) clustering of eight doc-\\numents. The ellipses correspond to successive clustering stages. Left: The single-link\\nsimilarity of the two upper two-point clusters is the similarity of d2 and d3 (solid\\nline), which is greater than the single-link similarity of the two left two-point clusters\\n(dashed line). Right: The complete-link similarity of the two upper two-point clusters\\nis the similarity of d1 and d4 (dashed line), which is smaller than the complete-link\\nsimilarity of the two left two-point clusters (solid line).\\n17.2\\nSingle-link and complete-link clustering\\nIn single-link clustering or single-linkage clustering, the similarity of two clus-\\nSINGLE-LINK\\nCLUSTERING\\nters is the similarity of their most similar members (see Figure 17.3, (a))3. This\\nsingle-link merge criterion is local. We pay attention solely to the area where\\nthe two clusters come closest to each other. Other, more distant parts of the\\ncluster and the clusters’ overall structure are not taken into account.\\nIn complete-link clustering or complete-linkage clustering, the similarity of two\\nCOMPLETE-LINK\\nCLUSTERING\\nclusters is the similarity of their most dissimilar members (see Figure 17.3, (b)).\\nThis is equivalent to choosing the cluster pair whose merge has the smallest\\ndiameter. This complete-link merge criterion is non-local; the entire structure\\nof the clustering can inﬂuence merge decisions. This results in a preference\\nfor compact clusters with small diameters over long, straggly clusters, but\\nalso causes sensitivity to outliers. A single document far from the center can\\nincrease diameters of candidate merge clusters dramatically and completely\\nchange the ﬁnal clustering.\\nFigure 17.4 depicts a single-link and a complete-link clustering of eight\\ndocuments. The ﬁrst four steps, each producing a cluster consisting of a pair\\nof two documents, are identical. Then single-link clustering joins the up-\\nper two pairs (and after that the lower two pairs) because on the maximum-\\nsimilarity deﬁnition of cluster similarity, those two clusters are closest. Complete-\\n3. Throughout this chapter, we equate similarity with proximity in 2D depictions of clustering.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n383\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\nNYSE closing averages\\nHog prices tumble\\nOil prices slip\\nAg trade reform.\\nChrysler / Latin America\\nJapanese prime minister / Mexico\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\nMexican markets\\nBritish FTSE index\\nWar hero Colin Powell\\nWar hero Colin Powell\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nOhio Blue Cross\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nViag stays positive\\nMost active stocks\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nBack−to−school spending is up\\nGerman unions split\\nChains may raise prices\\nClinton signs law\\n◮Figure 17.5\\nA dendrogram of a complete-link clustering. The same 30 documents\\nwere clustered with single-link clustering in Figure 17.1.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n384\\n17\\nHierarchical clustering\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n◮Figure 17.6\\nChaining in single-link clustering. The local criterion in single-link\\nclustering can cause undesirable elongated clusters.\\nlink clustering joins the left two pairs (and then the right two pairs) because\\nthose are the closest pairs according to the minimum-similarity deﬁnition of\\ncluster similarity.4\\nFigure 17.1 is an example of a single-link clustering of a set of documents\\nand Figure 17.5 is the complete-link clustering of the same set. When cutting\\nthe last merge in Figure 17.5, we obtain two clusters of similar size (doc-\\numents 1–16, from NYSE closing averages to Lloyd’s chief / U.S. grilling, and\\ndocuments 17–30, from Ohio Blue Cross to Clinton signs law). There is no cut\\nof the dendrogram in Figure 17.1 that would give us an equally balanced\\nclustering.\\nBoth single-link and complete-link clustering have graph-theoretic inter-\\npretations. Deﬁne sk to be the combination similarity of the two clusters\\nmerged in step k, and G(sk) the graph that links all data points with a similar-\\nity of at least sk. Then the clusters after step k in single-link clustering are the\\nconnected components of G(sk) and the clusters after step k in complete-link\\nclustering are maximal cliques of G(sk). A connected component is a maximal\\nCONNECTED\\nCOMPONENT\\nset of connected points such that there is a path connecting each pair. A clique\\nCLIQUE\\nis a set of points that are completely linked with each other.\\nThese graph-theoretic interpretations motivate the terms single-link and\\ncomplete-link clustering. Single-link clusters at step k are maximal sets of\\npoints that are linked via at least one link (a single link) of similarity s ≥sk;\\ncomplete-link clusters at step k are maximal sets of points that are completely\\nlinked with each other via links of similarity s ≥sk.\\nSingle-link and complete-link clustering reduce the assessment of cluster\\nquality to a single similarity between a pair of documents: the two most sim-\\nilar documents in single-link clustering and the two most dissimilar docu-\\nments in complete-link clustering. A measurement based on one pair cannot\\nfully reﬂect the distribution of documents in a cluster. It is therefore not sur-\\nprising that both algorithms often produce undesirable clusters. Single-link\\nclustering can produce straggling clusters as shown in Figure 17.6. Since the\\nmerge criterion is strictly local, a chain of points can be extended for long\\n4. If you are bothered by the possibility of ties, assume that d1 has coordinates (1 + ǫ, 3 −ǫ) and\\nthat all other points have integer coordinates.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n385\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n×\\nd5\\n◮Figure 17.7\\nOutliers in complete-link clustering.\\nThe ﬁve documents have\\nthe x-coordinates 1 + 2ǫ, 4, 5 + 2ǫ, 6 and 7 −ǫ.\\nComplete-link clustering cre-\\nates the two clusters shown as ellipses.\\nThe most intuitive two-cluster cluster-\\ning is {{d1}, {d2, d3, d4, d5}}, but in complete-link clustering, the outlier d1 splits\\n{d2, d3, d4, d5} as shown.\\ndistances without regard to the overall shape of the emerging cluster. This\\neffect is called chaining.\\nCHAINING\\nThe chaining effect is also apparent in Figure 17.1. The last eleven merges\\nof the single-link clustering (those above the 0.1 line) add on single docu-\\nments or pairs of documents, corresponding to a chain. The complete-link\\nclustering in Figure 17.5 avoids this problem. Documents are split into two\\ngroups of roughly equal size when we cut the dendrogram at the last merge.\\nIn general, this is a more useful organization of the data than a clustering\\nwith chains.\\nHowever, complete-link clustering suffers from a different problem. It\\npays too much attention to outliers, points that do not ﬁt well into the global\\nstructure of the cluster. In the example in Figure 17.7 the four documents\\nd2, d3, d4, d5 are split because of the outlier d1 at the left edge (Exercise 17.1).\\nComplete-link clustering does not ﬁnd the most intuitive cluster structure in\\nthis example.\\n17.2.1\\nTime complexity of HAC\\nThe complexity of the naive HAC algorithm in Figure 17.2 is Θ(N3) because\\nwe exhaustively scan the N × N matrix C for the largest similarity in each of\\nN −1 iterations.\\nFor the four HAC methods discussed in this chapter a more efﬁcient algo-\\nrithm is the priority-queue algorithm shown in Figure 17.8. Its time complex-\\nity is Θ(N2 log N). The rows C[k] of the N × N similarity matrix C are sorted\\nin decreasing order of similarity in the priority queues P. P[k].MAX() then\\nreturns the cluster in P[k] that currently has the highest similarity with ωk,\\nwhere we use ωk to denote the kth cluster as in Chapter 16. After creating the\\nmerged cluster of ωk1 and ωk2, ωk1 is used as its representative. The function\\nSIM computes the similarity function for potential merge pairs: largest simi-\\nlarity for single-link, smallest similarity for complete-link, average similarity\\nfor GAAC (Section 17.3), and centroid similarity for centroid clustering (Sec-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n386\\n17\\nHierarchical clustering\\nEFFICIENTHAC(⃗d1, . . . , ⃗dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i].sim ←⃗dn · ⃗di\\n4\\nC[n][i].index ←i\\n5\\nI[n] ←1\\n6\\nP[n] ←priority queue for C[n] sorted on sim\\n7\\nP[n].DELETE(C[n][n]) (don’t want self-similarities)\\n8\\nA ←[]\\n9\\nfor k ←1 to N −1\\n10\\ndo k1 ←arg max{k:I[k]=1} P[k].MAX().sim\\n11\\nk2 ←P[k1].MAX().index\\n12\\nA.APPEND(⟨k1, k2⟩)\\n13\\nI[k2] ←0\\n14\\nP[k1] ←[]\\n15\\nfor each i with I[i] = 1 ∧i ̸= k1\\n16\\ndo P[i].DELETE(C[i][k1])\\n17\\nP[i].DELETE(C[i][k2])\\n18\\nC[i][k1].sim ←SIM(i, k1, k2)\\n19\\nP[i].INSERT(C[i][k1])\\n20\\nC[k1][i].sim ←SIM(i, k1, k2)\\n21\\nP[k1].INSERT(C[k1][i])\\n22\\nreturn A\\nclustering algorithm\\nSIM(i, k1, k2)\\nsingle-link\\nmax(SIM(i, k1), SIM(i, k2))\\ncomplete-link\\nmin(SIM(i, k1), SIM(i, k2))\\ncentroid\\n( 1\\nNm⃗vm) · ( 1\\nNi⃗vi)\\ngroup-average\\n1\\n(Nm+Ni)(Nm+Ni−1)[(⃗vm +⃗vi)2 −(Nm + Ni)]\\ncompute C[5]\\n1\\n2\\n3\\n4\\n5\\n0.2\\n0.8\\n0.6\\n0.4\\n1.0\\ncreate P[5] (by sorting)\\n2\\n3\\n4\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\nmerge 2 and 3, update\\nsimilarity of 2, delete 3\\n2\\n4\\n1\\n0.3\\n0.4\\n0.2\\ndelete and reinsert 2\\n4\\n2\\n1\\n0.4\\n0.3\\n0.2\\n◮Figure 17.8\\nThe priority-queue algorithm for HAC. Top: The algorithm. Center:\\nFour different similarity measures. Bottom: An example for processing steps 6 and\\n16–19. This is a made up example showing P[5] for a 5 × 5 matrix C.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n387\\nSINGLELINKCLUSTERING(d1, . . . , dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i].sim ←SIM(dn, di)\\n4\\nC[n][i].index ←i\\n5\\nI[n] ←n\\n6\\nNBM[n] ←arg maxX∈{C[n][i]:n̸=i} X.sim\\n7\\nA ←[]\\n8\\nfor n ←1 to N −1\\n9\\ndo i1 ←arg max{i:I[i]=i} NBM[i].sim\\n10\\ni2 ←I[NBM[i1].index]\\n11\\nA.APPEND(⟨i1, i2⟩)\\n12\\nfor i ←1 to N\\n13\\ndo if I[i] = i ∧i ̸= i1 ∧i ̸= i2\\n14\\nthen C[i1][i].sim ←C[i][i1].sim ←max(C[i1][i].sim, C[i2][i].sim)\\n15\\nif I[i] = i2\\n16\\nthen I[i] ←i1\\n17\\nNBM[i1] ←arg maxX∈{C[i1][i]:I[i]=i∧i̸=i1} X.sim\\n18\\nreturn A\\n◮Figure 17.9\\nSingle-link clustering algorithm using an NBM array. After merging\\ntwo clusters i1 and i2, the ﬁrst one (i1) represents the merged cluster. If I[i] = i, then i\\nis the representative of its current cluster. If I[i] ̸= i, then i has been merged into the\\ncluster represented by I[i] and will therefore be ignored when updating NBM[i1].\\ntion 17.4). We give an example of how a row of C is processed (Figure 17.8,\\nbottom panel). The loop in lines 1–7 is Θ(N2) and the loop in lines 9–21 is\\nΘ(N2 log N) for an implementation of priority queues that supports deletion\\nand insertion in Θ(log N). The overall complexity of the algorithm is there-\\nfore Θ(N2 log N). In the deﬁnition of the function SIM, ⃗vm and ⃗vi are the\\nvector sums of ωk1 ∪ωk2 and ωi, respectively, and Nm and Ni are the number\\nof documents in ωk1 ∪ωk2 and ωi, respectively.\\nThe argument of EFFICIENTHAC in Figure 17.8 is a set of vectors (as op-\\nposed to a set of generic documents) because GAAC and centroid clustering\\n(Sections 17.3 and 17.4) require vectors as input. The complete-link version\\nof EFFICIENTHAC can also be applied to documents that are not represented\\nas vectors.\\nFor single-link, we can introduce a next-best-merge array (NBM) as a fur-\\nther optimization as shown in Figure 17.9. NBM keeps track of what the best\\nmerge is for each cluster. Each of the two top level for-loops in Figure 17.9\\nare Θ(N2), thus the overall complexity of single-link clustering is Θ(N2).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n388\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\n0\\n1\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n◮Figure 17.10\\nComplete-link clustering is not best-merge persistent. At ﬁrst, d2 is\\nthe best-merge cluster for d3. But after merging d1 and d2, d4 becomes d3’s best-merge\\ncandidate. In a best-merge persistent algorithm like single-link, d3’s best-merge clus-\\nter would be {d1, d2}.\\nCan we also speed up the other three HAC algorithms with an NBM ar-\\nray? We cannot because only single-link clustering is best-merge persistent.\\nBEST-MERGE\\nPERSISTENCE\\nSuppose that the best merge cluster for ωk is ωj in single-link clustering.\\nThen after merging ωj with a third cluster ωi ̸= ωk, the merge of ωi and ωj\\nwill be ωk’s best merge cluster (Exercise 17.6). In other words, the best-merge\\ncandidate for the merged cluster is one of the two best-merge candidates of\\nits components in single-link clustering. This means that C can be updated\\nin Θ(N) in each iteration – by taking a simple max of two values on line 14\\nin Figure 17.9 for each of the remaining ≤N clusters.\\nFigure 17.10 demonstrates that best-merge persistence does not hold for\\ncomplete-link clustering, which means that we cannot use an NBM array to\\nspeed up clustering. After merging d3’s best merge candidate d2 with cluster\\nd1, an unrelated cluster d4 becomes the best merge candidate for d3. This is\\nbecause the complete-link merge criterion is non-local and can be affected by\\npoints at a great distance from the area where two merge candidates meet.\\nIn practice, the efﬁciency penalty of the Θ(N2 log N) algorithm is small\\ncompared with the Θ(N2) single-link algorithm since computing the similar-\\nity between two documents (e.g., as a dot product) is an order of magnitude\\nslower than comparing two scalars in sorting. All four HAC algorithms in\\nthis chapter are Θ(N2) with respect to similarity computations. So the differ-\\nence in complexity is rarely a concern in practice when choosing one of the\\nalgorithms.\\n?\\nExercise 17.1\\nShow that complete-link clustering creates the two-cluster clustering depicted in Fig-\\nure 17.7.\\n17.3\\nGroup-average agglomerative clustering\\nGroup-average agglomerative clustering or GAAC (see Figure 17.3, (d)) evaluates\\nGROUP-AVERAGE\\nAGGLOMERATIVE\\nCLUSTERING\\ncluster quality based on all similarities between documents, thus avoiding\\nthe pitfalls of the single-link and complete-link criteria, which equate cluster\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.3\\nGroup-average agglomerative clustering\\n389\\nsimilarity with the similarity of a single pair of documents. GAAC is also\\ncalled group-average clustering and average-link clustering. GAAC computes\\nthe average similarity SIM-GA of all pairs of documents, including pairs from\\nthe same cluster. But self-similarities are not included in the average:\\nSIM-GA(ωi, ωj) =\\n1\\n(Ni + Nj)(Ni + Nj −1)\\n∑\\ndm∈ωi∪ωj\\n∑\\ndn∈ωi∪ωj,dn̸=dm\\n⃗dm · ⃗dn\\n(17.1)\\nwhere ⃗d is the length-normalized vector of document d, · denotes the dot\\nproduct, and Ni and Nj are the number of documents in ωi and ωj, respec-\\ntively.\\nThe motivation for GAAC is that our goal in selecting two clusters ωi\\nand ωj as the next merge in HAC is that the resulting merge cluster ωk =\\nωi ∪ωj should be coherent. To judge the coherence of ωk, we need to look\\nat all document-document similarities within ωk, including those that occur\\nwithin ωi and those that occur within ωj.\\nWe can compute the measure SIM-GA efﬁciently because the sum of indi-\\nvidual vector similarities is equal to the similarities of their sums:\\n∑\\ndm∈ωi ∑\\ndn∈ωj\\n(⃗dm · ⃗dn) = ( ∑\\ndm∈ωi\\n⃗dm) · ( ∑\\ndn∈ωj\\n⃗dn)\\n(17.2)\\nWith (17.2), we have:\\nSIM-GA(ωi, ωj)\\n=\\n1\\n(Ni + Nj)(Ni + Nj −1)[(\\n∑\\ndm∈ωi∪ωj\\n⃗dm)2 −(Ni + Nj)]\\n(17.3)\\nThe term (Ni + Nj) on the right is the sum of Ni + Nj self-similarities of value\\n1.0. With this trick we can compute cluster similarity in constant time (as-\\nsuming we have available the two vector sums ∑dm∈ωi ⃗dm and ∑dm∈ωj ⃗dm)\\ninstead of in Θ(NiNj). This is important because we need to be able to com-\\npute the function SIM on lines 18 and 20 in EFFICIENTHAC (Figure 17.8)\\nin constant time for efﬁcient implementations of GAAC. Note that for two\\nsingleton clusters, Equation (17.3) is equivalent to the dot product.\\nEquation (17.2) relies on the distributivity of the dot product with respect\\nto vector addition. Since this is crucial for the efﬁcient computation of a\\nGAAC clustering, the method cannot be easily applied to representations of\\ndocuments that are not real-valued vectors. Also, Equation (17.2) only holds\\nfor the dot product. While many algorithms introduced in this book have\\nnear-equivalent descriptions in terms of dot product, cosine similarity and\\nEuclidean distance (cf. Section 14.1, page 291), Equation (17.2) can only be\\nexpressed using the dot product. This is a fundamental difference between\\nsingle-link/complete-link clustering and GAAC. The ﬁrst two only require a\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n390\\n17\\nHierarchical clustering\\nsquare matrix of similarities as input and do not care how these similarities\\nwere computed.\\nTo summarize, GAAC requires (i) documents represented as vectors, (ii)\\nlength normalization of vectors, so that self-similarities are 1.0, and (iii) the\\ndot product as the measure of similarity between vectors and sums of vec-\\ntors.\\nThe merge algorithms for GAAC and complete-link clustering are the same\\nexcept that we use Equation (17.3) as similarity function in Figure 17.8. There-\\nfore, the overall time complexity of GAAC is the same as for complete-link\\nclustering: Θ(N2 log N). Like complete-link clustering, GAAC is not best-\\nmerge persistent (Exercise 17.6). This means that there is no Θ(N2) algorithm\\nfor GAAC that would be analogous to the Θ(N2) algorithm for single-link in\\nFigure 17.9.\\nWe can also deﬁne group-average similarity as including self-similarities:\\nSIM-GA′(ωi, ωj) =\\n1\\n(Ni+Nj)2 (\\n∑\\ndm∈ωi∪ωj\\n⃗dm)2 =\\n1\\nNi+Nj\\n∑\\ndm∈ωi∪ωj\\n[⃗dm ·⃗µ(ωi∪ωj)]\\n(17.4)\\nwhere the centroid ⃗µ(ω) is deﬁned as in Equation (14.1) (page 292). This\\ndeﬁnition is equivalent to the intuitive deﬁnition of cluster quality as average\\nsimilarity of documents ⃗dm to the cluster’s centroid ⃗µ.\\nSelf-similarities are always equal to 1.0, the maximum possible value for\\nlength-normalized vectors. The proportion of self-similarities in Equation (17.4)\\nis i/i2 = 1/i for a cluster of size i. This gives an unfair advantage to small\\nclusters since they will have proportionally more self-similarities. For two\\ndocuments d1, d2 with a similarity s, we have SIM-GA′(d1, d2) = (1 + s)/2.\\nIn contrast, SIM-GA(d1, d2) = s ≤(1 + s)/2. This similarity SIM-GA(d1, d2)\\nof two documents is the same as in single-link, complete-link and centroid\\nclustering. We prefer the deﬁnition in Equation (17.3), which excludes self-\\nsimilarities from the average, because we do not want to penalize large clus-\\nters for their smaller proportion of self-similarities and because we want a\\nconsistent similarity value s for document pairs in all four HAC algorithms.\\n?\\nExercise 17.2\\nApply group-average clustering to the points in Figures 17.6 and 17.7. Map them onto\\nthe surface of the unit sphere in a three-dimensional space to get length-normalized\\nvectors. Is the group-average clustering different from the single-link and complete-\\nlink clusterings?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.4\\nCentroid clustering\\n391\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n2\\n3\\n4\\n5\\n× d1\\n× d2\\n× d3\\n× d4\\n×\\nd5\\n× d6\\nb\\nc\\nµ1\\nb\\nc\\nµ3\\nb\\nc µ2\\n◮Figure 17.11\\nThree iterations of centroid clustering. Each iteration merges the\\ntwo clusters whose centroids are closest.\\n17.4\\nCentroid clustering\\nIn centroid clustering, the similarity of two clusters is deﬁned as the similar-\\nity of their centroids:\\nSIM-CENT(ωi, ωj)\\n=\\n⃗µ(ωi) ·⃗µ(ωj)\\n(17.5)\\n=\\n( 1\\nNi ∑\\ndm∈ωi\\n⃗dm) · ( 1\\nNj ∑\\ndn∈ωj\\n⃗dn)\\n=\\n1\\nNiNj ∑\\ndm∈ωi ∑\\ndn∈ωj\\n⃗dm · ⃗dn\\n(17.6)\\nEquation (17.5) is centroid similarity. Equation (17.6) shows that centroid\\nsimilarity is equivalent to average similarity of all pairs of documents from\\ndifferent clusters. Thus, the difference between GAAC and centroid clustering\\nis that GAAC considers all pairs of documents in computing average pair-\\nwise similarity (Figure 17.3, (d)) whereas centroid clustering excludes pairs\\nfrom the same cluster (Figure 17.3, (c)).\\nFigure 17.11 shows the ﬁrst three steps of a centroid clustering. The ﬁrst\\ntwo iterations form the clusters {d5, d6} with centroid µ1 and {d1, d2} with\\ncentroid µ2 because the pairs ⟨d5, d6⟩and ⟨d1, d2⟩have the highest centroid\\nsimilarities. In the third iteration, the highest centroid similarity is between\\nµ1 and d4 producing the cluster {d4, d5, d6} with centroid µ3.\\nLike GAAC, centroid clustering is not best-merge persistent and therefore\\nΘ(N2 log N) (Exercise 17.6).\\nIn contrast to the other three HAC algorithms, centroid clustering is not\\nmonotonic. So-called inversions can occur: Similarity can increase during\\nINVERSION\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n392\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n0\\n1\\n2\\n3\\n4\\n5\\n×\\n×\\n×\\nb\\nc\\nd1\\nd2\\nd3\\n−4\\n−3\\n−2\\n−1\\n0\\nd1\\nd2\\nd3\\n◮Figure 17.12\\nCentroid clustering is not monotonic. The documents d1 at (1+ ǫ, 1),\\nd2 at (5, 1), and d3 at (3, 1 + 2\\n√\\n3) are almost equidistant, with d1 and d2 closer to\\neach other than to d3. The non-monotonic inversion in the hierarchical clustering\\nof the three points appears as an intersecting merge line in the dendrogram. The\\nintersection is circled.\\nclustering as in the example in Figure 17.12, where we deﬁne similarity as\\nnegative distance. In the ﬁrst merge, the similarity of d1 and d2 is −(4 −ǫ). In\\nthe second merge, the similarity of the centroid of d1 and d2 (the circle) and d3\\nis ≈−cos(π/6) × 4 = −\\n√\\n3/2 × 4 ≈−3.46 > −(4 −ǫ). This is an example\\nof an inversion: similarity increases in this sequence of two clustering steps.\\nIn a monotonic HAC algorithm, similarity is monotonically decreasing from\\niteration to iteration.\\nIncreasing similarity in a series of HAC clustering steps contradicts the\\nfundamental assumption that small clusters are more coherent than large\\nclusters. An inversion in a dendrogram shows up as a horizontal merge line\\nthat is lower than the previous merge line. All merge lines in Figures 17.1\\nand 17.5 are higher than their predecessors because single-link and complete-\\nlink clustering are monotonic clustering algorithms.\\nDespite its non-monotonicity, centroid clustering is often used because its\\nsimilarity measure – the similarity of two centroids – is conceptually simpler\\nthan the average of all pairwise similarities in GAAC. Figure 17.11 is all one\\nneeds to understand centroid clustering. There is no equally simple graph\\nthat would explain how GAAC works.\\n?\\nExercise 17.3\\nFor a ﬁxed set of N documents there are up to N2 distinct similarities between clusters\\nin single-link and complete-link clustering. How many distinct cluster similarities are\\nthere in GAAC and centroid clustering?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.5\\nOptimality of HAC\\n393\\n$\\n17.5\\nOptimality of HAC\\nTo state the optimality conditions of hierarchical clustering precisely, we ﬁrst\\ndeﬁne the combination similarity COMB-SIM of a clustering Ω= {ω1, . . . , ωK}\\nas the smallest combination similarity of any of its K clusters:\\nCOMB-SIM({ω1, . . . , ωK}) = min\\nk\\nCOMB-SIM(ωk)\\nRecall that the combination similarity of a cluster ω that was created as the\\nmerge of ω1 and ω2 is the similarity of ω1 and ω2 (page 378).\\nWe then deﬁne Ω= {ω1, . . . , ωK} to be optimal if all clusterings Ω′ with k\\nOPTIMAL CLUSTERING\\nclusters, k ≤K, have lower combination similarities:\\n|Ω′| ≤|Ω| ⇒COMB-SIM(Ω′) ≤COMB-SIM(Ω)\\nFigure 17.12 shows that centroid clustering is not optimal. The cluster-\\ning {{d1, d2}, {d3}} (for K = 2) has combination similarity −(4 −ǫ) and\\n{{d1, d2, d3}} (for K = 1) has combination similarity -3.46. So the cluster-\\ning {{d1, d2}, {d3}} produced in the ﬁrst merge is not optimal since there is\\na clustering with fewer clusters ({{d1, d2, d3}}) that has higher combination\\nsimilarity. Centroid clustering is not optimal because inversions can occur.\\nThe above deﬁnition of optimality would be of limited use if it was only\\napplicable to a clustering together with its merge history. However, we can\\nshow (Exercise 17.4) that combination similarity for the three non-inversion\\nCOMBINATION\\nSIMILARITY\\nalgorithms can be read off from the cluster without knowing its history. These\\ndirect deﬁnitions of combination similarity are as follows.\\nsingle-link The combination similarity of a cluster ω is the smallest similar-\\nity of any bipartition of the cluster, where the similarity of a bipartition is\\nthe largest similarity between any two documents from the two parts:\\nCOMB-SIM(ω) =\\nmin\\n{ω′:ω′⊂ω} max\\ndi∈ω′\\nmax\\ndj∈ω−ω′ SIM(di, dj)\\nwhere each ⟨ω′, ω −ω′⟩is a bipartition of ω.\\ncomplete-link The combination similarity of a cluster ω is the smallest sim-\\nilarity of any two points in ω: mindi∈ω mindj∈ω SIM(di, dj).\\nGAAC The combination similarity of a cluster ω is the average of all pair-\\nwise similarities in ω (where self-similarities are not included in the aver-\\nage): Equation (17.3).\\nIf we use these deﬁnitions of combination similarity, then optimality is a\\nproperty of a set of clusters and not of a process that produces a set of clus-\\nters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n394\\n17\\nHierarchical clustering\\nWe can now prove the optimality of single-link clustering by induction\\nover the number of clusters K. We will give a proof for the case where no two\\npairs of documents have the same similarity, but it can easily be extended to\\nthe case with ties.\\nThe inductive basis of the proof is that a clustering with K = N clusters has\\ncombination similarity 1.0, which is the largest value possible. The induc-\\ntion hypothesis is that a single-link clustering ΩK with K clusters is optimal:\\nCOMB-SIM(ΩK) ≥COMB-SIM(Ω′\\nK) for all Ω′\\nK. Assume for contradiction that\\nthe clustering ΩK−1 we obtain by merging the two most similar clusters in\\nΩK is not optimal and that instead a different sequence of merges Ω′\\nK, Ω′\\nK−1\\nleads to the optimal clustering with K −1 clusters. We can write the as-\\nsumption that Ω′\\nK−1 is optimal and that ΩK−1 is not as COMB-SIM(Ω′\\nK−1) >\\nCOMB-SIM(ΩK−1).\\nCase 1: The two documents linked by s = COMB-SIM(Ω′\\nK−1) are in the\\nsame cluster in ΩK. They can only be in the same cluster if a merge with sim-\\nilarity smaller than s has occurred in the merge sequence producing ΩK. This\\nimplies s > COMB-SIM(ΩK). Thus, COMB-SIM(Ω′\\nK−1) = s > COMB-SIM(ΩK) >\\nCOMB-SIM(Ω′\\nK) > COMB-SIM(Ω′\\nK−1). Contradiction.\\nCase 2: The two documents linked by s = COMB-SIM(Ω′\\nK−1) are not in\\nthe same cluster in ΩK. But s = COMB-SIM(Ω′\\nK−1) > COMB-SIM(ΩK−1), so\\nthe single-link merging rule should have merged these two clusters when\\nprocessing ΩK. Contradiction.\\nThus, ΩK−1 is optimal.\\nIn contrast to single-link clustering, complete-link clustering and GAAC\\nare not optimal as this example shows:\\n×\\n×\\n×\\n×\\n1\\n3\\n3\\nd1\\nd2\\nd3\\nd4\\nBoth algorithms merge the two points with distance 1 (d2 and d3) ﬁrst and\\nthus cannot ﬁnd the two-cluster clustering {{d1, d2}, {d3, d4}}. But {{d1, d2}, {d3, d4}}\\nis optimal on the optimality criteria of complete-link clustering and GAAC.\\nHowever, the merge criteria of complete-link clustering and GAAC ap-\\nproximate the desideratum of approximate sphericity better than the merge\\ncriterion of single-link clustering. In many applications, we want spheri-\\ncal clusters. Thus, even though single-link clustering may seem preferable at\\nﬁrst because of its optimality, it is optimal with respect to the wrong criterion\\nin many document clustering applications.\\nTable 17.1 summarizes the properties of the four HAC algorithms intro-\\nduced in this chapter. We recommend GAAC for document clustering be-\\ncause it is generally the method that produces the clustering with the best\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.6\\nDivisive clustering\\n395\\nmethod\\ncombination similarity\\ntime compl.\\noptimal?\\ncomment\\nsingle-link\\nmax inter-similarity of any 2 docs\\nΘ(N2)\\nyes\\nchaining effect\\ncomplete-link\\nmin inter-similarity of any 2 docs\\nΘ(N2 log N)\\nno\\nsensitive to outliers\\ngroup-average\\naverage of all sims\\nΘ(N2 log N)\\nno\\nbest choice for\\nmost applications\\ncentroid\\naverage inter-similarity\\nΘ(N2 log N)\\nno\\ninversions can occur\\n◮Table 17.1\\nComparison of HAC algorithms.\\nproperties for applications. It does not suffer from chaining, from sensitivity\\nto outliers and from inversions.\\nThere are two exceptions to this recommendation. First, for non-vector\\nrepresentations, GAAC is not applicable and clustering should typically be\\nperformed with the complete-link method.\\nSecond, in some applications the purpose of clustering is not to create a\\ncomplete hierarchy or exhaustive partition of the entire document set. For\\ninstance, ﬁrst story detection or novelty detection is the task of detecting the ﬁrst\\nFIRST STORY\\nDETECTION\\noccurrence of an event in a stream of news stories. One approach to this task\\nis to ﬁnd a tight cluster within the documents that were sent across the wire\\nin a short period of time and are dissimilar from all previous documents. For\\nexample, the documents sent over the wire in the minutes after the World\\nTrade Center attack on September 11, 2001 form such a cluster. Variations of\\nsingle-link clustering can do well on this task since it is the structure of small\\nparts of the vector space – and not global structure – that is important in this\\ncase.\\nSimilarly, we will describe an approach to duplicate detection on the web\\nin Section 19.6 (page 440) where single-link clustering is used in the guise of\\nthe union-ﬁnd algorithm. Again, the decision whether a group of documents\\nare duplicates of each other is not inﬂuenced by documents that are located\\nfar away and single-link clustering is a good choice for duplicate detection.\\n?\\nExercise 17.4\\nShow the equivalence of the two deﬁnitions of combination similarity: the process\\ndeﬁnition on page 378 and the static deﬁnition on page 393.\\n17.6\\nDivisive clustering\\nSo far we have only looked at agglomerative clustering, but a cluster hierar-\\nchy can also be generated top-down. This variant of hierarchical clustering\\nis called top-down clustering or divisive clustering. We start at the top with all\\nTOP-DOWN\\nCLUSTERING\\ndocuments in one cluster. The cluster is split using a ﬂat clustering algo-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n396\\n17\\nHierarchical clustering\\nrithm. This procedure is applied recursively until each document is in its\\nown singleton cluster.\\nTop-down clustering is conceptually more complex than bottom-up clus-\\ntering since we need a second, ﬂat clustering algorithm as a “subroutine”. It\\nhas the advantage of being more efﬁcient if we do not generate a complete\\nhierarchy all the way down to individual document leaves. For a ﬁxed num-\\nber of top levels, using an efﬁcient ﬂat algorithm like K-means, top-down\\nalgorithms are linear in the number of documents and clusters. So they run\\nmuch faster than HAC algorithms, which are at least quadratic.\\nThere is evidence that divisive algorithms produce more accurate hierar-\\nchies than bottom-up algorithms in some circumstances. See the references\\non bisecting K-means in Section 17.9.\\nBottom-up methods make cluster-\\ning decisions based on local patterns without initially taking into account\\nthe global distribution. These early decisions cannot be undone. Top-down\\nclustering beneﬁts from complete information about the global distribution\\nwhen making top-level partitioning decisions.\\n17.7\\nCluster labeling\\nIn many applications of ﬂat clustering and hierarchical clustering, particu-\\nlarly in analysis tasks and in user interfaces (see applications in Table 16.1,\\npage 351), human users interact with clusters. In such settings, we must label\\nclusters, so that users can see what a cluster is about.\\nDifferential cluster labeling selects cluster labels by comparing the distribu-\\nDIFFERENTIAL CLUSTER\\nLABELING\\ntion of terms in one cluster with that of other clusters. The feature selection\\nmethods we introduced in Section 13.5 (page 271) can all be used for differen-\\ntial cluster labeling.5 In particular, mutual information (MI) (Section 13.5.1,\\npage 272) or, equivalently, information gain and the χ2-test (Section 13.5.2,\\npage 275) will identify cluster labels that characterize one cluster in contrast\\nto other clusters. A combination of a differential test with a penalty for rare\\nterms often gives the best labeling results because rare terms are not neces-\\nsarily representative of the cluster as a whole.\\nWe apply three labeling methods to a K-means clustering in Table 17.2. In\\nthis example, there is almost no difference between MI and χ2. We therefore\\nomit the latter.\\nCluster-internal labeling computes a label that solely depends on the cluster\\nCLUSTER-INTERNAL\\nLABELING\\nitself, not on other clusters. Labeling a cluster with the title of the document\\nclosest to the centroid is one cluster-internal method. Titles are easier to read\\nthan a list of terms. A full title can also contain important context that didn’t\\nmake it into the top 10 terms selected by MI. On the web, anchor text can\\n5. Selecting the most frequent terms is a non-differential feature selection technique we dis-\\ncussed in Section 13.5. It can also be used for labeling clusters.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.7\\nCluster labeling\\n397\\nlabeling method\\n# docs\\ncentroid\\nmutual information\\ntitle\\n4\\n622\\noil plant mexico pro-\\nduction crude power\\n000 reﬁnery gas bpd\\nplant\\noil\\nproduction\\nbarrels\\ncrude\\nbpd\\nmexico dolly capacity\\npetroleum\\nMEXICO:\\nHurri-\\ncane Dolly heads for\\nMexico coast\\n9\\n1017\\npolice security russian\\npeople military peace\\nkilled\\ntold\\ngrozny\\ncourt\\npolice killed military\\nsecurity\\npeace\\ntold\\ntroops\\nforces rebels\\npeople\\nRUSSIA:\\nRussia’s\\nLebed\\nmeets\\nrebel\\nchief in Chechnya\\n10\\n1259\\n00 000 tonnes traders\\nfutures wheat prices\\ncents\\nseptember\\ntonne\\ndelivery\\ntraders\\nfutures tonne tonnes\\ndesk wheat prices 000\\n00\\nUSA: Export Business\\n- Grain/oilseeds com-\\nplex\\n◮Table 17.2\\nAutomatically computed cluster labels. This is for three of ten clusters\\n(4, 9, and 10) in a K-means clustering of the ﬁrst 10,000 documents in Reuters-RCV1.\\nThe last three columns show cluster summaries computed by three labeling methods:\\nmost highly weighted terms in centroid (centroid), mutual information, and the title\\nof the document closest to the centroid of the cluster (title). Terms selected by only\\none of the ﬁrst two methods are in bold.\\nplay a role similar to a title since the anchor text pointing to a page can serve\\nas a concise summary of its contents.\\nIn Table 17.2, the title for cluster 9 suggests that many of its documents are\\nabout the Chechnya conﬂict, a fact the MI terms do not reveal. However, a\\nsingle document is unlikely to be representative of all documents in a cluster.\\nAn example is cluster 4, whose selected title is misleading. The main topic of\\nthe cluster is oil. Articles about hurricane Dolly only ended up in this cluster\\nbecause of its effect on oil prices.\\nWe can also use a list of terms with high weights in the centroid of the clus-\\nter as a label. Such highly weighted terms (or, even better, phrases, especially\\nnoun phrases) are often more representative of the cluster than a few titles\\ncan be, even if they are not ﬁltered for distinctiveness as in the differential\\nmethods. However, a list of phrases takes more time to digest for users than\\na well crafted title.\\nCluster-internal methods are efﬁcient, but they fail to distinguish terms\\nthat are frequent in the collection as a whole from those that are frequent only\\nin the cluster. Terms like year or Tuesday may be among the most frequent in\\na cluster, but they are not helpful in understanding the contents of a cluster\\nwith a speciﬁc topic like oil.\\nIn Table 17.2, the centroid method selects a few more uninformative terms\\n(000, court, cents, september) than MI (forces, desk), but most of the terms se-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n398\\n17\\nHierarchical clustering\\nlected by either method are good descriptors. We get a good sense of the\\ndocuments in a cluster from scanning the selected terms.\\nFor hierarchical clustering, additional complications arise in cluster label-\\ning. Not only do we need to distinguish an internal node in the tree from\\nits siblings, but also from its parent and its children. Documents in child\\nnodes are by deﬁnition also members of their parent node, so we cannot use\\na naive differential method to ﬁnd labels that distinguish the parent from\\nits children. However, more complex criteria, based on a combination of\\noverall collection frequency and prevalence in a given cluster, can determine\\nwhether a term is a more informative label for a child node or a parent node\\n(see Section 17.9).\\n17.8\\nImplementation notes\\nMost problems that require the computation of a large number of dot prod-\\nucts beneﬁt from an inverted index. This is also the case for HAC clustering.\\nComputational savings due to the inverted index are large if there are many\\nzero similarities – either because many documents do not share any terms or\\nbecause an aggressive stop list is used.\\nIn low dimensions, more aggressive optimizations are possible that make\\nthe computation of most pairwise similarities unnecessary (Exercise 17.10).\\nHowever, no such algorithms are known in higher dimensions. We encoun-\\ntered the same problem in kNN classiﬁcation (see Section 14.7, page 314).\\nWhen using GAAC on a large document set in high dimensions, we have\\nto take care to avoid dense centroids. For dense centroids, clustering can\\ntake time Θ(MN2 log N) where M is the size of the vocabulary, whereas\\ncomplete-link clustering is Θ(MaveN2 log N) where Mave is the average size\\nof the vocabulary of a document. So for large vocabularies complete-link\\nclustering can be more efﬁcient than an unoptimized implementation of GAAC.\\nWe discussed this problem in the context of K-means clustering in Chap-\\nter 16 (page 365) and suggested two solutions: truncating centroids (keeping\\nonly highly weighted terms) and representing clusters by means of sparse\\nmedoids instead of dense centroids. These optimizations can also be applied\\nto GAAC and centroid clustering.\\nEven with these optimizations, HAC algorithms are all Θ(N2) or Θ(N2 log N)\\nand therefore infeasible for large sets of 1,000,000 or more documents. For\\nsuch large sets, HAC can only be used in combination with a ﬂat clustering\\nalgorithm like K-means. Recall that K-means requires a set of seeds as initial-\\nization (Figure 16.5, page 361). If these seeds are badly chosen, then the re-\\nsulting clustering will be of poor quality. We can employ an HAC algorithm\\nto compute seeds of high quality. If the HAC algorithm is applied to a docu-\\nment subset of size\\n√\\nN, then the overall runtime of K-means cum HAC seed\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.9\\nReferences and further reading\\n399\\ngeneration is Θ(N). This is because the application of a quadratic algorithm\\nto a sample of size\\n√\\nN has an overall complexity of Θ(N). An appropriate\\nadjustment can be made for an Θ(N2 log N) algorithm to guarantee linear-\\nity. This algorithm is referred to as the Buckshot algorithm. It combines the\\nBUCKSHOT\\nALGORITHM\\ndeterminism and higher reliability of HAC with the efﬁciency of K-means.\\n17.9\\nReferences and further reading\\nAn excellent general review of clustering is (Jain et al. 1999). Early references\\nfor speciﬁc HAC algorithms are (King 1967) (single-link), (Sneath and Sokal\\n1973) (complete-link, GAAC) and (Lance and Williams 1967) (discussing a\\nlarge variety of hierarchical clustering algorithms). The single-link algorithm\\nin Figure 17.9 is similar to Kruskal’s algorithm for constructing a minimum\\nKRUSKAL’S\\nALGORITHM\\nspanning tree. A graph-theoretical proof of the correctness of Kruskal’s al-\\ngorithm (which is analogous to the proof in Section 17.5) is provided by Cor-\\nmen et al. (1990, Theorem 23.1). See Exercise 17.5 for the connection between\\nminimum spanning trees and single-link clusterings.\\nIt is often claimed that hierarchical clustering algorithms produce better\\nclusterings than ﬂat algorithms (Jain and Dubes (1988, p. 140), Cutting et al.\\n(1992), Larsen and Aone (1999)) although more recently there have been ex-\\nperimental results suggesting the opposite (Zhao and Karypis 2002). Even\\nwithout a consensus on average behavior, there is no doubt that results of\\nEM and K-means are highly variable since they will often converge to a local\\noptimum of poor quality. The HAC algorithms we have presented here are\\ndeterministic and thus more predictable.\\nThe complexity of complete-link, group-average and centroid clustering\\nis sometimes given as Θ(N2) (Day and Edelsbrunner 1984, Voorhees 1985b,\\nMurtagh 1983) because a document similarity computation is an order of\\nmagnitude more expensive than a simple comparison, the main operation\\nexecuted in the merging steps after the N × N similarity matrix has been\\ncomputed.\\nThe centroid algorithm described here is due to Voorhees (1985b). Voorhees\\nrecommends complete-link and centroid clustering over single-link for a re-\\ntrieval application. The Buckshot algorithm was originally published by Cut-\\nting et al. (1993). Allan et al. (1998) apply single-link clustering to ﬁrst story\\ndetection.\\nAn important HAC technique not discussed here is Ward’s method (Ward\\nWARD’S METHOD\\nJr. 1963, El-Hamdouchi and Willett 1986), also called minimum variance clus-\\ntering. In each step, it selects the merge with the smallest RSS (Chapter 16,\\npage 360). The merge criterion in Ward’s method (a function of all individual\\ndistances from the centroid) is closely related to the merge criterion in GAAC\\n(a function of all individual similarities to the centroid).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n400\\n17\\nHierarchical clustering\\nDespite its importance for making the results of clustering useful, compar-\\natively little work has been done on labeling clusters. Popescul and Ungar\\n(2000) obtain good results with a combination of χ2 and collection frequency\\nof a term. Glover et al. (2002b) use information gain for labeling clusters of\\nweb pages. Stein and zu Eissen’s approach is ontology-based (2004). The\\nmore complex problem of labeling nodes in a hierarchy (which requires dis-\\ntinguishing more general labels for parents from more speciﬁc labels for chil-\\ndren) is tackled by Glover et al. (2002a) and Treeratpituk and Callan (2006).\\nSome clustering algorithms attempt to ﬁnd a set of labels ﬁrst and then build\\n(often overlapping) clusters around the labels, thereby avoiding the problem\\nof labeling altogether (Zamir and Etzioni 1999, Käki 2005, Osi´nski and Weiss\\n2005). We know of no comprehensive study that compares the quality of\\nsuch “label-based” clustering to the clustering algorithms discussed in this\\nchapter and in Chapter 16. In principle, work on multi-document summa-\\nrization (McKeown and Radev 1995) is also applicable to cluster labeling, but\\nmulti-document summaries are usually longer than the short text fragments\\nneeded when labeling clusters (cf. Section 8.7, page 170). Presenting clusters\\nin a way that users can understand is a UI problem. We recommend read-\\ning (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) for an introduction to user\\ninterfaces in IR.\\nAn example of an efﬁcient divisive algorithm is bisecting K-means (Stein-\\nbach et al. 2000). Spectral clustering algorithms (Kannan et al. 2000, Dhillon\\nSPECTRAL CLUSTERING\\n2001, Zha et al. 2001, Ng et al. 2001a), including principal direction divisive\\npartitioning (PDDP) (whose bisecting decisions are based on SVD, see Chap-\\nter 18) (Boley 1998, Savaresi and Boley 2004), are computationally more ex-\\npensive than bisecting K-means, but have the advantage of being determin-\\nistic.\\nUnlike K-means and EM, most hierarchical clustering algorithms do not\\nhave a probabilistic interpretation. Model-based hierarchical clustering (Vaithyanathan\\nand Dom 2000, Kamvar et al. 2002, Castro et al. 2004) is an exception.\\nThe evaluation methodology described in Section 16.3 (page 356) is also\\napplicable to hierarchical clustering. Specialized evaluation measures for hi-\\nerarchies are discussed by Fowlkes and Mallows (1983), Larsen and Aone\\n(1999) and Sahoo et al. (2006).\\nThe R environment (R Development Core Team 2005) offers good support\\nfor hierarchical clustering. The R function hclust implements single-link,\\ncomplete-link, group-average, and centroid clustering; and Ward’s method.\\nAnother option provided is median clustering which represents each cluster\\nby its medoid (cf. k-medoids in Chapter 16, page 365). Support for cluster-\\ning vectors in high-dimensional spaces is provided by the software package\\nCLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n17.10\\nExercises\\n401\\n17.10\\nExercises\\n?\\nExercise 17.5\\nA single-link clustering can also be computed from the minimum spanning tree of a\\nMINIMUM SPANNING\\nTREE\\ngraph. The minimum spanning tree connects the vertices of a graph at the smallest\\npossible cost, where cost is deﬁned as the sum over all edges of the graph. In our\\ncase the cost of an edge is the distance between two documents. Show that if ∆k−1 >\\n∆k > . . . > ∆1 are the costs of the edges of a minimum spanning tree, then these\\nedges correspond to the k −1 merges in constructing a single-link clustering.\\nExercise 17.6\\nShow that single-link clustering is best-merge persistent and that GAAC and centroid\\nclustering are not best-merge persistent.\\nExercise 17.7\\na. Consider running 2-means clustering on a collection with documents from two\\ndifferent languages. What result would you expect?\\nb. Would you expect the same result when running an HAC algorithm?\\nExercise 17.8\\nDownload Reuters-21578. Keep only documents that are in the classes crude, inter-\\nest, and grain. Discard documents that are members of more than one of these three\\nclasses. Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid cluster-\\ning of the documents. (v) Cut each dendrogram at the second branch from the top to\\nobtain K = 3 clusters. Compute the Rand index for each of the 4 clusterings. Which\\nclustering method performs best?\\nExercise 17.9\\nSuppose a run of HAC ﬁnds the clustering with K = 7 to have the highest value on\\nsome prechosen goodness measure of clustering. Have we found the highest-value\\nclustering among all clusterings with K = 7?\\nExercise 17.10\\nConsider the task of producing a single-link clustering of N points on a line:\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nShow that we only need to compute a total of about N similarities. What is the overall\\ncomplexity of single-link clustering for a set of points on a line?\\nExercise 17.11\\nProve that single-link, complete-link, and group-average clustering are monotonic in\\nthe sense deﬁned on page 378.\\nExercise 17.12\\nFor N points, there are ≤NK different ﬂat clusterings into K clusters (Section 16.2,\\npage 356). What is the number of different hierarchical clusterings (or dendrograms)\\nof N documents? Are there more ﬂat clusterings or more hierarchical clusterings for\\ngiven K and N?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n403\\n18\\nMatrix decompositions and latent\\nsemantic indexing\\nOn page 123 we introduced the notion of a term-document matrix: an M × N\\nmatrix C, each of whose rows represents a term and each of whose columns\\nrepresents a document in the collection. Even for a collection of modest size,\\nthe term-document matrix C is likely to have several tens of thousands of\\nrows and columns. In Section 18.1.1 we ﬁrst develop a class of operations\\nfrom linear algebra, known as matrix decomposition. In Section 18.2 we use a\\nspecial form of matrix decomposition to construct a low-rank approximation\\nto the term-document matrix. In Section 18.3 we examine the application\\nof such low-rank approximations to indexing and retrieving documents, a\\ntechnique referred to as latent semantic indexing. While latent semantic in-\\ndexing has not been established as a signiﬁcant force in scoring and ranking\\nfor information retrieval, it remains an intriguing approach to clustering in a\\nnumber of domains including for collections of text documents (Section 16.6,\\npage 372). Understanding its full potential remains an area of active research.\\nReaders who do not require a refresher on linear algebra may skip Sec-\\ntion 18.1, although Example 18.1 is especially recommended as it highlights\\na property of eigenvalues that we exploit later in the chapter.\\n18.1\\nLinear algebra review\\nWe brieﬂy review some necessary background in linear algebra. Let C be\\nan M × N matrix with real-valued entries; for a term-document matrix, all\\nentries are in fact non-negative. The rank of a matrix is the number of linearly\\nRANK\\nindependent rows (or columns) in it; thus, rank(C) ≤min{M, N}. A square\\nr × r matrix all of whose off-diagonal entries are zero is called a diagonal\\nmatrix; its rank is equal to the number of non-zero diagonal entries. If all\\nr diagonal entries of such a diagonal matrix are 1, it is called the identity\\nmatrix of dimension r and represented by Ir.\\nFor a square M × M matrix C and a vector ⃗x that is not all zeros, the values\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n404\\n18\\nMatrix decompositions and latent semantic indexing\\nof λ satisfying\\nC⃗x = λ⃗x\\n(18.1)\\nare called the eigenvalues of C . The N-vector ⃗x satisfying Equation (18.1)\\nEIGENVALUE\\nfor an eigenvalue λ is the corresponding right eigenvector. The eigenvector\\ncorresponding to the eigenvalue of largest magnitude is called the principal\\neigenvector. In a similar fashion, the left eigenvectors of C are the M-vectors y\\nsuch that\\n⃗yT C = λ⃗yT.\\n(18.2)\\nThe number of non-zero eigenvalues of C is at most rank(C).\\nThe eigenvalues of a matrix are found by solving the characteristic equation,\\nwhich is obtained by rewriting Equation (18.1) in the form (C −λIM)⃗x = 0.\\nThe eigenvalues of C are then the solutions of |(C −λIM)| = 0, where |S|\\ndenotes the determinant of a square matrix S. The equation |(C −λIM)| = 0\\nis an Mth order polynomial equation in λ and can have at most M roots,\\nwhich are the eigenvalues of C. These eigenvalues can in general be complex,\\neven if all entries of C are real.\\nWe now examine some further properties of eigenvalues and eigenvectors,\\nto set up the central idea of singular value decompositions in Section 18.2 be-\\nlow. First, we look at the relationship between matrix-vector multiplication\\nand eigenvalues.\\n\\x0f\\nExample 18.1:\\nConsider the matrix\\nS =\\n\\uf8eb\\n\\uf8ed\\n30\\n0\\n0\\n0\\n20\\n0\\n0\\n0\\n1\\n\\uf8f6\\n\\uf8f8.\\nClearly the matrix has rank 3, and has 3 non-zero eigenvalues λ1 = 30, λ2 = 20 and\\nλ3 = 1, with the three corresponding eigenvectors\\n⃗x1 =\\n\\uf8eb\\n\\uf8ed\\n1\\n0\\n0\\n\\uf8f6\\n\\uf8f8, ⃗x2 =\\n\\uf8eb\\n\\uf8ed\\n0\\n1\\n0\\n\\uf8f6\\n\\uf8f8and ⃗x3 =\\n\\uf8eb\\n\\uf8ed\\n0\\n0\\n1\\n\\uf8f6\\n\\uf8f8.\\nFor each of the eigenvectors, multiplication by S acts as if we were multiplying the\\neigenvector by a multiple of the identity matrix; the multiple is different for each\\neigenvector. Now, consider an arbitrary vector, such as ⃗v =\\n\\uf8eb\\n\\uf8ed\\n2\\n4\\n6\\n\\uf8f6\\n\\uf8f8. We can always\\nexpress⃗v as a linear combination of the three eigenvectors of S; in the current example\\nwe have\\n⃗v =\\n\\uf8eb\\n\\uf8ed\\n2\\n4\\n6\\n\\uf8f6\\n\\uf8f8= 2⃗x1 + 4⃗x2 + 6⃗x3.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.1\\nLinear algebra review\\n405\\nSuppose we multiply⃗v by S:\\nS⃗v\\n=\\nS(2⃗x1 + 4⃗x2 + 6⃗x3)\\n=\\n2S⃗x1 + 4S⃗x2 + 6S⃗x3\\n=\\n2λ1⃗x1 + 4λ2⃗x2 + 6λ3⃗x3\\n=\\n60⃗x1 + 80⃗x2 + 6⃗x3.\\n(18.3)\\nExample 18.1 shows that even though ⃗v is an arbitrary vector, the effect of\\nmultiplication by S is determined by the eigenvalues and eigenvectors of S.\\nFurthermore, it is intuitively apparent from Equation (18.3) that the product\\nS⃗v is relatively unaffected by terms arising from the small eigenvalues of S;\\nin our example, since λ3 = 1, the contribution of the third term on the right\\nhand side of Equation (18.3) is small. In fact, if we were to completely ignore\\nthe contribution in Equation (18.3) from the third eigenvector corresponding\\nto λ3 = 1, then the product S⃗v would be computed to be\\n\\uf8eb\\n\\uf8ed\\n60\\n80\\n0\\n\\uf8f6\\n\\uf8f8rather than\\nthe correct product which is\\n\\uf8eb\\n\\uf8ed\\n60\\n80\\n6\\n\\uf8f6\\n\\uf8f8; these two vectors are relatively close\\nto each other by any of various metrics one could apply (such as the length\\nof their vector difference).\\nThis suggests that the effect of small eigenvalues (and their eigenvectors)\\non a matrix-vector product is small. We will carry forward this intuition\\nwhen studying matrix decompositions and low-rank approximations in Sec-\\ntion 18.2. Before doing so, we examine the eigenvectors and eigenvalues of\\nspecial forms of matrices that will be of particular interest to us.\\nFor a symmetric matrix S, the eigenvectors corresponding to distinct eigen-\\nvalues are orthogonal. Further, if S is both real and symmetric, the eigenvalues\\nare all real.\\n\\x0f\\nExample 18.2:\\nConsider the real, symmetric matrix\\nS =\\n\\x12 2\\n1\\n1\\n2\\n\\x13\\n.\\n(18.4)\\nFrom the characteristic equation |S −λI| = 0, we have the quadratic (2 −λ)2 −1 =\\n0, whose solutions yield the eigenvalues 3 and 1. The corresponding eigenvectors\\n\\x12\\n1\\n−1\\n\\x13\\nand\\n\\x12 1\\n1\\n\\x13\\nare orthogonal.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n406\\n18\\nMatrix decompositions and latent semantic indexing\\n18.1.1\\nMatrix decompositions\\nIn this section we examine ways in which a square matrix can be factored\\ninto the product of matrices derived from its eigenvectors; we refer to this\\nprocess as matrix decomposition. Matrix decompositions similar to the ones\\nMATRIX\\nDECOMPOSITION\\nin this section will form the basis of our principal text-analysis technique\\nin Section 18.3, where we will look at decompositions of non-square term-\\ndocument matrices. The square decompositions in this section are simpler\\nand can be treated with sufﬁcient mathematical rigor to help the reader un-\\nderstand how such decompositions work. The detailed mathematical deriva-\\ntion of the more complex decompositions in Section 18.2 are beyond the\\nscope of this book.\\nWe begin by giving two theorems on the decomposition of a square ma-\\ntrix into the product of three matrices of a special form. The ﬁrst of these,\\nTheorem 18.1, gives the basic factorization of a square real-valued matrix\\ninto three factors. The second, Theorem 18.2, applies to square symmetric\\nmatrices and is the basis of the singular value decomposition described in\\nTheorem 18.3.\\nTheorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valued\\nM × M matrix with M linearly independent eigenvectors. Then there exists an\\neigen decomposition\\nEIGEN DECOMPOSITION\\nS = UΛU−1,\\n(18.5)\\nwhere the columns of U are the eigenvectors of S and Λ is a diagonal matrix whose\\ndiagonal entries are the eigenvalues of S in decreasing order\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nλ1\\nλ2\\n· · ·\\nλM\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8, λi ≥λi+1.\\n(18.6)\\nIf the eigenvalues are distinct, then this decomposition is unique.\\nTo understand how Theorem 18.1 works, we note that U has the eigenvec-\\ntors of S as columns\\nU = (⃗u1 ⃗u2 · · · ⃗uM) .\\n(18.7)\\nThen we have\\nSU\\n=\\nS (⃗u1 ⃗u2 · · · ⃗uM)\\n=\\n(λ1⃗u1 λ2⃗u2 · · · λM ⃗uM)\\n=\\n(⃗u1 ⃗u2 · · · ⃗\\nuM)\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nλ1\\nλ2\\n· · ·\\nλM\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.2\\nTerm-document matrices and singular value decompositions\\n407\\nThus, we have SU = UΛ, or S = UΛU−1.\\nWe next state a closely related decomposition of a symmetric square matrix\\ninto the product of matrices derived from its eigenvectors. This will pave the\\nway for the development of our main tool for text analysis, the singular value\\ndecomposition (Section 18.2).\\nTheorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-\\nmetric real-valued M × M matrix with M linearly independent eigenvectors. Then\\nthere exists a symmetric diagonal decomposition\\nSYMMETRIC DIAGONAL\\nDECOMPOSITION\\nS = QΛQT,\\n(18.8)\\nwhere the columns of Q are the orthogonal and normalized (unit length, real) eigen-\\nvectors of S, and Λ is the diagonal matrix whose entries are the eigenvalues of S.\\nFurther, all entries of Q are real and we have Q−1 = QT.\\nWe will build on this symmetric diagonal decomposition to build low-rank\\napproximations to term-document matrices.\\n?\\nExercise 18.1\\nWhat is the rank of the 3 × 3 diagonal matrix below?\\n\\uf8eb\\n\\uf8ed\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n2\\n1\\n\\uf8f6\\n\\uf8f8\\nExercise 18.2\\nShow that λ = 2 is an eigenvalue of\\nC =\\n\\x12 6\\n−2\\n4\\n0\\n\\x13\\n.\\nFind the corresponding eigenvector.\\nExercise 18.3\\nCompute the unique eigen decomposition of the 2 × 2 matrix in (18.4).\\n18.2\\nTerm-document matrices and singular value decompositions\\nThe decompositions we have been studying thus far apply to square matri-\\nces. However, the matrix we are interested in is the M × N term-document\\nmatrix C where (barring a rare coincidence) M ̸= N; furthermore, C is very\\nunlikely to be symmetric. To this end we ﬁrst describe an extension of the\\nsymmetric diagonal decomposition known as the singular value decomposi-\\nSINGULAR VALUE\\nDECOMPOSITION\\ntion. We then show in Section 18.3 how this can be used to construct an ap-\\nproximate version of C. It is beyond the scope of this book to develop a full\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n408\\n18\\nMatrix decompositions and latent semantic indexing\\ntreatment of the mathematics underlying singular value decompositions; fol-\\nlowing the statement of Theorem 18.3 we relate the singular value decompo-\\nsition to the symmetric diagonal decompositions from Section 18.1.1. Given\\nSYMMETRIC DIAGONAL\\nDECOMPOSITION\\nC, let U be the M × M matrix whose columns are the orthogonal eigenvec-\\ntors of CCT, and V be the N × N matrix whose columns are the orthogonal\\neigenvectors of CTC. Denote by CT the transpose of a matrix C.\\nTheorem 18.3. Let r be the rank of the M × N matrix C. Then, there is a singular-\\nvalue decomposition (SVD for short) of C of the form\\nSVD\\nC = UΣVT,\\n(18.9)\\nwhere\\n1. The eigenvalues λ1, . . . , λr of CCT are the same as the eigenvalues of CTC;\\n2. For 1 ≤i ≤r, let σi = √λi, with λi ≥λi+1. Then the M × N matrix Σ is\\ncomposed by setting Σii = σi for 1 ≤i ≤r, and zero otherwise.\\nThe values σi are referred to as the singular values of C. It is instructive to\\nexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this rather\\nthan derive the general proof of Theorem 18.3, which is beyond the scope of\\nthis book.\\nBy multiplying Equation (18.9) by its transposed version, we have\\nCCT = UΣVT VΣUT = UΣ2UT.\\n(18.10)\\nNote now that in Equation (18.10), the left-hand side is a square symmetric\\nmatrix real-valued matrix, and the right-hand side represents its symmetric\\ndiagonal decomposition as in Theorem 18.2. What does the left-hand side\\nCCT represent? It is a square matrix with a row and a column correspond-\\ning to each of the M terms. The entry (i, j) in the matrix is a measure of the\\noverlap between the ith and jth terms, based on their co-occurrence in docu-\\nments. The precise mathematical meaning depends on the manner in which\\nC is constructed based on term weighting. Consider the case where C is the\\nterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then the\\nentry (i, j) in CCT is the number of documents in which both term i and term\\nj occur.\\nWhen writing down the numerical values of the SVD, it is conventional\\nto represent Σ as an r × r matrix with the singular values on the diagonals,\\nsince all its entries outside this sub-matrix are zeros. Accordingly, it is con-\\nventional to omit the rightmost M −r columns of U corresponding to these\\nomitted rows of Σ; likewise the rightmost N −r columns of V are omitted\\nsince they correspond in VT to the rows that will be multiplied by the N −r\\ncolumns of zeros in Σ. This written form of the SVD is sometimes known\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.2\\nTerm-document matrices and singular value decompositions\\n409\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nC\\n=\\nU\\nΣ\\nVT\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\n◮Figure 18.1\\nIllustration of the singular-value decomposition. In this schematic\\nillustration of (18.9), we see two cases illustrated. In the top half of the ﬁgure, we\\nhave a matrix C for which M > N. The lower half illustrates the case M < N.\\nas the reduced SVD or truncated SVD and we will encounter it again in Ex-\\nREDUCED SVD\\nTRUNCATED SVD\\nercise 18.9. Henceforth, our numerical examples and exercises will use this\\nreduced form.\\n\\x0f\\nExample 18.3:\\nWe now illustrate the singular-value decomposition of a 4 × 2 ma-\\ntrix of rank 2; the singular values are Σ11 = 2.236 and Σ22 = 1.\\nC =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n1\\n−1\\n0\\n1\\n1\\n0\\n−1\\n1\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8=\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n−0.632\\n0.000\\n0.316\\n−0.707\\n−0.316\\n−0.707\\n0.632\\n0.000\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\x12\\n2.236\\n0.000\\n0.000\\n1.000\\n\\x13 \\x12 −0.707\\n0.707\\n−0.707\\n−0.707\\n\\x13\\n.\\n(18.11)\\nAs with the matrix decompositions deﬁned in Section 18.1.1, the singu-\\nlar value decomposition of a matrix can be computed by a variety of algo-\\nrithms, many of which have been publicly available software implementa-\\ntions; pointers to these are given in Section 18.5.\\n?\\nExercise 18.4\\nLet\\nC =\\n\\uf8eb\\n\\uf8ed\\n1\\n1\\n0\\n1\\n1\\n0\\n\\uf8f6\\n\\uf8f8\\n(18.12)\\nbe the term-document incidence matrix for a collection. Compute the co-occurrence\\nmatrix CCT. What is the interpretation of the diagonal entries of CCT when C is a\\nterm-document incidence matrix?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n410\\n18\\nMatrix decompositions and latent semantic indexing\\nExercise 18.5\\nVerify that the SVD of the matrix in Equation (18.12) is\\nU =\\n\\uf8eb\\n\\uf8ed\\n−0.816\\n0.000\\n−0.408\\n−0.707\\n−0.408\\n0.707\\n\\uf8f6\\n\\uf8f8, Σ =\\n\\x12 1.732\\n0.000\\n0.000\\n1.000\\n\\x13\\nand VT =\\n\\x12 −0.707\\n−0.707\\n0.707\\n−0.707\\n\\x13\\n,\\n(18.13)\\nby verifying all of the properties in the statement of Theorem 18.3.\\nExercise 18.6\\nSuppose that C is a binary term-document incidence matrix. What do the entries of\\nCTC represent?\\nExercise 18.7\\nLet\\nC =\\n\\uf8eb\\n\\uf8ed\\n0\\n2\\n1\\n0\\n3\\n0\\n2\\n1\\n0\\n\\uf8f6\\n\\uf8f8\\n(18.14)\\nbe a term-document matrix whose entries are term frequencies; thus term 1 occurs 2\\ntimes in document 2 and once in document 3. Compute CCT; observe that its entries\\nare largest where two terms have their most frequent occurrences together in the same\\ndocument.\\n18.3\\nLow-rank approximations\\nWe next state a matrix approximation problem that at ﬁrst seems to have\\nlittle to do with information retrieval. We describe a solution to this matrix\\nproblem using singular-value decompositions, then develop its application\\nto information retrieval.\\nGiven an M × N matrix C and a positive integer k, we wish to ﬁnd an\\nM × N matrix Ck of rank at most k, so as to minimize the Frobenius norm of\\nFROBENIUS NORM\\nthe matrix difference X = C −Ck, deﬁned to be\\n∥X∥F =\\nv\\nu\\nu\\nt\\nM\\n∑\\ni=1\\nN\\n∑\\nj=1\\nX2\\nij.\\n(18.15)\\nThus, the Frobenius norm of X measures the discrepancy between Ck and C;\\nour goal is to ﬁnd a matrix Ck that minimizes this discrepancy, while con-\\nstraining Ck to have rank at most k. If r is the rank of C, clearly Cr = C\\nand the Frobenius norm of the discrepancy is zero in this case. When k is far\\nsmaller than r, we refer to Ck as a low-rank approximation.\\nLOW-RANK\\nAPPROXIMATION\\nThe singular value decomposition can be used to solve the low-rank ma-\\ntrix approximation problem. We then derive from it an application to ap-\\nproximating term-document matrices. We invoke the following three-step\\nprocedure to this end:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.3\\nLow-rank approximations\\n411\\nCk\\n=\\nU\\nΣk\\nVT\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\n◮Figure 18.2\\nIllustration of low rank approximation using the singular-value de-\\ncomposition. The dashed boxes indicate the matrix entries affected by “zeroing out”\\nthe smallest singular values.\\n1. Given C, construct its SVD in the form shown in (18.9); thus, C = UΣVT.\\n2. Derive from Σ the matrix Σk formed by replacing by zeros the r −k small-\\nest singular values on the diagonal of Σ.\\n3. Compute and output Ck = UΣkVT as the rank-k approximation to C.\\nThe rank of Ck is at most k: this follows from the fact that Σk has at most\\nk non-zero values. Next, we recall the intuition of Example 18.1: the effect\\nof small eigenvalues on matrix products is small. Thus, it seems plausible\\nthat replacing these small eigenvalues by zero will not substantially alter the\\nproduct, leaving it “close” to C. The following theorem due to Eckart and\\nYoung tells us that, in fact, this procedure yields the matrix of rank k with\\nthe lowest possible Frobenius error.\\nTheorem 18.4.\\nmin\\nZ| rank(Z)=k\\n∥C −Z∥F = ∥C −Ck∥F = σk+1.\\n(18.16)\\nRecalling that the singular values are in decreasing order σ1 ≥σ2 ≥· · ·,\\nwe learn from Theorem 18.4 that Ck is the best rank-k approximation to C,\\nincurring an error (measured by the Frobenius norm of C −Ck) equal to σk+1.\\nThus the larger k is, the smaller this error (and in particular, for k = r, the\\nerror is zero since Σr = Σ; provided r < M, N, then σr+1 = 0 and thus\\nCr = C).\\nTo derive further insight into why the process of truncating the smallest\\nr −k singular values in Σ helps generate a rank-k approximation of low error,\\nwe examine the form of Ck:\\nCk\\n=\\nUΣkVT\\n(18.17)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n412\\n18\\nMatrix decompositions and latent semantic indexing\\n=\\nU\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nσ1\\n0\\n0\\n0\\n0\\n0\\n· · ·\\n0\\n0\\n0\\n0\\n0\\nσk\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n· · ·\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\nVT\\n(18.18)\\n=\\nk\\n∑\\ni=1\\nσi⃗ui⃗vT\\ni ,\\n(18.19)\\nwhere ⃗ui and ⃗vi are the ith columns of U and V, respectively. Thus, ⃗ui⃗vT\\ni is\\na rank-1 matrix, so that we have just expressed Ck as the sum of k rank-1\\nmatrices each weighted by a singular value. As i increases, the contribution\\nof the rank-1 matrix ⃗ui⃗vT\\ni is weighted by a sequence of shrinking singular\\nvalues σi.\\n?\\nExercise 18.8\\nCompute a rank 1 approximation C1 to the matrix C in Example 18.12, using the SVD\\nas in Exercise 18.13. What is the Frobenius norm of the error of this approximation?\\nExercise 18.9\\nConsider now the computation in Exercise 18.8.\\nFollowing the schematic in Fig-\\nure 18.2, notice that for a rank 1 approximation we have σ1 being a scalar. Denote\\nby U1 the ﬁrst column of U and by V1 the ﬁrst column of V. Show that the rank-1\\napproximation to C can then be written as U1σ1VT\\n1 = σ1U1VT\\n1 .\\nExercise 18.10\\nExercise 18.9 can be generalized to rank k approximations: we let U′\\nk and V′\\nk denote\\nthe “reduced” matrices formed by retaining only the ﬁrst k columns of U and V,\\nrespectively. Thus U′\\nk is an M × k matrix while V′T\\nk is a k × N matrix. Then, we have\\nCk = U′\\nkΣ′\\nkV′T\\nk ,\\n(18.20)\\nwhere Σ′\\nk is the square k × k submatrix of Σk with the singular values σ1, . . . , σk on\\nthe diagonal. The primary advantage of using (18.20) is to eliminate a lot of redun-\\ndant columns of zeros in U and V, thereby explicitly eliminating multiplication by\\ncolumns that do not affect the low-rank approximation; this version of the SVD is\\nsometimes known as the reduced SVD or truncated SVD and is a computationally\\nsimpler representation from which to compute the low rank approximation.\\nFor the matrix C in Example 18.3, write down both Σ2 and Σ′\\n2.\\n18.4\\nLatent semantic indexing\\nWe now discuss the approximation of a term-document matrix C by one of\\nlower rank using the SVD. The low-rank approximation to C yields a new\\nrepresentation for each document in the collection.\\nWe will cast queries\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.4\\nLatent semantic indexing\\n413\\ninto this low-rank representation as well, enabling us to compute query-\\ndocument similarity scores in this low-rank representation. This process is\\nknown as latent semantic indexing (generally abbreviated LSI).\\nLATENT SEMANTIC\\nINDEXING\\nBut ﬁrst, we motivate such an approximation. Recall the vector space rep-\\nresentation of documents and queries introduced in Section 6.3 (page 120).\\nThis vector space representation enjoys a number of advantages including\\nthe uniform treatment of queries and documents as vectors, the induced\\nscore computation based on cosine similarity, the ability to weight differ-\\nent terms differently, and its extension beyond document retrieval to such\\napplications as clustering and classiﬁcation. The vector space representa-\\ntion suffers, however, from its inability to cope with two classic problems\\narising in natural languages: synonymy and polysemy. Synonymy refers to a\\ncase where two different words (say car and automobile) have the same mean-\\ning. Because the vector space representation fails to capture the relationship\\nbetween synonymous terms such as car and automobile – according each a\\nseparate dimension in the vector space. Consequently the computed simi-\\nlarity⃗q · ⃗d between a query⃗q (say, car) and a document ⃗d containing both car\\nand automobile underestimates the true similarity that a user would perceive.\\nPolysemy on the other hand refers to the case where a term such as charge\\nhas multiple meanings, so that the computed similarity ⃗q · ⃗d overestimates\\nthe similarity that a user would perceive. Could we use the co-occurrences\\nof terms (whether, for instance, charge occurs in a document containing steed\\nversus in a document containing electron) to capture the latent semantic as-\\nsociations of terms and alleviate these problems?\\nEven for a collection of modest size, the term-document matrix C is likely\\nto have several tens of thousand of rows and columns, and a rank in the\\ntens of thousands as well. In latent semantic indexing (sometimes referred\\nto as latent semantic analysis (LSA)), we use the SVD to construct a low-rank\\nLSA\\napproximation Ck to the term-document matrix, for a value of k that is far\\nsmaller than the original rank of C. In the experimental work cited later\\nin this section, k is generally chosen to be in the low hundreds. We thus\\nmap each row/column (respectively corresponding to a term/document) to\\na k-dimensional space; this space is deﬁned by the k principal eigenvectors\\n(corresponding to the largest eigenvalues) of CCT and CTC. Note that the\\nmatrix Ck is itself still an M × N matrix, irrespective of k.\\nNext, we use the new k-dimensional LSI representation as we did the orig-\\ninal representation – to compute similarities between vectors. A query vector\\n⃗q is mapped into its representation in the LSI space by the transformation\\n⃗qk = Σ−1\\nk UT\\nk ⃗q.\\n(18.21)\\nNow, we may use cosine similarities as in Section 6.3.1 (page 120) to com-\\npute the similarity between a query and a document, between two docu-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n414\\n18\\nMatrix decompositions and latent semantic indexing\\nments, or between two terms. Note especially that Equation (18.21) does not\\nin any way depend on ⃗q being a query; it is simply a vector in the space of\\nterms. This means that if we have an LSI representation of a collection of\\ndocuments, a new document not in the collection can be “folded in” to this\\nrepresentation using Equation (18.21). This allows us to incrementally add\\ndocuments to an LSI representation. Of course, such incremental addition\\nfails to capture the co-occurrences of the newly added documents (and even\\nignores any new terms they contain). As such, the quality of the LSI rep-\\nresentation will degrade as more documents are added and will eventually\\nrequire a recomputation of the LSI representation.\\nThe ﬁdelity of the approximation of Ck to C leads us to hope that the rel-\\native values of cosine similarities are preserved: if a query is close to a doc-\\nument in the original space, it remains relatively close in the k-dimensional\\nspace. But this in itself is not sufﬁciently interesting, especially given that\\nthe sparse query vector ⃗q turns into a dense query vector ⃗qk in the low-\\ndimensional space. This has a signiﬁcant computational cost, when com-\\npared with the cost of processing⃗q in its native form.\\n\\x0f\\nExample 18.4:\\nConsider the term-document matrix C =\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\nship\\n1\\n0\\n1\\n0\\n0\\n0\\nboat\\n0\\n1\\n0\\n0\\n0\\n0\\nocean\\n1\\n1\\n0\\n0\\n0\\n0\\nvoyage\\n1\\n0\\n0\\n1\\n1\\n0\\ntrip\\n0\\n0\\n0\\n1\\n0\\n1\\nIts singular value decomposition is the product of three matrices as below. First we\\nhave U which in this example is:\\n1\\n2\\n3\\n4\\n5\\nship\\n−0.44\\n−0.30\\n0.57\\n0.58\\n0.25\\nboat\\n−0.13\\n−0.33\\n−0.59\\n0.00\\n0.73\\nocean\\n−0.48\\n−0.51\\n−0.37\\n0.00\\n−0.61\\nvoyage\\n−0.70\\n0.35\\n0.15\\n−0.58\\n0.16\\ntrip\\n−0.26\\n0.65\\n−0.41\\n0.58\\n−0.09\\nWhen applying the SVD to a term-document matrix, U is known as the SVD term\\nmatrix. The singular values are Σ =\\n2.16\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.59\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.28\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.39\\nFinally we have VT, which in the context of a term-document matrix is known as\\nthe SVD document matrix:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.4\\nLatent semantic indexing\\n415\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−0.75\\n−0.28\\n−0.20\\n−0.45\\n−0.33\\n−0.12\\n2\\n−0.29\\n−0.53\\n−0.19\\n0.63\\n0.22\\n0.41\\n3\\n0.28\\n−0.75\\n0.45\\n−0.20\\n0.12\\n−0.33\\n4\\n0.00\\n0.00\\n0.58\\n0.00\\n−0.58\\n0.58\\n5\\n−0.53\\n0.29\\n0.63\\n0.19\\n0.41\\n−0.22\\nBy “zeroing out” all but the two largest singular values of Σ, we obtain Σ2 =\\n2.16\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.59\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nFrom this, we compute C2 =\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−1.62\\n−0.60\\n−0.44\\n−0.97\\n−0.70\\n−0.26\\n2\\n−0.46\\n−0.84\\n−0.30\\n1.00\\n0.35\\n0.65\\n3\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n4\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n5\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nNotice that the low-rank approximation, unlike the original matrix C, can have\\nnegative entries.\\nExamination of C2 and Σ2 in Example 18.4 shows that the last 3 rows of\\neach of these matrices are populated entirely by zeros. This suggests that\\nthe SVD product UΣVT in Equation (18.18) can be carried out with only two\\nrows in the representations of Σ2 and VT; we may then replace these matrices\\nby their truncated versions Σ′\\n2 and (V′)T. For instance, the truncated SVD\\ndocument matrix (V′)T in this example is:\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−1.62\\n−0.60\\n−0.44\\n−0.97\\n−0.70\\n−0.26\\n2\\n−0.46\\n−0.84\\n−0.30\\n1.00\\n0.35\\n0.65\\nFigure 18.3 illustrates the documents in (V′)T in two dimensions. Note\\nalso that C2 is dense relative to C.\\nWe may in general view the low-rank approximation of C by Ck as a con-\\nstrained optimization problem: subject to the constraint that Ck have rank at\\nmost k, we seek a representation of the terms and documents comprising C\\nwith low Frobenius norm for the error C −Ck. When forced to squeeze the\\nterms/documents down to a k-dimensional space, the SVD should bring to-\\ngether terms with similar co-occurrences. This intuition suggests, then, that\\nnot only should retrieval quality not suffer too much from the dimension\\nreduction, but in fact may improve.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n416\\n18\\nMatrix decompositions and latent semantic indexing\\n−0.5\\n−1.0\\n−1.5\\n0.5\\n1.0\\n−0.5\\n−1.0\\ndim 2\\ndim 1\\n×\\nd1\\n×\\nd2\\n× d3\\n×\\nd4\\n×\\nd5\\n× d6\\n◮Figure 18.3\\nThe documents of Example 18.4 reduced to two dimensions in (V′)T.\\nDumais (1993) and Dumais (1995) conducted experiments with LSI on\\nTREC documents and tasks, using the commonly-used Lanczos algorithm\\nto compute the SVD. At the time of their work in the early 1990’s, the LSI\\ncomputation on tens of thousands of documents took approximately a day\\non one machine. On these experiments, they achieved precision at or above\\nthat of the median TREC participant. On about 20% of TREC topics their\\nsystem was the top scorer, and reportedly slightly better on average than\\nstandard vector spaces for LSI at about 350 dimensions. Here are some con-\\nclusions on LSI ﬁrst suggested by their work, and subsequently veriﬁed by\\nmany other experiments.\\n• The computational cost of the SVD is signiﬁcant; at the time of this writ-\\ning, we know of no successful experiment with over one million docu-\\nments. This has been the biggest obstacle to the widespread adoption to\\nLSI. One approach to this obstacle is to build the LSI representation on a\\nrandomly sampled subset of the documents in the collection, following\\nwhich the remaining documents are “folded in” as detailed with Equa-\\ntion (18.21).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.5\\nReferences and further reading\\n417\\n• As we reduce k, recall tends to increase, as expected.\\n• Most surprisingly, a value of k in the low hundreds can actually increase\\nprecision on some query benchmarks. This appears to suggest that for a\\nsuitable value of k, LSI addresses some of the challenges of synonymy.\\n• LSI works best in applications where there is little overlap between queries\\nand documents.\\nThe experiments also documented some modes where LSI failed to match\\nthe effectiveness of more traditional indexes and score computations. Most\\nnotably (and perhaps obviously), LSI shares two basic drawbacks of vector\\nspace retrieval: there is no good way of expressing negations (ﬁnd docu-\\nments that contain german but not shepherd), and no way of enforcing Boolean\\nconditions.\\nLSI can be viewed as soft clustering by interpreting each dimension of the\\nSOFT CLUSTERING\\nreduced space as a cluster and the value that a document has on that dimen-\\nsion as its fractional membership in that cluster.\\n18.5\\nReferences and further reading\\nStrang (1986) provides an excellent introductory overview of matrix decom-\\npositions including the singular value decomposition. Theorem 18.4 is due\\nto Eckart and Young (1936). The connection between information retrieval\\nand low-rank approximations of the term-document matrix was introduced\\nin Deerwester et al. (1990), with a subsequent survey of results in Berry\\net al. (1995). Dumais (1993) and Dumais (1995) describe experiments on\\nTREC benchmarks giving evidence that at least on some benchmarks, LSI\\ncan produce better precision and recall than standard vector-space retrieval.\\nhttp://www.cs.utk.edu/˜berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.html\\noffer comprehensive pointers to the literature and software of LSI. Schütze\\nand Silverstein (1997) evaluate LSI and truncated representations of cen-\\ntroids for efﬁcient K-means clustering (Section 16.4). Bast and Majumdar\\n(2005) detail the role of the reduced dimension k in LSI and how different\\npairs of terms get coalesced together at differing values of k. Applications of\\nLSI to cross-language information retrieval (where documents in two or more\\nCROSS-LANGUAGE\\nINFORMATION\\nRETRIEVAL\\ndifferent languages are indexed, and a query posed in one language is ex-\\npected to retrieve documents in other languages) are developed in Berry and\\nYoung (1995) and Littman et al. (1998). LSI (referred to as LSA in more gen-\\neral settings) has been applied to host of other problems in computer science\\nranging from memory modeling to computer vision.\\nHofmann (1999a;b) provides an initial probabilistic extension of the basic\\nlatent semantic indexing technique. A more satisfactory formal basis for a\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n418\\n18\\nMatrix decompositions and latent semantic indexing\\nDocID\\nDocument text\\n1\\nhello\\n2\\nopen house\\n3\\nmi casa\\n4\\nhola Profesor\\n5\\nhola y bienvenido\\n6\\nhello and welcome\\n◮Figure 18.4\\nDocuments for Exercise 18.11.\\nSpanish\\nEnglish\\nmi\\nmy\\ncasa\\nhouse\\nhola\\nhello\\nprofesor\\nprofessor\\ny\\nand\\nbienvenido\\nwelcome\\n◮Figure 18.5\\nGlossary for Exercise 18.11.\\nprobabilistic latent variable model for dimensionality reduction is the Latent\\nDirichlet Allocation (LDA) model (Blei et al. 2003), which is generative and\\nassigns probabilities to documents outside of the training set. This model is\\nextended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft\\n(2006) present the ﬁrst large scale evaluation of LDA, ﬁnding it to signiﬁ-\\ncantly outperform the query likelihood model of Section 12.2 (page 242), but\\nto not perform quite as well as the relevance model mentioned in Section 12.4\\n(page 250) – but the latter does additional per-query processing unlike LDA.\\nTeh et al. (2006) generalize further by presenting Hierarchical Dirichlet Pro-\\ncesses, a probabilistic model which allows a group (for us, a document) to\\nbe drawn from an inﬁnite mixture of latent topics, while still allowing these\\ntopics to be shared across documents.\\n?\\nExercise 18.11\\nAssume you have a set of documents each of which is in either English or in Spanish.\\nThe collection is given in Figure 18.4.\\nFigure 18.5 gives a glossary relating the Spanish and English words above for your\\nown information. This glossary is NOT available to the retrieval system:\\n1. Construct the appropriate term-document matrix C to use for a collection con-\\nsisting of these documents. For simplicity, use raw term frequencies rather than\\nnormalized tf-idf weights. Make sure to clearly label the dimensions of your ma-\\ntrix.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n18.5\\nReferences and further reading\\n419\\n2. Write down the matrices U2, Σ′\\n2 and V2 and from these derive the rank 2 approxi-\\nmation C2.\\n3. State succinctly what the (i, j) entry in the matrix CTC represents.\\n4. State succinctly what the (i, j) entry in the matrix CT\\n2 C2 represents, and why it\\ndiffers from that in CTC.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n421\\n19\\nWeb search basics\\nIn this and the following two chapters, we consider web search engines. Sec-\\ntions 19.1–19.4 provide some background and history to help the reader ap-\\npreciate the forces that conspire to make the Web chaotic, fast-changing and\\n(from the standpoint of information retrieval) very different from the “tradi-\\ntional” collections studied thus far in this book. Sections 19.5–19.6 deal with\\nestimating the number of documents indexed by web search engines, and the\\nelimination of duplicate documents in web indexes, respectively. These two\\nlatter sections serve as background material for the following two chapters.\\n19.1\\nBackground and history\\nThe Web is unprecedented in many ways: unprecedented in scale, unprece-\\ndented in the almost-complete lack of coordination in its creation, and un-\\nprecedented in the diversity of backgrounds and motives of its participants.\\nEach of these contributes to making web search different – and generally far\\nharder – than searching “traditional” documents.\\nThe invention of hypertext, envisioned by Vannevar Bush in the 1940’s and\\nﬁrst realized in working systems in the 1970’s, signiﬁcantly precedes the for-\\nmation of the World Wide Web (which we will simply refer to as the Web), in\\nthe 1990’s. Web usage has shown tremendous growth to the point where it\\nnow claims a good fraction of humanity as participants, by relying on a sim-\\nple, open client-server design: (1) the server communicates with the client\\nvia a protocol (the http or hypertext transfer protocol) that is lightweight and\\nHTTP\\nsimple, asynchronously carrying a variety of payloads (text, images and –\\nover time – richer media such as audio and video ﬁles) encoded in a sim-\\nple markup language called HTML (for hypertext markup language); (2) the\\nHTML\\nclient – generally a browser, an application within a graphical user environ-\\nment – can ignore what it does not understand. Each of these seemingly\\ninnocuous features has contributed enormously to the growth of the Web, so\\nit is worthwhile to examine them further.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n422\\n19\\nWeb search basics\\nThe basic operation is as follows: a client (such as a browser) sends an http\\nrequest to a web server. The browser speciﬁes a URL (for Universal Resource Lo-\\nURL\\ncator) such as http://www.stanford.edu/home/atoz/contact.html.\\nIn this example URL, the string http refers to the protocol to be used for\\ntransmitting the data. The string www.stanford.edu is known as the do-\\nmain and speciﬁes the root of a hierarchy of web pages (typically mirroring a\\nﬁlesystem hierarchy underlying the web server). In this example, /home/atoz/contact.html\\nis a path in this hierarchy with a ﬁle contact.html that contains the infor-\\nmation to be returned by the web server at www.stanford.edu in response\\nto this request. The HTML-encoded ﬁle contact.html holds the hyper-\\nlinks and the content (in this instance, contact information for Stanford Uni-\\nversity), as well as formatting rules for rendering this content in a browser.\\nSuch an http request thus allows us to fetch the content of a page, some-\\nthing that will prove to be useful to us for crawling and indexing documents\\n(Chapter 20).\\nThe designers of the ﬁrst browsers made it easy to view the HTML markup\\ntags on the content of a URL. This simple convenience allowed new users to\\ncreate their own HTML content without extensive training or experience;\\nrather, they learned from example content that they liked. As they did so, a\\nsecond feature of browsers supported the rapid proliferation of web content\\ncreation and usage: browsers ignored what they did not understand. This\\ndid not, as one might fear, lead to the creation of numerous incompatible\\ndialects of HTML. What it did promote was amateur content creators who\\ncould freely experiment with and learn from their newly created web pages\\nwithout fear that a simple syntax error would “bring the system down.” Pub-\\nlishing on the Web became a mass activity that was not limited to a few\\ntrained programmers, but rather open to tens and eventually hundreds of\\nmillions of individuals. For most users and for most information needs, the\\nWeb quickly became the best way to supply and consume information on\\neverything from rare ailments to subway schedules.\\nThe mass publishing of information on the Web is essentially useless un-\\nless this wealth of information can be discovered and consumed by other\\nusers. Early attempts at making web information “discoverable” fell into two\\nbroad categories: (1) full-text index search engines such as Altavista, Excite\\nand Infoseek and (2) taxonomies populated with web pages in categories,\\nsuch as Yahoo! The former presented the user with a keyword search in-\\nterface supported by inverted indexes and ranking mechanisms building on\\nthose introduced in earlier chapters. The latter allowed the user to browse\\nthrough a hierarchical tree of category labels. While this is at ﬁrst blush a\\nconvenient and intuitive metaphor for ﬁnding web pages, it has a number of\\ndrawbacks: ﬁrst, accurately classifying web pages into taxonomy tree nodes\\nis for the most part a manual editorial process, which is difﬁcult to scale\\nwith the size of the Web. Arguably, we only need to have “high-quality”\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n423\\nweb pages in the taxonomy, with only the best web pages for each category.\\nHowever, just discovering these and classifying them accurately and consis-\\ntently into the taxonomy entails signiﬁcant human effort. Furthermore, in\\norder for a user to effectively discover web pages classiﬁed into the nodes of\\nthe taxonomy tree, the user’s idea of what sub-tree(s) to seek for a particu-\\nlar topic should match that of the editors performing the classiﬁcation. This\\nquickly becomes challenging as the size of the taxonomy grows; the Yahoo!\\ntaxonomy tree surpassed 1000 distinct nodes fairly early on. Given these\\nchallenges, the popularity of taxonomies declined over time, even though\\nvariants (such as About.com and the Open Directory Project) sprang up with\\nsubject-matter experts collecting and annotating web pages for each cate-\\ngory.\\nThe ﬁrst generation of web search engines transported classical search\\ntechniques such as those in the preceding chapters to the web domain, focus-\\ning on the challenge of scale. The earliest web search engines had to contend\\nwith indexes containing tens of millions of documents, which was a few or-\\nders of magnitude larger than any prior information retrieval system in the\\npublic domain. Indexing, query serving and ranking at this scale required\\nthe harnessing together of tens of machines to create highly available sys-\\ntems, again at scales not witnessed hitherto in a consumer-facing search ap-\\nplication. The ﬁrst generation of web search engines was largely successful\\nat solving these challenges while continually indexing a signiﬁcant fraction\\nof the Web, all the while serving queries with sub-second response times.\\nHowever, the quality and relevance of web search results left much to be\\ndesired owing to the idiosyncrasies of content creation on the Web that we\\ndiscuss in Section 19.2. This necessitated the invention of new ranking and\\nspam-ﬁghting techniques in order to ensure the quality of the search results.\\nWhile classical information retrieval techniques (such as those covered ear-\\nlier in this book) continue to be necessary for web search, they are not by\\nany means sufﬁcient. A key aspect (developed further in Chapter 21) is that\\nwhereas classical techniques measure the relevance of a document to a query,\\nthere remains a need to gauge the authoritativeness of a document based on\\ncues such as which website hosts it.\\n19.2\\nWeb characteristics\\nThe essential feature that led to the explosive growth of the web – decentral-\\nized content publishing with essentially no central control of authorship –\\nturned out to be the biggest challenge for web search engines in their quest to\\nindex and retrieve this content. Web page authors created content in dozens\\nof (natural) languages and thousands of dialects, thus demanding many dif-\\nferent forms of stemming and other linguistic operations. Because publish-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n424\\n19\\nWeb search basics\\ning was now open to tens of millions, web pages exhibited heterogeneity at a\\ndaunting scale, in many crucial aspects. First, content-creation was no longer\\nthe privy of editorially-trained writers; while this represented a tremendous\\ndemocratization of content creation, it also resulted in a tremendous varia-\\ntion in grammar and style (and in many cases, no recognizable grammar or\\nstyle). Indeed, web publishing in a sense unleashed the best and worst of\\ndesktop publishing on a planetary scale, so that pages quickly became rid-\\ndled with wild variations in colors, fonts and structure. Some web pages,\\nincluding the professionally created home pages of some large corporations,\\nconsisted entirely of images (which, when clicked, led to richer textual con-\\ntent) – and therefore, no indexable text.\\nWhat about the substance of the text in web pages? The democratization\\nof content creation on the web meant a new level of granularity in opinion on\\nvirtually any subject. This meant that the web contained truth, lies, contra-\\ndictions and suppositions on a grand scale. This gives rise to the question:\\nwhich web pages does one trust? In a simplistic approach, one might argue\\nthat some publishers are trustworthy and others not – begging the question\\nof how a search engine is to assign such a measure of trust to each website\\nor web page. In Chapter 21 we will examine approaches to understanding\\nthis question. More subtly, there may be no universal, user-independent no-\\ntion of trust; a web page whose contents are trustworthy to one user may\\nnot be so to another. In traditional (non-web) publishing this is not an issue:\\nusers self-select sources they ﬁnd trustworthy. Thus one reader may ﬁnd\\nthe reporting of The New York Times to be reliable, while another may prefer\\nThe Wall Street Journal. But when a search engine is the only viable means\\nfor a user to become aware of (let alone select) most content, this challenge\\nbecomes signiﬁcant.\\nWhile the question “how big is the Web?” has no easy answer (see Sec-\\ntion 19.5), the question “how many web pages are in a search engine’s index”\\nis more precise, although, even this question has issues. By the end of 1995,\\nAltavista reported that it had crawled and indexed approximately 30 million\\nstatic web pages. Static web pages are those whose content does not vary from\\nSTATIC WEB PAGES\\none request for that page to the next. For this purpose, a professor who man-\\nually updates his home page every week is considered to have a static web\\npage, but an airport’s ﬂight status page is considered to be dynamic. Dy-\\nnamic pages are typically mechanically generated by an application server\\nin response to a query to a database, as show in Figure 19.1. One sign of\\nsuch a page is that the URL has the character \"?\" in it. Since the number\\nof static web pages was believed to be doubling every few months in 1995,\\nearly web search engines such as Altavista had to constantly add hardware\\nand bandwidth for crawling and indexing web pages.\\n',\n",
       " \"Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n425\\n◮Figure 19.1\\nA dynamically generated web page. The browser sends a request for\\nﬂight information on ﬂight AA129 to the web application, that fetches the informa-\\ntion from back-end databases then creates a dynamic web page that it returns to the\\nbrowser.\\n&%\\n'$\\n&%\\n'$\\n-\\nanchor\\n◮Figure 19.2\\nTwo nodes of the web graph joined by a link.\\n19.2.1\\nThe web graph\\nWe can view the static Web consisting of static HTML pages together with\\nthe hyperlinks between them as a directed graph in which each web page is\\na node and each hyperlink a directed edge.\\nFigure 19.2 shows two nodes A and B from the web graph, each corre-\\nsponding to a web page, with a hyperlink from A to B. We refer to the set of\\nall such nodes and directed edges as the web graph. Figure 19.2 also shows\\nthat (as is the case with most links on web pages) there is some text surround-\\ning the origin of the hyperlink on page A. This text is generally encapsulated\\nin the href attribute of the <a> (for anchor) tag that encodes the hyperlink\\nin the HTML code of page A, and is referred to as anchor text. As one might\\nANCHOR TEXT\\nsuspect, this directed graph is not strongly connected: there are pairs of pages\\nsuch that one cannot proceed from one page of the pair to the other by follow-\\ning hyperlinks. We refer to the hyperlinks into a page as in-links and those\\nIN-LINKS\\nout of a page as out-links. The number of in-links to a page (also known as\\nOUT-LINKS\\nits in-degree) has averaged from roughly 8 to 15, in a range of studies. We\\nsimilarly deﬁne the out-degree of a web page to be the number of links out\\n\",\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n426\\n19\\nWeb search basics\\n◮Figure 19.3\\nA sample small web graph. In this example we have six pages labeled\\nA-F. Page B has in-degree 3 and out-degree 1. This example graph is not strongly\\nconnected: there is no path from any of pages B-F to page A.\\nof it. These notions are represented in Figure 19.3.\\nThere is ample evidence that these links are not randomly distributed; for\\none thing, the distribution of the number of links into a web page does not\\nfollow the Poisson distribution one would expect if every web page were\\nto pick the destinations of its links uniformly at random. Rather, this dis-\\ntribution is widely reported to be a power law, in which the total number of\\nPOWER LAW\\nweb pages with in-degree i is proportional to 1/iα; the value of α typically\\nreported by studies is 2.1.1 Furthermore, several studies have suggested that\\nthe directed graph connecting web pages has a bowtie shape: there are three\\nBOWTIE\\nmajor categories of web pages that are sometimes referred to as IN, OUT\\nand SCC. A web surfer can pass from any page in IN to any page in SCC, by\\nfollowing hyperlinks. Likewise, a surfer can pass from page in SCC to any\\npage in OUT. Finally, the surfer can surf from any page in SCC to any other\\npage in SCC. However, it is not possible to pass from a page in SCC to any\\npage in IN, or from a page in OUT to a page in SCC (or, consequently, IN).\\nNotably, in several studies IN and OUT are roughly equal in size, whereas\\n1. Cf. Zipf’s law of the distribution of words in text in Chapter 5 (page 90), which is a power\\nlaw with α = 1.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n427\\n◮Figure 19.4\\nThe bowtie structure of the Web. Here we show one tube and three\\ntendrils.\\nSCC is somewhat larger; most web pages fall into one of these three sets. The\\nremaining pages form into tubes that are small sets of pages outside SCC that\\nlead directly from IN to OUT, and tendrils that either lead nowhere from IN,\\nor from nowhere to OUT. Figure 19.4 illustrates this structure of the Web.\\n19.2.2\\nSpam\\nEarly in the history of web search, it became clear that web search engines\\nwere an important means for connecting advertisers to prospective buyers.\\nA user searching for maui golf real estate is not merely seeking news or en-\\ntertainment on the subject of housing on golf courses on the island of Maui,\\nbut instead likely to be seeking to purchase such a property. Sellers of such\\nproperty and their agents, therefore, have a strong incentive to create web\\npages that rank highly on this query. In a search engine whose scoring was\\nbased on term frequencies, a web page with numerous repetitions of maui golf\\nreal estate would rank highly. This led to the ﬁrst generation of spam, which\\nSPAM\\n(in the context of web search) is the manipulation of web page content for\\nthe purpose of appearing high up in search results for selected keywords.\\nTo avoid irritating users with these repetitions, sophisticated spammers re-\\nsorted to such tricks as rendering these repeated terms in the same color as\\nthe background. Despite these words being consequently invisible to the hu-\\nman user, a search engine indexer would parse the invisible words out of\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n428\\n19\\nWeb search basics\\n◮Figure 19.5\\nCloaking as used by spammers.\\nthe HTML representation of the web page and index these words as being\\npresent in the page.\\nAt its root, spam stems from the heterogeneity of motives in content cre-\\nation on the Web. In particular, many web content creators have commercial\\nmotives and therefore stand to gain from manipulating search engine results.\\nYou might argue that this is no different from a company that uses large fonts\\nto list its phone numbers in the yellow pages; but this generally costs the\\ncompany more and is thus a fairer mechanism. A more apt analogy, perhaps,\\nis the use of company names beginning with a long string of A’s to be listed\\nearly in a yellow pages category. In fact, the yellow pages’ model of com-\\npanies paying for larger/darker fonts has been replicated in web search: in\\nmany search engines, it is possible to pay to have one’s web page included\\nin the search engine’s index – a model known as paid inclusion. Different\\nPAID INCLUSION\\nsearch engines have different policies on whether to allow paid inclusion,\\nand whether such a payment has any effect on ranking in search results.\\nSearch engines soon became sophisticated enough in their spam detection\\nto screen out a large number of repetitions of particular keywords. Spam-\\nmers responded with a richer set of spam techniques, the best known of\\nwhich we now describe. The ﬁrst of these techniques is cloaking, shown in\\nFigure 19.5. Here, the spammer’s web server returns different pages depend-\\ning on whether the http request comes from a web search engine’s crawler\\n(the part of the search engine that gathers web pages, to be described in\\nChapter 20), or from a human user’s browser. The former causes the web\\npage to be indexed by the search engine under misleading keywords. When\\nthe user searches for these keywords and elects to view the page, he receives\\na web page that has altogether different content than that indexed by the\\nsearch engine. Such deception of search indexers is unknown in the tra-\\nditional world of information retrieval; it stems from the fact that the rela-\\ntionship between page publishers and web search engines is not completely\\ncollaborative.\\nA doorway page contains text and metadata carefully chosen to rank highly\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.3\\nAdvertising as the economic model\\n429\\non selected search keywords. When a browser requests the doorway page, it\\nis redirected to a page containing content of a more commercial nature. More\\ncomplex spamming techniques involve manipulation of the metadata related\\nto a page including (for reasons we will see in Chapter 21) the links into a\\nweb page. Given that spamming is inherently an economically motivated\\nactivity, there has sprung around it an industry of Search Engine Optimizers,\\nSEARCH ENGINE\\nOPTIMIZERS\\nor SEOs to provide consultancy services for clients who seek to have their\\nweb pages rank highly on selected keywords. Web search engines frown on\\nthis business of attempting to decipher and adapt to their proprietary rank-\\ning techniques and indeed announce policies on forms of SEO behavior they\\ndo not tolerate (and have been known to shut down search requests from cer-\\ntain SEOs for violation of these). Inevitably, the parrying between such SEOs\\n(who gradually infer features of each web search engine’s ranking methods)\\nand the web search engines (who adapt in response) is an unending struggle;\\nindeed, the research sub-area of adversarial information retrieval has sprung up\\nADVERSARIAL\\nINFORMATION\\nRETRIEVAL\\naround this battle. To combat spammers who manipulate the text of their\\nweb pages is the exploitation of the link structure of the Web – a technique\\nknown as link analysis. The ﬁrst web search engine known to apply link anal-\\nysis on a large scale (to be detailed in Chapter 21) was Google, although all\\nweb search engines currently make use of it (and correspondingly, spam-\\nmers now invest considerable effort in subverting it – this is known as link\\nLINK SPAM\\nspam).\\n?\\nExercise 19.1\\nIf the number of pages with in-degree i is proportional to 1/i2.1, what is the probabil-\\nity that a randomly chosen web page has in-degree 1?\\nExercise 19.2\\nIf the number of pages with in-degree i is proportional to 1/i2.1, what is the average\\nin-degree of a web page?\\nExercise 19.3\\nIf the number of pages with in-degree i is proportional to 1/i2.1, then as the largest\\nin-degree goes to inﬁnity, does the fraction of pages with in-degree i grow, stay the\\nsame, or diminish? How would your answer change for values of the exponent other\\nthan 2.1?\\nExercise 19.4\\nThe average in-degree of all nodes in a snapshot of the web graph is 9. What can we\\nsay about the average out-degree of all nodes in this snapshot?\\n19.3\\nAdvertising as the economic model\\nEarly in the history of the Web, companies used graphical banner advertise-\\nments on web pages at popular websites (news and entertainment sites such\\nas MSN, America Online, Yahoo! and CNN). The primary purpose of these\\nadvertisements was branding: to convey to the viewer a positive feeling about\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n430\\n19\\nWeb search basics\\nthe brand of the company placing the advertisement. Typically these adver-\\ntisements are priced on a cost per mil (CPM) basis: the cost to the company of\\nCPM\\nhaving its banner advertisement displayed 1000 times. Some websites struck\\ncontracts with their advertisers in which an advertisement was priced not by\\nthe number of times it is displayed (also known as impressions), but rather\\nby the number of times it was clicked on by the user. This pricing model is\\nknown as the cost per click (CPC) model. In such cases, clicking on the adver-\\nCPC\\ntisement leads the user to a web page set up by the advertiser, where the user\\nis induced to make a purchase. Here the goal of the advertisement is not so\\nmuch brand promotion as to induce a transaction. This distinction between\\nbrand and transaction-oriented advertising was already widely recognized\\nin the context of conventional media such as broadcast and print. The inter-\\nactivity of the web allowed the CPC billing model – clicks could be metered\\nand monitored by the website and billed to the advertiser.\\nThe pioneer in this direction was a company named Goto, which changed\\nits name to Overture prior to eventual acquisition by Yahoo! Goto was not,\\nin the traditional sense, a search engine; rather, for every query term q it ac-\\ncepted bids from companies who wanted their web page shown on the query\\nq. In response to the query q, Goto would return the pages of all advertisers\\nwho bid for q, ordered by their bids. Furthermore, when the user clicked\\non one of the returned results, the corresponding advertiser would make a\\npayment to Goto (in the initial implementation, this payment equaled the\\nadvertiser’s bid for q).\\nSeveral aspects of Goto’s model are worth highlighting. First, a user typing\\nthe query q into Goto’s search interface was actively expressing an interest\\nand intent related to the query q. For instance, a user typing golf clubs is more\\nlikely to be imminently purchasing a set than one who is simply browsing\\nnews on golf. Second, Goto only got compensated when a user actually ex-\\npressed interest in an advertisement – as evinced by the user clicking the ad-\\nvertisement. Taken together, these created a powerful mechanism by which\\nto connect advertisers to consumers, quickly raising the annual revenues of\\nGoto/Overture into hundreds of millions of dollars. This style of search en-\\ngine came to be known variously as sponsored search or search advertising.\\nSPONSORED SEARCH\\nSEARCH ADVERTISING\\nGiven these two kinds of search engines – the “pure” search engines such\\nas Google and Altavista, versus the sponsored search engines – the logi-\\ncal next step was to combine them into a single user experience. Current\\nsearch engines follow precisely this model: they provide pure search results\\n(generally known as algorithmic search results) as the primary response to a\\nALGORITHMIC SEARCH\\nuser’s search, together with sponsored search results displayed separately\\nand distinctively to the right of the algorithmic results. This is shown in Fig-\\nure 19.6. Retrieving sponsored search results and ranking them in response\\nto a query has now become considerably more sophisticated than the sim-\\nple Goto scheme; the process entails a blending of ideas from information\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.3\\nAdvertising as the economic model\\n431\\n◮Figure 19.6\\nSearch advertising triggered by query keywords. Here the query A320\\nreturns algorithmic search results about the Airbus aircraft, together with advertise-\\nments for various non-aircraft goods numbered A320, that advertisers seek to market\\nto those querying on this query. The lack of advertisements for the aircraft reﬂects the\\nfact that few marketers attempt to sell A320 aircraft on the web.\\nretrieval and microeconomics, and is beyond the scope of this book. For\\nadvertisers, understanding how search engines do this ranking and how to\\nallocate marketing campaign budgets to different keywords and to different\\nsponsored search engines has become a profession known as search engine\\nSEARCH ENGINE\\nMARKETING\\nmarketing (SEM).\\nThe inherently economic motives underlying sponsored search give rise\\nto attempts by some participants to subvert the system to their advantage.\\nThis can take many forms, one of which is known as click spam. There is\\nCLICK SPAM\\ncurrently no universally accepted deﬁnition of click spam. It refers (as the\\nname suggests) to clicks on sponsored search results that are not from bona\\nﬁde search users. For instance, a devious advertiser may attempt to exhaust\\nthe advertising budget of a competitor by clicking repeatedly (through the\\nuse of a robotic click generator) on that competitor’s sponsored search ad-\\nvertisements. Search engines face the challenge of discerning which of the\\nclicks they observe are part of a pattern of click spam, to avoid charging their\\nadvertiser clients for such clicks.\\n?\\nExercise 19.5\\nThe Goto method ranked advertisements matching a query by bid: the highest-bidding\\nadvertiser got the top position, the second-highest the next, and so on. What can go\\nwrong with this when the highest-bidding advertiser places an advertisement that is\\nirrelevant to the query? Why might an advertiser with an irrelevant advertisement\\nbid high in this manner?\\nExercise 19.6\\nSuppose that, in addition to bids, we had for each advertiser their click-through rate:\\nthe ratio of the historical number of times users click on their advertisement to the\\nnumber of times the advertisement was shown. Suggest a modiﬁcation of the Goto\\nscheme that exploits this data to avoid the problem in Exercise 19.5 above.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n432\\n19\\nWeb search basics\\n19.4\\nThe search user experience\\nIt is crucial that we understand the users of web search as well. This is\\nagain a signiﬁcant change from traditional information retrieval, where users\\nwere typically professionals with at least some training in the art of phrasing\\nqueries over a well-authored collection whose style and structure they un-\\nderstood well. In contrast, web search users tend to not know (or care) about\\nthe heterogeneity of web content, the syntax of query languages and the art\\nof phrasing queries; indeed, a mainstream tool (as web search has come to\\nbecome) should not place such onerous demands on billions of people. A\\nrange of studies has concluded that the average number of keywords in a\\nweb search is somewhere between 2 and 3. Syntax operators (Boolean con-\\nnectives, wildcards, etc.) are seldom used, again a result of the composition\\nof the audience – “normal” people, not information scientists.\\nIt is clear that the more user trafﬁc a web search engine can attract, the\\nmore revenue it stands to earn from sponsored search. How do search en-\\ngines differentiate themselves and grow their trafﬁc? Here Google identiﬁed\\ntwo principles that helped it grow at the expense of its competitors: (1) a\\nfocus on relevance, speciﬁcally precision rather than recall in the ﬁrst few re-\\nsults; (2) a user experience that is lightweight, meaning that both the search\\nquery page and the search results page are uncluttered and almost entirely\\ntextual, with very few graphical elements. The effect of the ﬁrst was simply\\nto save users time in locating the information they sought. The effect of the\\nsecond is to provide a user experience that is extremely responsive, or at any\\nrate not bottlenecked by the time to load the search query or results page.\\n19.4.1\\nUser query needs\\nThere appear to be three broad categories into which common web search\\nqueries can be grouped: (i) informational, (ii) navigational and (iii) transac-\\ntional. We now explain these categories; it should be clear that some queries\\nwill fall in more than one of these categories, while others will fall outside\\nthem.\\nInformational queries seek general information on a broad topic, such as\\nINFORMATIONAL\\nQUERIES\\nleukemia or Provence. There is typically not a single web page that con-\\ntains all the information sought; indeed, users with informational queries\\ntypically try to assimilate information from multiple web pages.\\nNavigational queries seek the website or home page of a single entity that the\\nNAVIGATIONAL\\nQUERIES\\nuser has in mind, say Lufthansa airlines. In such cases, the user’s expectation\\nis that the very ﬁrst search result should be the home page of Lufthansa.\\nThe user is not interested in a plethora of documents containing the term\\nLufthansa; for such a user, the best measure of user satisfaction is precision at\\n1.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.5\\nIndex size and estimation\\n433\\nA transactional query is one that is a prelude to the user performing a trans-\\nTRANSACTIONAL\\nQUERY\\naction on the Web – such as purchasing a product, downloading a ﬁle or\\nmaking a reservation. In such cases, the search engine should return results\\nlisting services that provide form interfaces for such transactions.\\nDiscerning which of these categories a query falls into can be challeng-\\ning. The category not only governs the algorithmic search results, but the\\nsuitability of the query for sponsored search results (since the query may re-\\nveal an intent to purchase). For navigational queries, some have argued that\\nthe search engine should return only a single result or even the target web\\npage directly. Nevertheless, web search engines have historically engaged in\\na battle of bragging rights over which one indexes more web pages. Does\\nthe user really care? Perhaps not, but the media does highlight estimates\\n(often statistically indefensible) of the sizes of various search engines. Users\\nare inﬂuenced by these reports and thus, search engines do have to pay at-\\ntention to how their index sizes compare to competitors’. For informational\\n(and to a lesser extent, transactional) queries, the user does care about the\\ncomprehensiveness of the search engine.\\nFigure 19.7 shows a composite picture of a web search engine including\\nthe crawler, as well as both the web page and advertisement indexes. The\\nportion of the ﬁgure under the curved dashed line is internal to the search\\nengine.\\n19.5\\nIndex size and estimation\\nTo a ﬁrst approximation, comprehensiveness grows with index size, although\\nit does matter which speciﬁc pages a search engine indexes – some pages are\\nmore informative than others. It is also difﬁcult to reason about the fraction\\nof the Web indexed by a search engine, because there is an inﬁnite number of\\ndynamic web pages; for instance, http://www.yahoo.com/any_string\\nreturns a valid HTML page rather than an error, politely informing the user\\nthat there is no such page at Yahoo! Such a \"soft 404 error\" is only one exam-\\nple of many ways in which web servers can generate an inﬁnite number of\\nvalid web pages. Indeed, some of these are malicious spider traps devised\\nto cause a search engine’s crawler (the component that systematically gath-\\ners web pages for the search engine’s index, described in Chapter 20) to stay\\nwithin a spammer’s website and index many pages from that site.\\nWe could ask the following better-deﬁned question: given two search en-\\ngines, what are the relative sizes of their indexes? Even this question turns\\nout to be imprecise, because:\\n1. In response to queries a search engine can return web pages whose con-\\ntents it has not (fully or even partially) indexed. For one thing, search\\nengines generally index only the ﬁrst few thousand words in a web page.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n434\\n19\\nWeb search basics\\nT\\nh\\ne\\nW\\ne\\nb\\nA\\nd\\ni\\nn\\nd\\ne\\nx\\ne\\ns\\nW\\ne\\nb\\nc\\nr\\na\\nw\\nl\\ne\\nr\\nI\\nn\\nd\\ne\\nx\\ne\\nr\\nI\\nn\\nd\\ne\\nx\\ne\\ns\\nS\\ne\\na\\nr\\nc\\nh\\nU\\ns\\ne\\nr\\n◮Figure 19.7\\nThe various components of a web search engine.\\nIn some cases, a search engine is aware of a page p that is linked to by pages\\nit has indexed, but has not indexed p itself. As we will see in Chapter 21,\\nit is still possible to meaningfully return p in search results.\\n2. Search engines generally organize their indexes in various tiers and parti-\\ntions, not all of which are examined on every search (recall tiered indexes\\nfrom Section 7.2.1). For instance, a web page deep inside a website may be\\nindexed but not retrieved on general web searches; it is however retrieved\\nas a result on a search that a user has explicitly restricted to that website\\n(such site-speciﬁc search is offered by most web search engines).\\nThus, search engine indexes include multiple classes of indexed pages, so\\nthat there is no single measure of index size. These issues notwithstanding,\\na number of techniques have been devised for crude estimates of the ratio of\\nthe index sizes of two search engines, E1 and E2. The basic hypothesis under-\\nlying these techniques is that each search engine indexes a fraction of the Web\\nchosen independently and uniformly at random. This involves some ques-\\ntionable assumptions: ﬁrst, that there is a ﬁnite size for the Web from which\\neach search engine chooses a subset, and second, that each engine chooses\\nan independent, uniformly chosen subset. As will be clear from the discus-\\nsion of crawling in Chapter 20, this is far from true. However, if we begin\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.5\\nIndex size and estimation\\n435\\nwith these assumptions, then we can invoke a classical estimation technique\\nknown as the capture-recapture method.\\nCAPTURE-RECAPTURE\\nMETHOD\\nSuppose that we could pick a random page from the index of E1 and test\\nwhether it is in E2’s index and symmetrically, test whether a random page\\nfrom E2 is in E1. These experiments give us fractions x and y such that our\\nestimate is that a fraction x of the pages in E1 are in E2, while a fraction y of\\nthe pages in E2 are in E1. Then, letting |Ei| denote the size of the index of\\nsearch engine Ei, we have\\nx|E1| ≈y|E2|,\\nfrom which we have the form we will use\\n|E1|\\n|E2| ≈y\\nx .\\n(19.1)\\nIf our assumption about E1 and E2 being independent and uniform random\\nsubsets of the Web were true, and our sampling process unbiased, then Equa-\\ntion (19.1) should give us an unbiased estimator for |E1|/|E2|. We distinguish\\nbetween two scenarios here. Either the measurement is performed by some-\\none with access to the index of one of the search engines (say an employee of\\nE1), or the measurement is performed by an independent party with no ac-\\ncess to the innards of either search engine. In the former case, we can simply\\npick a random document from one index. The latter case is more challeng-\\ning; by picking a random page from one search engine from outside the search\\nengine, then verify whether the random page is present in the other search\\nengine.\\nTo implement the sampling phase, we might generate a random page from\\nthe entire (idealized, ﬁnite) Web and test it for presence in each search engine.\\nUnfortunately, picking a web page uniformly at random is a difﬁcult prob-\\nlem. We brieﬂy outline several attempts to achieve such a sample, pointing\\nout the biases inherent to each; following this we describe in some detail one\\ntechnique that much research has built on.\\n1. Random searches: Begin with a search log of web searches; send a random\\nsearch from this log to E1 and a random page from the results. Since such\\nlogs are not widely available outside a search engine, one implementation\\nis to trap all search queries going out of a work group (say scientists in a\\nresearch center) that agrees to have all its searches logged. This approach\\nhas a number of issues, including the bias from the types of searches made\\nby the work group. Further, a random document from the results of such\\na random search to E1 is not the same as a random document from E1.\\n2. Random IP addresses: A second approach is to generate random IP ad-\\ndresses and send a request to a web server residing at the random ad-\\ndress, collecting all pages at that server. The biases here include the fact\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n436\\n19\\nWeb search basics\\nthat many hosts might share one IP (due to a practice known as virtual\\nhosting) or not accept http requests from the host where the experiment\\nis conducted. Furthermore, this technique is more likely to hit one of the\\nmany sites with few pages, skewing the document probabilities; we may\\nbe able to correct for this effect if we understand the distribution of the\\nnumber of pages on websites.\\n3. Random walks: If the web graph were a strongly connected directed graph,\\nwe could run a random walk starting at an arbitrary web page. This\\nwalk would converge to a steady state distribution (see Chapter 21, Sec-\\ntion 21.2.1 for more background material on this), from which we could in\\nprinciple pick a web page with a ﬁxed probability. This method, too has\\na number of biases. First, the Web is not strongly connected so that, even\\nwith various corrective rules, it is difﬁcult to argue that we can reach a\\nsteady state distribution starting from any page. Second, the time it takes\\nfor the random walk to settle into this steady state is unknown and could\\nexceed the length of the experiment.\\nClearly each of these approaches is far from perfect. We now describe a\\nfourth sampling approach, random queries. This approach is noteworthy for\\ntwo reasons: it has been successfully built upon for a series of increasingly\\nreﬁned estimates, and conversely it has turned out to be the approach most\\nlikely to be misinterpreted and carelessly implemented, leading to mislead-\\ning measurements. The idea is to pick a page (almost) uniformly at random\\nfrom a search engine’s index by posing a random query to it. It should be\\nclear that picking a set of random terms from (say) Webster’s dictionary is\\nnot a good way of implementing this idea. For one thing, not all vocabulary\\nterms occur equally often, so this approach will not result in documents be-\\ning chosen uniformly at random from the search engine. For another, there\\nare a great many terms in web documents that do not occur in a standard\\ndictionary such as Webster’s. To address the problem of vocabulary terms\\nnot in a standard dictionary, we begin by amassing a sample web dictionary.\\nThis could be done by crawling a limited portion of the Web, or by crawling a\\nmanually-assembled representative subset of the Web such as Yahoo! (as was\\ndone in the earliest experiments with this method). Consider a conjunctive\\nquery with two or more randomly chosen words from this dictionary.\\nOperationally, we proceed as follows: we use a random conjunctive query\\non E1 and pick from the top 100 returned results a page p at random. We\\nthen test p for presence in E2 by choosing 6-8 low-frequency terms in p and\\nusing them in a conjunctive query for E2. We can improve the estimate by\\nrepeating the experiment a large number of times. Both the sampling process\\nand the testing process have a number of issues.\\n1. Our sample is biased towards longer documents.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.6\\nNear-duplicates and shingling\\n437\\n2. Picking from the top 100 results of E1 induces a bias from the ranking\\nalgorithm of E1. Picking from all the results of E1 makes the experiment\\nslower. This is particularly so because most web search engines put up\\ndefenses against excessive robotic querying.\\n3. During the checking phase, a number of additional biases are introduced:\\nfor instance, E2 may not handle 8-word conjunctive queries properly.\\n4. Either E1 or E2 may refuse to respond to the test queries, treating them as\\nrobotic spam rather than as bona ﬁde queries.\\n5. There could be operational problems like connection time-outs.\\nA sequence of research has built on this basic paradigm to eliminate some\\nof these issues; there is no perfect solution yet, but the level of sophistica-\\ntion in statistics for understanding the biases is increasing. The main idea\\nis to address biases by estimating, for each document, the magnitude of the\\nbias. From this, standard statistical sampling methods can generate unbi-\\nased samples. In the checking phase, the newer work moves away from\\nconjunctive queries to phrase and other queries that appear to be better-\\nbehaved. Finally, newer experiments use other sampling methods besides\\nrandom queries. The best known of these is document random walk sampling,\\nin which a document is chosen by a random walk on a virtual graph de-\\nrived from documents. In this graph, nodes are documents; two documents\\nare connected by an edge if they share two or more words in common. The\\ngraph is never instantiated; rather, a random walk on it can be performed by\\nmoving from a document d to another by picking a pair of keywords in d,\\nrunning a query on a search engine and picking a random document from\\nthe results. Details may be found in the references in Section 19.7.\\n?\\nExercise 19.7\\nTwo web search engines A and B each generate a large number of pages uniformly at\\nrandom from their indexes. 30% of A’s pages are present in B’s index, while 50% of\\nB’s pages are present in A’s index. What is the number of pages in A’s index relative\\nto B’s?\\n19.6\\nNear-duplicates and shingling\\nOne aspect we have ignored in the discussion of index size in Section 19.5 is\\nduplication: the Web contains multiple copies of the same content. By some\\nestimates, as many as 40% of the pages on the Web are duplicates of other\\npages. Many of these are legitimate copies; for instance, certain information\\nrepositories are mirrored simply to provide redundancy and access reliabil-\\nity. Search engines try to avoid indexing multiple copies of the same content,\\nto keep down storage and processing overheads.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n438\\n19\\nWeb search basics\\nThe simplest approach to detecting duplicates is to compute, for each web\\npage, a ﬁngerprint that is a succinct (say 64-bit) digest of the characters on that\\npage. Then, whenever the ﬁngerprints of two web pages are equal, we test\\nwhether the pages themselves are equal and if so declare one of them to be a\\nduplicate copy of the other. This simplistic approach fails to capture a crucial\\nand widespread phenomenon on the Web: near duplication. In many cases,\\nthe contents of one web page are identical to those of another except for a\\nfew characters – say, a notation showing the date and time at which the page\\nwas last modiﬁed. Even in such cases, we want to be able to declare the two\\npages to be close enough that we only index one copy. Short of exhaustively\\ncomparing all pairs of web pages, an infeasible task at the scale of billions of\\npages, how can we detect and ﬁlter out such near duplicates?\\nWe now describe a solution to the problem of detecting near-duplicate web\\npages. The answer lies in a technique known as shingling. Given a positive\\nSHINGLING\\ninteger k and a sequence of terms in a document d, deﬁne the k-shingles of\\nd to be the set of all consecutive sequences of k terms in d. As an example,\\nconsider the following text: a rose is a rose is a rose. The 4-shingles for this text\\n(k = 4 is a typical value used in the detection of near-duplicate web pages)\\nare a rose is a, rose is a rose and is a rose is. The ﬁrst two of these shingles\\neach occur twice in the text. Intuitively, two documents are near duplicates if\\nthe sets of shingles generated from them are nearly the same. We now make\\nthis intuition precise, then develop a method for efﬁciently computing and\\ncomparing the sets of shingles for all web pages.\\nLet S(dj) denote the set of shingles of document dj. Recall the Jaccard\\ncoefﬁcient from page 61, which measures the degree of overlap between\\nthe sets S(d1) and S(d2) as |S(d1) ∩S(d2)|/|S(d1) ∪S(d2)|; denote this by\\nJ(S(d1), S(d2)). Our test for near duplication between d1 and d2 is to com-\\npute this Jaccard coefﬁcient; if it exceeds a preset threshold (say, 0.9), we\\ndeclare them near duplicates and eliminate one from indexing. However,\\nthis does not appear to have simpliﬁed matters: we still have to compute\\nJaccard coefﬁcients pairwise.\\nTo avoid this, we use a form of hashing. First, we map every shingle into\\na hash value over a large space, say 64 bits. For j = 1, 2, let H(dj) be the\\ncorresponding set of 64-bit hash values derived from S(dj). We now invoke\\nthe following trick to detect document pairs whose sets H() have large Jac-\\ncard overlaps. Let π be a random permutation from the 64-bit integers to the\\n64-bit integers. Denote by Π(dj) the set of permuted hash values in H(dj);\\nthus for each h ∈H(dj), there is a corresponding value π(h) ∈Π(dj).\\nLet xπ\\nj be the smallest integer in Π(dj). Then\\nTheorem 19.1.\\nJ(S(d1), S(d2)) = P(xπ\\n1 = xπ\\n2 ).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.6\\nNear-duplicates and shingling\\n439\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\nDocument 1\\nDocument 2\\nH(d1)\\nH(d2)\\nu\\n1\\nu\\n1\\nu\\n2\\nu\\n2\\nu\\n3\\nu\\n3\\nu\\n4\\nu\\n4\\nH(d1) and Π(d1)\\nH(d2) and Π(d2)\\nu\\nu\\nu\\nu\\nu\\nu\\nu\\nu\\n3\\n3\\n1\\n1\\n4\\n4\\n2\\n2\\n3\\n3\\n1\\n1\\n4\\n4\\n2\\n2\\n3\\n3\\nΠ(d1)\\nΠ(d2)\\nxπ\\n1\\nxπ\\n2\\n◮Figure 19.8\\nIllustration of shingle sketches. We see two documents going through\\nfour stages of shingle sketch computation. In the ﬁrst step (top row), we apply a 64-bit\\nhash to each shingle from each document to obtain H(d1) and H(d2) (circles). Next,\\nwe apply a random permutation Π to permute H(d1) and H(d2), obtaining Π(d1)\\nand Π(d2) (squares). The third row shows only Π(d1) and Π(d2), while the bottom\\nrow shows the minimum values xπ\\n1 and xπ\\n2 for each document.\\nProof. We give the proof in a slightly more general setting: consider a family\\nof sets whose elements are drawn from a common universe. View the sets\\nas columns of a matrix A, with one row for each element in the universe.\\nThe element aij = 1 if element i is present in the set Sj that the jth column\\nrepresents.\\nLet Π be a random permutation of the rows of A; denote by Π(Sj) the\\ncolumn that results from applying Π to the jth column. Finally, let xπ\\nj be the\\nindex of the ﬁrst row in which the column Π(Sj) has a 1. We then prove that\\nfor any two columns j1, j2,\\nP(xπ\\nj1 = xπ\\nj2) = J(Sj1, Sj2).\\nIf we can prove this, the theorem follows.\\nConsider two columns j1, j2 as shown in Figure 19.9. The ordered pairs of\\nentries of Sj1 and Sj2 partition the rows into four types: those with 0’s in both\\nof these columns, those with a 0 in Sj1 and a 1 in Sj2, those with a 1 in Sj1\\nand a 0 in Sj2, and ﬁnally those with 1’s in both of these columns. Indeed,\\nthe ﬁrst four rows of Figure 19.9 exemplify all of these four types of rows.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n440\\n19\\nWeb search basics\\nSj1\\nSj2\\n0\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n1\\n◮Figure 19.9\\nTwo sets Sj1 and Sj2; their Jaccard coefﬁcient is 2/5.\\nDenote by C00 the number of rows with 0’s in both columns, C01 the second,\\nC10 the third and C11 the fourth. Then,\\nJ(Sj1, Sj2) =\\nC11\\nC01 + C10 + C11\\n.\\n(19.2)\\nTo complete the proof by showing that the right-hand side of Equation (19.2)\\nequals P(xπ\\nj1 = xπ\\nj2), consider scanning columns j1, j2 in increasing row in-\\ndex until the ﬁrst non-zero entry is found in either column. Because Π is a\\nrandom permutation, the probability that this smallest row has a 1 in both\\ncolumns is exactly the right-hand side of Equation (19.2).\\nThus, our test for the Jaccard coefﬁcient of the shingle sets is probabilis-\\ntic: we compare the computed values xπ\\ni from different documents. If a pair\\ncoincides, we have candidate near duplicates. Repeat the process indepen-\\ndently for 200 random permutations π (a choice suggested in the literature).\\nCall the set of the 200 resulting values of xπ\\ni the sketch ψ(di) of di. We can\\nthen estimate the Jaccard coefﬁcient for any pair of documents di, dj to be\\n|ψi ∩ψj|/200; if this exceeds a preset threshold, we declare that di and dj are\\nsimilar.\\nHow can we quickly compute |ψi ∩ψj|/200 for all pairs i, j? Indeed, how\\ndo we represent all pairs of documents that are similar, without incurring\\na blowup that is quadratic in the number of documents? First, we use ﬁn-\\ngerprints to remove all but one copy of identical documents. We may also\\nremove common HTML tags and integers from the shingle computation, to\\neliminate shingles that occur very commonly in documents without telling\\nus anything about duplication. Next we use a union-ﬁnd algorithm to create\\nclusters that contain documents that are similar. To do this, we must accom-\\nplish a crucial step: going from the set of sketches to the set of pairs i, j such\\nthat di and dj are similar.\\nTo this end, we compute the number of shingles in common for any pair of\\ndocuments whose sketches have any members in common. We begin with\\nthe list < xπ\\ni , di > sorted by xπ\\ni pairs. For each xπ\\ni , we can now generate\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n19.7\\nReferences and further reading\\n441\\nall pairs i, j for which xπ\\ni is present in both their sketches. From these we\\ncan compute, for each pair i, j with non-zero sketch overlap, a count of the\\nnumber of xπ\\ni values they have in common. By applying a preset threshold,\\nwe know which pairs i, j have heavily overlapping sketches. For instance, if\\nthe threshold were 80%, we would need the count to be at least 160 for any\\ni, j. As we identify such pairs, we run the union-ﬁnd to group documents\\ninto near-duplicate “syntactic clusters”. This is essentially a variant of the\\nsingle-link clustering algorithm introduced in Section 17.2 (page 382).\\nOne ﬁnal trick cuts down the space needed in the computation of |ψi ∩\\nψj|/200 for pairs i, j, which in principle could still demand space quadratic\\nin the number of documents. To remove from consideration those pairs i, j\\nwhose sketches have few shingles in common, we preprocess the sketch for\\neach document as follows: sort the xπ\\ni in the sketch, then shingle this sorted\\nsequence to generate a set of super-shingles for each document. If two docu-\\nments have a super-shingle in common, we proceed to compute the precise\\nvalue of |ψi ∩ψj|/200. This again is a heuristic but can be highly effective\\nin cutting down the number of i, j pairs for which we accumulate the sketch\\noverlap counts.\\n?\\nExercise 19.8\\nWeb search engines A and B each crawl a random subset of the same size of the Web.\\nSome of the pages crawled are duplicates – exact textual copies of each other at dif-\\nferent URLs. Assume that duplicates are distributed uniformly amongst the pages\\ncrawled by A and B. Further, assume that a duplicate is a page that has exactly two\\ncopies – no pages have more than two copies. A indexes pages without duplicate\\nelimination whereas B indexes only one copy of each duplicate page. The two ran-\\ndom subsets have the same size before duplicate elimination. If, 45% of A’s indexed\\nURLs are present in B’s index, while 50% of B’s indexed URLs are present in A’s\\nindex, what fraction of the Web consists of pages that do not have a duplicate?\\nExercise 19.9\\nInstead of using the process depicted in Figure 19.8, consider instead the following\\nprocess for estimating the Jaccard coefﬁcient of the overlap between two sets S1 and\\nS2. We pick a random subset of the elements of the universe from which S1 and S2\\nare drawn; this corresponds to picking a random subset of the rows of the matrix A in\\nthe proof. We exhaustively compute the Jaccard coefﬁcient of these random subsets.\\nWhy is this estimate an unbiased estimator of the Jaccard coefﬁcient for S1 and S2?\\nExercise 19.10\\nExplain why this estimator would be very difﬁcult to use in practice.\\n19.7\\nReferences and further reading\\nBush (1945) foreshadowed the Web when he described an information man-\\nagement system that he called memex. Berners-Lee et al. (1992) describes\\none of the earliest incarnations of the Web. Kumar et al. (2000) and Broder\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n442\\n19\\nWeb search basics\\net al. (2000) provide comprehensive studies of the Web as a graph. The use\\nof anchor text was ﬁrst described in McBryan (1994). The taxonomy of web\\nqueries in Section 19.4 is due to Broder (2002). The observation of the power\\nlaw with exponent 2.1 in Section 19.2.1 appeared in Kumar et al. (1999).\\nChakrabarti (2002) is a good reference for many aspects of web search and\\nanalysis.\\nThe estimation of web search index sizes has a long history of develop-\\nment covered by Bharat and Broder (1998), Lawrence and Giles (1998), Rus-\\nmevichientong et al. (2001), Lawrence and Giles (1999), Henzinger et al. (2000),\\nBar-Yossef and Gurevich (2006). The state of the art is Bar-Yossef and Gure-\\nvich (2006), including several of the bias-removal techniques mentioned at\\nthe end of Section 19.5. Shingling was introduced by Broder et al. (1997) and\\nused for detecting websites (rather than simply pages) that are identical by\\nBharat et al. (2000).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n443\\n20\\nWeb crawling and indexes\\n20.1\\nOverview\\nWeb crawling is the process by which we gather pages from the Web, in\\norder to index them and support a search engine. The objective of crawling\\nis to quickly and efﬁciently gather as many useful web pages as possible,\\ntogether with the link structure that interconnects them. In Chapter 19 we\\nstudied the complexities of the Web stemming from its creation by millions of\\nuncoordinated individuals. In this chapter we study the resulting difﬁculties\\nfor crawling the Web. The focus of this chapter is the component shown in\\nFigure 19.7 as web crawler; it is sometimes referred to as a spider.\\nWEB CRAWLER\\nSPIDER\\nThe goal of this chapter is not to describe how to build the crawler for\\na full-scale commercial web search engine. We focus instead on a range of\\nissues that are generic to crawling from the student project scale to substan-\\ntial research projects. We begin (Section 20.1.1) by listing desiderata for web\\ncrawlers, and then discuss in Section 20.2 how each of these issues is ad-\\ndressed. The remainder of this chapter describes the architecture and some\\nimplementation details for a distributed web crawler that satisﬁes these fea-\\ntures. Section 20.3 discusses distributing indexes across many machines for\\na web-scale implementation.\\n20.1.1\\nFeatures a crawler must provide\\nWe list the desiderata for web crawlers in two categories: features that web\\ncrawlers must provide, followed by features they should provide.\\nRobustness: The Web contains servers that create spider traps, which are gen-\\nerators of web pages that mislead crawlers into getting stuck fetching an\\ninﬁnite number of pages in a particular domain. Crawlers must be de-\\nsigned to be resilient to such traps. Not all such traps are malicious; some\\nare the inadvertent side-effect of faulty website development.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n444\\n20\\nWeb crawling and indexes\\nPoliteness: Web servers have both implicit and explicit policies regulating\\nthe rate at which a crawler can visit them. These politeness policies must\\nbe respected.\\n20.1.2\\nFeatures a crawler should provide\\nDistributed: The crawler should have the ability to execute in a distributed\\nfashion across multiple machines.\\nScalable: The crawler architecture should permit scaling up the crawl rate\\nby adding extra machines and bandwidth.\\nPerformance and efﬁciency: The crawl system should make efﬁcient use of\\nvarious system resources including processor, storage and network band-\\nwidth.\\nQuality: Given that a signiﬁcant fraction of all web pages are of poor util-\\nity for serving user query needs, the crawler should be biased towards\\nfetching “useful” pages ﬁrst.\\nFreshness: In many applications, the crawler should operate in continuous\\nmode: it should obtain fresh copies of previously fetched pages. A search\\nengine crawler, for instance, can thus ensure that the search engine’s index\\ncontains a fairly current representation of each indexed web page. For\\nsuch continuous crawling, a crawler should be able to crawl a page with\\na frequency that approximates the rate of change of that page.\\nExtensible: Crawlers should be designed to be extensible in many ways –\\nto cope with new data formats, new fetch protocols, and so on. This de-\\nmands that the crawler architecture be modular.\\n20.2\\nCrawling\\nThe basic operation of any hypertext crawler (whether for the Web, an in-\\ntranet or other hypertext document collection) is as follows. The crawler\\nbegins with one or more URLs that constitute a seed set. It picks a URL from\\nthis seed set, then fetches the web page at that URL. The fetched page is then\\nparsed, to extract both the text and the links from the page (each of which\\npoints to another URL). The extracted text is fed to a text indexer (described\\nin Chapters 4 and 5). The extracted links (URLs) are then added to a URL\\nfrontier, which at all times consists of URLs whose corresponding pages have\\nyet to be fetched by the crawler. Initially, the URL frontier contains the seed\\nset; as pages are fetched, the corresponding URLs are deleted from the URL\\nfrontier. The entire process may be viewed as traversing the web graph (see\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n445\\nChapter 19). In continuous crawling, the URL of a fetched page is added\\nback to the frontier for fetching again in the future.\\nThis seemingly simple recursive traversal of the web graph is complicated\\nby the many demands on a practical web crawling system: the crawler has to\\nbe distributed, scalable, efﬁcient, polite, robust and extensible while fetching\\npages of high quality. We examine the effects of each of these issues. Our\\ntreatment follows the design of the Mercator crawler that has formed the ba-\\nMERCATOR\\nsis of a number of research and commercial crawlers. As a reference point,\\nfetching a billion pages (a small fraction of the static Web at present) in a\\nmonth-long crawl requires fetching several hundred pages each second. We\\nwill see how to use a multi-threaded design to address several bottlenecks in\\nthe overall crawler system in order to attain this fetch rate.\\nBefore proceeding to this detailed description, we reiterate for readers who\\nmay attempt to build crawlers of some basic properties any non-professional\\ncrawler should satisfy:\\n1. Only one connection should be open to any given host at a time.\\n2. A waiting time of a few seconds should occur between successive requests\\nto a host.\\n3. Politeness restrictions detailed in Section 20.2.1 should be obeyed.\\n20.2.1\\nCrawler architecture\\nThe simple scheme outlined above for crawling demands several modules\\nthat ﬁt together as shown in Figure 20.1.\\n1. The URL frontier, containing URLs yet to be fetched in the current crawl\\n(in the case of continuous crawling, a URL may have been fetched previ-\\nously but is back in the frontier for re-fetching). We describe this further\\nin Section 20.2.3.\\n2. A DNS resolution module that determines the web server from which to\\nfetch the page speciﬁed by a URL. We describe this further in Section 20.2.2.\\n3. A fetch module that uses the http protocol to retrieve the web page at a\\nURL.\\n4. A parsing module that extracts the text and set of links from a fetched web\\npage.\\n5. A duplicate elimination module that determines whether an extracted\\nlink is already in the URL frontier or has recently been fetched.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n446\\n20\\nWeb crawling and indexes\\nwww\\nFetch\\nDNS\\nParse\\nURL Frontier\\nContent\\nSeen?\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nDoc\\nFP’s\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nrobots\\ntemplates\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nURL\\nset\\nURL\\nFilter\\nDup\\nURL\\nElim\\n-\\n\\x1b\\n-\\n6\\n\\x1b-\\n?\\n6\\n-\\n-\\n-\\n\\x1b\\n6\\n?\\n6\\n?\\n6\\n?\\n◮Figure 20.1\\nThe basic crawler architecture.\\nCrawling is performed by anywhere from one to potentially hundreds of\\nthreads, each of which loops through the logical cycle in Figure 20.1. These\\nthreads may be run in a single process, or be partitioned amongst multiple\\nprocesses running at different nodes of a distributed system. We begin by\\nassuming that the URL frontier is in place and non-empty and defer our de-\\nscription of the implementation of the URL frontier to Section 20.2.3. We\\nfollow the progress of a single URL through the cycle of being fetched, pass-\\ning through various checks and ﬁlters, then ﬁnally (for continuous crawling)\\nbeing returned to the URL frontier.\\nA crawler thread begins by taking a URL from the frontier and fetching\\nthe web page at that URL, generally using the http protocol. The fetched\\npage is then written into a temporary store, where a number of operations\\nare performed on it. Next, the page is parsed and the text as well as the\\nlinks in it are extracted. The text (with any tag information – e.g., terms in\\nboldface) is passed on to the indexer. Link information including anchor text\\nis also passed on to the indexer for use in ranking in ways that are described\\nin Chapter 21. In addition, each extracted link goes through a series of tests\\nto determine whether the link should be added to the URL frontier.\\nFirst, the thread tests whether a web page with the same content has al-\\nready been seen at another URL. The simplest implementation for this would\\nuse a simple ﬁngerprint such as a checksum (placed in a store labeled \"Doc\\nFP’s\" in Figure 20.1). A more sophisticated test would use shingles instead\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n447\\nof ﬁngerprints, as described in Chapter 19.\\nNext, a URL ﬁlter is used to determine whether the extracted URL should\\nbe excluded from the frontier based on one of several tests. For instance, the\\ncrawl may seek to exclude certain domains (say, all .com URLs) – in this case\\nthe test would simply ﬁlter out the URL if it were from the .com domain.\\nA similar test could be inclusive rather than exclusive. Many hosts on the\\nWeb place certain portions of their websites off-limits to crawling, under a\\nstandard known as the Robots Exclusion Protocol. This is done by placing a\\nROBOTS EXCLUSION\\nPROTOCOL\\nﬁle with the name robots.txt at the root of the URL hierarchy at the site. Here\\nis an example robots.txt ﬁle that speciﬁes that no robot should visit any URL\\nwhose position in the ﬁle hierarchy starts with /yoursite/temp/, except for the\\nrobot called “searchengine”.\\nUser-agent: *\\nDisallow: /yoursite/temp/\\nUser-agent: searchengine\\nDisallow:\\nThe robots.txt ﬁle must be fetched from a website in order to test whether\\nthe URL under consideration passes the robot restrictions, and can there-\\nfore be added to the URL frontier. Rather than fetch it afresh for testing on\\neach URL to be added to the frontier, a cache can be used to obtain a re-\\ncently fetched copy of the ﬁle for the host. This is especially important since\\nmany of the links extracted from a page fall within the host from which the\\npage was fetched and therefore can be tested against the host’s robots.txt\\nﬁle. Thus, by performing the ﬁltering during the link extraction process, we\\nwould have especially high locality in the stream of hosts that we need to test\\nfor robots.txt ﬁles, leading to high cache hit rates. Unfortunately, this runs\\nafoul of webmasters’ politeness expectations. A URL (particularly one refer-\\nring to a low-quality or rarely changing document) may be in the frontier for\\ndays or even weeks. If we were to perform the robots ﬁltering before adding\\nsuch a URL to the frontier, its robots.txt ﬁle could have changed by the time\\nthe URL is dequeued from the frontier and fetched. We must consequently\\nperform robots-ﬁltering immediately before attempting to fetch a web page.\\nAs it turns out, maintaining a cache of robots.txt ﬁles is still highly effective;\\nthere is sufﬁcient locality even in the stream of URLs dequeued from the URL\\nfrontier.\\nNext, a URL should be normalized in the following sense: often the HTML\\nURL NORMALIZATION\\nencoding of a link from a web page p indicates the target of that link relative\\nto the page p. Thus, there is a relative link encoded thus in the HTML of the\\npage en.wikipedia.org/wiki/Main_Page:\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n448\\n20\\nWeb crawling and indexes\\n<a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General\\ndisclaimer\">Disclaimers</a>\\npoints to the URL http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer.\\nFinally, the URL is checked for duplicate elimination: if the URL is already\\nin the frontier or (in the case of a non-continuous crawl) already crawled,\\nwe do not add it to the frontier. When the URL is added to the frontier, it is\\nassigned a priority based on which it is eventually removed from the frontier\\nfor fetching. The details of this priority queuing are in Section 20.2.3.\\nCertain housekeeping tasks are typically performed by a dedicated thread.\\nThis thread is generally quiescent except that it wakes up once every few\\nseconds to log crawl progress statistics (URLs crawled, frontier size, etc.),\\ndecide whether to terminate the crawl, or (once every few hours of crawling)\\ncheckpoint the crawl. In checkpointing, a snapshot of the crawler’s state (say,\\nthe URL frontier) is committed to disk. In the event of a catastrophic crawler\\nfailure, the crawl is restarted from the most recent checkpoint.\\nDistributing the crawler\\nWe have mentioned that the threads in a crawler could run under different\\nprocesses, each at a different node of a distributed crawling system. Such\\ndistribution is essential for scaling; it can also be of use in a geographically\\ndistributed crawler system where each node crawls hosts “near” it. Parti-\\ntioning the hosts being crawled amongst the crawler nodes can be done by\\na hash function, or by some more speciﬁcally tailored policy. For instance,\\nwe may locate a crawler node in Europe to focus on European domains, al-\\nthough this is not dependable for several reasons – the routes that packets\\ntake through the internet do not always reﬂect geographic proximity, and in\\nany case the domain of a host does not always reﬂect its physical location.\\nHow do the various nodes of a distributed crawler communicate and share\\nURLs? The idea is to replicate the ﬂow of Figure 20.1 at each node, with one\\nessential difference: following the URL ﬁlter, we use a host splitter to dispatch\\neach surviving URL to the crawler node responsible for the URL; thus the set\\nof hosts being crawled is partitioned among the nodes. This modiﬁed ﬂow is\\nshown in Figure 20.2. The output of the host splitter goes into the Duplicate\\nURL Eliminator block of each other node in the distributed system.\\nThe “Content Seen?” module in the distributed architecture of Figure 20.2\\nis, however, complicated by several factors:\\n1. Unlike the URL frontier and the duplicate elimination module, document\\nﬁngerprints/shingles cannot be partitioned based on host name. There is\\nnothing preventing the same (or highly similar) content from appearing\\non different web servers. Consequently, the set of ﬁngerprints/shingles\\nmust be partitioned across the nodes based on some property of the ﬁn-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n449\\nwww\\nFetch\\nDNS\\nParse\\nURL Frontier\\nContent\\nSeen?\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nDoc\\nFP’s\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nURL\\nset\\nURL\\nFilter\\nHost\\nsplitter\\nTo\\nother\\nnodes\\nFrom\\nother\\nnodes\\nDup\\nURL\\nElim\\n-\\n\\x1b\\n-\\n6\\n\\x1b-\\n?\\n6\\n-\\n-\\n-\\n-\\n\\x1b\\n6\\n?\\n6\\n?\\n666\\n-\\n-\\n-\\n◮Figure 20.2\\nDistributing the basic crawl architecture.\\ngerprint/shingle (say by taking the ﬁngerprint modulo the number of\\nnodes). The result of this locality-mismatch is that most “Content Seen?”\\ntests result in a remote procedure call (although it is possible to batch\\nlookup requests).\\n2. There is very little locality in the stream of document ﬁngerprints/shingles.\\nThus, caching popular ﬁngerprints does not help (since there are no pop-\\nular ﬁngerprints).\\n3. Documents change over time and so, in the context of continuous crawl-\\ning, we must be able to delete their outdated ﬁngerprints/shingles from\\nthe content-seen set(s). In order to do so, it is necessary to save the ﬁnger-\\nprint/shingle of the document in the URL frontier, along with the URL\\nitself.\\n20.2.2\\nDNS resolution\\nEach web server (and indeed any host connected to the internet) has a unique\\nIP address: a sequence of four bytes generally represented as four integers\\nIP ADDRESS\\nseparated by dots; for instance 207.142.131.248is the numerical IP address as-\\nsociated with the host www.wikipedia.org. Given a URL such as www.wikipedia.org\\nin textual form, translating it to an IP address (in this case, 207.142.131.248)is\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n450\\n20\\nWeb crawling and indexes\\na process known as DNS resolution or DNS lookup; here DNS stands for Do-\\nDNS RESOLUTION\\nmain Name Service. During DNS resolution, the program that wishes to per-\\nform this translation (in our case, a component of the web crawler) contacts a\\nDNS server that returns the translated IP address. (In practice the entire trans-\\nDNS SERVER\\nlation may not occur at a single DNS server; rather, the DNS server contacted\\ninitially may recursively call upon other DNS servers to complete the transla-\\ntion.) For a more complex URL such as en.wikipedia.org/wiki/Domain_Name_System,\\nthe crawler component responsible for DNS resolution extracts the host name\\n– in this case en.wikipedia.org – and looks up the IP address for the host\\nen.wikipedia.org.\\nDNS resolution is a well-known bottleneck in web crawling. Due to the\\ndistributed nature of the Domain Name Service, DNS resolution may entail\\nmultiple requests and round-trips across the internet, requiring seconds and\\nsometimes even longer. Right away, this puts in jeopardy our goal of fetching\\nseveral hundred documents a second. A standard remedy is to introduce\\ncaching: URLs for which we have recently performed DNS lookups are likely\\nto be found in the DNS cache, avoiding the need to go to the DNS servers\\non the internet. However, obeying politeness constraints (see Section 20.2.3)\\nlimits the of cache hit rate.\\nThere is another important difﬁculty in DNS resolution; the lookup imple-\\nmentations in standard libraries (likely to be used by anyone developing a\\ncrawler) are generally synchronous. This means that once a request is made\\nto the Domain Name Service, other crawler threads at that node are blocked\\nuntil the ﬁrst request is completed. To circumvent this, most web crawlers\\nimplement their own DNS resolver as a component of the crawler. Thread\\ni executing the resolver code sends a message to the DNS server and then\\nperforms a timed wait: it resumes either when being signaled by another\\nthread or when a set time quantum expires. A single, separate DNS thread\\nlistens on the standard DNS port (port 53) for incoming response packets\\nfrom the name service. Upon receiving a response, it signals the appropriate\\ncrawler thread (in this case, i) and hands it the response packet if i has not\\nyet resumed because its time quantum has expired. A crawler thread that re-\\nsumes because its wait time quantum has expired retries for a ﬁxed number\\nof attempts, sending out a new message to the DNS server and performing\\na timed wait each time; the designers of Mercator recommend of the order\\nof ﬁve attempts. The time quantum of the wait increases exponentially with\\neach of these attempts; Mercator started with one second and ended with\\nroughly 90 seconds, in consideration of the fact that there are host names\\nthat take tens of seconds to resolve.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n451\\n20.2.3\\nThe URL frontier\\nThe URL frontier at a node is given a URL by its crawl process (or by the\\nhost splitter of another crawl process). It maintains the URLs in the frontier\\nand regurgitates them in some order whenever a crawler thread seeks a URL.\\nTwo important considerations govern the order in which URLs are returned\\nby the frontier. First, high-quality pages that change frequently should be\\nprioritized for frequent crawling. Thus, the priority of a page should be a\\nfunction of both its change rate and its quality (using some reasonable quality\\nestimate). The combination is necessary because a large number of spam\\npages change completely on every fetch.\\nThe second consideration is politeness: we must avoid repeated fetch re-\\nquests to a host within a short time span. The likelihood of this is exacerbated\\nbecause of a form of locality of reference: many URLs link to other URLs at\\nthe same host. As a result, a URL frontier implemented as a simple priority\\nqueue might result in a burst of fetch requests to a host. This might occur\\neven if we were to constrain the crawler so that at most one thread could\\nfetch from any single host at any time. A common heuristic is to insert a\\ngap between successive fetch requests to a host that is an order of magnitude\\nlarger than the time taken for the most recent fetch from that host.\\nFigure 20.3 shows a polite and prioritizing implementation of a URL fron-\\ntier. Its goals are to ensure that (i) only one connection is open at a time to any\\nhost; (ii) a waiting time of a few seconds occurs between successive requests\\nto a host and (iii) high-priority pages are crawled preferentially.\\nThe two major sub-modules are a set of F front queues in the upper por-\\ntion of the ﬁgure, and a set of B back queues in the lower part; all of these are\\nFIFO queues. The front queues implement the prioritization, while the back\\nqueues implement politeness. In the ﬂow of a URL added to the frontier as\\nit makes its way through the front and back queues, a prioritizer ﬁrst assigns\\nto the URL an integer priority i between 1 and F based on its fetch history\\n(taking into account the rate at which the web page at this URL has changed\\nbetween previous crawls). For instance, a document that has exhibited fre-\\nquent change would be assigned a higher priority. Other heuristics could be\\napplication-dependent and explicit – for instance, URLs from news services\\nmay always be assigned the highest priority. Now that it has been assigned\\npriority i, the URL is now appended to the ith of the front queues.\\nEach of the B back queues maintains the following invariants: (i) it is non-\\nempty while the crawl is in progress and (ii) it only contains URLs from a\\nsingle host1. An auxiliary table T (Figure 20.4) is used to maintain the map-\\nping from hosts to back queues. Whenever a back-queue is empty and is\\nbeing re-ﬁlled from a front-queue, table T must be updated accordingly.\\n1. The number of hosts is assumed to far exceed B.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n452\\n20\\nWeb crawling and indexes\\nBack queue\\nselector\\n-\\n\\x1b\\nBiased front queue selector\\nBack queue router\\nPrioritizer\\nr\\nr\\nr\\nr\\nB back queues\\nSingle host on each\\nr\\nr\\nr\\nr\\nr\\nF front queues\\n1\\n2\\nF\\n1\\n2\\nB\\n?\\nXXXXXXXXXXXX\\nz\\nXXXXXXXXXXXX\\nz\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\nXXXXXXXXXXXXXX\\nz\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n)\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n)\\nPPPPPPPPPPP\\nq\\n?\\nHHHHHHHHHHH\\nj\\nHHHHHHHHHHH\\nj\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x19\\n@\\n@@\\n\\x00\\x00\\x00 Heap\\n◮Figure 20.3\\nThe URL frontier. URLs extracted from already crawled pages ﬂow in\\nat the top of the ﬁgure. A crawl thread requesting a URL extracts it from the bottom of\\nthe ﬁgure. En route, a URL ﬂows through one of several front queues that manage its\\npriority for crawling, followed by one of several back queues that manage the crawler’s\\npoliteness.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n453\\nHost\\nBack queue\\nstanford.edu\\n23\\nmicrosoft.com\\n47\\nacm.org\\n12\\n◮Figure 20.4\\nExample of an auxiliary hosts-to-back queues table.\\nIn addition, we maintain a heap with one entry for each back queue, the\\nentry being the earliest time te at which the host corresponding to that queue\\ncan be contacted again.\\nA crawler thread requesting a URL from the frontier extracts the root of\\nthis heap and (if necessary) waits until the corresponding time entry te. It\\nthen takes the URL u at the head of the back queue j corresponding to the\\nextracted heap root, and proceeds to fetch the URL u. After fetching u, the\\ncalling thread checks whether j is empty. If so, it picks a front queue and\\nextracts from its head a URL v. The choice of front queue is biased (usually\\nby a random process) towards queues of higher priority, ensuring that URLs\\nof high priority ﬂow more quickly into the back queues. We examine v to\\ncheck whether there is already a back queue holding URLs from its host.\\nIf so, v is added to that queue and we reach back to the front queues to\\nﬁnd another candidate URL for insertion into the now-empty queue j. This\\nprocess continues until j is non-empty again. In any case, the thread inserts\\na heap entry for j with a new earliest time te based on the properties of the\\nURL in j that was last fetched (such as when its host was last contacted as\\nwell as the time taken for the last fetch), then continues with its processing.\\nFor instance, the new entry te could be the current time plus ten times the\\nlast fetch time.\\nThe number of front queues, together with the policy of assigning priori-\\nties and picking queues, determines the priority properties we wish to build\\ninto the system. The number of back queues governs the extent to which we\\ncan keep all crawl threads busy while respecting politeness. The designers\\nof Mercator recommend a rough rule of three times as many back queues as\\ncrawler threads.\\nOn a Web-scale crawl, the URL frontier may grow to the point where it\\ndemands more memory at a node than is available. The solution is to let\\nmost of the URL frontier reside on disk. A portion of each queue is kept in\\nmemory, with more brought in from disk as it is drained in memory.\\n?\\nExercise 20.1\\nWhy is it better to partition hosts (rather than individual URLs) between the nodes of\\na distributed crawl system?\\nExercise 20.2\\nWhy should the host splitter precede the Duplicate URL Eliminator?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n454\\n20\\nWeb crawling and indexes\\nExercise 20.3\\n[⋆⋆⋆]\\nIn the preceding discussion we encountered two recommended “hard constants” –\\nthe increment on te being ten times the last fetch time, and the number of back\\nqueues being three times the number of crawl threads. How are these two constants\\nrelated?\\n20.3\\nDistributing indexes\\nIn Section 4.4 we described distributed indexing. We now consider the distri-\\nbution of the index across a large computer cluster2 that supports querying.\\nTwo obvious alternative index implementations suggest themselves: parti-\\nTERM PARTITIONING\\ntioning by terms, also known as global index organization, and partitioning by\\nDOCUMENT\\nPARTITIONING\\ndocuments, also know as local index organization. In the former, the diction-\\nary of index terms is partitioned into subsets, each subset residing at a node.\\nAlong with the terms at a node, we keep the postings for those terms. A\\nquery is routed to the nodes corresponding to its query terms. In principle,\\nthis allows greater concurrency since a stream of queries with different query\\nterms would hit different sets of machines.\\nIn practice, partitioning indexes by vocabulary terms turns out to be non-\\ntrivial. Multi-word queries require the sending of long postings lists between\\nsets of nodes for merging, and the cost of this can outweigh the greater con-\\ncurrency. Load balancing the partition is governed not by an a priori analysis\\nof relative term frequencies, but rather by the distribution of query terms\\nand their co-occurrences, which can drift with time or exhibit sudden bursts.\\nAchieving good partitions is a function of the co-occurrences of query terms\\nand entails the clustering of terms to optimize objectives that are not easy to\\nquantify. Finally, this strategy makes implementation of dynamic indexing\\nmore difﬁcult.\\nA more common implementation is to partition by documents: each node\\ncontains the index for a subset of all documents. Each query is distributed to\\nall nodes, with the results from various nodes being merged before presenta-\\ntion to the user. This strategy trades more local disk seeks for less inter-node\\ncommunication. One difﬁculty in this approach is that global statistics used\\nin scoring – such as idf – must be computed across the entire document col-\\nlection even though the index at any single node only contains a subset of\\nthe documents. These are computed by distributed “background” processes\\nthat periodically refresh the node indexes with fresh global statistics.\\nHow do we decide the partition of documents to nodes? Based on our de-\\nvelopment of the crawler architecture in Section 20.2.1, one simple approach\\nwould be to assign all pages from a host to a single node. This partitioning\\n2. Please note the different usage of “clusters” elsewhere in this book, in the sense of Chapters\\n16 and 17.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.4\\nConnectivity servers\\n455\\ncould follow the partitioning of hosts to crawler nodes. A danger of such\\npartitioning is that on many queries, a preponderance of the results would\\ncome from documents at a small number of hosts (and hence a small number\\nof index nodes).\\nA hash of each URL into the space of index nodes results in a more uni-\\nform distribution of query-time computation across nodes. At query time,\\nthe query is broadcast to each of the nodes, with the top k results from each\\nnode being merged to ﬁnd the top k documents for the query. A common\\nimplementation heuristic is to partition the document collection into indexes\\nof documents that are more likely to score highly on most queries (using,\\nfor instance, techniques in Chapter 21) and low-scoring indexes with the re-\\nmaining documents. We only search the low-scoring indexes when there are\\ntoo few matches in the high-scoring indexes, as described in Section 7.2.1.\\n20.4\\nConnectivity servers\\nFor reasons to become clearer in Chapter 21, web search engines require a\\nconnectivity server that supports fast connectivity queries on the web graph.\\nCONNECTIVITY SERVER\\nCONNECTIVITY\\nQUERIES\\nTypical connectivity queries are which URLs link to a given URL? and which\\nURLs does a given URL link to? To this end, we wish to store mappings in\\nmemory from URL to out-links, and from URL to in-links. Applications in-\\nclude crawl control, web graph analysis, sophisticated crawl optimization\\nand link analysis (to be covered in Chapter 21).\\nSuppose that the Web had four billion pages, each with ten links to other\\npages. In the simplest form, we would require 32 bits or 4 bytes to specify\\neach end (source and destination) of each link, requiring a total of\\n4 × 109 × 10 × 8 = 3.2 × 1011\\nbytes of memory. Some basic properties of the web graph can be exploited to\\nuse well under 10% of this memory requirement. At ﬁrst sight, we appear to\\nhave a data compression problem – which is amenable to a variety of stan-\\ndard solutions. However, our goal is not to simply compress the web graph\\nto ﬁt into memory; we must do so in a way that efﬁciently supports connec-\\ntivity queries; this challenge is reminiscent of index compression (Chapter 5).\\nWe assume that each web page is represented by a unique integer; the\\nspeciﬁc scheme used to assign these integers is described below. We build\\nan adjacency table that resembles an inverted index: it has a row for each web\\npage, with the rows ordered by the corresponding integers. The row for any\\npage p contains a sorted list of integers, each corresponding to a web page\\nthat links to p. This table permits us to respond to queries of the form which\\npages link to p? In similar fashion we build a table whose entries are the pages\\nlinked to by p.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n456\\n20\\nWeb crawling and indexes\\n1: www.stanford.edu/alchemy\\n2: www.stanford.edu/biology\\n3: www.stanford.edu/biology/plant\\n4: www.stanford.edu/biology/plant/copyright\\n5: www.stanford.edu/biology/plant/people\\n6: www.stanford.edu/chemistry\\n◮Figure 20.5\\nA lexicographically ordered set of URLs.\\nThis table representation cuts the space taken by the naive representation\\n(in which we explicitly represent each link by its two end points, each a 32-bit\\ninteger) by 50%. Our description below will focus on the table for the links\\nfrom each page; it should be clear that the techniques apply just as well to\\nthe table of links to each page. To further reduce the storage for the table, we\\nexploit several ideas:\\n1. Similarity between lists: Many rows of the table have many entries in\\ncommon.\\nThus, if we explicitly represent a prototype row for several\\nsimilar rows, the remainder can be succinctly expressed in terms of the\\nprototypical row.\\n2. Locality: many links from a page go to “nearby” pages – pages on the\\nsame host, for instance. This suggests that in encoding the destination of\\na link, we can often use small integers and thereby save space.\\n3. We use gap encodings in sorted lists: rather than store the destination of\\neach link, we store the offset from the previous entry in the row.\\nWe now develop each of these techniques.\\nIn a lexicographic ordering of all URLs, we treat each URL as an alphanu-\\nmeric string and sort these strings. Figure 20.5 shows a segment of this sorted\\norder. For a true lexicographic sort of web pages, the domain name part of\\nthe URL should be inverted, so that www.stanford.edu becomes edu.stanford.www,\\nbut this is not necessary here since we are mainly concerned with links local\\nto a single host.\\nTo each URL, we assign its position in this ordering as the unique identi-\\nfying integer. Figure 20.6 shows an example of such a numbering and the\\nresulting table. In this example sequence, www.stanford.edu/biology\\nis assigned the integer 2 since it is second in the sequence.\\nWe next exploit a property that stems from the way most websites are\\nstructured to get similarity and locality. Most websites have a template with\\na set of links from each page in the site to a ﬁxed set of pages on the site (such\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.4\\nConnectivity servers\\n457\\n1: 1, 2, 4, 8, 16, 32, 64\\n2: 1, 4, 9, 16, 25, 36, 49, 64\\n3: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\\n4: 1, 4, 8, 16, 25, 36, 49, 64\\n◮Figure 20.6\\nA four-row segment of the table of links.\\nas its copyright notice, terms of use, and so on). In this case, the rows cor-\\nresponding to pages in a website will have many table entries in common.\\nMoreover, under the lexicographic ordering of URLs, it is very likely that the\\npages from a website appear as contiguous rows in the table.\\nWe adopt the following strategy: we walk down the table, encoding each\\ntable row in terms of the seven preceding rows. In the example of Figure 20.6,\\nwe could encode the fourth row as “the same as the row at offset 2 (mean-\\ning, two rows earlier in the table), with 9 replaced by 8”. This requires the\\nspeciﬁcation of the offset, the integer(s) dropped (in this case 9) and the in-\\nteger(s) added (in this case 8). The use of only the seven preceding rows has\\ntwo advantages: (i) the offset can be expressed with only 3 bits; this choice\\nis optimized empirically (the reason for seven and not eight preceding rows\\nis the subject of Exercise 20.4) and (ii) ﬁxing the maximum offset to a small\\nvalue like seven avoids having to perform an expensive search among many\\ncandidate prototypes in terms of which to express the current row.\\nWhat if none of the preceding seven rows is a good prototype for express-\\ning the current row? This would happen, for instance, at each boundary\\nbetween different websites as we walk down the rows of the table. In this\\ncase we simply express the row as starting from the empty set and “adding\\nin” each integer in that row. By using gap encodings to store the gaps (rather\\nthan the actual integers) in each row, and encoding these gaps tightly based\\non the distribution of their values, we obtain further space reduction. In ex-\\nperiments mentioned in Section 20.5, the series of techniques outlined here\\nappears to use as few as 3 bits per link, on average – a dramatic reduction\\nfrom the 64 required in the naive representation.\\nWhile these ideas give us a representation of sizable web graphs that com-\\nfortably ﬁt in memory, we still need to support connectivity queries. What\\nis entailed in retrieving from this representation the set of links from a page?\\nFirst, we need an index lookup from (a hash of) the URL to its row number\\nin the table. Next, we need to reconstruct these entries, which may be en-\\ncoded in terms of entries in other rows. This entails following the offsets to\\nreconstruct these other rows – a process that in principle could lead through\\nmany levels of indirection. In practice however, this does not happen very\\noften. A heuristic for controlling this can be introduced into the construc-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n458\\n20\\nWeb crawling and indexes\\ntion of the table: when examining the preceding seven rows as candidates\\nfrom which to model the current row, we demand a threshold of similarity\\nbetween the current row and the candidate prototype. This threshold must\\nbe chosen with care. If the threshold is set too high, we seldom use proto-\\ntypes and express many rows afresh. If the threshold is too low, most rows\\nget expressed in terms of prototypes, so that at query time the reconstruction\\nof a row leads to many levels of indirection through preceding prototypes.\\n?\\nExercise 20.4\\nWe noted that expressing a row in terms of one of seven preceding rows allowed us\\nto use no more than three bits to specify which of the preceding rows we are using\\nas prototype. Why seven and not eight preceding rows? (Hint: consider the case when\\nnone of the preceding seven rows is a good prototype.)\\nExercise 20.5\\nWe noted that for the scheme in Section 20.4, decoding the links incident on a URL\\ncould result in many levels of indirection. Construct an example in which the number\\nof levels of indirection grows linearly with the number of URLs.\\n20.5\\nReferences and further reading\\nThe ﬁrst web crawler appears to be Matthew Gray’s Wanderer, written in the\\nspring of 1993. The Mercator crawler is due to Najork and Heydon (Najork\\nand Heydon 2001; 2002); the treatment in this chapter follows their work.\\nOther classic early descriptions of web crawling include Burner (1997), Brin\\nand Page (1998), Cho et al. (1998) and the creators of the Webbase system\\nat Stanford (Hirai et al. 2000). Cho and Garcia-Molina (2002) give a taxon-\\nomy and comparative study of different modes of communication between\\nthe nodes of a distributed crawler. The Robots Exclusion Protocol standard\\nis described at http://www.robotstxt.org/wc/exclusion.html. Boldi et al. (2002) and\\nShkapenyuk and Suel (2002) provide more recent details of implementing\\nlarge-scale distributed web crawlers.\\nOur discussion of DNS resolution (Section 20.2.2) uses the current conven-\\ntion for internet addresses, known as IPv4 (for Internet Protocol version 4) –\\neach IP address is a sequence of four bytes. In the future, the convention for\\naddresses (collectively known as the internet address space) is likely to use a\\nnew standard known as IPv6 (http://www.ipv6.org/).\\nTomasic and Garcia-Molina (1993) and Jeong and Omiecinski (1995) are\\nkey early papers evaluating term partitioning versus document partitioning\\nfor distributed indexes. Document partitioning is found to be superior, at\\nleast when the distribution of terms is skewed, as it typically is in practice.\\nThis result has generally been conﬁrmed in more recent work (MacFarlane\\net al. 2000). But the outcome depends on the details of the distributed system;\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n20.5\\nReferences and further reading\\n459\\nat least one thread of work has reached the opposite conclusion (Ribeiro-\\nNeto and Barbosa 1998, Badue et al. 2001).\\nSornil (2001) argues for a par-\\ntitioning scheme that is a hybrid between term and document partitioning.\\nBarroso et al. (2003) describe the distribution methods used at Google. The\\nﬁrst implementation of a connectivity server was described by Bharat et al.\\n(1998). The scheme discussed in this chapter, currently believed to be the\\nbest published scheme (achieving as few as 3 bits per link for encoding), is\\ndescribed in a series of papers by Boldi and Vigna (2004a;b).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n461\\n21\\nLink analysis\\nThe analysis of hyperlinks and the graph structure of the Web has been in-\\nstrumental in the development of web search. In this chapter we focus on the\\nuse of hyperlinks for ranking web search results. Such link analysis is one\\nof many factors considered by web search engines in computing a compos-\\nite score for a web page on any given query. We begin by reviewing some\\nbasics of the Web as a graph in Section 21.1, then proceed to the technical\\ndevelopment of the elements of link analysis for ranking.\\nLink analysis for web search has intellectual antecedents in the ﬁeld of cita-\\ntion analysis, aspects of which overlap with an area known as bibliometrics.\\nThese disciplines seek to quantify the inﬂuence of scholarly articles by ana-\\nlyzing the pattern of citations amongst them. Much as citations represent the\\nconferral of authority from a scholarly article to others, link analysis on the\\nWeb treats hyperlinks from a web page to another as a conferral of authority.\\nClearly, not every citation or hyperlink implies such authority conferral; for\\nthis reason, simply measuring the quality of a web page by the number of\\nin-links (citations from other pages) is not robust enough. For instance, one\\nmay contrive to set up multiple web pages pointing to a target web page,\\nwith the intent of artiﬁcially boosting the latter’s tally of in-links. This phe-\\nnomenon is referred to as link spam. Nevertheless, the phenomenon of ci-\\ntation is prevalent and dependable enough that it is feasible for web search\\nengines to derive useful signals for ranking from more sophisticated link\\nanalysis. Link analysis also proves to be a useful indicator of what page(s)\\nto crawl next while crawling the web; this is done by using link analysis to\\nguide the priority assignment in the front queues of Chapter 20.\\nSection 21.1 develops the basic ideas underlying the use of the web graph\\nin link analysis. Sections 21.2 and 21.3 then develop two distinct methods for\\nlink analysis, PageRank and HITS.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n462\\n21\\nLink analysis\\n21.1\\nThe Web as a graph\\nRecall the notion of the web graph from Section 19.2.1 and particularly Fig-\\nure 19.2. Our study of link analysis builds on two intuitions:\\n1. The anchor text pointing to page B is a good description of page B.\\n2. The hyperlink from A to B represents an endorsement of page B, by the\\ncreator of page A. This is not always the case; for instance, many links\\namongst pages within a single website stem from the user of a common\\ntemplate. For instance, most corporate websites have a pointer from ev-\\nery page to a page containing a copyright notice – this is clearly not an\\nendorsement. Accordingly, implementations of link analysis algorithms\\nwill typical discount such “internal” links.\\n21.1.1\\nAnchor text and the web graph\\nThe following fragment of HTML code from a web page shows a hyperlink\\npointing to the home page of the Journal of the ACM:\\n<a href=\"http://www.acm.org/jacm/\">Journal of the ACM.</a>\\nIn this case, the link points to the page http://www.acm.org/jacm/ and\\nthe anchor text is Journal of the ACM. Clearly, in this example the anchor is de-\\nscriptive of the target page. But then the target page (B = http://www.acm.org/jacm/)\\nitself contains the same description as well as considerable additional infor-\\nmation on the journal. So what use is the anchor text?\\nThe Web is full of instances where the page B does not provide an accu-\\nrate description of itself. In many cases this is a matter of how the publish-\\ners of page B choose to present themselves; this is especially common with\\ncorporate web pages, where a web presence is a marketing statement. For\\nexample, at the time of the writing of this book the home page of the IBM\\ncorporation (http://www.ibm.com)did not contain the term computer any-\\nwhere in its HTML code, despite the fact that IBM is widely viewed as the\\nworld’s largest computer maker. Similarly, the HTML code for the home\\npage of Yahoo! (http://www.yahoo.com) does not at this time contain the\\nword portal.\\nThus, there is often a gap between the terms in a web page, and how web\\nusers would describe that web page. Consequently, web searchers need not\\nuse the terms in a page to query for it. In addition, many web pages are rich\\nin graphics and images, and/or embed their text in these images; in such\\ncases, the HTML parsing performed when crawling will not extract text that\\nis useful for indexing these pages. The “standard IR” approach to this would\\nbe to use the methods outlined in Chapter 9 and Section 12.4. The insight\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.1\\nThe Web as a graph\\n463\\nbehind anchor text is that such methods can be supplanted by anchor text,\\nthereby tapping the power of the community of web page authors.\\nThe fact that the anchors of many hyperlinks pointing to http://www.ibm.com\\ninclude the word computer can be exploited by web search engines. For in-\\nstance, the anchor text terms can be included as terms under which to index\\nthe target web page. Thus, the postings for the term computer would include\\nthe document http://www.ibm.com and that for the term portal would in-\\nclude the document http://www.yahoo.com, using a special indicator to\\nshow that these terms occur as anchor (rather than in-page) text. As with\\nin-page terms, anchor text terms are generally weighted based on frequency,\\nwith a penalty for terms that occur very often (the most common terms in an-\\nchor text across the Web are Click and here, using methods very similar to idf).\\nThe actual weighting of terms is determined by machine-learned scoring, as\\nin Section 15.4.1; current web search engines appear to assign a substantial\\nweighting to anchor text terms.\\nThe use of anchor text has some interesting side-effects. Searching for big\\nblue on most web search engines returns the home page of the IBM corpora-\\ntion as the top hit; this is consistent with the popular nickname that many\\npeople use to refer to IBM. On the other hand, there have been (and con-\\ntinue to be) many instances where derogatory anchor text such as evil empire\\nleads to somewhat unexpected results on querying for these terms on web\\nsearch engines. This phenomenon has been exploited in orchestrated cam-\\npaigns against speciﬁc sites. Such orchestrated anchor text may be a form\\nof spamming, since a website can create misleading anchor text pointing to\\nitself, to boost its ranking on selected query terms. Detecting and combating\\nsuch systematic abuse of anchor text is another form of spam detection that\\nweb search engines perform.\\nThe window of text surrounding anchor text (sometimes referred to as ex-\\ntended anchor text) is often usable in the same manner as anchor text itself;\\nconsider for instance the fragment of web text there is good discussion\\nof vedic scripture <a>here</a>. This has been considered in a num-\\nber of settings and the useful width of this window has been studied; see\\nSection 21.4 for references.\\n?\\nExercise 21.1\\nIs it always possible to follow directed edges (hyperlinks) in the web graph from any\\nnode (web page) to any other? Why or why not?\\nExercise 21.2\\nFind an instance of misleading anchor-text on the Web.\\nExercise 21.3\\nGiven the collection of anchor-text phrases for a web page x, suggest a heuristic for\\nchoosing one term or phrase from this collection that is most descriptive of x.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n464\\n21\\nLink analysis\\n\\x12\\x11\\n\\x13\\x10\\nA\\n\\x12\\x11\\n\\x13\\x10\\nC\\n\\x12\\x11\\n\\x13\\x10\\nB\\n\\x12\\x11\\n\\x13\\x10\\nD\\n-\\n\\x00\\x00\\x12\\n@\\n@\\nR\\n◮Figure 21.1\\nThe random surfer at node A proceeds with probability 1/3 to each\\nof B, C and D.\\nExercise 21.4\\nDoes your heuristic in the previous exercise take into account a single domain D\\nrepeating anchor text for x from multiple pages in D?\\n21.2\\nPageRank\\nWe now focus on scoring and ranking measures derived from the link struc-\\nture alone. Our ﬁrst technique for link analysis assigns to every node in\\nthe web graph a numerical score between 0 and 1, known as its PageRank.\\nPAGERANK\\nThe PageRank of a node will depend on the link structure of the web graph.\\nGiven a query, a web search engine computes a composite score for each\\nweb page that combines hundreds of features such as cosine similarity (Sec-\\ntion 6.3) and term proximity (Section 7.2.2), together with the PageRank score.\\nThis composite score, developed using the methods of Section 15.4.1, is used\\nto provide a ranked list of results for the query.\\nConsider a random surfer who begins at a web page (a node of the web\\ngraph) and executes a random walk on the Web as follows. At each time\\nstep, the surfer proceeds from his current page A to a randomly chosen web\\npage that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of\\nwhich there are three hyperlinks to nodes B, C and D; the surfer proceeds at\\nthe next time step to one of these three nodes, with equal probabilities 1/3.\\nAs the surfer proceeds in this random walk from node to node, he visits\\nsome nodes more often than others; intuitively, these are nodes with many\\nlinks coming in from other frequently visited nodes. The idea behind Page-\\nRank is that pages visited more often in this walk are more important.\\nWhat if the current location of the surfer, the node A, has no out-links?\\nTo address this we introduce an additional operation for our random surfer:\\nthe teleport operation. In the teleport operation the surfer jumps from a node\\nTELEPORT\\nto any other node in the web graph. This could happen because he types\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n465\\nan address into the URL bar of his browser. The destination of a teleport\\noperation is modeled as being chosen uniformly at random from all web\\npages. In other words, if N is the total number of nodes in the web graph1,\\nthe teleport operation takes the surfer to each node with probability 1/N.\\nThe surfer would also teleport to his present position with probability 1/N.\\nIn assigning a PageRank score to each node of the web graph, we use the\\nteleport operation in two ways: (1) When at a node with no out-links, the\\nsurfer invokes the teleport operation. (2) At any node that has outgoing links,\\nthe surfer invokes the teleport operation with probability 0 < α < 1 and the\\nstandard random walk (follow an out-link chosen uniformly at random as in\\nFigure 21.1) with probability 1 −α, where α is a ﬁxed parameter chosen in\\nadvance. Typically, α might be 0.1.\\nIn Section 21.2.1, we will use the theory of Markov chains to argue that\\nwhen the surfer follows this combined process (random walk plus teleport)\\nhe visits each node v of the web graph a ﬁxed fraction of the time π(v) that\\ndepends on (1) the structure of the web graph and (2) the value of α. We call\\nthis value π(v) the PageRank of v and will show how to compute this value\\nin Section 21.2.2.\\n21.2.1\\nMarkov chains\\nA Markov chain is a discrete-time stochastic process: a process that occurs in\\na series of time-steps in each of which a random choice is made. A Markov\\nchain consists of N states. Each web page will correspond to a state in the\\nMarkov chain we will formulate.\\nA Markov chain is characterized by an N × N transition probability matrix P\\neach of whose entries is in the interval [0, 1]; the entries in each row of P add\\nup to 1. The Markov chain can be in one of the N states at any given time-\\nstep; then, the entry Pij tells us the probability that the state at the next time-\\nstep is j, conditioned on the current state being i. Each entry Pij is known as a\\ntransition probability and depends only on the current state i; this is known\\nas the Markov property. Thus, by the Markov property,\\n∀i, j, Pij ∈[0, 1]\\nand\\n∀i,\\nN\\n∑\\nj=1\\nPij = 1.\\n(21.1)\\nA matrix with non-negative entries that satisﬁes Equation (21.1) is known\\nas a stochastic matrix. A key property of a stochastic matrix is that it has a\\nSTOCHASTIC MATRIX\\nprincipal left eigenvector corresponding to its largest eigenvalue, which is 1.\\nPRINCIPAL LEFT\\nEIGENVECTOR\\n1. This is consistent with our usage of N for the number of documents in the collection.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n466\\n21\\nLink analysis\\n\\x16\\x15\\n\\x17\\x14\\nA\\n\\x16\\x15\\n\\x17\\x14\\nB\\n\\x16\\x15\\n\\x17\\x14\\nC\\n-\\n1\\n-\\n0.5\\n\\x1b\\n0.5\\n\\x1b\\n1\\n◮Figure 21.2\\nA simple Markov chain with three states; the numbers on the links\\nindicate the transition probabilities.\\nIn a Markov chain, the probability distribution of next states for a Markov\\nchain depends only on the current state, and not on how the Markov chain\\narrived at the current state. Figure 21.2 shows a simple Markov chain with\\nthree states. From the middle state A, we proceed with (equal) probabilities\\nof 0.5 to either B or C. From either B or C, we proceed with probability 1 to\\nA. The transition probability matrix of this Markov chain is then\\n\\uf8eb\\n\\uf8ed\\n0\\n0.5\\n0.5\\n1\\n0\\n0\\n1\\n0\\n0\\n\\uf8f6\\n\\uf8f8\\nA Markov chain’s probability distribution over its states may be viewed as\\na probability vector: a vector all of whose entries are in the interval [0, 1], and\\nPROBABILITY VECTOR\\nthe entries add up to 1. An N-dimensional probability vector each of whose\\ncomponents corresponds to one of the N states of a Markov chain can be\\nviewed as a probability distribution over its states. For our simple Markov\\nchain of Figure 21.2, the probability vector would have 3 components that\\nsum to 1.\\nWe can view a random surfer on the web graph as a Markov chain, with\\none state for each web page, and each transition probability representing the\\nprobability of moving from one web page to another. The teleport operation\\ncontributes to these transition probabilities. The adjacency matrix A of the\\nweb graph is deﬁned as follows: if there is a hyperlink from page i to page\\nj, then Aij = 1, otherwise Aij = 0. We can readily derive the transition\\nprobability matrix P for our Markov chain from the N × N matrix A:\\n1. If a row of A has no 1’s, then replace each element by 1/N. For all other\\nrows proceed as follows.\\n2. Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row\\nwith three 1’s, then each of them is replaced by 1/3.\\n3. Multiply the resulting matrix by 1 −α.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n467\\n4. Add α/N to every entry of the resulting matrix, to obtain P.\\nWe can depict the probability distribution of the surfer’s position at any\\ntime by a probability vector ⃗x. At t = 0 the surfer may begin at a state whose\\ncorresponding entry in ⃗x is 1 while all others are zero. By deﬁnition, the\\nsurfer’s distribution at t = 1 is given by the probability vector ⃗xP; at t = 2\\nby (⃗xP)P = ⃗xP2, and so on. We will detail this process in Section 21.2.2. We\\ncan thus compute the surfer’s distribution over the states at any time, given\\nonly the initial distribution and the transition probability matrix P.\\nIf a Markov chain is allowed to run for many time steps, each state is vis-\\nited at a (different) frequency that depends on the structure of the Markov\\nchain. In our running analogy, the surfer visits certain web pages (say, pop-\\nular news home pages) more often than other pages. We now make this in-\\ntuition precise, establishing conditions under which such the visit frequency\\nconverges to ﬁxed, steady-state quantity. Following this, we set the Page-\\nRank of each node v to this steady-state visit frequency and show how it can\\nbe computed.\\nDeﬁnition:\\nA Markov chain is said to be ergodic if there exists a positive\\nERGODIC MARKOV\\nCHAIN\\ninteger T0 such that for all pairs of states i, j in the Markov chain, if it is\\nstarted at time 0 in state i then for all t > T0, the probability of being in state\\nj at time t is greater than 0.\\nFor a Markov chain to be ergodic, two technical conditions are required\\nof its states and the non-zero transition probabilities; these conditions are\\nknown as irreducibility and aperiodicity. Informally, the ﬁrst ensures that there\\nis a sequence of transitions of non-zero probability from any state to any\\nother, while the latter ensures that the states are not partitioned into sets\\nsuch that all state transitions occur cyclically from one set to another.\\nTheorem 21.1. For any ergodic Markov chain, there is a unique steady-state prob-\\nSTEADY-STATE\\nability vector ⃗π that is the principal left eigenvector of P, such that if η(i, t) is the\\nnumber of visits to state i in t steps, then\\nlim\\nt→∞\\nη(i, t)\\nt\\n= π(i),\\nwhere π(i) > 0 is the steady-state probability for state i.\\nIt follows from Theorem 21.1 that the random walk with teleporting re-\\nsults in a unique distribution of steady-state probabilities over the states of\\nthe induced Markov chain. This steady-state probability for a state is the\\nPageRank of the corresponding web page.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n468\\n21\\nLink analysis\\n21.2.2\\nThe PageRank computation\\nHow do we compute PageRank values? Recall the deﬁnition of a left eigen-\\nvector from Equation 18.2; the left eigenvectors of the transition probability\\nmatrix P are N-vectors ⃗π such that\\n⃗π P = λ⃗π.\\n(21.2)\\nThe N entries in the principal eigenvector ⃗π are the steady-state proba-\\nbilities of the random walk with teleporting, and thus the PageRank values\\nfor the corresponding web pages. We may interpret Equation (21.2) as fol-\\nlows: if ⃗π is the probability distribution of the surfer across the web pages,\\nhe remains in the steady-state distribution ⃗π. Given that ⃗π is the steady-state\\ndistribution, we have that πP = 1π, so 1 is an eigenvalue of P. Thus if we\\nwere to compute the principal left eigenvector of the matrix P — the one with\\neigenvalue 1 — we would have computed the PageRank values.\\nThere are many algorithms available for computing left eigenvectors; the\\nreferences at the end of Chapter 18 and the present chapter are a guide to\\nthese. We give here a rather elementary method, sometimes known as power\\niteration. If ⃗x is the initial distribution over the states, then the distribution at\\ntime t is ⃗xPt. As t grows large, we would expect that the distribution ⃗xPt2\\nis very similar to the distribution ⃗xPt+1, since for large t we would expect\\nthe Markov chain to attain its steady state. By Theorem 21.1 this is indepen-\\ndent of the initial distribution ⃗x. The power iteration method simulates the\\nsurfer’s walk: begin at a state and run the walk for a large number of steps\\nt, keeping track of the visit frequencies for each of the states. After a large\\nnumber of steps t, these frequencies “settle down” so that the variation in the\\ncomputed frequencies is below some predetermined threshold. We declare\\nthese tabulated frequencies to be the PageRank values.\\nWe consider the web graph in Exercise 21.6 with α = 0.5. The transition\\nprobability matrix of the surfer’s walk with teleportation is then\\nP =\\n\\uf8eb\\n\\uf8ed\\n1/6\\n2/3\\n1/6\\n5/12\\n1/6\\n5/12\\n1/6\\n2/3\\n1/6\\n\\uf8f6\\n\\uf8f8.\\n(21.3)\\nImagine that the surfer starts in state 1, corresponding to the initial proba-\\nbility distribution vector ⃗x0 = (1 0 0). Then, after one step the distribution\\nis\\n⃗x0P =\\n\\x001/6\\n2/3\\n1/6\\n\\x01 = ⃗x1.\\n(21.4)\\n2. Note that Pt represents P raised to the tth power, not the transpose of P which is denoted PT.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n469\\n⃗x0\\n1\\n0\\n0\\n⃗x1\\n1/6\\n2/3\\n1/6\\n⃗x2\\n1/3\\n1/3\\n1/3\\n⃗x3\\n1/4\\n1/2\\n1/4\\n⃗x4\\n7/24\\n5/12\\n7/24\\n. . .\\n· · ·\\n· · ·\\n· · ·\\n⃗x\\n5/18\\n4/9\\n5/18\\n◮Figure 21.3\\nThe sequence of probability vectors.\\nAfter two steps it is\\n⃗x1P =\\n\\x00 1/6\\n2/3\\n1/6 \\x01\\n\\uf8eb\\n\\uf8ed\\n1/6\\n2/3\\n1/6\\n5/12\\n1/6\\n5/12\\n1/6\\n2/3\\n1/6\\n\\uf8f6\\n\\uf8f8=\\n\\x00 1/3\\n1/3\\n1/3 \\x01 = ⃗x2.\\n(21.5)\\nContinuing in this fashion gives a sequence of probability vectors as shown\\nin Figure 21.3.\\nContinuing for several steps, we see that the distribution converges to the\\nsteady state of ⃗x = (5/18\\n4/9\\n5/18). In this simple example, we may\\ndirectly calculate this steady-state probability distribution by observing the\\nsymmetry of the Markov chain: states 1 and 3 are symmetric, as evident from\\nthe fact that the ﬁrst and third rows of the transition probability matrix in\\nEquation (21.3) are identical. Postulating, then, that they both have the same\\nsteady-state probability and denoting this probability by p, we know that the\\nsteady-state distribution is of the form ⃗π = (p 1 −2p p). Now, using the\\nidentity ⃗π = ⃗πP, we solve a simple linear equation to obtain p = 5/18 and\\nconsequently, ⃗π = (5/18 4/9 5/18).\\nThe PageRank values of pages (and the implicit ordering amongst them)\\nare independent of any query a user might pose; PageRank is thus a query-\\nindependent measure of the static quality of each web page (recall such static\\nquality measures from Section 7.1.4). On the other hand, the relative order-\\ning of pages should, intuitively, depend on the query being served. For this\\nreason, search engines use static quality measures such as PageRank as just\\none of many factors in scoring a web page on a query. Indeed, the relative\\ncontribution of PageRank to the overall score may again be determined by\\nmachine-learned scoring as in Section 15.4.1.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n470\\n21\\nLink analysis\\nd0\\nd2\\nd1\\nd5\\nd3\\nd6\\nd4\\ncar\\nbenz\\nford\\ngm\\nhonda\\njaguar\\njag\\ncat\\nleopard\\ntiger\\njaguar\\nlion\\ncheetah\\nspeed\\n◮Figure 21.4\\nA small web graph. Arcs are annotated with the word that occurs in\\nthe anchor text of the corresponding link.\\n\\x0f\\nExample 21.1:\\nConsider the graph in Figure 21.4. For a teleportation rate of 0.14\\nits (stochastic) transition probability matrix is:\\n0.02\\n0.02\\n0.88\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.02\\n0.31\\n0.02\\n0.31\\n0.31\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.88\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.31\\n0.31\\n0.02\\n0.31\\nThe PageRank vector of this matrix is:\\n⃗x = (0.05\\n0.04\\n0.11\\n0.25\\n0.21\\n0.04\\n0.31)\\n(21.6)\\nObserve that in Figure 21.4, q2, q3, q4 and q6 are the nodes with at least two in-links.\\nOf these, q2 has the lowest PageRank since the random walk tends to drift out of the\\ntop part of the graph – the walker can only return there through teleportation.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n471\\n21.2.3\\nTopic-speciﬁc PageRank\\nThus far we have discussed the PageRank computation with a teleport op-\\neration in which the surfer jumps to a random web page chosen uniformly\\nat random. We now consider teleporting to a random web page chosen non-\\nuniformly. In doing so, we are able to derive PageRank values tailored to\\nparticular interests. For instance, a sports aﬁcionado might wish that pages\\non sports be ranked higher than non-sports pages. Suppose that web pages\\non sports are “near” one another in the web graph. Then, a random surfer\\nwho frequently ﬁnds himself on random sports pages is likely (in the course\\nof the random walk) to spend most of his time at sports pages, so that the\\nsteady-state distribution of sports pages is boosted.\\nSuppose our random surfer, endowed with a teleport operation as before,\\nteleports to a random web page on the topic of sports instead of teleporting to a\\nuniformly chosen random web page. We will not focus on how we collect all\\nweb pages on the topic of sports; in fact, we only need a non-zero subset S of\\nsports-related web pages, so that the teleport operation is feasible. This may\\nbe obtained, for instance, from a manually built directory of sports pages\\nsuch as the open directory project (http://www.dmoz.org/) or that of Yahoo.\\nProvided the set S of sports-related pages is non-empty, it follows that\\nthere is a non-empty set of web pages Y ⊇S over which the random walk\\nhas a steady-state distribution; let us denote this sports PageRank distribution\\nby ⃗πs. For web pages not in Y, we set the PageRank values to zero. We call\\n⃗πs the topic-speciﬁc PageRank for sports.\\nTOPIC-SPECIFIC\\nPAGERANK\\nWe do not demand that teleporting takes the random surfer to a uniformly\\nchosen sports page; the distribution over teleporting targets S could in fact\\nbe arbitrary.\\nIn like manner we can envision topic-speciﬁc PageRank distributions for\\neach of several topics such as science, religion, politics and so on. Each of\\nthese distributions assigns to each web page a PageRank value in the interval\\n[0, 1). For a user interested in only a single topic from among these topics,\\nwe may invoke the corresponding PageRank distribution when scoring and\\nranking search results. This gives us the potential of considering settings in\\nwhich the search engine knows what topic a user is interested in. This may\\nhappen because users either explicitly register their interests, or because the\\nsystem learns by observing each user’s behavior over time.\\nBut what if a user is known to have a mixture of interests from multiple\\ntopics? For instance, a user may have an interest mixture (or proﬁle) that is\\n60% sports and 40% politics; can we compute a personalized PageRank for this\\nPERSONALIZED\\nPAGERANK\\nuser? At ﬁrst glance, this appears daunting: how could we possibly compute\\na different PageRank distribution for each user proﬁle (with, potentially, in-\\nﬁnitely many possible proﬁles)? We can in fact address this provided we\\nassume that an individual’s interests can be well-approximated as a linear\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n472\\n21\\nLink analysis\\n◮Figure 21.5\\nTopic-speciﬁc PageRank. In this example we consider a user whose\\ninterests are 60% sports and 40% politics. If the teleportation probability is 10%, this\\nuser is modeled as teleporting 6% to sports pages and 4% to politics pages.\\ncombination of a small number of topic page distributions. A user with this\\nmixture of interests could teleport as follows: determine ﬁrst whether to tele-\\nport to the set S of known sports pages, or to the set of known politics pages.\\nThis choice is made at random, choosing sports pages 60% of the time and\\npolitics pages 40% of the time. Once we choose that a particular teleport step\\nis to (say) a random sports page, we choose a web page in S uniformly at\\nrandom to teleport to. This in turn leads to an ergodic Markov chain with a\\nsteady-state distribution that is personalized to this user’s preferences over\\ntopics (see Exercise 21.16).\\nWhile this idea has intuitive appeal, its implementation appears cumber-\\nsome: it seems to demand that for each user, we compute a transition prob-\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n473\\nability matrix and compute its steady-state distribution. We are rescued by\\nthe fact that the evolution of the probability distribution over the states of\\na Markov chain can be viewed as a linear system. In Exercise 21.16 we will\\nshow that it is not necessary to compute a PageRank vector for every distinct\\ncombination of user interests over topics; the personalized PageRank vector\\nfor any user can be expressed as a linear combination of the underlying topic-\\nspeciﬁc PageRanks. For instance, the personalized PageRank vector for the\\nuser whose interests are 60% sports and 40% politics can be computed as\\n0.6⃗πs + 0.4⃗πp,\\n(21.7)\\nwhere ⃗πs and ⃗πp are the topic-speciﬁc PageRank vectors for sports and for\\npolitics, respectively.\\n?\\nExercise 21.5\\nWrite down the transition probability matrix for the example in Figure 21.2.\\nExercise 21.6\\nConsider a web graph with three nodes 1, 2 and 3. The links are as follows: 1 →\\n2, 3 →2, 2 →1, 2 →3. Write down the transition probability matrices for the surfer’s\\nwalk with teleporting, for the following three values of the teleport probability: (a)\\nα = 0; (b) α = 0.5 and (c) α = 1.\\nExercise 21.7\\nA user of a browser can, in addition to clicking a hyperlink on the page x he is cur-\\nrently browsing, use the back button to go back to the page from which he arrived at\\nx. Can such a user of back buttons be modeled as a Markov chain? How would we\\nmodel repeated invocations of the back button?\\nExercise 21.8\\nConsider a Markov chain with three states A, B and C, and transition probabilities as\\nfollows. From state A, the next state is B with probability 1. From B, the next state is\\neither A with probability pA, or state C with probability 1 −pA. From C the next state\\nis A with probability 1. For what values of pA ∈[0, 1] is this Markov chain ergodic?\\nExercise 21.9\\nShow that for any directed graph, the Markov chain induced by a random walk with\\nthe teleport operation is ergodic.\\nExercise 21.10\\nShow that the PageRank of every page is at least α/N. What does this imply about\\nthe difference in PageRank values (over the various pages) as α becomes close to 1?\\nExercise 21.11\\nFor the data in Example 21.1, write a small routine or use a scientiﬁc calculator to\\ncompute the PageRank values stated in Equation (21.6).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n474\\n21\\nLink analysis\\nExercise 21.12\\nSuppose that the web graph is stored on disk as an adjacency list, in such a way that\\nyou may only query for the out-neighbors of pages in the order in which they are\\nstored. You cannot load the graph in main memory but you may do multiple reads\\nover the full graph. Write the algorithm for computing the PageRank in this setting.\\nExercise 21.13\\nRecall the sets S and Y introduced near the beginning of Section 21.2.3. How does the\\nset Y relate to S?\\nExercise 21.14\\nIs the set Y always the set of all web pages? Why or why not?\\nExercise 21.15\\n[⋆⋆⋆]\\nIs the sports PageRank of any page in S at least as large as its PageRank?\\nExercise 21.16\\n[⋆⋆⋆]\\nConsider a setting where we have two topic-speciﬁc PageRank values for each web\\npage: a sports PageRank ⃗πs, and a politics PageRank ⃗πp. Let α be the (common)\\nteleportation probability used in computing both sets of topic-speciﬁc PageRanks.\\nFor q ∈[0, 1], consider a user whose interest proﬁle is divided between a fraction q in\\nsports and a fraction 1 −q in politics. Show that the user’s personalized PageRank is\\nthe steady-state distribution of a random walk in which – on a teleport step – the walk\\nteleports to a sports page with probability q and to a politics page with probability\\n1 −q.\\nExercise 21.17\\nShow that the Markov chain corresponding to the walk in Exercise 21.16 is ergodic\\nand hence the user’s personalized PageRank can be obtained by computing the steady-\\nstate distribution of this Markov chain.\\nExercise 21.18\\nShow that in the steady-state distribution of Exercise 21.17, the steady-state probabil-\\nity for any web page i equals qπs(i) + (1 −q)πp(i).\\n21.3\\nHubs and Authorities\\nWe now develop a scheme in which, given a query, every web page is as-\\nsigned two scores. One is called its hub score and the other its authority score.\\nHUB SCORE\\nAUTHORITY SCORE\\nFor any query, we compute two ranked lists of results rather than one. The\\nranking of one list is induced by the hub scores and that of the other by the\\nauthority scores.\\nThis approach stems from a particular insight into the creation of web\\npages, that there are two primary kinds of web pages useful as results for\\nbroad-topic searches. By a broad topic search we mean an informational query\\nsuch as \"I wish to learn about leukemia\". There are authoritative sources of\\ninformation on the topic; in this case, the National Cancer Institute’s page on\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n475\\nleukemia would be such a page. We will call such pages authorities; in the\\ncomputation we are about to describe, they are the pages that will emerge\\nwith high authority scores.\\nOn the other hand, there are many pages on the Web that are hand-compiled\\nlists of links to authoritative web pages on a speciﬁc topic. These hub pages\\nare not in themselves authoritative sources of topic-speciﬁc information, but\\nrather compilations that someone with an interest in the topic has spent time\\nputting together. The approach we will take, then, is to use these hub pages\\nto discover the authority pages. In the computation we now develop, these\\nhub pages are the pages that will emerge with high hub scores.\\nA good hub page is one that points to many good authorities; a good au-\\nthority page is one that is pointed to by many good hub pages. We thus\\nappear to have a circular deﬁnition of hubs and authorities; we will turn this\\ninto an iterative computation. Suppose that we have a subset of the web con-\\ntaining good hub and authority pages, together with the hyperlinks amongst\\nthem. We will iteratively compute a hub score and an authority score for ev-\\nery web page in this subset, deferring the discussion of how we pick this\\nsubset until Section 21.3.1.\\nFor a web page v in our subset of the web, we use h(v) to denote its hub\\nscore and a(v) its authority score. Initially, we set h(v) = a(v) = 1 for all\\nnodes v. We also denote by v 7→y the existence of a hyperlink from v to\\ny. The core of the iterative algorithm is a pair of updates to the hub and au-\\nthority scores of all pages given by Equation 21.8, which capture the intuitive\\nnotions that good hubs point to good authorities and that good authorities\\nare pointed to by good hubs.\\nh(v)\\n←\\n∑\\nv7→y\\na(y)\\n(21.8)\\na(v)\\n←\\n∑\\ny7→v\\nh(y).\\nThus, the ﬁrst line of Equation (21.8) sets the hub score of page v to the sum\\nof the authority scores of the pages it links to. In other words, if v links to\\npages with high authority scores, its hub score increases. The second line\\nplays the reverse role; if page v is linked to by good hubs, its authority score\\nincreases.\\nWhat happens as we perform these updates iteratively, recomputing hub\\nscores, then new authority scores based on the recomputed hub scores, and\\nso on? Let us recast the equations Equation (21.8) into matrix-vector form.\\nLet⃗h and⃗a denote the vectors of all hub and all authority scores respectively,\\nfor the pages in our subset of the web graph. Let A denote the adjacency\\nmatrix of the subset of the web graph that we are dealing with: A is a square\\nmatrix with one row and one column for each page in the subset. The entry\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n476\\n21\\nLink analysis\\nAij is 1 if there is a hyperlink from page i to page j, and 0 otherwise. Then,\\nwe may write Equation (21.8)\\n⃗h\\n←\\nA⃗a\\n(21.9)\\n⃗a\\n←\\nAT⃗h,\\nwhere AT denotes the transpose of the matrix A. Now the right hand side of\\neach line of Equation (21.9) is a vector that is the left hand side of the other\\nline of Equation (21.9). Substituting these into one another, we may rewrite\\nEquation (21.9) as\\n⃗h\\n←\\nAAT⃗h\\n(21.10)\\n⃗a\\n←\\nATA⃗a.\\nNow, Equation (21.10) bears an uncanny resemblance to a pair of eigenvector\\nequations (Section 18.1); indeed, if we replace the ←symbols by = symbols\\nand introduce the (unknown) eigenvalue, the ﬁrst line of Equation (21.10)\\nbecomes the equation for the eigenvectors of AAT, while the second becomes\\nthe equation for the eigenvectors of ATA:\\n⃗h\\n=\\n(1/λh)AAT⃗h\\n⃗a\\n=\\n(1/λa)ATA⃗a.\\n(21.11)\\nHere we have used λh to denote the eigenvalue of AAT and λa to denote the\\neigenvalue of ATA.\\nThis leads to some key consequences:\\n1. The iterative updates in Equation (21.8) (or equivalently, Equation (21.9)),\\nif scaled by the appropriate eigenvalues, are equivalent to the power iter-\\nation method for computing the eigenvectors of AAT and ATA. Provided\\nthat the principal eigenvalue of AAT is unique, the iteratively computed\\nentries of⃗h and⃗a settle into unique steady-state values determined by the\\nentries of A and hence the link structure of the graph.\\n2. In computing these eigenvector entries, we are not restricted to using the\\npower iteration method; indeed, we could use any fast method for com-\\nputing the principal eigenvector of a stochastic matrix.\\nThe resulting computation thus takes the following form:\\n1. Assemble the target subset of web pages, form the graph induced by their\\nhyperlinks and compute AAT and ATA.\\n2. Compute the principal eigenvectors of AAT and ATA to form the vector\\nof hub scores⃗h and authority scores⃗a.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n477\\n3. Output the top-scoring hubs and the top-scoring authorities.\\nThis method of link analysis is known as HITS, which is an acronym for\\nHITS\\nHyperlink-Induced Topic Search.\\n\\x0f\\nExample 21.2:\\nAssuming the query jaguar and double-weighting of links whose\\nanchors contain the query word, the matrix A for Figure 21.4 is as follows:\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n2\\n1\\n0\\n1\\nThe hub and authority vectors are:\\n⃗h = (0.03\\n0.04\\n0.33\\n0.18\\n0.04\\n0.04\\n0.35)\\n⃗a = (0.10\\n0.01\\n0.12\\n0.47\\n0.16\\n0.01\\n0.13)\\nHere, q3 is the main authority – two hubs (q2 and q6) are pointing to it via highly\\nweighted jaguar links.\\nSince the iterative updates captured the intuition of good hubs and good\\nauthorities, the high-scoring pages we output would give us good hubs and\\nauthorities from the target subset of web pages. In Section 21.3.1 we describe\\nthe remaining detail: how do we gather a target subset of web pages around\\na topic such as leukemia?\\n21.3.1\\nChoosing the subset of the Web\\nIn assembling a subset of web pages around a topic such as leukemia, we must\\ncope with the fact that good authority pages may not contain the speciﬁc\\nquery term leukemia. This is especially true, as we noted in Section 21.1.1,\\nwhen an authority page uses its web presence to project a certain market-\\ning image. For instance, many pages on the IBM website are authoritative\\nsources of information on computer hardware, even though these pages may\\nnot contain the term computer or hardware. However, a hub compiling com-\\nputer hardware resources is likely to use these terms and also link to the\\nrelevant pages on the IBM website.\\nBuilding on these observations, the following procedure has been sug-\\ngested for compiling the subset of the Web for which to compute hub and\\nauthority scores.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n478\\n21\\nLink analysis\\n1. Given a query (say leukemia), use a text index to get all pages containing\\nleukemia. Call this the root set of pages.\\n2. Build the base set of pages, to include the root set as well as any page that\\neither links to a page in the root set, or is linked to by a page in the root\\nset.\\nWe then use the base set for computing hub and authority scores. The base\\nset is constructed in this manner for three reasons:\\n1. A good authority page may not contain the query text (such as computer\\nhardware).\\n2. If the text query manages to capture a good hub page vh in the root set,\\nthen the inclusion of all pages linked to by any page in the root set will\\ncapture all the good authorities linked to by vh in the base set.\\n3. Conversely, if the text query manages to capture a good authority page\\nva in the root set, then the inclusion of pages which point to va will bring\\nother good hubs into the base set. In other words, the “expansion” of\\nthe root set into the base set enriches the common pool of good hubs and\\nauthorities.\\nRunning HITS across a variety of queries reveals some interesting insights\\nabout link analysis. Frequently, the documents that emerge as top hubs and\\nauthorities include languages other than the language of the query. These\\npages were presumably drawn into the base set, following the assembly of\\nthe root set. Thus, some elements of cross-language retrieval (where a query\\nin one language retrieves documents in another) are evident here; interest-\\ningly, this cross-language effect resulted purely from link analysis, with no\\nlinguistic translation taking place.\\nWe conclude this section with some notes on implementing this algorithm.\\nThe root set consists of all pages matching the text query; in fact, implemen-\\ntations (see the references in Section 21.4) suggest that it sufﬁces to use 200 or\\nso web pages for the root set, rather than all pages matching the text query.\\nAny algorithm for computing eigenvectors may be used for computing the\\nhub/authority score vector. In fact, we need not compute the exact values\\nof these scores; it sufﬁces to know the relative values of the scores so that\\nwe may identify the top hubs and authorities. To this end, it is possible that\\na small number of iterations of the power iteration method yields the rela-\\ntive ordering of the top hubs and authorities. Experiments have suggested\\nthat in practice, about ﬁve iterations of Equation (21.8) yield fairly good re-\\nsults. Moreover, since the link structure of the web graph is fairly sparse\\n(the average web page links to about ten others), we do not perform these as\\nmatrix-vector products but rather as additive updates as in Equation (21.8).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n479\\n◮Figure 21.6\\nA sample run of HITS on the query japan elementary schools.\\nFigure 21.6 shows the results of running HITS on the query japan elemen-\\ntary schools. The ﬁgure shows the top hubs and authorities; each row lists the\\ntitle tag from the corresponding HTML page. Because the resulting string\\nis not necessarily in Latin characters, the resulting print is (in many cases)\\na string of gibberish. Each of these corresponds to a web page that does\\nnot use Latin characters, in this case very likely pages in Japanese. There\\nalso appear to be pages in other non-English languages, which seems sur-\\nprising given that the query string is in English. In fact, this result is em-\\nblematic of the functioning of HITS – following the assembly of the root set,\\nthe (English) query string is ignored. The base set is likely to contain pages\\nin other languages, for instance if an English-language hub page links to\\nthe Japanese-language home pages of Japanese elementary schools. Because\\nthe subsequent computation of the top hubs and authorities is entirely link-\\nbased, some of these non-English pages will appear among the top hubs and\\nauthorities.\\n?\\nExercise 21.19\\nIf all the hub and authority scores are initialized to 1, what is the hub/authority score\\nof a node after one iteration?\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n480\\n21\\nLink analysis\\nExercise 21.20\\nHow would you interpret the entries of the matrices AAT and ATA? What is the\\nconnection to the co-occurrence matrix CCT in Chapter 18?\\nExercise 21.21\\nWhat are the principal eigenvalues of AAT and ATA?\\nd1\\nd2\\nd3\\n◮Figure 21.7\\nWeb graph for Exercise 21.22.\\nExercise 21.22\\nFor the web graph in Figure 21.7, compute PageRank, hub and authority scores for\\neach of the three pages. Also give the relative ordering of the 3 nodes for each of these\\nscores, indicating any ties.\\nPageRank: Assume that at each step of the PageRank random walk, we teleport to a\\nrandom page with probability 0.1, with a uniform distribution over which particular\\npage we teleport to.\\nHubs/Authorities: Normalize the hub (authority) scores so that the maximum hub\\n(authority) score is 1.\\nHint 1: Using symmetries to simplify and solving with linear equations might be\\neasier than using iterative methods.\\nHint 2: Provide the relative ordering (indicating any ties) of the three nodes for each\\nof the three scoring measures.\\n21.4\\nReferences and further reading\\nGarﬁeld (1955) is seminal in the science of citation analysis. This was built\\non by Pinski and Narin (1976) to develop a journal inﬂuence weight, whose\\ndeﬁnition is remarkably similar to that of the PageRank measure.\\nThe use of anchor text as an aid to searching and ranking stems from the\\nwork of McBryan (1994). Extended anchor-text was implicit in his work, with\\nsystematic experiments reported in Chakrabarti et al. (1998).\\nKemeny and Snell (1976) is a classic text on Markov chains. The PageRank\\nmeasure was developed in Brin and Page (1998) and in Page et al. (1998).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n21.4\\nReferences and further reading\\n481\\nA number of methods for the fast computation of PageRank values are sur-\\nveyed in Berkhin (2005) and in Langville and Meyer (2006); the former also\\ndetails how the PageRank eigenvector solution may be viewed as solving a\\nlinear system, leading to one way of solving Exercise 21.16. The effect of the\\nteleport probability α has been studied by Baeza-Yates et al. (2005) and by\\nBoldi et al. (2005). Topic-speciﬁc PageRank and variants were developed in\\nHaveliwala (2002), Haveliwala (2003) and in Jeh and Widom (2003). Berkhin\\n(2006a) develops an alternate view of topic-speciﬁc PageRank.\\nNg et al. (2001b) suggests that the PageRank score assignment is more ro-\\nbust than HITS in the sense that scores are less sensitive to small changes in\\ngraph topology. However, it has also been noted that the teleport operation\\ncontributes signiﬁcantly to PageRank’s robustness in this sense. Both Page-\\nRank and HITS can be “spammed” by the orchestrated insertion of links into\\nthe web graph; indeed, the Web is known to have such link farms that col-\\nLINK FARMS\\nlude to increase the score assigned to certain pages by various link analysis\\nalgorithms.\\nThe HITS algorithm is due to Kleinberg (1999). Chakrabarti et al. (1998) de-\\nveloped variants that weighted links in the iterative computation based on\\nthe presence of query terms in the pages being linked and compared these\\nto results from several web search engines. Bharat and Henzinger (1998) fur-\\nther developed these and other heuristics, showing that certain combinations\\noutperformed the basic HITS algorithm. Borodin et al. (2001) provides a sys-\\ntematic study of several variants of the HITS algorithm. Ng et al. (2001b)\\nintroduces a notion of stability for link analysis, arguing that small changes\\nto link topology should not lead to signiﬁcant changes in the ranked list of\\nresults for a query. Numerous other variants of HITS have been developed\\nby a number of authors, the best know of which is perhaps SALSA (Lempel\\nand Moran 2000).\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n483\\nBibliography\\nWe use the following abbreviated journal and conference names in the bibliography:\\nCACM Communications of the Association for Computing Machinery.\\nIP&M Information Processing and Management.\\nIR Information Retrieval.\\nJACM Journal of the Association for Computing Machinery.\\nJASIS Journal of the American Society for Information Science.\\nJASIST Journal of the American Society for Information Science and Technology.\\nJMLR Journal of Machine Learning Research.\\nTOIS ACM Transactions on Information Systems.\\nProc. ACL Proceedings of the Annual Meeting of the Association for Computational\\nLinguistics. Available from: http://www.aclweb.org/anthology-index/\\nProc. CIKM Proceedings of the ACM CIKM Conference on Information and Know-\\nledge Management. ACM Press.\\nProc. ECIR Proceedings of the European Conference on Information Retrieval.\\nProc. ECML Proceedings of the European Conference on Machine Learning.\\nProc. ICML Proceedings of the International Conference on Machine Learning.\\nProc. IJCAI Proceedings of the International Joint Conference on Artiﬁcial Intelli-\\ngence.\\nProc. INEX Proceedings of the Initiative for the Evaluation of XML Retrieval.\\nProc. KDD Proceedings of the ACM SIGKDD International Conference on Know-\\nledge Discovery and Data Mining.\\nProc. NIPS Proceedings of the Neural Information Processing Systems Conference.\\nProc. PODS Proceedings of the ACM Conference on Principles of Database Systems.\\nProc. SDAIR Proceedings of the Annual Symposium on Document Analysis and In-\\nformation Retrieval.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n484\\nBibliography\\nProc. SIGIR Proceedings of the Annual International ACM/SIGIR Conference on\\nResearch and Development in Information Retrieval. Available from: http://www.sigir.org/proceedings/Proc-\\nBrowse.html\\nProc. SPIRE Proceedings of the Symposium on String Processing and Information\\nRetrieval.\\nProc. TREC Proceedings of the Text Retrieval Conference.\\nProc. UAI Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence.\\nProc. VLDB Proceedings of the Very Large Data Bases Conference.\\nProc. WWW Proceedings of the International World Wide Web Conference.\\nAberer, Karl. 2001. P-Grid: A self-organizing access structure for P2P information\\nsystems.\\nIn Proc. International Conference on Cooperative Information Systems, pp.\\n179–194. Springer. xxxiv, 519\\nAizerman, Mark A., Emmanuel M. Braverman, and Lev I. Rozonoér. 1964. Theoret-\\nical foundations of the potential function method in pattern recognition learning.\\nAutomation and Remote Control 25:821–837. 347, 519, 520, 530\\nAkaike, Hirotugu. 1974. A new look at the statistical model identiﬁcation. IEEE\\nTransactions on automatic control 19(6):716–723. 373, 519\\nAllan, James. 2005. HARD track overview in TREC 2005: High accuracy retrieval\\nfrom documents. In Proc. TREC. 174, 519\\nAllan, James, Ron Papka, and Victor Lavrenko.\\n1998.\\nOn-line new event\\ndetection and tracking.\\nIn Proc. SIGIR, pp. 37–45. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/290941.290954. 399, 519, 526, 528\\nAllwein, Erin L., Robert E. Schapire, and Yoram Singer. 2000. Reducing multiclass\\nto binary: A unifying approach for margin classiﬁers.\\nJMLR 1:113–141.\\nURL:\\nwww.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf. 315, 519, 530, 531\\nAlonso, Omar, Sandeepan Banerjee, and Mark Drake. 2006. GIO: A semantic web\\napplication using the information grid framework. In Proc. WWW, pp. 857–858.\\nACM Press. DOI: doi.acm.org/10.1145/1135777.1135913. 373, 519, 522\\nAltingövde, Ismail Sengör, Engin Demir, Fazli Can, and Özgür Ulusoy. 2008. In-\\ncremental cluster-based retrieval using compressed cluster-skipping inverted ﬁles.\\nTOIS. To appear. 372\\nAltingövde, Ismail Sengör, Rifat Ozcan, Huseyin Cagdas Ocalan, Fazli Can, and\\nÖzgür Ulusoy. 2007. Large-scale cluster-based retrieval experiments on Turkish\\ntexts. In Proc. SIGIR, pp. 891–892. ACM Press. 519, 521, 528, 532\\nAmer-Yahia, Sihem, Chavdar Botev, Jochen Dörre, and Jayavel Shanmugasundaram.\\n2006. XQuery full-text extensions explained. IBM Systems Journal 45(2):335–352.\\n217, 519, 520, 522, 530\\nAmer-Yahia, Sihem, Pat Case, Thomas Rölleke, Jayavel Shanmugasundaram, and\\nGerhard Weikum. 2005. Report on the DB/IR panel at SIGMOD 2005. SIGMOD\\nRecord 34(4):71–74. DOI: doi.acm.org/10.1145/1107499.1107514. 217, 519, 521, 530, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n485\\nAmer-Yahia, Sihem, and Mounia Lalmas. 2006. XML search: Languages, INEX and\\nscoring. SIGMOD Record 35(4):16–23.\\nDOI: doi.acm.org/10.1145/1228268.1228271.\\n217, 519, 526\\nAnagnostopoulos, Aris, Andrei Z. Broder, and Kunal Punera. 2006. Effective and\\nefﬁcient classiﬁcation on a search-engine model. In Proc. CIKM, pp. 208–217. ACM\\nPress. DOI: doi.acm.org/10.1145/1183614.1183648. 315, 519, 520, 529\\nAnderberg, Michael R. 1973. Cluster analysis for applications. Academic Press. 372, 519\\nAndoni, Alexandr, Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab Mirrokni.\\n2006.\\nLocality-sensitive hashing using stable distributions.\\nIn Nearest Neighbor\\nMethods in Learning and Vision: Theory and Practice. MIT Press. 314, 519, 522, 524,\\n527\\nAnh, Vo Ngoc, Owen de Kretser, and Alistair Moffat. 2001. Vector-space ranking with\\neffective early termination. In Proc. SIGIR, pp. 35–42. ACM Press. 149, 519, 526, 527\\nAnh,\\nVo\\nNgoc,\\nand\\nAlistair\\nMoffat.\\n2005.\\nInverted\\nindex\\ncom-\\npression\\nusing\\nword-aligned\\nbinary\\ncodes.\\nIR\\n8(1):151–166.\\nDOI:\\ndx.doi.org/10.1023/B:INRT.0000048490.99518.5c. 106, 519, 527\\nAnh, Vo Ngoc, and Alistair Moffat. 2006a. Improved word-aligned binary compres-\\nsion for text indexing. IEEE Transactions on Knowledge and Data Engineering 18(6):\\n857–861. 106, 519, 527\\nAnh, Vo Ngoc, and Alistair Moffat.\\n2006b.\\nPruned query evaluation us-\\ning pre-computed impacts.\\nIn Proc. SIGIR, pp. 372–379. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1148170.1148235. 149, 519, 527\\nAnh, Vo Ngoc, and Alistair Moffat. 2006c. Structured index organizations for high-\\nthroughput text querying. In Proc. SPIRE, pp. 304–315. Springer. 149, 519, 527\\nApté, Chidanand, Fred Damerau, and Sholom M. Weiss. 1994. Automated learning\\nof decision rules for text categorization. TOIS 12(1):233–251. 286, 519, 522, 532\\nArthur, David, and Sergei Vassilvitskii. 2006. How slow is the k-means method? In\\nProc. ACM Symposium on Computational Geometry, pp. 144–153. 373, 519, 532\\nArvola, Paavo, Marko Junkkari, and Jaana Kekäläinen. 2005. Generalized contextual-\\nization method for XML information retrieval. In Proc. CIKM, pp. 20–27. 216, 519,\\n525\\nAslam, Javed A., and Emine Yilmaz. 2005. A geometric interpretation and analysis\\nof R-precision. In Proc. CIKM, pp. 664–671. ACM Press. 174, 519, 533\\nAult, Thomas Galen, and Yiming Yang. 2002. Information ﬁltering in TREC-9 and\\nTDT-3: A comparative analysis. IR 5(2-3):159–187. 315, 519, 533\\nBadue, Claudine Santos, Ricardo A. Baeza-Yates, Berthier Ribeiro-Neto, and Nivio\\nZiviani. 2001. Distributed query processing using partitioned inverted ﬁles. In\\nProc. SPIRE, pp. 10–20. 459, 519, 529, 533\\nBaeza-Yates, Ricardo, Paolo Boldi, and Carlos Castillo. 2005. The choice of a damp-\\ning function for propagating importance in link-based ranking. Technical report,\\nDipartimento di Scienze dell’Informazione, Università degli Studi di Milano. 481,\\n519, 520, 521\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n486\\nBibliography\\nBaeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval.\\nAddison Wesley. xxxiv, 84, 105, 175, 400, 519, 529\\nBahle, Dirk, Hugh E. Williams, and Justin Zobel. 2002. Efﬁcient phrase querying with\\nan auxiliary index. In Proc. SIGIR, pp. 215–221. ACM Press. 47, 519, 533\\nBaldridge, Jason, and Miles Osborne. 2004.\\nActive learning and the total cost of\\nannotation. In Proc. Empirical Methods in Natural Language Processing, pp. 9–16. 348,\\n519, 528\\nBall, G. H. 1965. Data analysis in the social sciences: What about the details? In Proc.\\nFall Joint Computer Conference, pp. 533–560. Spartan Books. 373, 519\\nBanko, Michele, and Eric Brill. 2001. Scaling to very very large corpora for natural\\nlanguage disambiguation. In Proc. ACL. 337, 519, 520\\nBar-Ilan, Judit, and Tatyana Gutman. 2005. How do search engines respond to some\\nnon-English queries? Journal of Information Science 31(1):13–28. 46, 519, 523\\nBar-Yossef, Ziv,\\nand Maxim Gurevich.\\n2006.\\nRandom sampling from a\\nsearch engine’s index.\\nIn Proc. WWW, pp. 367–376. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1135777.1135833. 442, 519, 523\\nBarroso, Luiz André, Jeffrey Dean, and Urs Hölzle.\\n2003.\\nWeb search for\\na planet:\\nThe Google cluster architecture.\\nIEEE Micro 23(2):22–28.\\nDOI:\\ndx.doi.org/10.1109/MM.2003.1196112. 459, 519, 522, 524\\nBartell, Brian Theodore. 1994. Optimizing ranking functions: A connectionist approach to\\nadaptive information retrieval. PhD thesis, University of California at San Diego, La\\nJolla, CA. 150, 519\\nBartell, Brian T., Garrison W. Cottrell, and Richard K. Belew. 1998. Optimizing sim-\\nilarity using multi-query relevance feedback. JASIS 49(8):742–761. 150, 519, 520,\\n521\\nBarzilay, Regina, and Michael Elhadad. 1997. Using lexical chains for text summa-\\nrization. In Workshop on Intelligent Scalable Text Summarization, pp. 10–17. 174, 520,\\n522\\nBast, Holger, and Debapriyo Majumdar. 2005. Why spectral retrieval works. In Proc.\\nSIGIR, pp. 11–18. ACM Press. DOI: doi.acm.org/10.1145/1076034.1076040. 417, 520,\\n527\\nBasu, Sugato, Arindam Banerjee, and Raymond J. Mooney.\\n2004.\\nActive semi-\\nsupervision for pairwise constrained clustering. In Proc. SIAM International Con-\\nference on Data Mining, pp. 333–344. 373, 519, 520, 528\\nBeesley, Kenneth R. 1998. Language identiﬁer: A computer program for automatic\\nnatural-language identiﬁcation of on-line text. In Languages at Crossroads: Proc.\\nAnnual Conference of the American Translators Association, pp. 47–54. 46, 520\\nBeesley, Kenneth R., and Lauri Karttunen. 2003. Finite State Morphology. CSLI Publi-\\ncations. 46, 520, 525\\nBennett, Paul N. 2000. Assessing the calibration of naive Bayes’ posterior estimates.\\nTechnical Report CMU-CS-00-155, School of Computer Science, Carnegie Mellon\\nUniversity. 286, 520\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n487\\nBerger, Adam, and John Lafferty. 1999. Information retrieval as statistical translation.\\nIn Proc. SIGIR, pp. 222–229. ACM Press. 251, 252, 520, 526\\nBerkhin, Pavel. 2005. A survey on pagerank computing. Internet Mathematics 2(1):\\n73–120. 481, 520\\nBerkhin, Pavel. 2006a. Bookmark-coloring algorithm for personalized pagerank com-\\nputing. Internet Mathematics 3(1):41–62. 481, 520\\nBerkhin, Pavel. 2006b. A survey of clustering data mining techniques. In Jacob Kogan,\\nCharles Nicholas, and Marc Teboulle (eds.), Grouping Multidimensional Data: Recent\\nAdvances in Clustering, pp. 25–71. Springer. 372, 520\\nBerners-Lee, Tim, Robert Cailliau, Jean-Francois Groff, and Bernd Pollermann.\\n1992.\\nWorld-Wide Web: The information universe.\\nElectronic Networking: Re-\\nsearch, Applications and Policy 1(2):74–82.\\nURL:\\nciteseer.ist.psu.edu/article/berners-\\nlee92worldwide.html. 441, 520, 521, 523, 529\\nBerry, Michael, and Paul Young. 1995. Using latent semantic indexing for multilan-\\nguage information retrieval. Computers and the Humanities 29(6):413–429. 417, 520,\\n533\\nBerry, Michael W., Susan T. Dumais, and Gavin W. O’Brien. 1995. Using linear algebra\\nfor intelligent information retrieval. SIAM Review 37(4):573–595. 417, 520, 522, 528\\nBetsi, Stamatina, Mounia Lalmas, Anastasios Tombros, and Theodora Tsikrika. 2006.\\nUser expectations from XML element retrieval. In Proc. SIGIR, pp. 611–612. ACM\\nPress. 217, 520, 526, 531, 532\\nBharat, Krishna, and Andrei Broder. 1998. A technique for measuring the relative size\\nand overlap of public web search engines. Computer Networks and ISDN Systems 30\\n(1-7):379–388. DOI: dx.doi.org/10.1016/S0169-7552(98)00127-5. 442, 520\\nBharat, Krishna, Andrei Broder, Monika Henzinger, Puneet Kumar, and Suresh\\nVenkatasubramanian. 1998.\\nThe connectivity server: Fast access to linkage in-\\nformation on the web. In Proc. WWW, pp. 469–477. 459, 520, 524, 526, 532\\nBharat, Krishna, Andrei Z. Broder, Jeffrey Dean, and Monika Rauch Henzinger. 2000.\\nA comparison of techniques to ﬁnd mirrored hosts on the WWW. JASIS 51(12):\\n1114–1122. URL: citeseer.ist.psu.edu/bharat99comparison.html. 442, 520, 522, 524\\nBharat, Krishna, and Monika R. Henzinger. 1998. Improved algorithms for topic\\ndistillation in a hyperlinked environment. In Proc. SIGIR, pp. 104–111. ACM Press.\\nURL: citeseer.ist.psu.edu/bharat98improved.html. 481, 520, 524\\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer. 315,\\n520\\nBlair, David C., and M. E. Maron. 1985. An evaluation of retrieval effectiveness for a\\nfull-text document-retrieval system. CACM 28(3):289–299. 193, 520, 527\\nBlanco, Roi, and Alvaro Barreiro. 2006. TSP and cluster-based solutions to the reas-\\nsignment of document identiﬁers. IR 9(4):499–517. 106, 519, 520\\nBlanco, Roi, and Alvaro Barreiro. 2007. Boosting static pruning of inverted ﬁles. In\\nProc. SIGIR. ACM Press. 105, 519, 520\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n488\\nBibliography\\nBlandford, Dan, and Guy Blelloch. 2002. Index compression through document re-\\nordering. In Proc. Data Compression Conference, p. 342. IEEE Computer Society. 106,\\n520\\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation.\\nJMLR 3:993–1022. 418, 520, 525, 528\\nBoldi, Paolo, Bruno Codenotti, Massimo Santini, and Sebastiano Vigna. 2002. Ubi-\\ncrawler: A scalable fully distributed web crawler. In Proc. Australian World Wide\\nWeb Conference.\\nURL: citeseer.ist.psu.edu/article/boldi03ubicrawler.html. 458, 520, 521,\\n530, 532\\nBoldi, Paolo, Massimo Santini, and Sebastiano Vigna. 2005. PageRank as a function\\nof the damping factor. In Proc. WWW. URL: citeseer.ist.psu.edu/boldi05pagerank.html.\\n481, 520, 530, 532\\nBoldi, Paolo, and Sebastiano Vigna. 2004a. Codes for the World-Wide Web. Internet\\nMathematics 2(4):405–427. 459, 520, 532\\nBoldi, Paolo, and Sebastiano Vigna. 2004b. The WebGraph framework I: Compression\\ntechniques. In Proc. WWW, pp. 595–601. ACM Press. 459, 520, 532\\nBoldi, Paolo, and Sebastiano Vigna. 2005. Compressed perfect embedded skip lists\\nfor quick inverted-index lookups. In Proc. SPIRE. Springer. 46, 520, 532\\nBoley, Daniel. 1998. Principal direction divisive partitioning. Data Mining and Know-\\nledge Discovery 2(4):325–344. DOI: dx.doi.org/10.1023/A:1009740529316. 400, 520\\nBorodin, Allan, Gareth O. Roberts, Jeffrey S. Rosenthal, and Panayiotis Tsaparas.\\n2001. Finding authorities and hubs from link structures on the World Wide Web.\\nIn Proc. WWW, pp. 415–429. 481, 520, 529, 530, 532\\nBourne, Charles P., and Donald F. Ford.\\n1961.\\nA study of methods for sys-\\ntematically abbreviating English words and names.\\nJACM 8(4):538–552.\\nDOI:\\ndoi.acm.org/10.1145/321088.321094. 65, 520, 523\\nBradley, Paul S., and Usama M. Fayyad. 1998. Reﬁning initial points for K-means\\nclustering. In Proc. ICML, pp. 91–99. 373, 520, 522\\nBradley, Paul S., Usama M. Fayyad, and Cory Reina. 1998. Scaling clustering algo-\\nrithms to large databases. In Proc. KDD, pp. 9–15. 374, 520, 522, 529\\nBrill, Eric, and Robert C. Moore. 2000. An improved error model for noisy channel\\nspelling correction. In Proc. ACL, pp. 286–293. 65, 520, 528\\nBrin, Sergey, and Lawrence Page. 1998. The anatomy of a large-scale hypertextual\\nweb search engine. In Proc. WWW, pp. 107–117. 149, 458, 480, 520, 528\\nBrisaboa, Nieves R., Antonio Fariña, Gonzalo Navarro, and José R. Paramá. 2007.\\nLightweight natural language text compression. IR 10(1):1–33. 107, 520, 522, 528\\nBroder, Andrei. 2002. A taxonomy of web search. SIGIR Forum 36(2):3–10.\\nDOI:\\ndoi.acm.org/10.1145/792550.792552. 442, 520\\nBroder, Andrei, S. Ravi Kumar, Farzin Maghoul, Prabhakar Raghavan, Sridhar Ra-\\njagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph struc-\\nture in the web. Computer Networks 33(1):309–320.\\n441, 520, 526, 527, 529, 531,\\n532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n489\\nBroder, Andrei Z., Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. 1997.\\nSyntactic clustering of the web. In Proc. WWW, pp. 391–404. 442, 520, 523, 527, 533\\nBrown, Eric W. 1995. Execution Performance Issues in Full-Text Information Retrieval.\\nPhD thesis, University of Massachusetts, Amherst. 149, 520\\nBuckley, Chris, James Allan, and Gerard Salton. 1994a. Automatic routing and ad-hoc\\nretrieval using SMART: TREC 2. In Proc. TREC, pp. 45–55. 314, 519, 520, 530\\nBuckley, Chris, and Gerard Salton. 1995. Optimization of relevance feedback weights.\\nIn Proc. SIGIR, pp. 351–357. ACM Press.\\nDOI: doi.acm.org/10.1145/215206.215383.\\n315, 520, 530\\nBuckley, Chris, Gerard Salton, and James Allan. 1994b. The effect of adding relevance\\ninformation in a relevance feedback environment.\\nIn Proc. SIGIR, pp. 292–300.\\nACM Press. 185, 194, 314, 519, 520, 530\\nBuckley, Chris, Amit Singhal, and Mandar Mitra. 1995. New retrieval approaches\\nusing SMART: TREC 4. In Proc. TREC. 187, 520, 527, 531\\nBuckley, Chris, and Ellen M. Voorhees. 2000. Evaluating evaluation measure stability.\\nIn Proc. SIGIR, pp. 33–40. 173, 174, 520, 532\\nBurges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,\\nand Greg Hullender. 2005. Learning to rank using gradient descent. In Proc. ICML.\\n348, 520, 522, 523, 524, 526, 529, 530\\nBurges, Christopher J. C. 1998. A tutorial on support vector machines for pattern\\nrecognition. Data Mining and Knowledge Discovery 2(2):121–167. 346, 520\\nBurner, Mike. 1997. Crawling towards eternity: Building an archive of the World\\nWide Web. Web Techniques Magazine 2(5). 458, 520\\nBurnham, Kenneth P., and David Anderson. 2002. Model Selection and Multi-Model\\nInference. Springer. 373, 519, 521\\nBush, Vannevar.\\n1945.\\nAs we may think.\\nThe Atlantic Monthly.\\nURL:\\nwww.theatlantic.com/doc/194507/bush. 17, 441, 521\\nBüttcher, Stefan, and Charles L. A. Clarke. 2005a.\\nIndexing time vs. query time:\\nTrade-offs in dynamic information retrieval systems. In Proc. CIKM, pp. 317–318.\\nACM Press. DOI: doi.acm.org/10.1145/1099554.1099645. 84, 521\\nBüttcher, Stefan, and Charles L. A. Clarke.\\n2005b.\\nA security model for full-\\ntext ﬁle system search in multi-user environments.\\nIn Proc. FAST.\\nURL:\\nwww.usenix.org/events/fast05/tech/buettcher.html. 84, 521\\nBüttcher, Stefan, and Charles L. A. Clarke. 2006. A document-centric approach to\\nstatic index pruning in text retrieval systems. In Proc. CIKM, pp. 182–189.\\nDOI:\\ndoi.acm.org/10.1145/1183614.1183644. 105, 521\\nBüttcher, Stefan, Charles L. A. Clarke, and Brad Lushman. 2006. Hybrid index main-\\ntenance for growing text collections. In Proc. SIGIR, pp. 356–363. ACM Press. DOI:\\ndoi.acm.org/10.1145/1148170.1148233. 84, 521, 527\\nCacheda, Fidel, Victor Carneiro, Carmen Guerrero, and Ángel Viña. 2003. Optimiza-\\ntion of restricted searches in web directories using hybrid data structures. In Proc.\\nECIR, pp. 436–451. 372, 521, 523, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n490\\nBibliography\\nCallan, Jamie. 2000. Distributed information retrieval. In W. Bruce Croft (ed.), Ad-\\nvances in information retrieval, pp. 127–150. Kluwer. 84, 521\\nCan, Fazli, Ismail Sengör Altingövde, and Engin Demir. 2004. Efﬁciency and effec-\\ntiveness of query processing in cluster-based retrieval. Information Systems 29(8):\\n697–717. DOI: dx.doi.org/10.1016/S0306-4379(03)00062-0. 372, 519, 521, 522\\nCan, Fazli, and Esen A. Ozkarahan. 1990. Concepts and effectiveness of the cover-\\ncoefﬁcient-based clustering methodology for text databases. ACM Trans. Database\\nSyst. 15(4):483–517. 372, 521, 528\\nCao, Guihong, Jian-Yun Nie, and Jing Bai. 2005. Integrating word relationships into\\nlanguage models. In Proc. SIGIR, pp. 298–305. ACM Press. 252, 519, 521, 528\\nCao, Yunbo, Jun Xu, Tie-Yan Liu, Hang Li, Yalou Huang, and Hsiao-Wuen Hon. 2006.\\nAdapting Ranking SVM to document retrieval. In Proc. SIGIR. ACM Press. 348, 521,\\n524, 526, 533\\nCarbonell, Jaime, and Jade Goldstein. 1998. The use of MMR, diversity-based rerank-\\ning for reordering documents and producing summaries. In Proc. SIGIR, pp. 335–\\n336. ACM Press. DOI: doi.acm.org/10.1145/290941.291025. 167, 521, 523\\nCarletta, Jean. 1996. Assessing agreement on classiﬁcation tasks: The kappa statistic.\\nComputational Linguistics 22:249–254. 174, 521\\nCarmel, David, Doron Cohen, Ronald Fagin, Eitan Farchi, Michael Herscovici,\\nYoelle S. Maarek, and Aya Soffer.\\n2001.\\nStatic index pruning for infor-\\nmation retrieval systems.\\nIn Proc. SIGIR, pp. 43–50. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/383952.383958. 105, 149, 521, 522, 524, 527, 531\\nCarmel, David, Yoelle S. Maarek, Matan Mandelbrod, Yosi Mass, and Aya Soffer.\\n2003. Searching XML documents via XML fragments. In Proc. SIGIR, pp. 151–158.\\nACM Press. DOI: doi.acm.org/10.1145/860435.860464. 216, 521, 527, 531\\nCaruana, Rich, and Alexandru Niculescu-Mizil. 2006. An empirical comparison of\\nsupervised learning algorithms. In Proc. ICML. 347, 521, 528\\nCastro, R. M., M. J. Coates, and R. D. Nowak. 2004. Likelihood based hierarchical\\nclustering. IEEE Transactions in Signal Processing 52(8):2308–2321. 400, 521, 528\\nCavnar, William B., and John M. Trenkle. 1994. N-gram-based text categorization. In\\nProc. SDAIR, pp. 161–175. 46, 521, 532\\nChakrabarti, Soumen. 2002. Mining the Web: Analysis of Hypertext and Semi Structured\\nData. Morgan Kaufmann. 442, 521\\nChakrabarti, Soumen, Byron Dom, David Gibson, Jon Kleinberg, Prabhakar Ragha-\\nvan, and Sridhar Rajagopalan.\\n1998.\\nAutomatic resource list compilation by\\nanalyzing hyperlink structure and associated text.\\nIn Proc. WWW.\\nURL: cite-\\nseer.ist.psu.edu/chakrabarti98automatic.html. 480, 481, 521, 522, 523, 525, 529\\nChapelle, Olivier, Bernhard Schölkopf, and Alexander Zien (eds.).\\n2006.\\nSemi-\\nSupervised Learning. MIT Press. 347, 500, 507, 521, 533\\nChaudhuri,\\nSurajit,\\nGautam\\nDas,\\nVagelis Hristidis,\\nand\\nGerhard Weikum.\\n2006.\\nProbabilistic information retrieval approach for ranking of database\\nquery results.\\nACM Transactions on Database Systems 31(3):1134–1168.\\nDOI:\\ndoi.acm.org/10.1145/1166074.1166085. 217, 521, 522, 524, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n491\\nCheeseman, Peter, and John Stutz. 1996. Bayesian classiﬁcation (AutoClass): Theory\\nand results. In Advances in Knowledge Discovery and Data Mining, pp. 153–180. MIT\\nPress. 374, 521, 531\\nChen, Hsin-Hsi, and Chuan-Jie Lin. 2000. A multilingual news summarizer. In Proc.\\nCOLING, pp. 159–165. 373, 521, 526\\nChen, Pai-Hsuen, Chih-Jen Lin, and Bernhard Schölkopf.\\n2005.\\nA tutorial on ν-\\nsupport vector machines.\\nApplied Stochastic Models in Business and Industry 21:\\n111–136. 346, 521, 526, 530\\nChiaramella, Yves, Philippe Mulhem, and Franck Fourel. 1996. A model for multime-\\ndia information retrieval. Technical Report 4-96, University of Glasgow. 216, 521,\\n523, 528\\nChierichetti, Flavio, Alessandro Panconesi, Prabhakar Raghavan, Mauro Sozio,\\nAlessandro Tiberi, and Eli Upfal. 2007. Finding near neighbors through cluster\\npruning. In Proc. PODS. 149, 521, 528, 529, 531, 532\\nCho, Junghoo, and Hector Garcia-Molina. 2002. Parallel crawlers. In Proc. WWW, pp.\\n124–135. ACM Press. DOI: doi.acm.org/10.1145/511446.511464. 458, 521, 523\\nCho, Junghoo, Hector Garcia-Molina, and Lawrence Page. 1998. Efﬁcient crawling\\nthrough URL ordering. In Proc. WWW, pp. 161–172. 458, 521, 523, 528\\nChu-Carroll,\\nJennifer,\\nJohn\\nPrager,\\nKrzysztof\\nCzuba,\\nDavid\\nFerrucci,\\nand\\nPablo Duboue.\\n2006.\\nSemantic search via XML fragments:\\nA high-\\nprecision approach to IR.\\nIn Proc. SIGIR, pp. 445–452. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1148170.1148247. 216, 521, 522, 529\\nClarke, Charles L.A., Gordon V. Cormack, and Elizabeth A. Tudhope. 2000. Relevance\\nranking for one to three term queries. IP&M 36:291–311. 149, 521, 532\\nCleverdon, Cyril W. 1991. The signiﬁcance of the Cranﬁeld tests on index languages.\\nIn Proc. SIGIR, pp. 3–12. ACM Press. 17, 173, 521\\nCoden, Anni R., Eric W. Brown, and Savitha Srinivasan (eds.). 2002.\\nInformation\\nRetrieval Techniques for Speech Applications. Springer. xxxiv, 520, 521, 531\\nCohen, Paul R. 1995. Empirical methods for artiﬁcial intelligence. MIT Press. 286, 521\\nCohen, William W. 1998. Integration of heterogeneous databases without common\\ndomains using queries based on textual similarity. In Proc. SIGMOD, pp. 201–212.\\nACM Press. 217, 521\\nCohen, William W., Robert E. Schapire, and Yoram Singer.\\n1998.\\nLearn-\\ning\\nto\\norder\\nthings.\\nIn\\nProc.\\nNIPS.\\nThe\\nMIT\\nPress.\\nURL:\\ncite-\\nseer.ist.psu.edu/article/cohen98learning.html. 150, 521, 530, 531\\nCohen, William W., and Yoram Singer. 1999. Context-sensitive learning methods for\\ntext categorization. TOIS 17(2):141–173. 339, 521, 531\\nComtet, Louis. 1974. Advanced Combinatorics. Reidel. 356, 521\\nCooper, William S., Aitao Chen, and Fredric C. Gey. 1994. Full text retrieval based on\\nprobabilistic equations with coefﬁcients ﬁtted by logistic regression. In Proc. TREC,\\npp. 57–66. 150, 521, 523\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n492\\nBibliography\\nCormen, Thomas H., Charles Eric Leiserson, and Ronald L. Rivest. 1990. Introduction\\nto Algorithms. MIT Press. 11, 79, 399, 521, 526, 529\\nCover, Thomas M., and Peter E. Hart. 1967. Nearest neighbor pattern classiﬁcation.\\nIEEE Transactions on Information Theory 13(1):21–27. 315, 521, 524\\nCover, Thomas M., and Joy A. Thomas. 1991. Elements of Information Theory. Wiley.\\n106, 251, 521, 531\\nCrammer, Koby, and Yoram Singer. 2001.\\nOn the algorithmic implementation of\\nmulticlass kernel-based machines. JMLR 2:265–292. 347, 521, 531\\nCreecy, Robert H., Brij M. Masand, Stephen J. Smith, and David L. Waltz.\\n1992.\\nTrading MIPS and memory for knowledge engineering. CACM 35(8):48–64. DOI:\\ndoi.acm.org/10.1145/135226.135228. 314, 521, 527, 531, 532\\nCrestani, Fabio, Mounia Lalmas, Cornelis J. Van Rijsbergen, and Iain Campbell.\\n1998.\\nIs this document relevant? . . . probably: A survey of probabilistic mod-\\nels in information retrieval.\\nACM Computing Surveys 30(4):528–552.\\nDOI:\\ndoi.acm.org/10.1145/299917.299920. 235, 521, 526, 529\\nCristianini, Nello, and John Shawe-Taylor. 2000. Introduction to Support Vector Ma-\\nchines and Other Kernel-based Learning Methods. Cambridge University Press. 346,\\n521, 530\\nCroft, W. Bruce. 1978. A ﬁle organization for cluster-based retrieval. In Proc. SIGIR,\\npp. 65–82. ACM Press. 372, 521\\nCroft, W. Bruce, and David J. Harper. 1979. Using probabilistic models of document\\nretrieval without relevance information. Journal of Documentation 35(4):285–295.\\n133, 227, 521, 524\\nCroft, W. Bruce, and John Lafferty (eds.). 2003. Language Modeling for Information\\nRetrieval. Springer. 252, 522, 526\\nCrouch, Carolyn J. 1988. A cluster-based approach to thesaurus construction. In Proc.\\nSIGIR, pp. 309–320. ACM Press. DOI: doi.acm.org/10.1145/62437.62467. 374, 522\\nCucerzan, Silviu, and Eric Brill. 2004. Spelling correction as an iterative process that\\nexploits the collective knowledge of web users. In Proc. Empirical Methods in Natural\\nLanguage Processing. 65, 520, 522\\nCutting, Douglas R., David R. Karger, and Jan O. Pedersen.\\n1993.\\nConstant\\ninteraction-time Scatter/Gather browsing of very large document collections. In\\nProc. SIGIR, pp. 126–134. ACM Press. 399, 522, 525, 528\\nCutting, Douglas R., Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scat-\\nter/Gather: A cluster-based approach to browsing large document collections. In\\nProc. SIGIR, pp. 318–329. ACM Press. 372, 399, 522, 525, 528, 532\\nDamerau, Fred J. 1964. A technique for computer detection and correction of spelling\\nerrors. CACM 7(3):171–176. DOI: doi.acm.org/10.1145/363958.363994. 65, 522\\nDavidson, Ian, and Ashwin Satyanarayana. 2003. Speeding up k-means clustering\\nby bootstrap averaging. In ICDM 2003 Workshop on Clustering Large Data Sets. 373,\\n522, 530\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n493\\nDay, William H., and Herbert Edelsbrunner. 1984. Efﬁcient algorithms for agglomer-\\native hierarchical clustering methods. Journal of Classiﬁcation 1:1–24. 399, 522\\nde Moura, Edleno Silva, Gonzalo Navarro, Nivio Ziviani, and Ricardo Baeza-Yates.\\n2000. Fast and ﬂexible word searching on compressed text. TOIS 18(2):113–139.\\nDOI: doi.acm.org/10.1145/348751.348754. 107, 519, 528, 533\\nDean, Jeffrey, and Sanjay Ghemawat. 2004. MapReduce: Simpliﬁed data processing\\non large clusters. In Proc. Symposium on Operating System Design and Implementation.\\nxx, 76, 83, 522, 523\\nDeerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and\\nRichard Harshman. 1990. Indexing by latent semantic analysis. JASIS 41(6):391–\\n407. 417, 522, 523, 524, 526\\ndel Bimbo, Alberto. 1999. Visual Information Retrieval. Morgan Kaufmann. xxxiv, 533\\nDempster, A.P., N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incom-\\nplete data via the EM algorithm. Journal of the Royal Statistical Society Series B 39:\\n1–38. 373, 522, 526, 530\\nDhillon, Inderjit S. 2001. Co-clustering documents and words using bipartite spectral\\ngraph partitioning. In Proc. KDD, pp. 269–274. 374, 400, 522\\nDhillon, Inderjit S., and Dharmendra S. Modha. 2001. Concept decompositions for\\nlarge sparse text data using clustering. Machine Learning 42(1/2):143–175.\\nDOI:\\ndx.doi.org/10.1023/A:1007612920971. 373, 522, 527\\nDi Eugenio, Barbara, and Michael Glass. 2004. The kappa statistic: A second look.\\nComputational Linguistics 30(1):95–101. DOI: dx.doi.org/10.1162/089120104773633402.\\n174, 522, 523\\nDietterich, Thomas G. 2002. Ensemble learning. In Michael A. Arbib (ed.), The Hand-\\nbook of Brain Theory and Neural Networks, 2nd edition. MIT Press. 347, 522\\nDietterich, Thomas G., and Ghulum Bakiri. 1995. Solving multiclass learning prob-\\nlems via error-correcting output codes. Journal of Artiﬁcial Intelligence Research 2:\\n263–286. 315, 519, 522\\nDom, Byron E. 2002. An information-theoretic external cluster-validity measure. In\\nProc. UAI. 373, 522\\nDomingos, Pedro. 2000. A uniﬁed bias-variance decomposition for zero-one and\\nsquared loss. In Proc. National Conference on Artiﬁcial Intelligence and Proc. Conference\\nInnovative Applications of Artiﬁcial Intelligence, pp. 564–569. AAAI Press / The MIT\\nPress. 315, 522\\nDomingos, Pedro, and Michael J. Pazzani. 1997. On the optimality of the simple\\nBayesian classiﬁer under zero-one loss. Machine Learning 29(2-3):103–130.\\nURL:\\nciteseer.ist.psu.edu/domingos97optimality.html. 286, 522, 528\\nDownie, J. Stephen. 2006. The Music Information Retrieval Evaluation eXchange\\n(MIREX). D-Lib Magazine 12(12). xxxiv, 522\\nDuda, Richard O., Peter E. Hart, and David G. Stork. 2000. Pattern Classiﬁcation, 2nd\\nedition. Wiley-Interscience. 286, 372, 522, 524, 531\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n494\\nBibliography\\nDumais, Susan, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive\\nlearning algorithms and representations for text categorization. In Proc. CIKM, pp.\\n148–155. ACM Press.\\nDOI: doi.acm.org/10.1145/288627.288651. xvii, 282, 333, 334,\\n347, 522, 524, 529, 530\\nDumais, Susan T. 1993. Latent semantic indexing (LSI) and TREC-2. In Proc. TREC,\\npp. 105–115. 415, 417, 522\\nDumais, Susan T. 1995. Latent semantic indexing (LSI): TREC-3 report. In Proc. TREC,\\npp. 219–230. 416, 417, 522\\nDumais, Susan T., and Hao Chen. 2000. Hierarchical classiﬁcation of Web content. In\\nProc. SIGIR, pp. 256–263. ACM Press. 347, 521, 522\\nDunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence.\\nComputational Linguistics 19(1):61–74. 286, 522\\nDunning, Ted. 1994. Statistical identiﬁcation of language. Technical Report 94-273,\\nComputing Research Laboratory, New Mexico State University. 46, 522\\nEckart, Carl, and Gale Young. 1936. The approximation of a matrix by another of\\nlower rank. Psychometrika 1:211–218. 417, 522, 533\\nEl-Hamdouchi, Abdelmoula, and Peter Willett. 1986. Hierarchic document classiﬁca-\\ntion using Ward’s clustering method. In Proc. SIGIR, pp. 149–156. ACM Press. DOI:\\ndoi.acm.org/10.1145/253168.253200. 399, 522, 532\\nElias, Peter. 1975. Universal code word sets and representations of the integers. IEEE\\nTransactions on Information Theory 21(2):194–203. 106, 522\\nEyheramendy, Susana, David Lewis, and David Madigan. 2003. On the Naive Bayes\\nmodel for text categorization. In International Workshop on Artiﬁcial Intelligence and\\nStatistics. Society for Artiﬁcial Intelligence and Statistics. 286, 522, 526, 527\\nFallows,\\nDeborah,\\n2004.\\nThe\\ninternet\\nand\\ndaily\\nlife.\\nURL:\\nwww.pewinternet.org/pdfs/PIP_Internet_and_Daily_Life.pdf. Pew/Internet and American\\nLife Project. xxxi, 522\\nFayyad, Usama M., Cory Reina, and Paul S. Bradley. 1998. Initialization of iterative\\nreﬁnement clustering algorithms. In Proc. KDD, pp. 194–198. 374, 520, 522, 529\\nFellbaum, Christiane D. 1998. WordNet – An Electronic Lexical Database. MIT Press.\\n194, 522\\nFerragina, Paolo, and Rossano Venturini. 2007. Compressed permuterm indexes. In\\nProc. SIGIR. ACM Press. 65, 522, 532\\nForman, George. 2004. A pitfall and solution in multi-class feature selection for text\\nclassiﬁcation. In Proc. ICML. 286, 523\\nForman, George. 2006. Tackling concept drift by temporal inductive transfer. In Proc.\\nSIGIR, pp. 252–259. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148216. 286, 523\\nForman, George, and Ira Cohen. 2004. Learning from little: Comparison of classiﬁers\\ngiven little training. In Proc. PKDD, pp. 161–172. 336, 521, 523\\nFowlkes, Edward B., and Colin L. Mallows. 1983. A method for comparing two\\nhierarchical clusterings. Journal of the American Statistical Association 78(383):553–\\n569. URL: www.jstor.org/view/01621459/di985957/98p0926l/0. 400, 523, 527\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n495\\nFox, Edward A., and Whay C. Lee. 1991. FAST-INV: A fast algorithm for building\\nlarge inverted ﬁles. Technical report, Virginia Polytechnic Institute & State Univer-\\nsity, Blacksburg, VA, USA. 83, 523, 526\\nFraenkel, Aviezri S., and Shmuel T. Klein. 1985. Novel compression of sparse bit-\\nstrings – preliminary report. In Combinatorial Algorithms on Words, NATO ASI Series\\nVol F12, pp. 169–183. Springer. 106, 523, 525\\nFrakes, William B., and Ricardo Baeza-Yates (eds.). 1992. Information Retrieval: Data\\nStructures and Algorithms. Prentice Hall. 497, 509, 519, 523\\nFraley, Chris, and Adrian E. Raftery. 1998. How many clusters? Which clustering\\nmethod? Answers via model-based cluster analysis. Computer Journal 41(8):578–\\n588. 373, 523, 529\\nFriedl, Jeffrey E. F. 2006. Mastering Regular Expressions, 3rd edition. O’Reilly. 18, 523\\nFriedman, Jerome H.\\n1997.\\nOn bias, variance, 0/1–loss, and the curse-of-\\ndimensionality. Data Mining and Knowledge Discovery 1(1):55–77. 286, 315, 523\\nFriedman, Nir, and Moises Goldszmidt. 1996. Building classiﬁers using Bayesian\\nnetworks. In Proc. National Conference on Artiﬁcial Intelligence, pp. 1277–1284. 231,\\n523\\nFuhr, Norbert. 1989. Optimum polynomial retrieval functions based on the probabil-\\nity ranking principle. TOIS 7(3):183–204. 150, 523\\nFuhr, Norbert. 1992. Probabilistic models in information retrieval. Computer Journal\\n35(3):243–255. 235, 348, 523\\nFuhr, Norbert, Norbert Gövert, Gabriella Kazai, and Mounia Lalmas (eds.). 2003a.\\nINitiative for the Evaluation of XML Retrieval (INEX). Proc. First INEX Workshop.\\nERCIM. 216, 523, 525, 526\\nFuhr, Norbert, and Kai Großjohann.\\n2004.\\nXIRQL: An XML query lan-\\nguage based on information retrieval concepts.\\nTOIS 22(2):313–356.\\nURL:\\ndoi.acm.org/10.1145/984321.984326. 216, 523\\nFuhr, Norbert, and Mounia Lalmas. 2007. Advances in XML retrieval: The INEX\\ninitiative. In International Workshop on Research Issues in Digital Libraries. 216, 523,\\n526\\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Gabriella Kazai (eds.). 2006. Ad-\\nvances in XML Information Retrieval and Evaluation, 4th International Workshop of the\\nInitiative for the Evaluation of XML Retrieval, INEX 2005. Springer. 216, 523, 525, 526,\\n527\\nFuhr, Norbert, Mounia Lalmas, Saadia Malik, and Zoltán Szlávik (eds.). 2005. Ad-\\nvances in XML Information Retrieval, Third International Workshop of the Initiative for\\nthe Evaluation of XML Retrieval, INEX 2004. Springer. 216, 507, 515, 523, 526, 527,\\n531\\nFuhr, Norbert, Mounia Lalmas, and Andrew Trotman (eds.). 2007. Comparative Evalu-\\nation of XML Information Retrieval Systems, 5th International Workshop of the Initiative\\nfor the Evaluation of XML Retrieval, INEX 2006. Springer. 216, 502, 504, 523, 526, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n496\\nBibliography\\nFuhr, Norbert, Saadia Malik, and Mounia Lalmas (eds.). 2003b. INEX 2003 Workshop.\\nURL: inex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. 216, 496, 505, 523, 526, 527\\nFuhr, Norbert, and Ulrich Pfeifer. 1994. Probabilistic information retrieval as a com-\\nbination of abstraction, inductive learning, and probabilistic assumptions. TOIS 12\\n(1):92–115. DOI: doi.acm.org/10.1145/174608.174612. 150, 523, 529\\nFuhr, Norbert, and Thomas Rölleke. 1997. A probabilistic relational algebra for the\\nintegration of information retrieval and database systems. TOIS 15(1):32–66. DOI:\\ndoi.acm.org/10.1145/239041.239045. 217, 523, 530\\nGaertner, Thomas, John W. Lloyd, and Peter A. Flach. 2002. Kernels for structured\\ndata. In Proc. International Conference on Inductive Logic Programming, pp. 66–83. 347,\\n522, 523, 527\\nGao, Jianfeng, Mu Li, Chang-Ning Huang, and Andi Wu. 2005. Chinese word seg-\\nmentation and named entity recognition: A pragmatic approach. Computational\\nLinguistics 31(4):531–574. 46, 523, 524, 526, 533\\nGao, Jianfeng, Jian-Yun Nie, Guangyuan Wu, and Guihong Cao. 2004. Dependence\\nlanguage model for information retrieval. In Proc. SIGIR, pp. 170–177. ACM Press.\\n252, 521, 523, 528, 533\\nGarcia, Steven, Hugh E. Williams, and Adam Cannane. 2004. Access-ordered indexes.\\nIn Proc. Australasian Conference on Computer Science, pp. 7–14. 149, 521, 523, 533\\nGarcia-Molina, Hector, Jennifer Widom, and Jeffrey D. Ullman. 1999. Database System\\nImplementation. Prentice Hall. 84, 523, 532\\nGarﬁeld, Eugene. 1955. Citation indexes to science: A new dimension in documenta-\\ntion through association of ideas. Science 122:108–111. 480, 523\\nGarﬁeld, Eugene. 1976. The permuterm subject index: An autobiographic review.\\nJASIS 27(5-6):288–291. 65, 523\\nGeman, Stuart, Elie Bienenstock, and René Doursat. 1992. Neural networks and the\\nbias/variance dilemma. Neural Computation 4(1):1–58. 315, 520, 522, 523\\nGeng, Xiubo, Tie-Yan Liu, Tao Qin, and Hang Li. 2007. Feature selection for ranking.\\nIn Proc. SIGIR, pp. 407–414. ACM Press. 348, 523, 526, 529\\nGerrand, Peter. 2007. Estimating linguistic diversity on the internet: A taxonomy\\nto avoid pitfalls and paradoxes. Journal of Computer-Mediated Communication 12(4).\\nURL: jcmc.indiana.edu/vol12/issue4/gerrand.html. article 8. 30, 523\\nGey, Fredric C. 1994. Inferring probability of relevance using the method of logistic\\nregression. In Proc. SIGIR, pp. 222–231. ACM Press. 348, 523\\nGhamrawi, Nadia, and Andrew McCallum. 2005. Collective multi-label classiﬁcation.\\nIn Proc. CIKM, pp. 195–200. ACM Press. DOI: doi.acm.org/10.1145/1099554.1099591.\\n315, 523, 527\\nGlover, Eric, David M. Pennock, Steve Lawrence, and Robert Krovetz. 2002a. In-\\nferring hierarchical descriptions. In Proc. CIKM, pp. 507–514. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/584792.584876. 400, 523, 526, 529\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n497\\nGlover, Eric J.,\\nKostas Tsioutsiouliklis, Steve Lawrence,\\nDavid M. Pennock,\\nand Gary W. Flake.\\n2002b.\\nUsing web structure for classifying and de-\\nscribing web pages.\\nIn Proc. WWW, pp. 562–569. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/511446.511520. 400, 522, 523, 526, 529, 532\\nGövert, Norbert, and Gabriella Kazai.\\n2003.\\nOverview of the INitiative for the\\nEvaluation of XML retrieval (INEX) 2002. In Fuhr et al. (2003b), pp. 1–17.\\nURL:\\ninex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. 216, 523, 525\\nGrabs, Torsten, and Hans-Jörg Schek. 2002. Generating vector spaces on-the-ﬂy for\\nﬂexible XML retrieval. In XML and Information Retrieval Workshop at SIGIR 2002.\\n216, 523, 530\\nGreiff, Warren R. 1998. A theory of term weighting based on exploratory data analy-\\nsis. In Proc. SIGIR, pp. 11–19. ACM Press. 227, 523\\nGrinstead,\\nCharles\\nM.,\\nand\\nJ.\\nLaurie\\nSnell.\\n1997.\\nIntroduction\\nto\\nProbability,\\n2nd\\nedition.\\nAmerican\\nMathematical\\nSociety.\\nURL:\\nwww.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf.\\n235, 523, 531\\nGrossman, David A., and Ophir Frieder. 2004. Information Retrieval: Algorithms and\\nHeuristics, 2nd edition. Springer. xxxiv, 84, 217, 523\\nGusﬁeld, Dan. 1997. Algorithms on Strings, Trees and Sequences: Computer Science and\\nComputational Biology. Cambridge University Press. 65, 523\\nHamerly, Greg, and Charles Elkan. 2003. Learning the k in k-means. In Proc. NIPS.\\nURL: books.nips.cc/papers/ﬁles/nips16/NIPS2003_AA36.pdf. 373, 522, 523\\nHan, Eui-Hong, and George Karypis. 2000. Centroid-based document classiﬁcation:\\nAnalysis and experimental results. In Proc. PKDD, pp. 424–431. 314, 524, 525\\nHand, David J. 2006. Classiﬁer technology and the illusion of progress. Statistical\\nScience 21:1–14. 286, 524\\nHand, David J., and Keming Yu. 2001. Idiot’s Bayes: Not so stupid after all. Interna-\\ntional Statistical Review 69(3):385–398. 286, 524, 533\\nHarman, Donna. 1991. How effective is sufﬁxing? JASIS 42:7–15. 46, 524\\nHarman, Donna. 1992. Relevance feedback revisited. In Proc. SIGIR, pp. 1–10. ACM\\nPress. 185, 194, 524\\nHarman, Donna, Ricardo Baeza-Yates, Edward Fox, and W. Lee. 1992. Inverted ﬁles.\\nIn Frakes and Baeza-Yates (1992), pp. 28–43. 83, 519, 523, 524, 526\\nHarman, Donna, and Gerald Candela. 1990. Retrieving records from a gigabyte of\\ntext on a minicomputer using statistical ranking. JASIS 41(8):581–589. 83, 521, 524\\nHarold, Elliotte Rusty, and Scott W. Means. 2004. XML in a Nutshell, 3rd edition.\\nO’Reilly. 216, 524, 527\\nHarter, Stephen P. 1998. Variations in relevance assessments and the measurement of\\nretrieval effectiveness. JASIS 47:37–49. 174, 524\\nHartigan, J. A., and M. A. Wong. 1979. A K-means clustering algorithm. Applied\\nStatistics 28:100–108. 373, 524, 533\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n498\\nBibliography\\nHastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2001. The Elements of\\nStatistical Learning: Data Mining, Inference, and Prediction. Springer. 286, 314, 315,\\n347, 523, 524, 531\\nHatzivassiloglou,\\nVasileios,\\nLuis Gravano,\\nand Ankineedu Maganti.\\n2000.\\nAn investigation of linguistic features and clustering algorithms for topi-\\ncal document clustering.\\nIn Proc. SIGIR, pp. 224–231. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/345508.345582. 373, 523, 524, 527\\nHaveliwala, Taher. 2003. Topic-sensitive PageRank: A context-sensitive ranking al-\\ngorithm for web search. IEEE Transactions on Knowledge and Data Engineering 15(4):\\n784–796. URL: citeseer.ist.psu.edu/article/haveliwala03topicsensitive.html. 481, 524\\nHaveliwala, Taher H. 2002. Topic-sensitive PageRank. In Proc. WWW.\\nURL: cite-\\nseer.ist.psu.edu/haveliwala02topicsensitive.html. 481, 524\\nHayes, Philip J., and Steven P. Weinstein.\\n1990.\\nCONSTRUE/TIS: A system for\\ncontent-based indexing of a database of news stories. In Proc. Conference on In-\\nnovative Applications of Artiﬁcial Intelligence, pp. 49–66. 335, 524, 532\\nHeaps, Harold S. 1978. Information Retrieval: Computational and Theoretical Aspects.\\nAcademic Press. 105, 524\\nHearst, Marti A. 1997. TextTiling: Segmenting text into multi-paragraph subtopic\\npassages. Computational Linguistics 23(1):33–64. 217, 524\\nHearst, Marti A. 2006. Clustering versus faceted categories for information explo-\\nration. CACM 49(4):59–61. DOI: doi.acm.org/10.1145/1121949.1121983. 372, 524\\nHearst, Marti A., and Jan O. Pedersen. 1996. Reexamining the cluster hypothesis. In\\nProc. SIGIR, pp. 76–84. ACM Press. 372, 524, 528\\nHearst, Marti A., and Christian Plaunt.\\n1993.\\nSubtopic structuring for full-\\nlength document access.\\nIn Proc. SIGIR, pp. 59–68. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/160688.160695. 217, 524, 529\\nHeinz, Steffen, and Justin Zobel. 2003. Efﬁcient single-pass index construction for\\ntext databases. JASIST 54(8):713–729. DOI: dx.doi.org/10.1002/asi.10268. 83, 524, 533\\nHeinz, Steffen, Justin Zobel, and Hugh E. Williams.\\n2002.\\nBurst tries:\\nA\\nfast, efﬁcient data structure for string keys.\\nTOIS 20(2):192–223.\\nDOI:\\ndoi.acm.org/10.1145/506309.506312. 84, 524, 533\\nHenzinger, Monika R., Allan Heydon, Michael Mitzenmacher, and Marc Najork.\\n2000. On near-uniform URL sampling. In Proc. WWW, pp. 295–308. North-Holland.\\nDOI: dx.doi.org/10.1016/S1389-1286(00)00055-4. 442, 524, 527, 528\\nHerbrich, Ralf, Thore Graepel, and Klaus Obermayer.\\n2000.\\nLarge margin rank\\nboundaries for ordinal regression. In Advances in Large Margin Classiﬁers, pp. 115–\\n132. MIT Press. 348, 523, 524, 528\\nHersh, William, Chris Buckley, T. J. Leone, and David Hickam. 1994. OHSUMED: An\\ninteractive retrieval evaluation and new large test collection for research. In Proc.\\nSIGIR, pp. 192–201. ACM Press. 174, 520, 524, 526\\nHersh, William R., Andrew Turpin, Susan Price, Benjamin Chan, Dale Kraemer,\\nLynetta Sacherek, and Daniel Olson. 2000a. Do batch and user evaluation give\\nthe same results? In Proc. SIGIR, pp. 17–24. 175, 521, 524, 526, 528, 529, 530, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n499\\nHersh, William R., Andrew Turpin, Susan Price, Dale Kraemer, Daniel Olson, Ben-\\njamin Chan, and Lynetta Sacherek. 2001. Challenging conventional assumptions\\nof automated information retrieval with real users: Boolean searching and batch\\nretrieval evaluations. IP&M 37(3):383–402. 175, 521, 524, 526, 528, 529, 530, 532\\nHersh, William R., Andrew Turpin, Lynetta Sacherek, Daniel Olson, Susan Price, Ben-\\njamin Chan, and Dale Kraemer. 2000b. Further analysis of whether batch and user\\nevaluations give the same results with a question-answering task. In Proc. TREC.\\n175, 521, 524, 526, 528, 529, 530, 532\\nHiemstra, Djoerd. 1998. A linguistically motivated probabilistic model of information\\nretrieval. In Proc. ECDL, volume 1513 of LNCS, pp. 569–584. 252, 524\\nHiemstra, Djoerd. 2000. A probabilistic justiﬁcation for using tf.idf term weighting in\\ninformation retrieval. International Journal on Digital Libraries 3(2):131–139. 246, 524\\nHiemstra, Djoerd, and Wessel Kraaij. 2005. A language-modeling approach to TREC.\\nIn Voorhees and Harman (2005), pp. 373–395. 252, 524, 526\\nHirai, Jun, Sriram Raghavan, Hector Garcia-Molina, and Andreas Paepcke.\\n2000.\\nWebBase: A repository of web pages. In Proc. WWW, pp. 277–293. 458, 523, 524,\\n528, 529\\nHofmann, Thomas. 1999a. Probabilistic Latent Semantic Indexing. In Proc. UAI. URL:\\nciteseer.ist.psu.edu/hofmann99probabilistic.html. 417, 524\\nHofmann, Thomas. 1999b. Probabilistic Latent Semantic Indexing. In Proc. SIGIR, pp.\\n50–57. ACM Press.\\nURL: citeseer.ist.psu.edu/article/hofmann99probabilistic.html.\\n417,\\n524\\nHollink, Vera, Jaap Kamps, Christof Monz, and Maarten de Rijke. 2004. Monolingual\\ndocument retrieval for European languages. IR 7(1):33–52. 46, 524, 525, 528, 529\\nHopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman. 2000. Introduction to Au-\\ntomata Theory, Languages, and Computation, 2nd edition. Addison Wesley. 18, 524,\\n528, 532\\nHuang,\\nYifen,\\nand\\nTom M. Mitchell.\\n2006.\\nText clustering with ex-\\ntended user feedback.\\nIn Proc. SIGIR, pp. 413–420. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1148170.1148242. 374, 524, 527\\nHubert, Lawrence, and Phipps Arabie. 1985. Comparing partitions. Journal of Classi-\\nﬁcation 2:193–218. 373, 519, 524\\nHughes, Baden, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew\\nMacKinlay. 2006. Reconsidering language identiﬁcation for written language re-\\nsources. In Proc. International Conference on Language Resources and Evaluation, pp.\\n485–488. 46, 519, 520, 524, 527, 528\\nHull, David. 1993. Using statistical testing in the evaluation of retrieval performance.\\nIn Proc. SIGIR, pp. 329–338. ACM Press. 173, 524\\nHull, David. 1996. Stemming algorithms – A case study for detailed evaluation. JASIS\\n47(1):70–84. 46, 524\\nIde, E. 1971. New experiments in relevance feedback. In Salton (1971b), pp. 337–354.\\n193, 524\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n500\\nBibliography\\nIndyk, Piotr. 2004. Nearest neighbors in high-dimensional spaces. In J. E. Good-\\nman and J. O’Rourke (eds.), Handbook of Discrete and Computational Geometry, 2nd\\nedition, pp. 877–892. Chapman and Hall/CRC Press. 314, 524\\nIngwersen, Peter, and Kalervo Järvelin. 2005.\\nThe Turn: Integration of Information\\nSeeking and Retrieval in Context. Springer. xxxiv, 524\\nIttner, David J., David D. Lewis, and David D. Ahn. 1995. Text categorization of low\\nquality images. In Proc. SDAIR, pp. 301–315. 314, 519, 524, 526\\nIwayama, Makoto, and Takenobu Tokunaga. 1995. Cluster-based text categorization:\\nA comparison of category search strategies.\\nIn Proc. SIGIR, pp. 273–280. ACM\\nPress. 314, 524, 531\\nJackson, Peter, and Isabelle Moulinier. 2002. Natural Language Processing for Online\\nApplications: Text Retrieval, Extraction and Categorization. John Benjamins. 334, 335,\\n524, 528\\nJacobs, Paul S., and Lisa F. Rau. 1990. SCISOR: Extracting information from on-line\\nnews. CACM 33:88–97. 335, 524, 529\\nJain, Anil, M. Narasimha Murty, and Patrick Flynn. 1999. Data clustering: A review.\\nACM Computing Surveys 31(3):264–323. 399, 523, 524, 528\\nJain, Anil K., and Richard C. Dubes. 1988. Algorithms for Clustering Data. Prentice\\nHall. 399, 522, 524\\nJardine, N., and Cornelis Joost van Rijsbergen. 1971. The use of hierarchic clustering\\nin information retrieval. Information Storage and Retrieval 7:217–240. 372, 525, 529\\nJärvelin, Kalervo, and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of\\nIR techniques. TOIS 20(4):422–446. 174, 525\\nJeh, Glen, and Jennifer Widom. 2003.\\nScaling personalized web search. In Proc.\\nWWW, pp. 271–279. ACM Press. 481, 525, 532\\nJensen, Finn V., and Finn B. Jensen. 2001. Bayesian Networks and Decision Graphs.\\nSpringer. 234, 525\\nJeong, Byeong-Soo, and Edward Omiecinski. 1995. Inverted ﬁle partitioning schemes\\nin multiple disk systems. IEEE Transactions on Parallel and Distributed Systems 6(2):\\n142–153. 458, 525, 528\\nJi, Xiang, and Wei Xu. 2006. Document clustering with prior knowledge. In Proc.\\nSIGIR, pp. 405–412. ACM Press.\\nDOI: doi.acm.org/10.1145/1148170.1148241.\\n374,\\n525, 533\\nJing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proc.\\nConference on Applied Natural Language Processing, pp. 310–315. 174, 525\\nJoachims, Thorsten. 1997. A probabilistic analysis of the Rocchio algorithm with tﬁdf\\nfor text categorization. In Proc. ICML, pp. 143–151. Morgan Kaufmann. 314, 525\\nJoachims, Thorsten. 1998. Text categorization with support vector machines: Learn-\\ning with many relevant features. In Proc. ECML, pp. 137–142. Springer. xvii, 282,\\n333, 334, 525\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n501\\nJoachims, Thorsten. 1999. Making large-scale SVM learning practical. In B. Schölkopf,\\nC. Burges, and A. Smola (eds.), Advances in Kernel Methods - Support Vector Learning.\\nMIT Press. 347, 525\\nJoachims, Thorsten. 2002a. Learning to Classify Text Using Support Vector Machines.\\nKluwer. xvii, 334, 347, 525\\nJoachims, Thorsten. 2002b. Optimizing search engines using clickthrough data. In\\nProc. KDD, pp. 133–142. 175, 185, 348, 525\\nJoachims, Thorsten. 2006a. Training linear SVMs in linear time. In Proc. KDD, pp.\\n217–226. ACM Press. DOI: doi.acm.org/10.1145/1150402.1150429. 286, 329, 347, 525\\nJoachims, Thorsten. 2006b. Transductive support vector machines. In Chapelle et al.\\n(2006), pp. 105–118. 347, 525\\nJoachims, Thorsten, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2005.\\nAccurately interpreting clickthrough data as implicit feedback. In Proc. SIGIR, pp.\\n154–161. ACM Press. 175, 185, 523, 524, 525, 528\\nJohnson, David, Vishv Malhotra, and Peter Vamplew. 2006. More effective web search\\nusing bigrams and trigrams. Webology 3(4). URL: www.webology.ir/2006/v3n4/a35.html.\\nArticle 35. 47, 525, 527, 532\\nJurafsky, Dan, and James H. Martin. 2008. Speech and Language Processing: An Introduc-\\ntion to Natural Language Processing, Computational Linguistics and Speech Recognition,\\n2nd edition. Prentice Hall. xxxiv, 252, 525, 527\\nKäki, Mika.\\n2005.\\nFindex:\\nSearch result categories help users when doc-\\nument ranking fails.\\nIn Proc. SIGCHI, pp. 131–140. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1054972.1054991. 372, 400, 526\\nKammenhuber, Nils, Julia Luxenburger, Anja Feldmann, and Gerhard Weikum. 2006.\\nWeb search clickstreams. In Proc. ACM SIGCOMM on Internet Measurement, pp.\\n245–250. ACM Press. 47, 522, 525, 527, 532\\nKamps, Jaap, Maarten de Rijke, and Börkur Sigurbjörnsson.\\n2004.\\nLength nor-\\nmalization in XML retrieval.\\nIn Proc. SIGIR, pp. 80–87. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1008992.1009009. 216, 525, 529, 530\\nKamps, Jaap, Maarten Marx, Maarten de Rijke, and Börkur Sigurbjörnsson. 2006.\\nArticulating information needs in XML query languages. TOIS 24(4):407–436. DOI:\\ndoi.acm.org/10.1145/1185877.1185879. 216, 525, 527, 529, 530\\nKamvar, Sepandar D., Dan Klein, and Christopher D. Manning. 2002. Interpreting\\nand extending classical agglomerative clustering algorithms using a model-based\\napproach. In Proc. ICML, pp. 283–290. Morgan Kaufmann. 400, 525, 527\\nKannan, Ravi, Santosh Vempala, and Adrian Vetta. 2000. On clusterings – Good, bad\\nand spectral. In Proc. Symposium on Foundations of Computer Science, pp. 367–377.\\nIEEE Computer Society. 400, 525, 532\\nKaszkiel, Marcin, and Justin Zobel. 1997. Passage retrieval revisited. In Proc. SIGIR,\\npp. 178–185. ACM Press. DOI: doi.acm.org/10.1145/258525.258561. 217, 525, 533\\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding groups in data. Wiley. 373,\\n525, 530\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n502\\nBibliography\\nKazai, Gabriella, and Mounia Lalmas. 2006.\\neXtended cumulated gain measures\\nfor the evaluation of content-oriented XML retrieval. TOIS 24(4):503–542.\\nDOI:\\ndoi.acm.org/10.1145/1185883. 217, 525, 526\\nKekäläinen, Jaana. 2005. Binary and graded relevance in IR evaluations – Comparison\\nof the effects on ranking of IR systems. IP&M 41:1019–1033. 174, 525\\nKekäläinen, Jaana, and Kalervo Järvelin. 2002. Using graded relevance assessments\\nin IR evaluation. JASIST 53(13):1120–1129. 174, 525\\nKemeny, John G., and J. Laurie Snell. 1976. Finite Markov Chains. Springer. 480, 525,\\n531\\nKent, Allen, Madeline M. Berry, Fred U. Luehrs, Jr., and J. W. Perry. 1955. Machine\\nliterature searching VIII. Operational criteria for designing information retrieval\\nsystems. American Documentation 6(2):93–101. 173, 520, 525, 527, 529\\nKernighan, Mark D., Kenneth W. Church, and William A. Gale. 1990. A spelling\\ncorrection program based on a noisy channel model. In Proc. ACL, pp. 205–210. 65,\\n521, 523, 525\\nKing, Benjamin. 1967. Step-wise clustering procedures. Journal of the American Statis-\\ntical Association 69:86–101. 399, 525\\nKishida, Kazuaki, Kuang-Hua Chen, Sukhoon Lee, Kazuko Kuriyama, Noriko\\nKando, Hsin-Hsi Chen, and Sung Hyon Myaeng. 2005. Overview of CLIR task\\nat the ﬁfth NTCIR workshop. In NTCIR Workshop Meeting on Evaluation of Informa-\\ntion Access Technologies: Information Retrieval, Question Answering and Cross-Lingual\\nInformation Access. National Institute of Informatics. 45, 521, 525, 526, 528\\nKlein, Dan, and Christopher D. Manning. 2002. Conditional structure versus con-\\nditional estimation in NLP models. In Proc. Empirical Methods in Natural Language\\nProcessing, pp. 9–16. 336, 525, 527\\nKleinberg, Jon M. 1997. Two algorithms for nearest-neighbor search in high dimen-\\nsions. In Proc. ACM Symposium on Theory of Computing, pp. 599–608. ACM Press.\\nDOI: doi.acm.org/10.1145/258533.258653. 314, 525\\nKleinberg, Jon M. 1999. Authoritative sources in a hyperlinked environment. JACM\\n46(5):604–632. URL: citeseer.ist.psu.edu/article/kleinberg98authoritative.html. 481, 525\\nKleinberg, Jon M. 2002. An impossibility theorem for clustering. In Proc. NIPS. 373,\\n525\\nKnuth, Donald E.\\n1997.\\nThe Art of Computer Programming, Volume 3: Sorting and\\nSearching, 3rd edition. Addison Wesley. 65, 525\\nKo, Youngjoong, Jinwoo Park, and Jungyun Seo. 2004. Improving text categorization\\nusing the importance of sentences. IP&M 40(1):65–79. 340, 525, 528, 530\\nKoenemann, Jürgen, and Nicholas J. Belkin. 1996. A case for interaction: A study of\\ninteractive information retrieval behavior and effectiveness. In Proc. SIGCHI, pp.\\n205–212. ACM Press. DOI: doi.acm.org/10.1145/238386.238487. 194, 520, 525\\nKołcz, Aleksander, Vidya Prabakarmurthi, and Jugal Kalita. 2000. Summarization as\\nfeature selection for text categorization. In Proc. CIKM, pp. 365–370. ACM Press.\\n340, 525, 529\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n503\\nKołcz, Aleksander, and Wen-Tau Yih. 2007. Raising the baseline for high-precision\\ntext classiﬁers. In Proc. KDD. 286, 525, 533\\nKoller, Daphne, and Mehran Sahami. 1997. Hierarchically classifying documents\\nusing very few words. In Proc. ICML, pp. 170–178. 347, 525, 530\\nKonheim, Alan G. 1981. Cryptography: A Primer. John Wiley & Sons. 46, 525\\nKorfhage, Robert R. 1997. Information Storage and Retrieval. Wiley. xxxiv, 175, 525\\nKozlov, M. K., S. P. Tarasov, and L. G. Khachiyan. 1979. Polynomial solvability of con-\\nvex quadratic programming. Soviet Mathematics Doklady 20:1108–1111. Translated\\nfrom original in Doklady Akademiia Nauk SSR, 228 (1979). 328, 525, 531\\nKraaij, Wessel, and Martijn Spitters. 2003. Language models for topic tracking. In\\nW. B. Croft and J. Lafferty (eds.), Language Modeling for Information Retrieval, pp.\\n95–124. Kluwer. 251, 526, 531\\nKraaij, Wessel, Thijs Westerveld, and Djoerd Hiemstra. 2002. The importance of prior\\nprobabilities for entry page search. In Proc. SIGIR, pp. 27–34. ACM Press. 252, 524,\\n526, 532\\nKrippendorff, Klaus. 2003. Content Analysis: An Introduction to its Methodology. Sage.\\n174, 526\\nKrovetz, Bob. 1995. Word sense disambiguation for large text databases. PhD thesis,\\nUniversity of Massachusetts Amherst. 46, 526\\nKukich, Karen. 1992. Techniques for automatically correcting words in text. ACM\\nComputing Surveys 24(4):377–439. DOI: doi.acm.org/10.1145/146370.146380. 65, 526\\nKumar, Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, and Andrew Tomkins. 1999.\\nTrawling the Web for emerging cyber-communities. Computer Networks 31(11–16):\\n1481–1493. URL: citeseer.ist.psu.edu/kumar99trawling.html. 442, 526, 529, 531\\nKumar, S. Ravi, Prabhakar Raghavan, Sridhar Rajagopalan, Dandapani Sivakumar,\\nAndrew Tomkins, and Eli Upfal. 2000. The Web as a graph. In Proc. PODS, pp.\\n1–10. ACM Press. URL: citeseer.ist.psu.edu/article/kumar00web.html. 441, 526, 529, 531,\\n532\\nKupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A trainable document sum-\\nmarizer. In Proc. SIGIR, pp. 68–73. ACM Press. 174, 521, 526, 529\\nKurland, Oren, and Lillian Lee.\\n2004.\\nCorpus structure, language models, and\\nad hoc information retrieval.\\nIn Proc. SIGIR, pp. 194–201. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1008992.1009027. 372, 526\\nLafferty, John, and Chengxiang Zhai. 2001. Document language models, query mod-\\nels, and risk minimization for information retrieval. In Proc. SIGIR, pp. 111–119.\\nACM Press. 250, 251, 526, 533\\nLafferty, John, and Chengxiang Zhai. 2003. Probabilistic relevance models based\\non document and query generation. In W. Bruce Croft and John Lafferty (eds.),\\nLanguage Modeling for Information Retrieval. Kluwer. 252, 526, 533\\nLalmas, Mounia, Gabriella Kazai, Jaap Kamps, Jovan Pehcevski, Benjamin Pi-\\nwowarski, and Stephen E. Robertson. 2007. INEX 2006 evaluation measures. In\\nFuhr et al. (2007), pp. 20–34. 217, 525, 526, 529\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n504\\nBibliography\\nLalmas, Mounia, and Anastasios Tombros. 2007. Evaluating XML retrieval effective-\\nness at INEX. SIGIR Forum 41(1):40–57. DOI: doi.acm.org/10.1145/1273221.1273225.\\n216, 526, 531\\nLance, G. N., and W. T. Williams. 1967. A general theory of classiﬁcatory sorting\\nstrategies 1. Hierarchical systems. Computer Journal 9(4):373–380. 399, 526, 533\\nLangville, Amy, and Carl Meyer. 2006. Google’s PageRank and Beyond: The Science of\\nSearch Engine Rankings. Princeton University Press. 481, 526, 527\\nLarsen, Bjornar, and Chinatsu Aone.\\n1999.\\nFast and effective text mining using\\nlinear-time document clustering.\\nIn Proc. KDD, pp. 16–22. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/312129.312186. 399, 400, 519, 526\\nLarson, Ray R. 2005. A fusion approach to XML structured document retrieval. IR 8\\n(4):601–629. DOI: dx.doi.org/10.1007/s10791-005-0749-0. 216, 526\\nLavrenko, Victor, and W. Bruce Croft. 2001. Relevance-based language models. In\\nProc. SIGIR, pp. 120–127. ACM Press. 250, 522, 526\\nLawrence, Steve, and C. Lee Giles. 1998. Searching the World Wide Web. Science 280\\n(5360):98–100. URL: citeseer.ist.psu.edu/lawrence98searching.html. 442, 523, 526\\nLawrence, Steve, and C. Lee Giles. 1999. Accessibility of information on the web.\\nNature 500:107–109. 442, 523, 526\\nLee, Whay C., and Edward A. Fox. 1988. Experimental comparison of schemes for in-\\nterpreting Boolean queries. Technical Report TR-88-27, Computer Science, Virginia\\nPolytechnic Institute and State University. 17, 523, 526\\nLempel, Ronny, and Shlomo Moran. 2000. The stochastic approach for link-structure\\nanalysis (SALSA) and the TKC effect. Computer Networks 33(1–6):387–401.\\nURL:\\nciteseer.ist.psu.edu/lempel00stochastic.html. 481, 526, 528\\nLesk, Michael. 1988. Grab – Inverted indexes with low storage overhead. Computing\\nSystems 1:207–220. 83, 526\\nLesk, Michael. 2004. Understanding Digital Libraries, 2nd edition. Morgan Kaufmann.\\nxxxiv, 526\\nLester, Nicholas, Alistair Moffat, and Justin Zobel. 2005.\\nFast on-line index con-\\nstruction by geometric partitioning. In Proc. CIKM, pp. 776–783. ACM Press. DOI:\\ndoi.acm.org/10.1145/1099554.1099739. 84, 526, 527, 533\\nLester, Nicholas, Justin Zobel, and Hugh E. Williams.\\n2006.\\nEfﬁcient online\\nindex maintenance for contiguous inverted lists.\\nIP&M 42(4):916–933.\\nDOI:\\ndx.doi.org/10.1016/j.ipm.2005.09.005. 84, 526, 533\\nLevenshtein, Vladimir I. 1965. Binary codes capable of correcting spurious insertions\\nand deletions of ones. Problems of Information Transmission 1:8–17. 65, 526\\nLew, Michael S. 2001. Principles of Visual Information Retrieval. Springer. xxxiv, 526\\nLewis, David D. 1995. Evaluating and optimizing autonomous text classiﬁcation\\nsystems. In Proc. SIGIR. ACM Press. 286, 526\\nLewis, David D.\\n1998.\\nNaive (Bayes) at forty: The independence assumption in\\ninformation retrieval. In Proc. ECML, pp. 4–15. Springer. 286, 526\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n505\\nLewis, David D., and Karen Spärck Jones. 1996. Natural language processing for\\ninformation retrieval. CACM 39(1):92–101. DOI: doi.acm.org/10.1145/234173.234210.\\nxxxiv, 525, 526\\nLewis, David D., and Marc Ringuette. 1994. A comparison of two learning algorithms\\nfor text categorization. In Proc. SDAIR, pp. 81–93. 286, 526, 529\\nLewis, David D., Robert E. Schapire, James P. Callan, and Ron Papka. 1996. Training\\nalgorithms for linear text classiﬁers. In Proc. SIGIR, pp. 298–306. ACM Press. DOI:\\ndoi.acm.org/10.1145/243199.243277. 315, 521, 526, 528, 530\\nLewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A new bench-\\nmark collection for text categorization research. JMLR 5:361–397. 84, 287, 526, 530,\\n533\\nLi, Fan, and Yiming Yang. 2003. A loss function analysis for classiﬁcation methods in\\ntext categorization. In Proc. ICML, pp. 472–479. xvii, 282, 347, 526, 533\\nLiddy, Elizabeth D. 2005. Automatic document retrieval. In Encyclopedia of Language\\nand Linguistics, 2nd edition. Elsevier. 17, 526\\nList, Johan, Vojkan Mihajlovic, Georgina Ramírez, Arjen P. Vries, Djoerd Hiemstra,\\nand Henk Ernst Blok. 2005. TIJAH: Embracing IR methods in XML databases. IR\\n8(4):547–570. DOI: dx.doi.org/10.1007/s10791-005-0747-2. 216, 520, 524, 526, 527, 529,\\n532\\nLita, Lucian Vlad, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuE-\\ncasIng. In Proc. ACL, pp. 152–159. 46, 524, 525, 526, 530\\nLittman, Michael L., Susan T. Dumais, and Thomas K. Landauer. 1998. Automatic\\ncross-language information retrieval using latent semantic indexing. In Gregory\\nGrefenstette (ed.), Proc. Cross-Language Information Retrieval. Kluwer.\\nURL: cite-\\nseer.ist.psu.edu/littman98automatic.html. 417, 522, 526\\nLiu, Tie-Yan, Yiming Yang, Hao Wan, Hua-Jun Zeng, Zheng Chen, and Wei-Ying Ma.\\n2005. Support vector machines classiﬁcation with very large scale taxonomy. ACM\\nSIGKDD Explorations 7(1):36–43. 347, 521, 526, 527, 532, 533\\nLiu,\\nXiaoyong,\\nand W. Bruce Croft.\\n2004.\\nCluster-based retrieval us-\\ning language models.\\nIn Proc. SIGIR, pp. 186–193. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1008992.1009026. 252, 351, 372, 522, 526\\nLloyd, Stuart P. 1982. Least squares quantization in PCM. IEEE Transactions on Infor-\\nmation Theory 28(2):129–136. 373, 527\\nLodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris\\nWatkins. 2002. Text classiﬁcation using string kernels. JMLR 2:419–444. 347, 521,\\n527, 530, 532\\nLombard, Matthew, Cheryl C. Bracken, and Jennifer Snyder-Duch. 2002. Content\\nanalysis in mass communication: Assessment and reporting of intercoder reliabil-\\nity. Human Communication Research 28:587–604. 174, 520, 527, 531\\nLong, Xiaohui, and Torsten Suel.\\n2003.\\nOptimized query execution in large\\nsearch engines with global page ordering.\\nIn Proc. VLDB.\\nURL:\\ncite-\\nseer.ist.psu.edu/long03optimized.html. 149, 527, 531\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n506\\nBibliography\\nLovins, Julie Beth. 1968.\\nDevelopment of a stemming algorithm. Translation and\\nComputational Linguistics 11(1):22–31. 33, 527\\nLu, Wei, Stephen E. Robertson, and Andrew MacFarlane. 2007. CISR at INEX 2006.\\nIn Fuhr et al. (2007), pp. 57–63. 216, 527, 529\\nLuhn, Hans Peter. 1957. A statistical approach to mechanized encoding and searching\\nof literary information. IBM Journal of Research and Development 1(4):309–317. 133,\\n527\\nLuhn, Hans Peter. 1958. The automatic creation of literature abstracts. IBM Journal of\\nResearch and Development 2(2):159–165, 317. 133, 527\\nLuk, Robert W. P., and Kui-Lam Kwok. 2002. A comparison of Chinese document\\nindexing strategies and retrieval models. ACM Transactions on Asian Language In-\\nformation Processing 1(3):225–268. 45, 526, 527\\nLunde, Ken. 1998. CJKV Information Processing. O’Reilly. 45, 527\\nMacFarlane, A., J.A. McCann, and S.E. Robertson. 2000. Parallel search using parti-\\ntioned inverted ﬁles. In Proc. SPIRE, pp. 209–220. 458, 527, 529\\nMacQueen, James B. 1967.\\nSome methods for classiﬁcation and analysis of mul-\\ntivariate observations. In Proc. Berkeley Symposium on Mathematics, Statistics and\\nProbability, pp. 281–297. University of California Press. 373, 527\\nManning, Christopher D., and Hinrich Schütze. 1999. Foundations of Statistical Natural\\nLanguage Processing. MIT Press. xxxiv, 40, 105, 251, 252, 286, 372, 527, 530\\nMaron, M. E., and J. L. Kuhns. 1960. On relevance, probabilistic indexing, and infor-\\nmation retrieval. JACM 7(3):216–244. 235, 286, 526, 527\\nMass, Yosi, Matan Mandelbrod, Einat Amitay, David Carmel, Yoëlle S. Maarek, and\\nAya Soffer. 2003. JuruXML – An XML retrieval system at INEX’02. In Fuhr et al.\\n(2003b), pp. 73–80.\\nURL: inex.is.informatik.uni-duisburg.de:2003/proceedings.pdf. 216,\\n519, 521, 527, 531\\nMcBryan, Oliver A. 1994. GENVL and WWWW: Tools for Taming the Web. In Proc.\\nWWW. URL: citeseer.ist.psu.edu/mcbryan94genvl.html. 442, 480, 527\\nMcCallum, Andrew, and Kamal Nigam. 1998. A comparison of event models for\\nNaive Bayes text classiﬁcation. In AAAI/ICML Workshop on Learning for Text Cate-\\ngorization, pp. 41–48. 286, 527, 528\\nMcCallum, Andrew, Ronald Rosenfeld, Tom M. Mitchell, and Andrew Y. Ng. 1998.\\nImproving text classiﬁcation by shrinkage in a hierarchy of classes. In Proc. ICML,\\npp. 359–367. Morgan Kaufmann. 347, 527, 528, 530\\nMcCallum, Andrew Kachites. 1996. Bow: A toolkit for statistical language modeling,\\ntext retrieval, classiﬁcation and clustering. www.cs.cmu.edu/~mccallum/bow. 316, 527\\nMcKeown, Kathleen, and Dragomir R. Radev.\\n1995.\\nGenerating summaries\\nof multiple news articles.\\nIn Proc. SIGIR, pp. 74–82. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/215206.215334. 400, 527, 529\\nMcKeown, Kathleen R., Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Ju-\\ndith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman.\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n507\\n2002. Tracking and summarizing news on a daily basis with Columbia’s News-\\nblaster. In Proc. Human Language Technology Conference. 351, 373, 520, 522, 524, 525,\\n527, 528, 530\\nMcLachlan, Geoffrey J., and Thiriyambakam Krishnan. 1996. The EM Algorithm and\\nExtensions. John Wiley & Sons. 373, 526, 527\\nMeadow, Charles T., Donald H. Kraft, and Bert R. Boyce. 1999. Text Information Re-\\ntrieval Systems. Academic Press. xxxiv, 520, 526, 527\\nMeil˘a, Marina. 2005. Comparing clusterings – An axiomatic view. In Proc. ICML. 373,\\n527\\nMelnik, Sergey, Sriram Raghavan, Beverly Yang, and Hector Garcia-Molina. 2001.\\nBuilding a distributed full-text index for the web. In Proc. WWW, pp. 396–406.\\nACM Press. DOI: doi.acm.org/10.1145/371920.372095. 83, 523, 527, 529, 533\\nMihajlovi´c, Vojkan, Henk Ernst Blok, Djoerd Hiemstra, and Peter M. G. Apers. 2005.\\nScore region algebra: Building a transparent XML-R database. In Proc. CIKM, pp.\\n12–19. DOI: doi.acm.org/10.1145/1099554.1099560. 216, 519, 520, 524, 527\\nMiller, David R. H., Tim Leek, and Richard M. Schwartz. 1999. A hidden Markov\\nmodel information retrieval system. In Proc. SIGIR, pp. 214–221. ACM Press. 246,\\n252, 526, 527, 530\\nMinsky, Marvin Lee, and Seymour Papert (eds.). 1988. Perceptrons: An introduction to\\ncomputational geometry. MIT Press. Expanded edition. 315, 527, 528\\nMitchell, Tom M. 1997. Machine Learning. McGraw Hill. 286, 527\\nMoffat, Alistair, and Timothy A. H. Bell. 1995.\\nIn situ generation of compressed\\ninverted ﬁles. JASIS 46(7):537–550. 83, 520, 527\\nMoffat, Alistair, and Lang Stuiver. 1996. Exploiting clustering in inverted ﬁle com-\\npression. In Proc. Conference on Data Compression, pp. 82–91. IEEE Computer Soci-\\nety. 106, 527, 531\\nMoffat,\\nAlistair,\\nand\\nJustin\\nZobel.\\n1992.\\nParameterised\\ncompression\\nfor sparse bitmaps.\\nIn Proc. SIGIR, pp. 274–285.\\nACM Press.\\nDOI:\\ndoi.acm.org/10.1145/133160.133210. 106, 528, 533\\nMoffat, Alistair, and Justin Zobel. 1996. Self-indexing inverted ﬁles for fast text re-\\ntrieval. TOIS 14(4):349–379. 46, 47, 528, 533\\nMoffat, Alistair, and Justin Zobel. 1998. Exploring the similarity space. SIGIR Forum\\n32(1). 133, 528, 533\\nMooers, Calvin. 1961. From a point of view of mathematical etc. techniques. In R. A.\\nFairthorne (ed.), Towards information retrieval, pp. xvii–xxiii. Butterworths. 17, 528\\nMooers, Calvin E. 1950. Coding, information retrieval, and the rapid selector. Ameri-\\ncan Documentation 1(4):225–229. 17, 528\\nMoschitti, Alessandro. 2003. A study on optimal parameter tuning for Rocchio text\\nclassiﬁer. In Proc. ECIR, pp. 420–435. 315, 528\\nMoschitti, Alessandro, and Roberto Basili. 2004. Complex linguistic features for text\\nclassiﬁcation: A comprehensive study. In Proc. ECIR, pp. 181–196. 347, 520, 528\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n508\\nBibliography\\nMurata, Masaki, Qing Ma, Kiyotaka Uchimoto, Hiromi Ozaku, Masao Utiyama, and\\nHitoshi Isahara. 2000. Japanese probabilistic information retrieval using location\\nand category information. In International Workshop on Information Retrieval With\\nAsian Languages, pp. 81–88. URL: portal.acm.org/citation.cfm?doid=355214.355226. 340,\\n524, 527, 528, 532\\nMuresan, Gheorghe, and David J. Harper.\\n2004.\\nTopic modeling for medi-\\nated access to very large document collections.\\nJASIST 55(10):892–910.\\nDOI:\\ndx.doi.org/10.1002/asi.20034. 372, 524, 528\\nMurtagh, Fionn. 1983. A survey of recent advances in hierarchical clustering algo-\\nrithms. Computer Journal 26(4):354–359. 399, 528\\nNajork, Marc, and Allan Heydon. 2001. High-performance web crawling. Technical\\nReport 173, Compaq Systems Research Center. 458, 524, 528\\nNajork, Marc, and Allan Heydon. 2002. High-performance web crawling. In James\\nAbello, Panos Pardalos, and Mauricio Resende (eds.), Handbook of Massive Data\\nSets, chapter 2. Kluwer. 458, 524, 528\\nNavarro, Gonzalo, and Ricardo Baeza-Yates.\\n1997.\\nProximal nodes: A model to\\nquery document databases by content and structure. TOIS 15(4):400–435.\\nDOI:\\ndoi.acm.org/10.1145/263479.263482. 217, 519, 528\\nNewsam, Shawn, Sitaram Bhagavathy, and B. S. Manjunath. 2001. Category-based\\nimage retrieval. In Proc. IEEE International Conference on Image Processing, Special\\nSession on Multimedia Indexing, Browsing and Retrieval, pp. 596–599. 179, 520, 527,\\n528\\nNg, Andrew Y., and Michael I. Jordan. 2001. On discriminative vs. generative clas-\\nsiﬁers: A comparison of logistic regression and naive Bayes. In Proc. NIPS, pp.\\n841–848. URL: www-2.cs.cmu.edu/Groups/NIPS/NIPS2001/papers/psgz/AA28.ps.gz. 286,\\n336, 525, 528\\nNg, Andrew Y., Michael I. Jordan, and Yair Weiss. 2001a. On spectral clustering:\\nAnalysis and an algorithm. In Proc. NIPS, pp. 849–856. 400, 525, 528, 532\\nNg, Andrew Y., Alice X. Zheng, and Michael I. Jordan. 2001b. Link analysis, eigenvec-\\ntors and stability. In Proc. IJCAI, pp. 903–910. URL: citeseer.ist.psu.edu/ng01link.html.\\n481, 525, 528, 533\\nNigam, Kamal, Andrew McCallum, and Tom Mitchell. 2006. Semi-supervised text\\nclassiﬁcation using EM. In Chapelle et al. (2006), pp. 33–56. 347, 527, 528\\nNtoulas, Alexandros, and Junghoo Cho. 2007. Pruning policies for two-tiered in-\\nverted index with correctness guarantee. In Proc. SIGIR, pp. 191–198. ACM Press.\\n105, 521, 528\\nOard, Douglas W., and Bonnie J. Dorr. 1996. A survey of multilingual text retrieval.\\nTechnical Report UMIACS-TR-96-19, Institute for Advanced Computer Studies,\\nUniversity of Maryland, College Park, MD, USA. xxxiv, 522, 528\\nOgilvie, Paul, and Jamie Callan.\\n2005.\\nParameter estimation for a simple hierar-\\nchical generative model for XML retrieval.\\nIn Proc. INEX, pp. 211–224.\\nDOI:\\ndx.doi.org/10.1007/11766278_16. 216, 521, 528\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n509\\nO’Keefe, Richard A., and Andrew Trotman. 2004. The simplest query language that\\ncould possibly work. In Fuhr et al. (2005), pp. 167–174. 217, 528, 532\\nOsi´nski, Stanisław, and Dawid Weiss. 2005. A concept-driven algorithm for clustering\\nsearch results. IEEE Intelligent Systems 20(3):48–54. 400, 528, 532\\nPage, Lawrence, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The Page-\\nRank citation ranking: Bringing order to the web. Technical report, Stanford Digital\\nLibrary Technologies Project. URL: citeseer.ist.psu.edu/page98pagerank.html. 480, 520,\\n528, 533\\nPaice, Chris D. 1990. Another stemmer. SIGIR Forum 24(3):56–61. 33, 528\\nPapineni, Kishore. 2001. Why inverse document frequency? In Proc. North American\\nChapter of the Association for Computational Linguistics, pp. 1–8. 133, 528\\nPavlov, Dmitry, Ramnath Balasubramanyan, Byron Dom, Shyam Kapur, and Jig-\\nnashu Parikh. 2004. Document preprocessing for naive Bayes classiﬁcation and\\nclustering with mixture of multinomials. In Proc. KDD, pp. 829–834. 286, 519, 522,\\n525, 528\\nPelleg, Dan, and Andrew Moore.\\n1999.\\nAccelerating exact k-means algorithms\\nwith geometric reasoning.\\nIn Proc. KDD, pp. 277–281. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/312129.312248. 373, 528, 529\\nPelleg, Dan, and Andrew Moore. 2000. X-means: Extending k-means with efﬁcient es-\\ntimation of the number of clusters. In Proc. ICML, pp. 727–734. Morgan Kaufmann.\\n373, 528, 529\\nPerkins, Simon, Kevin Lacker, and James Theiler. 2003. Grafting: Fast, incremental\\nfeature selection by gradient descent in function space. JMLR 3:1333–1356. 286,\\n526, 529, 531\\nPersin, Michael. 1994. Document ﬁltering for fast ranking. In Proc. SIGIR, pp. 339–348.\\nACM Press. 149, 529\\nPersin, Michael, Justin Zobel, and Ron Sacks-Davis. 1996. Filtered document retrieval\\nwith frequency-sorted indexes. JASIS 47(10):749–764. 149, 529, 530, 533\\nPeterson, James L. 1980. Computer programs for detecting and correcting spelling\\nerrors. CACM 23(12):676–687. DOI: doi.acm.org/10.1145/359038.359041. 65, 529\\nPicca, Davide, Benoît Curdy, and François Bavaud. 2006. Non-linear correspondence\\nanalysis in text retrieval: A kernel view. In Proc. JADT. 308, 520, 522, 529\\nPinski, Gabriel, and Francis Narin. 1976. Citation inﬂuence for journal aggregates of\\nscientiﬁc publications: Theory, with application to the literature of Physics. IP&M\\n12:297–326. 480, 528, 529\\nPirolli, Peter L. T. 2007. Information Foraging Theory: Adaptive Interaction With Informa-\\ntion. Oxford University Press. 373, 529\\nPlatt, John. 2000. Probabilistic outputs for support vector machines and comparisons\\nto regularized likelihood methods. In A.J. Smola, P.L. Bartlett, B. Schölkopf, and\\nD. Schuurmans (eds.), Advances in Large Margin Classiﬁers, pp. 61–74. MIT Press.\\n325, 529\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n510\\nBibliography\\nPonte, Jay M., and W. Bruce Croft. 1998. A language modeling approach to informa-\\ntion retrieval. In Proc. SIGIR, pp. 275–281. ACM Press. xxii, 246, 247, 249, 252, 522,\\n529\\nPopescul, Alexandrin, and Lyle H. Ungar. 2000. Automatic labeling of document\\nclusters. Unpublished MS, U. Pennsylvania.\\nURL: http://www.cis.upenn.edu/ popes-\\ncul/Publications/popescul00labeling.pdf. 400, 529, 532\\nPorter, Martin F. 1980. An algorithm for sufﬁx stripping. Program 14(3):130–137. 33,\\n529\\nPugh, William. 1990. Skip lists: A probabilistic alternative to balanced trees. CACM\\n33(6):668–676. 46, 529\\nQin, Tao, Tie-Yan Liu, Wei Lai, Xu-Dong Zhang, De-Sheng Wang, and Hang Li. 2007.\\nRanking with multiple hyperplanes. In Proc. SIGIR. ACM Press. 348, 526, 529, 532,\\n533\\nQiu, Yonggang, and H.P. Frei. 1993. Concept based query expansion. In Proc. SIGIR,\\npp. 160–169. ACM Press. 194, 523, 529\\nR Development Core Team. 2005. R: A language and environment for statistical comput-\\ning. R Foundation for Statistical Computing, Vienna. URL: www.R-project.org. ISBN\\n3-900051-07-0. 374, 400, 529\\nRadev, Dragomir R., Sasha Blair-Goldensohn, Zhu Zhang, and Revathi Sundara\\nRaghavan. 2001. Interactive, domain-independent identiﬁcation and summariza-\\ntion of topically related news articles. In Proc. European Conference on Research and\\nAdvanced Technology for Digital Libraries, pp. 225–238. 373, 520, 529, 533\\nRahm,\\nErhard, and Philip A. Bernstein.\\n2001.\\nA survey of approaches\\nto automatic schema matching.\\nVLDB Journal 10(4):334–350.\\nURL:\\ncite-\\nseer.ist.psu.edu/rahm01survey.html. 216, 520, 529\\nRand, William M. 1971. Objective criteria for the evaluation of clustering methods.\\nJournal of the American Statistical Association 66(336):846–850. 373, 529\\nRasmussen, Edie. 1992. Clustering algorithms. In Frakes and Baeza-Yates (1992), pp.\\n419–442. 372, 529\\nRennie, Jason D., Lawrence Shih, Jaime Teevan, and David R. Karger. 2003. Tackling\\nthe poor assumptions of naive Bayes text classiﬁers. In Proc. ICML, pp. 616–623.\\n286, 525, 529, 530, 531\\nRibeiro-Neto, Berthier, Edleno S. Moura, Marden S. Neubert, and Nivio Ziviani. 1999.\\nEfﬁcient distributed algorithms to build inverted ﬁles. In Proc. SIGIR, pp. 105–112.\\nACM Press. DOI: doi.acm.org/10.1145/312624.312663. 83, 528, 529, 533\\nRibeiro-Neto, Berthier A., and Ramurti A. Barbosa. 1998. Query performance for\\ntightly coupled distributed digital libraries.\\nIn Proc. ACM Conference on Digital\\nLibraries, pp. 182–190. 459, 519, 529\\nRice, John A. 2006. Mathematical Statistics and Data Analysis. Duxbury Press. 99, 235,\\n276, 529\\nRichardson, M., A. Prakash, and E. Brill. 2006. Beyond PageRank: machine learning\\nfor static ranking. In Proc. WWW, pp. 707–715. 348, 520, 529\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n511\\nRiezler, Stefan, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and\\nYi Liu. 2007. Statistical machine translation for query expansion in answer re-\\ntrieval. In Proc. ACL, pp. 464–471. Association for Computational Linguistics. URL:\\nwww.aclweb.org/anthology/P/P07/P07-1059. 194, 527, 529, 532\\nRipley, B. D. 1996. Pattern Recognition and Neural Networks. Cambridge University\\nPress. 222, 235, 529\\nRobertson, Stephen. 2005. How Okapi came to TREC. In Voorhees and Harman\\n(2005), pp. 287–299. 235, 529\\nRobertson, Stephen, Hugo Zaragoza, and Michael Taylor.\\n2004.\\nSimple BM25\\nextension to multiple weighted ﬁelds.\\nIn Proc. CIKM, pp. 42–49.\\nDOI:\\ndoi.acm.org/10.1145/1031171.1031181. 235, 529, 531, 533\\nRobertson, Stephen E., and Karen Spärck Jones. 1976. Relevance weighting of search\\nterms. JASIS 27:129–146. 133, 235, 525, 529\\nRocchio, J. J. 1971. Relevance feedback in information retrieval. In Salton (1971b), pp.\\n313–323. 181, 193, 314, 530\\nRoget, P. M. 1946. Roget’s International Thesaurus. Thomas Y. Crowell. 194, 530\\nRosen-Zvi, Michal, Thomas Grifﬁths, Mark Steyvers, and Padhraic Smyth. 2004. The\\nauthor-topic model for authors and documents. In Proc. UAI, pp. 487–494. 418,\\n523, 530, 531\\nRoss, Sheldon. 2006. A First Course in Probability. Pearson Prentice Hall. 99, 235, 530\\nRusmevichientong, Paat, David M. Pennock, Steve Lawrence, and C. Lee Giles. 2001.\\nMethods for sampling pages uniformly from the world wide web. In Proc. AAAI\\nFall Symposium on Using Uncertainty Within Computation, pp. 121–128.\\nURL: cite-\\nseer.ist.psu.edu/rusmevichientong01methods.html. 442, 523, 526, 529, 530\\nRuthven, Ian, and Mounia Lalmas. 2003. A survey on the use of relevance feedback\\nfor information access systems. Knowledge Engineering Review 18(1). 194, 526, 530\\nSahoo, Nachiketa, Jamie Callan, Ramayya Krishnan, George Duncan, and Rema Pad-\\nman. 2006. Incremental hierarchical clustering of text documents. In Proc. CIKM,\\npp. 357–366. DOI: doi.acm.org/10.1145/1183614.1183667. 400, 521, 522, 526, 528, 530\\nSakai, Tetsuya. 2007. On the reliability of information retrieval metrics based on\\ngraded relevance. IP&M 43(2):531–548. 174, 530\\nSalton, Gerard. 1971a. Cluster search strategies and the optimization of retrieval\\neffectiveness. In The SMART Retrieval System – Experiments in Automatic Document\\nProcessing Salton (1971b), pp. 223–242. 351, 372, 530\\nSalton, Gerard (ed.). 1971b. The SMART Retrieval System – Experiments in Automatic\\nDocument Processing. Prentice Hall. 133, 173, 193, 499, 509, 510, 530\\nSalton, Gerard. 1975. Dynamic information and library processing. Prentice Hall. 372,\\n530\\nSalton, Gerard.\\n1989.\\nAutomatic Text Processing: The Transformation, Analysis, and\\nRetrieval of Information by Computer. Addison Wesley. 46, 194, 530\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n512\\nBibliography\\nSalton, Gerard. 1991. The Smart project in automatic document retrieval. In Proc.\\nSIGIR, pp. 356–358. ACM Press. 173, 530\\nSalton, Gerard, James Allan, and Chris Buckley. 1993. Approaches to passage re-\\ntrieval in full text information systems. In Proc. SIGIR, pp. 49–58. ACM Press. DOI:\\ndoi.acm.org/10.1145/160688.160693. 217, 519, 520, 530\\nSalton, Gerard, and Chris Buckley. 1987. Term weighting approaches in automatic\\ntext retrieval. Technical report, Cornell University, Ithaca, NY, USA. 133, 520, 530\\nSalton, Gerard, and Christopher Buckley. 1988. Term-weighting approaches in auto-\\nmatic text retrieval. IP&M 24(5):513–523. 133, 520, 530\\nSalton, Gerard, and Chris Buckley. 1990. Improving retrieval performance by rele-\\nvance feedback. JASIS 41(4):288–297. 194, 520, 530\\nSaracevic, Tefko, and Paul Kantor. 1988. A study of information seeking and retriev-\\ning. II: Users, questions and effectiveness. JASIS 39:177–196. 173, 525, 530\\nSaracevic, Tefko, and Paul Kantor. 1996. A study of information seeking and retriev-\\ning. III: Searchers, searches, overlap. JASIS 39(3):197–216. 173, 525, 530\\nSavaresi, Sergio M., and Daniel Boley. 2004. A comparative analysis on the bisecting\\nK-means and the PDDP clustering algorithms. Intelligent Data Analysis 8(4):345–\\n362. 400, 520, 530\\nSchamber, Linda, Michael Eisenberg, and Michael S. Nilan. 1990. A re-examination\\nof relevance: toward a dynamic, situational deﬁnition. IP&M 26(6):755–776. 174,\\n522, 528, 530\\nSchapire, Robert E. 2003. The boosting approach to machine learning: An overview.\\nIn D. D. Denison, M. H. Hansen, C. Holmes, B. Mallick, and B. Yu (eds.), Nonlinear\\nEstimation and Classiﬁcation. Springer. 347, 530\\nSchapire, Robert E., and Yoram Singer. 2000. Boostexter: A boosting-based system\\nfor text categorization. Machine Learning 39(2/3):135–168. 347, 530, 531\\nSchapire, Robert E., Yoram Singer, and Amit Singhal. 1998. Boosting and Rocchio\\napplied to text ﬁltering. In Proc. SIGIR, pp. 215–223. ACM Press. 314, 315, 530, 531\\nSchlieder, Torsten, and Holger Meuss. 2002. Querying and ranking XML documents.\\nJASIST 53(6):489–503. DOI: dx.doi.org/10.1002/asi.10060. 216, 527, 530\\nScholer, Falk, Hugh E. Williams, John Yiannis, and Justin Zobel. 2002. Compression\\nof inverted indexes for fast query evaluation. In Proc. SIGIR, pp. 222–229. ACM\\nPress. DOI: doi.acm.org/10.1145/564376.564416. 106, 530, 533\\nSchölkopf, Bernhard, and Alexander J. Smola. 2001. Learning with Kernels: Support\\nVector Machines, Regularization, Optimization, and Beyond. MIT Press. 346, 530, 531\\nSchütze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguis-\\ntics 24(1):97–124. 192, 194, 530\\nSchütze, Hinrich, David A. Hull, and Jan O. Pedersen. 1995. A comparison of clas-\\nsiﬁers and document representations for the routing problem. In Proc. SIGIR, pp.\\n229–237. ACM Press. 193, 286, 315, 524, 529, 530\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n513\\nSchütze, Hinrich, and Jan O. Pedersen. 1995. Information retrieval based on word\\nsenses. In Proc. SDAIR, pp. 161–175. 374, 529, 530\\nSchütze, Hinrich, and Craig Silverstein.\\n1997.\\nProjections for efﬁcient document\\nclustering. In Proc. SIGIR, pp. 74–81. ACM Press. 373, 417, 530\\nSchwarz, Gideon. 1978. Estimating the dimension of a model. Annals of Statistics 6\\n(2):461–464. 373, 530\\nSebastiani, Fabrizio. 2002. Machine learning in automated text categorization. ACM\\nComputing Surveys 34(1):1–47. 286, 530\\nShawe-Taylor, John, and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis.\\nCambridge University Press. 346, 521, 530\\nShkapenyuk, Vladislav, and Torsten Suel. 2002. Design and implementation of a\\nhigh-performance distributed web crawler. In Proc. International Conference on Data\\nEngineering. URL: citeseer.ist.psu.edu/shkapenyuk02design.html. 458, 530, 531\\nSiegel, Sidney, and N. John Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral\\nSciences, 2nd edition. McGraw Hill. 174, 521, 530\\nSifry, Dave, 2007.\\nThe state of the Live Web, April 2007.\\nURL:\\ntechno-\\nrati.com/weblog/2007/04/328.html. 30, 530\\nSigurbjörnsson, Börkur, Jaap Kamps, and Maarten de Rijke. 2004. Mixture models,\\noverlap, and structural hints in XML element retrieval. In Proc. INEX, pp. 196–210.\\n216, 525, 529, 530\\nSilverstein, Craig, Monika Rauch Henzinger, Hannes Marais, and Michael Moricz.\\n1999. Analysis of a very large web search engine query log. SIGIR Forum 33(1):\\n6–12. 47, 524, 527, 528, 530\\nSilvestri, Fabrizio. 2007. Sorting out the document identiﬁer assignment problem. In\\nProc. ECIR, pp. 101–112. 106, 531\\nSilvestri, Fabrizio, Raffaele Perego, and Salvatore Orlando. 2004. Assigning docu-\\nment identiﬁers to enhance compressibility of web search engines indexes. In Proc.\\nACM Symposium on Applied Computing, pp. 600–605. 106, 528, 529, 531\\nSindhwani, V., and S. S. Keerthi. 2006. Large scale semi-supervised linear SVMs. In\\nProc. SIGIR, pp. 477–484. 348, 525, 531\\nSinghal, Amit, Chris Buckley, and Mandar Mitra.\\n1996a.\\nPivoted document\\nlength normalization.\\nIn Proc. SIGIR, pp. 21–29. ACM Press.\\nURL:\\ncite-\\nseer.ist.psu.edu/singhal96pivoted.html. 133, 520, 527, 531\\nSinghal, Amit, Mandar Mitra, and Chris Buckley. 1997. Learning routing queries in a\\nquery zone. In Proc. SIGIR, pp. 25–32. ACM Press. 193, 520, 527, 531\\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1995.\\nLength normalization in\\ndegraded text collections. Technical report, Cornell University, Ithaca, NY. 133,\\n520, 530, 531\\nSinghal, Amit, Gerard Salton, and Chris Buckley. 1996b. Length normalization in\\ndegraded text collections. In Proc. SDAIR, pp. 149–162. 133, 520, 530, 531\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n514\\nBibliography\\nSingitham, Pavan Kumar C., Mahathi S. Mahabhashyam, and Prabhakar Raghavan.\\n2004. Efﬁciency-quality tradeoffs for vector score aggregation. In Proc. VLDB, pp.\\n624–635. URL: www.vldb.org/conf/2004/RS17P1.PDF. 149, 372, 527, 529, 531\\nSmeulders, Arnold W. M., Marcel Worring, Simone Santini, Amarnath Gupta,\\nand Ramesh Jain.\\n2000.\\nContent-based image retrieval at the end of the\\nearly years.\\nIEEE Trans. Pattern Anal. Mach. Intell. 22(12):1349–1380.\\nDOI:\\ndx.doi.org/10.1109/34.895972. xxxiv, 523, 524, 530, 531, 533\\nSneath, Peter H.A., and Robert R. Sokal. 1973. Numerical Taxonomy: The Principles and\\nPractice of Numerical Classiﬁcation. W.H. Freeman. 399, 531\\nSnedecor, George Waddel, and William G. Cochran. 1989. Statistical methods. Iowa\\nState University Press. 286, 521, 531\\nSomogyi, Zoltan. 1990. The Melbourne University bibliography system. Technical\\nReport 90/3, Melbourne University, Parkville, Victoria, Australia. 83, 531\\nSong, Ruihua, Ji-Rong Wen, and Wei-Ying Ma. 2005. Viewing term proximity from a\\ndifferent perspective. Technical Report MSR-TR-2005-69, Microsoft Research. 149,\\n527, 531, 532\\nSornil, Ohm. 2001. Parallel Inverted Index for Large-Scale, Dynamic Digital Libraries. PhD\\nthesis, Virginia Tech.\\nURL:\\nscholar.lib.vt.edu/theses/available/etd-02062001-114915/.\\n459, 531\\nSpärck Jones, Karen. 1972. A statistical interpretation of term speciﬁcity and its ap-\\nplication in retrieval. Journal of Documentation 28(1):11–21. 133, 525\\nSpärck Jones, Karen.\\n2004.\\nLanguage modelling’s generative model:\\nIs it\\nrational?\\nMS, Computer Laboratory, University of Cambridge.\\nURL:\\nwww.cl.cam.ac.uk/~ksj21/langmodnote4.pdf. 252, 525\\nSpärck Jones, Karen, S. Walker, and Stephen E. Robertson. 2000. A probabilistic model\\nof information retrieval: Development and comparative experiments. IP&M 36(6):\\n779–808, 809–840. 232, 234, 235, 525, 529, 532\\nSpink, Amanda, and Charles Cole (eds.). 2005. New Directions in Cognitive Information\\nRetrieval. Springer. 175, 521, 531\\nSpink,\\nAmanda,\\nBernard J. Jansen,\\nand H. Cenk Ozmultu.\\n2000.\\nUse\\nof query reformulation and relevance feedback by Excite users.\\nInternet\\nResearch:\\nElectronic Networking Applications and Policy 10(4):317–328.\\nURL:\\nist.psu.edu/faculty_pages/jjansen/academic/pubs/internetresearch2000.pdf. 185, 524, 528,\\n531\\nSproat, Richard, and Thomas Emerson. 2003. The ﬁrst international Chinese word\\nsegmentation bakeoff. In SIGHAN Workshop on Chinese Language Processing. 46,\\n522, 531\\nSproat, Richard, William Gale, Chilin Shih, and Nancy Chang. 1996. A stochastic\\nﬁnite-state word-segmentation algorithm for Chinese. Computational Linguistics 22\\n(3):377–404. 46, 521, 523, 530, 531\\nSproat, Richard William. 1992. Morphology and computation. MIT Press. 46, 531\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n515\\nStein, Benno, and Sven Meyer zu Eissen. 2004. Topic identiﬁcation: Framework and\\napplication. In Proc. International Conference on Knowledge Management. 400, 522,\\n531\\nStein, Benno, Sven Meyer zu Eissen, and Frank Wißbrock. 2003. On cluster validity\\nand the information need of users. In Proc. Artiﬁcial Intelligence and Applications.\\n373, 522, 531, 533\\nSteinbach, Michael, George Karypis, and Vipin Kumar. 2000. A comparison of docu-\\nment clustering techniques. In KDD Workshop on Text Mining. 400, 525, 526, 531\\nStrang, Gilbert (ed.). 1986. Introduction to Applied Mathematics. Wellesley-Cambridge\\nPress. 417, 531\\nStrehl, Alexander. 2002. Relationship-based Clustering and Cluster Ensembles for High-\\ndimensional Data Mining. PhD thesis, The University of Texas at Austin. 373, 531\\nStrohman, Trevor, and W. Bruce Croft. 2007. Efﬁcient document retrieval in main\\nmemory. In Proc. SIGIR, pp. 175–182. ACM Press. 47, 522, 531\\nSwanson, Don R. 1988. Historical note: Information retrieval and the future of an\\nillusion. JASIS 39(2):92–98. 173, 193, 531\\nTague-Sutcliffe, Jean, and James Blustein. 1995. A statistical analysis of the TREC-3\\ndata. In Proc. TREC, pp. 385–398. 174, 520, 531\\nTan, Songbo, and Xueqi Cheng. 2007. Using hypothesis margin to boost centroid text\\nclassiﬁer. In Proc. ACM Symposium on Applied Computing, pp. 398–403. ACM Press.\\nDOI: doi.acm.org/10.1145/1244002.1244096. 314, 521, 531\\nTannier, Xavier, and Shlomo Geva. 2005.\\nXML retrieval with a natural language\\ninterface. In Proc. SPIRE, pp. 29–40. 217, 523, 531\\nTao, Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai.\\n2006.\\nLanguage\\nmodel information retrieval with document expansion. In Proc. Human Language\\nTechnology Conference / North American Chapter of the Association for Computational\\nLinguistics, pp. 407–414. 252, 527, 531, 532, 533\\nTaube, Mortimer, and Harold Wooster (eds.). 1958. Information storage and retrieval:\\nTheory, systems, and devices. Columbia University Press. 17, 531, 533\\nTaylor, Michael, Hugo Zaragoza, Nick Craswell, Stephen Robertson, and Chris\\nBurges. 2006. Optimisation methods for ranking functions with multiple parame-\\nters. In Proc. CIKM. ACM Press. 348, 520, 521, 529, 531, 533\\nTeh, Yee Whye, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hier-\\narchical Dirichlet processes. Journal of the American Statistical Association 101(476):\\n1566–1581. 418, 520, 525, 531\\nTheobald, Martin, Holger Bast, Debapriyo Majumdar, Ralf Schenkel, and Gerhard\\nWeikum. 2008. TopX: Efﬁcient and versatile top-k query processing for semistruc-\\ntured data. VLDB Journal 17(1):81–115. 216, 520, 527, 530, 531, 532\\nTheobald, Martin, Ralf Schenkel, and Gerhard Weikum. 2005. An efﬁcient and versa-\\ntile query engine for TopX search. In Proc. VLDB, pp. 625–636. VLDB Endowment.\\n216, 530, 531, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n516\\nBibliography\\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. Estimating the num-\\nber of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society\\nSeries B 63:411–423. 374, 524, 531, 532\\nTishby, Naftali, and Noam Slonim. 2000. Data clustering by Markovian relaxation\\nand the information bottleneck method. In Proc. NIPS, pp. 640–646. 374, 531\\nToda, Hiroyuki, and Ryoji Kataoka. 2005. A search result clustering method using\\ninformatively named entities. In International Workshop on Web Information and Data\\nManagement, pp. 81–86. ACM Press. DOI: doi.acm.org/10.1145/1097047.1097063. 372,\\n525, 531\\nTomasic, Anthony, and Hector Garcia-Molina. 1993. Query processing and inverted\\nindices in shared-nothing document information retrieval systems. VLDB Journal\\n2(3):243–275. 458, 523, 531\\nTombros, Anastasios, and Mark Sanderson.\\n1998.\\nAdvantages of query biased\\nsummaries in information retrieval. In Proc. SIGIR, pp. 2–10. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/290941.290947. 174, 530, 531\\nTombros, Anastasios, Robert Villa, and Cornelis Joost van Rijsbergen.\\n2002.\\nThe\\neffectiveness of query-speciﬁc hierarchic clustering in information retrieval. IP&M\\n38(4):559–582. DOI: dx.doi.org/10.1016/S0306-4573(01)00048-6. 372, 529, 531, 532\\nTomlinson, Stephen. 2003. Lexical and algorithmic stemming compared for 9 Eu-\\nropean languages with Hummingbird Searchserver at CLEF 2003. In Proc. Cross-\\nLanguage Evaluation Forum, pp. 286–300. 46, 531\\nTong, Simon, and Daphne Koller. 2001. Support vector machine active learning with\\napplications to text classiﬁcation. JMLR 2:45–66. 348, 525, 531\\nToutanova, Kristina, and Robert C. Moore. 2002. Pronunciation modeling for im-\\nproved spelling correction. In Proc. ACL, pp. 144–151. 65, 528, 531\\nTreeratpituk, Pucktada, and Jamie Callan. 2006. An experimental study on automat-\\nically labeling hierarchical clusters using statistical features. In Proc. SIGIR, pp.\\n707–708. ACM Press. DOI: doi.acm.org/10.1145/1148170.1148328. 400, 521, 532\\nTrotman, Andrew.\\n2003.\\nCompressing inverted ﬁles.\\nIR 6(1):5–19.\\nDOI:\\ndx.doi.org/10.1023/A:1022949613039. 106, 532\\nTrotman, Andrew, and Shlomo Geva. 2006. Passage retrieval and other XML-retrieval\\ntasks. In SIGIR 2006 Workshop on XML Element Retrieval Methodology, pp. 43–50. 217,\\n523, 532\\nTrotman, Andrew, Shlomo Geva, and Jaap Kamps (eds.). 2007. SIGIR Workshop on\\nFocused Retrieval. University of Otago. 217, 523, 525, 532\\nTrotman, Andrew, Nils Pharo, and Miro Lehtonen. 2006. XML-IR users and use cases.\\nIn Proc. INEX, pp. 400–412. 216, 526, 529, 532\\nTrotman, Andrew, and Börkur Sigurbjörnsson. 2004. Narrowed Extended XPath I\\n(NEXI). In Fuhr et al. (2005), pp. 16–40.\\nDOI: dx.doi.org/10.1007/11424550_2. 217,\\n530, 532\\nTseng, Huihsin, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher\\nManning. 2005. A conditional random ﬁeld word segmenter. In SIGHAN Workshop\\non Chinese Language Processing. 46, 519, 521, 525, 527, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n517\\nTsochantaridis, Ioannis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.\\n2005. Large margin methods for structured and interdependent output variables.\\nJMLR 6:1453–1484. 347, 519, 524, 525, 532\\nTurpin, Andrew, and William R. Hersh. 2001. Why batch and user evaluations do not\\ngive the same results. In Proc. SIGIR, pp. 225–231. 175, 524, 532\\nTurpin, Andrew, and William R. Hersh. 2002. User interface effects in past batch\\nversus user experiments. In Proc. SIGIR, pp. 431–432. 175, 524, 532\\nTurpin, Andrew, Yohannes Tsegay, David Hawking, and Hugh E. Williams. 2007.\\nFast generation of result snippets in web search. In Proc. SIGIR, pp. 127–134. ACM\\nPress. 174, 524, 532, 533\\nTurtle, Howard. 1994. Natural language vs. Boolean query evaluation: A comparison\\nof retrieval performance. In Proc. SIGIR, pp. 212–220. ACM Press. 15, 532\\nTurtle, Howard, and W. Bruce Croft. 1989. Inference networks for document retrieval.\\nIn Proc. SIGIR, pp. 1–24. ACM Press. 234, 522, 532\\nTurtle, Howard, and W. Bruce Croft. 1991. Evaluation of an inference network-based\\nretrieval model. TOIS 9(3):187–222. 234, 522, 532\\nTurtle, Howard, and James Flood. 1995. Query evaluation: strategies and optimiza-\\ntions. IP&M 31(6):831–850. DOI: dx.doi.org/10.1016/0306-4573(95)00020-H. 133, 522,\\n532\\nVaithyanathan, Shivakumar, and Byron Dom. 2000. Model-based hierarchical clus-\\ntering. In Proc. UAI, pp. 599–608. Morgan Kaufmann. 400, 522, 532\\nvan Rijsbergen, Cornelis Joost. 1979. Information Retrieval, 2nd edition. Butterworths.\\n173, 216, 221, 231, 235, 529\\nvan Rijsbergen, Cornelis Joost. 1989. Towards an information logic. In Proc. SIGIR,\\npp. 77–86. ACM Press. DOI: doi.acm.org/10.1145/75334.75344. xxxiv, 529\\nvan Zwol, Roelof, Jeroen Baas, Herre van Oostendorp, and Frans Wiering.\\n2006.\\nBricks: The building blocks to tackle query formulation in structured document\\nretrieval. In Proc. ECIR, pp. 314–325. 217, 519, 528, 532, 533\\nVapnik, Vladimir N. 1998. Statistical Learning Theory. Wiley-Interscience. 346, 532\\nVittaut, Jean-Noël, and Patrick Gallinari. 2006. Machine learning ranking for struc-\\ntured information retrieval. In Proc. ECIR, pp. 338–349. 216, 523, 532\\nVoorhees, Ellen M. 1985a. The cluster hypothesis revisited. In Proc. SIGIR, pp. 188–\\n196. ACM Press. 372, 532\\nVoorhees, Ellen M. 1985b. The effectiveness and efﬁciency of agglomerative hierar-\\nchic clustering in document retrieval. Technical Report TR 85-705, Cornell. 399,\\n532\\nVoorhees, Ellen M. 2000. Variations in relevance judgments and the measurement of\\nretrieval effectiveness. IP&M 36:697–716. 174, 532\\nVoorhees, Ellen M., and Donna Harman (eds.). 2005. TREC: Experiment and Evaluation\\nin Information Retrieval. MIT Press. 173, 314, 498, 509, 524, 532\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n518\\nBibliography\\nWagner, Robert A., and Michael J. Fischer. 1974. The string-to-string correction prob-\\nlem. JACM 21(1):168–173. DOI: doi.acm.org/10.1145/321796.321811. 65, 522, 532\\nWard Jr., J. H. 1963. Hierarchical grouping to optimize an objective function. Journal\\nof the American Statistical Association 58:236–244. 399, 532\\nWei,\\nXing,\\nand\\nW.\\nBruce Croft.\\n2006.\\nLDA-based document models\\nfor ad-hoc retrieval.\\nIn Proc. SIGIR, pp. 178–185. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1148170.1148204. 418, 522, 532\\nWeigend, Andreas S., Erik D. Wiener, and Jan O. Pedersen. 1999. Exploiting hierarchy\\nin text categorization. IR 1(3):193–216. 347, 529, 532\\nWeston, Jason, and Chris Watkins. 1999. Support vector machines for multi-class\\npattern recognition. In Proc. European Symposium on Artiﬁcial Neural Networks, pp.\\n219–224. 347, 532\\nWilliams, Hugh E., and Justin Zobel. 2005. Searchable words on the web. International\\nJournal on Digital Libraries 5(2):99–105.\\nDOI: dx.doi.org/10.1007/s00799-003-0050-z.\\n105, 533\\nWilliams, Hugh E., Justin Zobel, and Dirk Bahle. 2004. Fast phrase querying with\\ncombined indexes. TOIS 22(4):573–594. 43, 519, 533\\nWitten, Ian H., and Timothy C. Bell. 1990. Source models for natural language text.\\nInternational Journal Man-Machine Studies 32(5):545–579. 105, 520, 533\\nWitten, Ian H., and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools\\nand Techniques, 2nd edition. Morgan Kaufmann. 374, 523, 533\\nWitten, Ian H., Alistair Moffat, and Timothy C. Bell. 1999. Managing Gigabytes: Com-\\npressing and Indexing Documents and Images, 2nd edition. Morgan Kaufmann. 18,\\n83, 105, 106, 520, 528, 533\\nWong, S. K. Michael, Yiyu Yao, and Peter Bollmann. 1988. Linear structure in infor-\\nmation retrieval. In Proc. SIGIR, pp. 219–232. ACM Press. 348, 520, 533\\nWoodley, Alan, and Shlomo Geva. 2006. NLPX at INEX 2006. In Proc. INEX, pp.\\n302–311. 217, 523, 533\\nXu, Jinxi, and W. Bruce Croft. 1996. Query expansion using local and global document\\nanalysis. In Proc. SIGIR, pp. 4–11. ACM Press. 194, 522, 533\\nXu, Jinxi, and W. Bruce Croft.\\n1999.\\nCluster-based language models for\\ndistributed retrieval.\\nIn Proc. SIGIR, pp. 254–261. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/312624.312687. 372, 522, 533\\nYang, Hui, and Jamie Callan.\\n2006.\\nNear-duplicate detection by instance-\\nlevel constrained clustering.\\nIn Proc. SIGIR, pp. 421–428. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/1148170.1148243. 373, 521, 533\\nYang, Yiming. 1994. Expert network: Effective and efﬁcient learning from human\\ndecisions in text categorization and retrieval. In Proc. SIGIR, pp. 13–22. ACM Press.\\n314, 533\\nYang, Yiming. 1999. An evaluation of statistical approaches to text categorization. IR\\n1:69–90. 347, 533\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nBibliography\\n519\\nYang, Yiming. 2001. A study of thresholding strategies for text categorization. In\\nProc. SIGIR, pp. 137–145. ACM Press. DOI: doi.acm.org/10.1145/383952.383975. 315,\\n533\\nYang, Yiming, and Bryan Kisiel. 2003. Margin-based local regression for adaptive\\nﬁltering. In Proc. CIKM, pp. 191–198. DOI: doi.acm.org/10.1145/956863.956902. 315,\\n525, 533\\nYang, Yiming, and Xin Liu. 1999. A re-examination of text categorization methods.\\nIn Proc. SIGIR, pp. 42–49. ACM Press. 287, 347, 527, 533\\nYang, Yiming, and Jan Pedersen. 1997. Feature selection in statistical learning of text\\ncategorization. In Proc. ICML. 286, 529, 533\\nYue, Yisong, Thomas Finley, Filip Radlinski, and Thorsten Joachims. 2007. A support\\nvector method for optimizing average precision. In Proc. SIGIR. ACM Press. 348,\\n522, 525, 529, 533\\nZamir, Oren, and Oren Etzioni. 1999. Grouper: A dynamic clustering interface to\\nweb search results. In Proc. WWW, pp. 1361–1374. Elsevier North-Holland. DOI:\\ndx.doi.org/10.1016/S1389-1286(99)00054-7. 372, 400, 522, 533\\nZaragoza, Hugo, Djoerd Hiemstra, Michael Tipping, and Stephen Robertson. 2003.\\nBayesian extension to the language model for ad hoc information retrieval. In Proc.\\nSIGIR, pp. 4–9. ACM Press. 252, 524, 529, 531, 533\\nZavrel, Jakub, Peter Berck, and Willem Lavrijssen. 2000. Information extraction by\\ntext classiﬁcation: Corpus mining for features. In Workshop Information Extraction\\nMeets Corpus Linguistics.\\nURL: www.cnts.ua.ac.be/Publications/2000/ZBL00. Held in\\nconjunction with LREC-2000. 315, 520, 526, 533\\nZha, Hongyuan, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and Horst D. Simon. 2001.\\nBipartite graph partitioning and data clustering. In Proc. CIKM, pp. 25–32. 374,\\n400, 522, 523, 524, 531, 533\\nZhai, Chengxiang, and John Lafferty. 2001a. Model-based feedback in the language\\nmodeling approach to information retrieval. In Proc. CIKM. ACM Press. 250, 526,\\n533\\nZhai, Chengxiang, and John Lafferty. 2001b.\\nA study of smoothing methods for\\nlanguage models applied to ad hoc information retrieval. In Proc. SIGIR, pp. 334–\\n342. ACM Press. 252, 526, 533\\nZhai, ChengXiang, and John Lafferty.\\n2002.\\nTwo-stage language models\\nfor information retrieval.\\nIn Proc. SIGIR, pp. 49–56. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/564376.564387. 252, 526, 533\\nZhang, Jiangong, Xiaohui Long, and Torsten Suel. 2007. Performance of compressed\\ninverted list caching in search engines. In Proc. CIKM. 106, 527, 531, 533\\nZhang, Tong, and Frank J. Oles. 2001. Text categorization based on regularized linear\\nclassiﬁcation methods. IR 4(1):5–31. URL: citeseer.ist.psu.edu/zhang00text.html. 347,\\n528, 533\\nZhao, Ying, and George Karypis.\\n2002.\\nEvaluation of hierarchical clustering al-\\ngorithms for document datasets. In Proc. CIKM, pp. 515–524. ACM Press.\\nDOI:\\ndoi.acm.org/10.1145/584792.584877. 399, 525, 533\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n520\\nBibliography\\nZipf, George Kingsley. 1949. Human Behavior and the Principle of Least Effort. Addison\\nWesley. 105, 533\\nZobel, Justin. 1998. How reliable are the results of large-scale information retrieval\\nexperiments? In Proc. SIGIR, pp. 307–314. 174, 533\\nZobel,\\nJustin,\\nand Philip Dart.\\n1995.\\nFinding approximate matches in\\nlarge lexicons.\\nSoftware Practice and Experience 25(3):331–345.\\nURL:\\ncite-\\nseer.iﬁ.unizh.ch/zobel95ﬁnding.html. 65, 522, 533\\nZobel, Justin, and Philip Dart. 1996. Phonetic string matching: Lessons from infor-\\nmation retrieval. In Proc. SIGIR, pp. 166–173. ACM Press. 65, 522, 533\\nZobel, Justin, and Alistair Moffat. 2006. Inverted ﬁles for text search engines. ACM\\nComputing Surveys 38(2). 18, 83, 106, 133, 528, 533\\nZobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron Sacks-Davis. 1995. Efﬁcient\\nretrieval of partial documents. IP&M 31(3):361–377.\\nDOI: dx.doi.org/10.1016/0306-\\n4573(94)00052-5. 217, 528, 530, 532, 533\\nZukowski, Marcin, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar\\nRAM-CPU cache compression. In Proc. International Conference on Data Engineering,\\np. 59. IEEE Computer Society. DOI: dx.doi.org/10.1109/ICDE.2006.150. 106, 520, 524,\\n528, 533\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\nAberer: Aberer (2001)\\nAhn: Ittner et al. (1995)\\nAizerman: Aizerman et al. (1964)\\nAkaike: Akaike (1974)\\nAllan: Allan (2005), Allan et al. (1998),\\nBuckley et al. (1994a), Buckley\\net al. (1994b), Salton et al. (1993)\\nAllwein: Allwein et al. (2000)\\nAlonso: Alonso et al. (2006)\\nAltingövde: Can et al. (2004)\\nAltingövde: Altingövde et al. (2007)\\nAltun: Tsochantaridis et al. (2005)\\nAmer-Yahia: Amer-Yahia et al. (2006),\\nAmer-Yahia et al. (2005),\\nAmer-Yahia and Lalmas (2006)\\nAmitay: Mass et al. (2003)\\nAnagnostopoulos: Anagnostopoulos\\net al. (2006)\\nAnderberg: Anderberg (1973)\\nAnderson: Burnham and Anderson\\n(2002)\\nAndoni: Andoni et al. (2006)\\nAndrew: Tseng et al. (2005)\\nAnh: Anh et al. (2001), Anh and\\nMoffat (2005), Anh and Moffat\\n(2006a), Anh and Moffat (2006b),\\nAnh and Moffat (2006c)\\nAone: Larsen and Aone (1999)\\nApers: Mihajlovi´c et al. (2005)\\nApté: Apté et al. (1994)\\nArabie: Hubert and Arabie (1985)\\nArthur: Arthur and Vassilvitskii\\n(2006)\\nArvola: Arvola et al. (2005)\\nAslam: Aslam and Yilmaz (2005)\\nAult: Ault and Yang (2002)\\nBaas: van Zwol et al. (2006)\\nBadue: Badue et al. (2001)\\nBaeza-Yates: Badue et al. (2001),\\nBaeza-Yates et al. (2005),\\nBaeza-Yates and Ribeiro-Neto\\n(1999), de Moura et al. (2000),\\nFrakes and Baeza-Yates (1992),\\nHarman et al. (1992), Navarro\\nand Baeza-Yates (1997)\\nBahle: Bahle et al. (2002), Williams\\net al. (2004)\\nBai: Cao et al. (2005)\\nBakiri: Dietterich and Bakiri (1995)\\nBalasubramanyan: Pavlov et al.\\n(2004)\\nBaldridge: Baldridge and Osborne\\n(2004)\\nBaldwin: Hughes et al. (2006)\\nBall: Ball (1965)\\nBanerjee: Alonso et al. (2006), Basu\\net al. (2004)\\nBanko: Banko and Brill (2001)\\nBar-Ilan: Bar-Ilan and Gutman (2005)\\nBar-Yossef: Bar-Yossef and Gurevich\\n(2006)\\nBarbosa: Ribeiro-Neto and Barbosa\\n(1998)\\nBarreiro: Blanco and Barreiro (2006),\\nBlanco and Barreiro (2007)\\nBarroso: Barroso et al. (2003)\\nBartell: Bartell (1994), Bartell et al.\\n(1998)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n522\\nAuthor Index\\nBarzilay: Barzilay and Elhadad\\n(1997), McKeown et al. (2002)\\nBasili: Moschitti and Basili (2004)\\nBast: Bast and Majumdar (2005),\\nTheobald et al. (2008)\\nBasu: Basu et al. (2004)\\nBavaud: Picca et al. (2006)\\nBeal: Teh et al. (2006)\\nBeesley: Beesley (1998), Beesley and\\nKarttunen (2003)\\nBelew: Bartell et al. (1998)\\nBelkin: Koenemann and Belkin (1996)\\nBell: Moffat and Bell (1995), Witten\\nand Bell (1990), Witten et al.\\n(1999)\\nBennett: Bennett (2000)\\nBerck: Zavrel et al. (2000)\\nBerger: Berger and Lafferty (1999)\\nBerkhin: Berkhin (2005), Berkhin\\n(2006a), Berkhin (2006b)\\nBerners-Lee: Berners-Lee et al. (1992)\\nBernstein: Rahm and Bernstein (2001)\\nBerry: Berry and Young (1995), Berry\\net al. (1995), Kent et al. (1955)\\nBetsi: Betsi et al. (2006)\\nBhagavathy: Newsam et al. (2001)\\nBharat: Bharat and Broder (1998),\\nBharat et al. (1998), Bharat et al.\\n(2000), Bharat and Henzinger\\n(1998)\\nBienenstock: Geman et al. (1992)\\nBird: Hughes et al. (2006)\\nBishop: Bishop (2006)\\nBlair: Blair and Maron (1985)\\nBlair-Goldensohn: Radev et al. (2001)\\nBlanco: Blanco and Barreiro (2006),\\nBlanco and Barreiro (2007)\\nBlandford: Blandford and Blelloch\\n(2002)\\nBlei: Blei et al. (2003), Teh et al. (2006)\\nBlelloch: Blandford and Blelloch\\n(2002)\\nBlok: List et al. (2005), Mihajlovi´c\\net al. (2005)\\nBlustein: Tague-Sutcliffe and Blustein\\n(1995)\\nBoldi: Baeza-Yates et al. (2005), Boldi\\net al. (2002), Boldi et al. (2005),\\nBoldi and Vigna (2004a), Boldi\\nand Vigna (2004b), Boldi and\\nVigna (2005)\\nBoley: Boley (1998), Savaresi and\\nBoley (2004)\\nBollmann: Wong et al. (1988)\\nBoncz: Zukowski et al. (2006)\\nBorodin: Borodin et al. (2001)\\nBotev: Amer-Yahia et al. (2006)\\nBourne: Bourne and Ford (1961)\\nBoyce: Meadow et al. (1999)\\nBracken: Lombard et al. (2002)\\nBradley: Bradley and Fayyad (1998),\\nBradley et al. (1998), Fayyad\\net al. (1998)\\nBraverman: Aizerman et al. (1964)\\nBrill: Banko and Brill (2001), Brill and\\nMoore (2000), Cucerzan and Brill\\n(2004), Richardson et al. (2006)\\nBrin: Brin and Page (1998), Page et al.\\n(1998)\\nBrisaboa: Brisaboa et al. (2007)\\nBroder: Anagnostopoulos et al.\\n(2006), Bharat and Broder (1998),\\nBharat et al. (1998), Bharat et al.\\n(2000), Broder (2002), Broder\\net al. (2000), Broder et al. (1997)\\nBrown: Brown (1995), Coden et al.\\n(2002)\\nBuckley: Buckley et al. (1994a),\\nBuckley and Salton (1995),\\nBuckley et al. (1994b), Buckley\\net al. (1995), Buckley and\\nVoorhees (2000), Hersh et al.\\n(1994), Salton et al. (1993), Salton\\nand Buckley (1987), Salton and\\nBuckley (1988), Salton and\\nBuckley (1990), Singhal et al.\\n(1996a), Singhal et al. (1997),\\nSinghal et al. (1995), Singhal et al.\\n(1996b)\\nBurges: Burges et al. (2005), Burges\\n(1998), Taylor et al. (2006)\\nBurner: Burner (1997)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n523\\nBurnham: Burnham and Anderson\\n(2002)\\nBush: Bush (1945)\\nBüttcher: Büttcher and Clarke\\n(2005a), Büttcher and Clarke\\n(2005b), Büttcher and Clarke\\n(2006), Büttcher et al. (2006)\\nCacheda: Cacheda et al. (2003)\\nCailliau: Berners-Lee et al. (1992)\\nCallan: Callan (2000), Lewis et al.\\n(1996), Ogilvie and Callan (2005),\\nSahoo et al. (2006), Treeratpituk\\nand Callan (2006), Yang and\\nCallan (2006)\\nCampbell: Crestani et al. (1998)\\nCan: Altingövde et al. (2007), Can\\net al. (2004), Can and Ozkarahan\\n(1990)\\nCandela: Harman and Candela (1990)\\nCannane: Garcia et al. (2004)\\nCao: Cao et al. (2005), Cao et al.\\n(2006), Gao et al. (2004)\\nCarbonell: Carbonell and Goldstein\\n(1998)\\nCarletta: Carletta (1996)\\nCarmel: Carmel et al. (2001), Carmel\\net al. (2003), Mass et al. (2003)\\nCarneiro: Cacheda et al. (2003)\\nCaruana: Caruana and\\nNiculescu-Mizil (2006)\\nCase: Amer-Yahia et al. (2005)\\nCastellan: Siegel and Castellan (1988)\\nCastillo: Baeza-Yates et al. (2005)\\nCastro: Castro et al. (2004)\\nCavnar: Cavnar and Trenkle (1994)\\nChakrabarti: Chakrabarti (2002),\\nChakrabarti et al. (1998)\\nChan: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b)\\nChang: Sproat et al. (1996), Tseng\\net al. (2005)\\nChapelle: Chapelle et al. (2006)\\nChaudhuri: Chaudhuri et al. (2006)\\nCheeseman: Cheeseman and Stutz\\n(1996)\\nChen: Chen and Lin (2000), Chen\\net al. (2005), Cooper et al. (1994),\\nDumais and Chen (2000),\\nKishida et al. (2005), Kishida\\net al. (2005), Kupiec et al. (1995),\\nLiu et al. (2005)\\nCheng: Tan and Cheng (2007)\\nChiaramella: Chiaramella et al. (1996)\\nChierichetti: Chierichetti et al. (2007)\\nCho: Cho and Garcia-Molina (2002),\\nCho et al. (1998), Ntoulas and\\nCho (2007)\\nChu-Carroll: Chu-Carroll et al. (2006)\\nChurch: Kernighan et al. (1990)\\nClarke: Büttcher and Clarke (2005a),\\nBüttcher and Clarke (2005b),\\nBüttcher and Clarke (2006),\\nBüttcher et al. (2006), Clarke\\net al. (2000)\\nCleverdon: Cleverdon (1991)\\nCoates: Castro et al. (2004)\\nCochran: Snedecor and Cochran\\n(1989)\\nCoden: Coden et al. (2002)\\nCodenotti: Boldi et al. (2002)\\nCohen: Carmel et al. (2001), Cohen\\n(1995), Cohen (1998), Cohen et al.\\n(1998), Cohen and Singer (1999),\\nForman and Cohen (2004)\\nCole: Spink and Cole (2005)\\nComtet: Comtet (1974)\\nCooper: Cooper et al. (1994)\\nCormack: Clarke et al. (2000)\\nCormen: Cormen et al. (1990)\\nCottrell: Bartell et al. (1998)\\nCover: Cover and Hart (1967), Cover\\nand Thomas (1991)\\nCrammer: Crammer and Singer\\n(2001)\\nCraswell: Taylor et al. (2006)\\nCreecy: Creecy et al. (1992)\\nCrestani: Crestani et al. (1998)\\nCristianini: Cristianini and\\nShawe-Taylor (2000), Lodhi et al.\\n(2002), Shawe-Taylor and\\nCristianini (2004)\\nCroft: Croft (1978), Croft and Harper\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n524\\nAuthor Index\\n(1979), Croft and Lafferty (2003),\\nLavrenko and Croft (2001), Liu\\nand Croft (2004), Ponte and Croft\\n(1998), Strohman and Croft\\n(2007), Turtle and Croft (1989),\\nTurtle and Croft (1991), Wei and\\nCroft (2006), Xu and Croft (1996),\\nXu and Croft (1999)\\nCrouch: Crouch (1988)\\nCucerzan: Cucerzan and Brill (2004)\\nCurdy: Picca et al. (2006)\\nCutting: Cutting et al. (1993), Cutting\\net al. (1992)\\nCzuba: Chu-Carroll et al. (2006)\\nDamerau: Apté et al. (1994),\\nDamerau (1964)\\nDart: Zobel and Dart (1995), Zobel\\nand Dart (1996)\\nDas: Chaudhuri et al. (2006)\\nDatar: Andoni et al. (2006)\\nDavidson: Davidson and\\nSatyanarayana (2003)\\nDay: Day and Edelsbrunner (1984)\\nDean: Barroso et al. (2003), Bharat\\net al. (2000), Dean and\\nGhemawat (2004)\\nDeeds: Burges et al. (2005)\\nDeerwester: Deerwester et al. (1990)\\nDemir: Can et al. (2004)\\nDempster: Dempster et al. (1977)\\nDhillon: Dhillon (2001), Dhillon and\\nModha (2001)\\nDi Eugenio: Di Eugenio and Glass\\n(2004)\\nDietterich: Dietterich (2002),\\nDietterich and Bakiri (1995)\\nDing: Zha et al. (2001)\\nDom: Chakrabarti et al. (1998), Dom\\n(2002), Pavlov et al. (2004),\\nVaithyanathan and Dom (2000)\\nDomingos: Domingos (2000),\\nDomingos and Pazzani (1997)\\nDorr: Oard and Dorr (1996)\\nDoursat: Geman et al. (1992)\\nDownie: Downie (2006)\\nDrake: Alonso et al. (2006)\\nDubes: Jain and Dubes (1988)\\nDuboue: Chu-Carroll et al. (2006)\\nDuda: Duda et al. (2000)\\nDumais: Berry et al. (1995),\\nDeerwester et al. (1990), Dumais\\net al. (1998), Dumais (1993),\\nDumais (1995), Dumais and\\nChen (2000), Littman et al. (1998)\\nDuncan: Sahoo et al. (2006)\\nDunning: Dunning (1993), Dunning\\n(1994)\\nDörre: Amer-Yahia et al. (2006)\\nEckart: Eckart and Young (1936)\\nEdelsbrunner: Day and Edelsbrunner\\n(1984)\\nEisenberg: Schamber et al. (1990)\\nEissen: Stein and zu Eissen (2004),\\nStein et al. (2003)\\nEl-Hamdouchi: El-Hamdouchi and\\nWillett (1986)\\nElhadad: Barzilay and Elhadad (1997)\\nElias: Elias (1975)\\nElkan: Hamerly and Elkan (2003)\\nEmerson: Sproat and Emerson (2003)\\nEtzioni: Zamir and Etzioni (1999)\\nEvans: McKeown et al. (2002)\\nEyheramendy: Eyheramendy et al.\\n(2003)\\nFagin: Carmel et al. (2001)\\nFallows: Fallows (2004)\\nFarchi: Carmel et al. (2001)\\nFariña: Brisaboa et al. (2007)\\nFayyad: Bradley and Fayyad (1998),\\nBradley et al. (1998), Fayyad\\net al. (1998)\\nFeldmann: Kammenhuber et al.\\n(2006)\\nFellbaum: Fellbaum (1998)\\nFerragina: Ferragina and Venturini\\n(2007)\\nFerrucci: Chu-Carroll et al. (2006)\\nFinley: Yue et al. (2007)\\nFischer: Wagner and Fischer (1974)\\nFlach: Gaertner et al. (2002)\\nFlake: Glover et al. (2002b)\\nFlood: Turtle and Flood (1995)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n525\\nFlynn: Jain et al. (1999)\\nFord: Bourne and Ford (1961)\\nForman: Forman (2004), Forman\\n(2006), Forman and Cohen (2004)\\nFourel: Chiaramella et al. (1996)\\nFowlkes: Fowlkes and Mallows\\n(1983)\\nFox: Fox and Lee (1991), Harman\\net al. (1992), Lee and Fox (1988)\\nFraenkel: Fraenkel and Klein (1985)\\nFrakes: Frakes and Baeza-Yates (1992)\\nFraley: Fraley and Raftery (1998)\\nFrank: Witten and Frank (2005)\\nFrei: Qiu and Frei (1993)\\nFrieder: Grossman and Frieder (2004)\\nFriedl: Friedl (2006)\\nFriedman: Friedman (1997),\\nFriedman and Goldszmidt\\n(1996), Hastie et al. (2001)\\nFuhr: Fuhr (1989), Fuhr (1992), Fuhr\\net al. (2003a), Fuhr and\\nGroßjohann (2004), Fuhr and\\nLalmas (2007), Fuhr et al. (2006),\\nFuhr et al. (2005), Fuhr et al.\\n(2007), Fuhr et al. (2003b), Fuhr\\nand Pfeifer (1994), Fuhr and\\nRölleke (1997)\\nFurnas: Deerwester et al. (1990)\\nGaertner: Gaertner et al. (2002)\\nGale: Kernighan et al. (1990), Sproat\\net al. (1996)\\nGallinari: Vittaut and Gallinari (2006)\\nGao: Gao et al. (2005), Gao et al.\\n(2004)\\nGarcia: Garcia et al. (2004)\\nGarcia-Molina: Cho and\\nGarcia-Molina (2002), Cho et al.\\n(1998), Garcia-Molina et al.\\n(1999), Hirai et al. (2000), Melnik\\net al. (2001), Tomasic and\\nGarcia-Molina (1993)\\nGarﬁeld: Garﬁeld (1955), Garﬁeld\\n(1976)\\nGay: Joachims et al. (2005)\\nGeman: Geman et al. (1992)\\nGeng: Geng et al. (2007)\\nGerrand: Gerrand (2007)\\nGeva: Tannier and Geva (2005),\\nTrotman and Geva (2006),\\nTrotman et al. (2007), Woodley\\nand Geva (2006)\\nGey: Cooper et al. (1994), Gey (1994)\\nGhamrawi: Ghamrawi and\\nMcCallum (2005)\\nGhemawat: Dean and Ghemawat\\n(2004)\\nGibson: Chakrabarti et al. (1998)\\nGiles: Lawrence and Giles (1998),\\nLawrence and Giles (1999),\\nRusmevichientong et al. (2001)\\nGlass: Di Eugenio and Glass (2004)\\nGlassman: Broder et al. (1997)\\nGlover: Glover et al. (2002a), Glover\\net al. (2002b)\\nGoldstein: Carbonell and Goldstein\\n(1998)\\nGoldszmidt: Friedman and\\nGoldszmidt (1996)\\nGrabs: Grabs and Schek (2002)\\nGraepel: Herbrich et al. (2000)\\nGranka: Joachims et al. (2005)\\nGravano: Hatzivassiloglou et al.\\n(2000)\\nGreiff: Greiff (1998)\\nGrifﬁths: Rosen-Zvi et al. (2004)\\nGrinstead: Grinstead and Snell (1997)\\nGroff: Berners-Lee et al. (1992)\\nGrossman: Grossman and Frieder\\n(2004)\\nGroßjohann: Fuhr and Großjohann\\n(2004)\\nGu: Zha et al. (2001)\\nGuerrero: Cacheda et al. (2003)\\nGupta: Smeulders et al. (2000)\\nGurevich: Bar-Yossef and Gurevich\\n(2006)\\nGusﬁeld: Gusﬁeld (1997)\\nGutman: Bar-Ilan and Gutman (2005)\\nGövert: Fuhr et al. (2003a), Gövert\\nand Kazai (2003)\\nHamerly: Hamerly and Elkan (2003)\\nHamilton: Burges et al. (2005)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n526\\nAuthor Index\\nHan: Han and Karypis (2000)\\nHand: Hand (2006), Hand and Yu\\n(2001)\\nHarman: Harman (1991), Harman\\n(1992), Harman et al. (1992),\\nHarman and Candela (1990),\\nVoorhees and Harman (2005)\\nHarold: Harold and Means (2004)\\nHarper: Croft and Harper (1979),\\nMuresan and Harper (2004)\\nHarshman: Deerwester et al. (1990)\\nHart: Cover and Hart (1967), Duda\\net al. (2000)\\nHarter: Harter (1998)\\nHartigan: Hartigan and Wong (1979)\\nHastie: Hastie et al. (2001), Tibshirani\\net al. (2001)\\nHatzivassiloglou: Hatzivassiloglou\\net al. (2000), McKeown et al.\\n(2002)\\nHaveliwala: Haveliwala (2003),\\nHaveliwala (2002)\\nHawking: Turpin et al. (2007)\\nHayes: Hayes and Weinstein (1990)\\nHe: Zha et al. (2001)\\nHeaps: Heaps (1978)\\nHearst: Hearst (1997), Hearst (2006),\\nHearst and Pedersen (1996),\\nHearst and Plaunt (1993)\\nHeckerman: Dumais et al. (1998)\\nHeinz: Heinz and Zobel (2003), Heinz\\net al. (2002)\\nHeman: Zukowski et al. (2006)\\nHembrooke: Joachims et al. (2005)\\nHenzinger: Bharat et al. (1998),\\nBharat et al. (2000), Bharat and\\nHenzinger (1998), Henzinger\\net al. (2000), Silverstein et al.\\n(1999)\\nHerbrich: Herbrich et al. (2000)\\nHerscovici: Carmel et al. (2001)\\nHersh: Hersh et al. (1994), Hersh\\net al. (2000a), Hersh et al. (2001),\\nHersh et al. (2000b), Turpin and\\nHersh (2001), Turpin and Hersh\\n(2002)\\nHeydon: Henzinger et al. (2000),\\nNajork and Heydon (2001),\\nNajork and Heydon (2002)\\nHickam: Hersh et al. (1994)\\nHiemstra: Hiemstra (1998), Hiemstra\\n(2000), Hiemstra and Kraaij\\n(2005), Kraaij et al. (2002), List\\net al. (2005), Mihajlovi´c et al.\\n(2005), Zaragoza et al. (2003)\\nHirai: Hirai et al. (2000)\\nHofmann: Hofmann (1999a),\\nHofmann (1999b), Tsochantaridis\\net al. (2005)\\nHollink: Hollink et al. (2004)\\nHon: Cao et al. (2006)\\nHopcroft: Hopcroft et al. (2000)\\nHristidis: Chaudhuri et al. (2006)\\nHuang: Cao et al. (2006), Gao et al.\\n(2005), Huang and Mitchell\\n(2006)\\nHubert: Hubert and Arabie (1985)\\nHughes: Hughes et al. (2006)\\nHull: Hull (1993), Hull (1996),\\nSchütze et al. (1995)\\nHullender: Burges et al. (2005)\\nHölzle: Barroso et al. (2003)\\nIde: Ide (1971)\\nImmorlica: Andoni et al. (2006)\\nIndyk: Andoni et al. (2006), Indyk\\n(2004)\\nIngwersen: Ingwersen and Järvelin\\n(2005)\\nIsahara: Murata et al. (2000)\\nIttner: Ittner et al. (1995)\\nIttycheriah: Lita et al. (2003)\\nIwayama: Iwayama and Tokunaga\\n(1995)\\nJärvelin: Ingwersen and Järvelin\\n(2005)\\nJackson: Jackson and Moulinier\\n(2002)\\nJacobs: Jacobs and Rau (1990)\\nJain: Jain et al. (1999), Jain and Dubes\\n(1988), Smeulders et al. (2000)\\nJansen: Spink et al. (2000)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n527\\nJardine: Jardine and van Rijsbergen\\n(1971)\\nJeh: Jeh and Widom (2003)\\nJensen: Jensen and Jensen (2001),\\nJensen and Jensen (2001)\\nJeong: Jeong and Omiecinski (1995)\\nJi: Ji and Xu (2006)\\nJing: Jing (2000)\\nJoachims: Joachims (1997), Joachims\\n(1998), Joachims (1999), Joachims\\n(2002a), Joachims (2002b),\\nJoachims (2006a), Joachims\\n(2006b), Joachims et al. (2005),\\nTsochantaridis et al. (2005), Yue\\net al. (2007)\\nJohnson: Johnson et al. (2006)\\nJones: Lewis and Jones (1996),\\nRobertson and Jones (1976),\\nSpärck Jones (1972), Spärck Jones\\n(2004), Spärck Jones et al. (2000)\\nJordan: Blei et al. (2003), Ng and\\nJordan (2001), Ng et al. (2001a),\\nNg et al. (2001b), Teh et al. (2006)\\nJr: Kent et al. (1955)\\nJunkkari: Arvola et al. (2005)\\nJurafsky: Jurafsky and Martin (2008),\\nTseng et al. (2005)\\nJärvelin: Järvelin and Kekäläinen\\n(2002), Kekäläinen and Järvelin\\n(2002)\\nKalita: Kołcz et al. (2000)\\nKambhatla: Lita et al. (2003)\\nKammenhuber: Kammenhuber et al.\\n(2006)\\nKamps: Hollink et al. (2004), Kamps\\net al. (2004), Kamps et al. (2006),\\nLalmas et al. (2007),\\nSigurbjörnsson et al. (2004),\\nTrotman et al. (2007)\\nKamvar: Kamvar et al. (2002)\\nKando: Kishida et al. (2005)\\nKannan: Kannan et al. (2000)\\nKantor: Saracevic and Kantor (1988),\\nSaracevic and Kantor (1996)\\nKapur: Pavlov et al. (2004)\\nKarger: Cutting et al. (1993), Cutting\\net al. (1992), Rennie et al. (2003)\\nKarttunen: Beesley and Karttunen\\n(2003)\\nKarypis: Han and Karypis (2000),\\nSteinbach et al. (2000), Zhao and\\nKarypis (2002)\\nKaszkiel: Kaszkiel and Zobel (1997)\\nKataoka: Toda and Kataoka (2005)\\nKaufman: Kaufman and Rousseeuw\\n(1990)\\nKazai: Fuhr et al. (2003a), Fuhr et al.\\n(2006), Gövert and Kazai (2003),\\nKazai and Lalmas (2006), Lalmas\\net al. (2007)\\nKeerthi: Sindhwani and Keerthi\\n(2006)\\nKekäläinen: Arvola et al. (2005),\\nJärvelin and Kekäläinen (2002),\\nKekäläinen (2005), Kekäläinen\\nand Järvelin (2002)\\nKemeny: Kemeny and Snell (1976)\\nKent: Kent et al. (1955)\\nKernighan: Kernighan et al. (1990)\\nKhachiyan: Kozlov et al. (1979)\\nKing: King (1967)\\nKishida: Kishida et al. (2005)\\nKisiel: Yang and Kisiel (2003)\\nKlavans: McKeown et al. (2002)\\nKlein: Fraenkel and Klein (1985),\\nKamvar et al. (2002), Klein and\\nManning (2002)\\nKleinberg: Chakrabarti et al. (1998),\\nKleinberg (1997), Kleinberg\\n(1999), Kleinberg (2002)\\nKnuth: Knuth (1997)\\nKo: Ko et al. (2004)\\nKoenemann: Koenemann and Belkin\\n(1996)\\nKoller: Koller and Sahami (1997),\\nTong and Koller (2001)\\nKonheim: Konheim (1981)\\nKorfhage: Korfhage (1997)\\nKozlov: Kozlov et al. (1979)\\nKołcz: Kołcz et al. (2000), Kołcz and\\nYih (2007)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n528\\nAuthor Index\\nKraaij: Hiemstra and Kraaij (2005),\\nKraaij and Spitters (2003), Kraaij\\net al. (2002)\\nKraemer: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b)\\nKraft: Meadow et al. (1999)\\nKretser: Anh et al. (2001)\\nKrippendorff: Krippendorff (2003)\\nKrishnan: McLachlan and Krishnan\\n(1996), Sahoo et al. (2006)\\nKrovetz: Glover et al. (2002a),\\nKrovetz (1995)\\nKuhns: Maron and Kuhns (1960)\\nKukich: Kukich (1992)\\nKumar: Bharat et al. (1998), Broder\\net al. (2000), Kumar et al. (1999),\\nKumar et al. (2000), Steinbach\\net al. (2000)\\nKupiec: Kupiec et al. (1995)\\nKuriyama: Kishida et al. (2005)\\nKurland: Kurland and Lee (2004)\\nKwok: Luk and Kwok (2002)\\nKäki: Käki (2005)\\nLacker: Perkins et al. (2003)\\nLafferty: Berger and Lafferty (1999),\\nCroft and Lafferty (2003),\\nLafferty and Zhai (2001), Lafferty\\nand Zhai (2003), Zhai and\\nLafferty (2001a), Zhai and\\nLafferty (2001b), Zhai and\\nLafferty (2002)\\nLai: Qin et al. (2007)\\nLaird: Dempster et al. (1977)\\nLalmas: Amer-Yahia and Lalmas\\n(2006), Betsi et al. (2006), Crestani\\net al. (1998), Fuhr et al. (2003a),\\nFuhr and Lalmas (2007), Fuhr\\net al. (2006), Fuhr et al. (2005),\\nFuhr et al. (2007), Fuhr et al.\\n(2003b), Kazai and Lalmas\\n(2006), Lalmas et al. (2007),\\nLalmas and Tombros (2007),\\nRuthven and Lalmas (2003)\\nLance: Lance and Williams (1967)\\nLandauer: Deerwester et al. (1990),\\nLittman et al. (1998)\\nLangville: Langville and Meyer\\n(2006)\\nLarsen: Larsen and Aone (1999)\\nLarson: Larson (2005)\\nLavrenko: Allan et al. (1998),\\nLavrenko and Croft (2001)\\nLavrijssen: Zavrel et al. (2000)\\nLawrence: Glover et al. (2002a),\\nGlover et al. (2002b), Lawrence\\nand Giles (1998), Lawrence and\\nGiles (1999), Rusmevichientong\\net al. (2001)\\nLazier: Burges et al. (2005)\\nLee: Fox and Lee (1991), Harman\\net al. (1992), Kishida et al. (2005),\\nKurland and Lee (2004), Lee and\\nFox (1988)\\nLeek: Miller et al. (1999)\\nLehtonen: Trotman et al. (2006)\\nLeiserson: Cormen et al. (1990)\\nLempel: Lempel and Moran (2000)\\nLeone: Hersh et al. (1994)\\nLesk: Lesk (1988), Lesk (2004)\\nLester: Lester et al. (2005), Lester\\net al. (2006)\\nLevenshtein: Levenshtein (1965)\\nLew: Lew (2001)\\nLewis: Eyheramendy et al. (2003),\\nIttner et al. (1995), Lewis (1995),\\nLewis (1998), Lewis and Jones\\n(1996), Lewis and Ringuette\\n(1994), Lewis et al. (1996), Lewis\\net al. (2004)\\nLi: Cao et al. (2006), Gao et al. (2005),\\nGeng et al. (2007), Lewis et al.\\n(2004), Li and Yang (2003), Qin\\net al. (2007)\\nLiddy: Liddy (2005)\\nLin: Chen and Lin (2000), Chen et al.\\n(2005)\\nList: List et al. (2005)\\nLita: Lita et al. (2003)\\nLittman: Littman et al. (1998)\\nLiu: Cao et al. (2006), Geng et al.\\n(2007), Liu et al. (2005), Liu and\\nCroft (2004), Qin et al. (2007),\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n529\\nRiezler et al. (2007), Yang and Liu\\n(1999)\\nLloyd: Gaertner et al. (2002), Lloyd\\n(1982)\\nLodhi: Lodhi et al. (2002)\\nLombard: Lombard et al. (2002)\\nLong: Long and Suel (2003), Zhang\\net al. (2007)\\nLovins: Lovins (1968)\\nLu: Lu et al. (2007)\\nLuehrs: Kent et al. (1955)\\nLuhn: Luhn (1957), Luhn (1958)\\nLuk: Luk and Kwok (2002)\\nLunde: Lunde (1998)\\nLushman: Büttcher et al. (2006)\\nLuxenburger: Kammenhuber et al.\\n(2006)\\nMa: Liu et al. (2005), Murata et al.\\n(2000), Song et al. (2005)\\nMaarek: Carmel et al. (2001), Carmel\\net al. (2003), Mass et al. (2003)\\nMacFarlane: Lu et al. (2007),\\nMacFarlane et al. (2000)\\nMacKinlay: Hughes et al. (2006)\\nMacQueen: MacQueen (1967)\\nMadigan: Eyheramendy et al. (2003)\\nMaganti: Hatzivassiloglou et al.\\n(2000)\\nMaghoul: Broder et al. (2000)\\nMahabhashyam: Singitham et al.\\n(2004)\\nMajumdar: Bast and Majumdar\\n(2005), Theobald et al. (2008)\\nMalhotra: Johnson et al. (2006)\\nMalik: Fuhr et al. (2006), Fuhr et al.\\n(2005), Fuhr et al. (2003b)\\nMallows: Fowlkes and Mallows\\n(1983)\\nManasse: Broder et al. (1997)\\nMandelbrod: Carmel et al. (2003),\\nMass et al. (2003)\\nManjunath: Newsam et al. (2001)\\nManning: Kamvar et al. (2002), Klein\\nand Manning (2002), Manning\\nand Schütze (1999), Tseng et al.\\n(2005)\\nMarais: Silverstein et al. (1999)\\nMaron: Blair and Maron (1985),\\nMaron and Kuhns (1960)\\nMartin: Jurafsky and Martin (2008)\\nMarx: Kamps et al. (2006)\\nMasand: Creecy et al. (1992)\\nMass: Carmel et al. (2003), Mass et al.\\n(2003)\\nMcBryan: McBryan (1994)\\nMcCallum: Ghamrawi and\\nMcCallum (2005), McCallum and\\nNigam (1998), McCallum et al.\\n(1998), McCallum (1996), Nigam\\net al. (2006)\\nMcCann: MacFarlane et al. (2000)\\nMcKeown: McKeown and Radev\\n(1995), McKeown et al. (2002)\\nMcLachlan: McLachlan and Krishnan\\n(1996)\\nMeadow: Meadow et al. (1999)\\nMeans: Harold and Means (2004)\\nMei: Tao et al. (2006)\\nMeil˘a: Meil˘a (2005)\\nMelnik: Melnik et al. (2001)\\nMeuss: Schlieder and Meuss (2002)\\nMeyer: Langville and Meyer (2006)\\nMihajlovi´c: Mihajlovi´c et al. (2005)\\nMihajlovic: List et al. (2005)\\nMiller: Miller et al. (1999)\\nMinsky: Minsky and Papert (1988)\\nMirrokni: Andoni et al. (2006)\\nMitchell: Huang and Mitchell (2006),\\nMcCallum et al. (1998), Mitchell\\n(1997), Nigam et al. (2006)\\nMitra: Buckley et al. (1995), Singhal\\net al. (1996a), Singhal et al. (1997)\\nMittal: Riezler et al. (2007)\\nMitzenmacher: Henzinger et al.\\n(2000)\\nModha: Dhillon and Modha (2001)\\nMoffat: Anh et al. (2001), Anh and\\nMoffat (2005), Anh and Moffat\\n(2006a), Anh and Moffat (2006b),\\nAnh and Moffat (2006c), Lester\\net al. (2005), Moffat and Bell\\n(1995), Moffat and Stuiver (1996),\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n530\\nAuthor Index\\nMoffat and Zobel (1992), Moffat\\nand Zobel (1996), Moffat and\\nZobel (1998), Witten et al. (1999),\\nZobel and Moffat (2006), Zobel\\net al. (1995)\\nMonz: Hollink et al. (2004)\\nMooers: Mooers (1961), Mooers\\n(1950)\\nMooney: Basu et al. (2004)\\nMoore: Brill and Moore (2000), Pelleg\\nand Moore (1999), Pelleg and\\nMoore (2000), Toutanova and\\nMoore (2002)\\nMoran: Lempel and Moran (2000)\\nMoricz: Silverstein et al. (1999)\\nMoschitti: Moschitti (2003), Moschitti\\nand Basili (2004)\\nMotwani: Hopcroft et al. (2000), Page\\net al. (1998)\\nMoulinier: Jackson and Moulinier\\n(2002)\\nMoura: de Moura et al. (2000),\\nRibeiro-Neto et al. (1999)\\nMulhem: Chiaramella et al. (1996)\\nMurata: Murata et al. (2000)\\nMuresan: Muresan and Harper (2004)\\nMurtagh: Murtagh (1983)\\nMurty: Jain et al. (1999)\\nMyaeng: Kishida et al. (2005)\\nNajork: Henzinger et al. (2000),\\nNajork and Heydon (2001),\\nNajork and Heydon (2002)\\nNarin: Pinski and Narin (1976)\\nNavarro: Brisaboa et al. (2007),\\nde Moura et al. (2000), Navarro\\nand Baeza-Yates (1997)\\nNenkova: McKeown et al. (2002)\\nNes: Zukowski et al. (2006)\\nNeubert: Ribeiro-Neto et al. (1999)\\nNewsam: Newsam et al. (2001)\\nNg: Blei et al. (2003), McCallum et al.\\n(1998), Ng and Jordan (2001), Ng\\net al. (2001a), Ng et al. (2001b)\\nNicholson: Hughes et al. (2006)\\nNiculescu-Mizil: Caruana and\\nNiculescu-Mizil (2006)\\nNie: Cao et al. (2005), Gao et al. (2004)\\nNigam: McCallum and Nigam (1998),\\nNigam et al. (2006)\\nNilan: Schamber et al. (1990)\\nNowak: Castro et al. (2004)\\nNtoulas: Ntoulas and Cho (2007)\\nO’Brien: Berry et al. (1995)\\nO’Keefe: O’Keefe and Trotman (2004)\\nOard: Oard and Dorr (1996)\\nObermayer: Herbrich et al. (2000)\\nOcalan: Altingövde et al. (2007)\\nOgilvie: Ogilvie and Callan (2005)\\nOles: Zhang and Oles (2001)\\nOlson: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b)\\nOmiecinski: Jeong and Omiecinski\\n(1995)\\nOostendorp: van Zwol et al. (2006)\\nOrlando: Silvestri et al. (2004)\\nOsborne: Baldridge and Osborne\\n(2004)\\nOsi´nski: Osi´nski and Weiss (2005)\\nOzaku: Murata et al. (2000)\\nOzcan: Altingövde et al. (2007)\\nOzkarahan: Can and Ozkarahan\\n(1990)\\nOzmultu: Spink et al. (2000)\\nPadman: Sahoo et al. (2006)\\nPaepcke: Hirai et al. (2000)\\nPage: Brin and Page (1998), Cho et al.\\n(1998), Page et al. (1998)\\nPaice: Paice (1990)\\nPan: Joachims et al. (2005)\\nPanconesi: Chierichetti et al. (2007)\\nPapert: Minsky and Papert (1988)\\nPapineni: Papineni (2001)\\nPapka: Allan et al. (1998), Lewis et al.\\n(1996)\\nParamá: Brisaboa et al. (2007)\\nParikh: Pavlov et al. (2004)\\nPark: Ko et al. (2004)\\nPavlov: Pavlov et al. (2004)\\nPazzani: Domingos and Pazzani\\n(1997)\\nPedersen: Cutting et al. (1993),\\nCutting et al. (1992), Hearst and\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n531\\nPedersen (1996), Kupiec et al.\\n(1995), Schütze et al. (1995),\\nSchütze and Pedersen (1995),\\nWeigend et al. (1999), Yang and\\nPedersen (1997)\\nPehcevski: Lalmas et al. (2007)\\nPelleg: Pelleg and Moore (1999),\\nPelleg and Moore (2000)\\nPennock: Glover et al. (2002a), Glover\\net al. (2002b), Rusmevichientong\\net al. (2001)\\nPerego: Silvestri et al. (2004)\\nPerkins: Perkins et al. (2003)\\nPerry: Kent et al. (1955)\\nPersin: Persin (1994), Persin et al.\\n(1996)\\nPeterson: Peterson (1980)\\nPfeifer: Fuhr and Pfeifer (1994)\\nPharo: Trotman et al. (2006)\\nPicca: Picca et al. (2006)\\nPinski: Pinski and Narin (1976)\\nPirolli: Pirolli (2007)\\nPiwowarski: Lalmas et al. (2007)\\nPlatt: Dumais et al. (1998), Platt (2000)\\nPlaunt: Hearst and Plaunt (1993)\\nPollermann: Berners-Lee et al. (1992)\\nPonte: Ponte and Croft (1998)\\nPopescul: Popescul and Ungar (2000)\\nPorter: Porter (1980)\\nPrabakarmurthi: Kołcz et al. (2000)\\nPrager: Chu-Carroll et al. (2006)\\nPrakash: Richardson et al. (2006)\\nPrice: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b)\\nPugh: Pugh (1990)\\nPunera: Anagnostopoulos et al.\\n(2006)\\nQin: Geng et al. (2007), Qin et al.\\n(2007)\\nQiu: Qiu and Frei (1993)\\nR Development Core Team: R\\nDevelopment Core Team (2005)\\nRadev: McKeown and Radev (1995),\\nRadev et al. (2001)\\nRadlinski: Yue et al. (2007)\\nRaftery: Fraley and Raftery (1998)\\nRaghavan: Broder et al. (2000),\\nChakrabarti et al. (1998),\\nChierichetti et al. (2007), Hirai\\net al. (2000), Kumar et al. (1999),\\nKumar et al. (2000), Melnik et al.\\n(2001), Radev et al. (2001),\\nSingitham et al. (2004)\\nRahm: Rahm and Bernstein (2001)\\nRajagopalan: Broder et al. (2000),\\nChakrabarti et al. (1998), Kumar\\net al. (1999), Kumar et al. (2000)\\nRamírez: List et al. (2005)\\nRand: Rand (1971)\\nRasmussen: Rasmussen (1992)\\nRau: Jacobs and Rau (1990)\\nReina: Bradley et al. (1998), Fayyad\\net al. (1998)\\nRennie: Rennie et al. (2003)\\nRenshaw: Burges et al. (2005)\\nRibeiro-Neto: Badue et al. (2001),\\nBaeza-Yates and Ribeiro-Neto\\n(1999), Ribeiro-Neto et al. (1999),\\nRibeiro-Neto and Barbosa (1998)\\nRice: Rice (2006)\\nRichardson: Richardson et al. (2006)\\nRiezler: Riezler et al. (2007)\\nRijke: Hollink et al. (2004), Kamps\\net al. (2004), Kamps et al. (2006),\\nSigurbjörnsson et al. (2004)\\nRijsbergen: Crestani et al. (1998),\\nJardine and van Rijsbergen\\n(1971), Tombros et al. (2002),\\nvan Rijsbergen (1979),\\nvan Rijsbergen (1989)\\nRinguette: Lewis and Ringuette\\n(1994)\\nRipley: Ripley (1996)\\nRivest: Cormen et al. (1990)\\nRoberts: Borodin et al. (2001)\\nRobertson: Lalmas et al. (2007), Lu\\net al. (2007), MacFarlane et al.\\n(2000), Robertson (2005),\\nRobertson et al. (2004), Robertson\\nand Jones (1976), Spärck Jones\\net al. (2000), Taylor et al. (2006),\\nZaragoza et al. (2003)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n532\\nAuthor Index\\nRocchio: Rocchio (1971)\\nRoget: Roget (1946)\\nRose: Lewis et al. (2004)\\nRosen-Zvi: Rosen-Zvi et al. (2004)\\nRosenfeld: McCallum et al. (1998)\\nRosenthal: Borodin et al. (2001)\\nRoss: Ross (2006)\\nRoukos: Lita et al. (2003)\\nRousseeuw: Kaufman and\\nRousseeuw (1990)\\nRozonoér: Aizerman et al. (1964)\\nRubin: Dempster et al. (1977)\\nRusmevichientong:\\nRusmevichientong et al. (2001)\\nRuthven: Ruthven and Lalmas (2003)\\nRölleke: Amer-Yahia et al. (2005),\\nFuhr and Rölleke (1997)\\nSable: McKeown et al. (2002)\\nSacherek: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b)\\nSacks-Davis: Persin et al. (1996),\\nZobel et al. (1995)\\nSahami: Dumais et al. (1998), Koller\\nand Sahami (1997)\\nSahoo: Sahoo et al. (2006)\\nSakai: Sakai (2007)\\nSalton: Buckley et al. (1994a), Buckley\\nand Salton (1995), Buckley et al.\\n(1994b), Salton (1971a), Salton\\n(1971b), Salton (1975), Salton\\n(1989), Salton (1991), Salton et al.\\n(1993), Salton and Buckley\\n(1987), Salton and Buckley\\n(1988), Salton and Buckley\\n(1990), Singhal et al. (1995),\\nSinghal et al. (1996b)\\nSanderson: Tombros and Sanderson\\n(1998)\\nSantini: Boldi et al. (2002), Boldi et al.\\n(2005), Smeulders et al. (2000)\\nSaracevic: Saracevic and Kantor\\n(1988), Saracevic and Kantor\\n(1996)\\nSatyanarayana: Davidson and\\nSatyanarayana (2003)\\nSaunders: Lodhi et al. (2002)\\nSavaresi: Savaresi and Boley (2004)\\nSchamber: Schamber et al. (1990)\\nSchapire: Allwein et al. (2000), Cohen\\net al. (1998), Lewis et al. (1996),\\nSchapire (2003), Schapire and\\nSinger (2000), Schapire et al.\\n(1998)\\nSchek: Grabs and Schek (2002)\\nSchenkel: Theobald et al. (2008),\\nTheobald et al. (2005)\\nSchiffman: McKeown et al. (2002)\\nSchlieder: Schlieder and Meuss (2002)\\nScholer: Scholer et al. (2002)\\nSchwartz: Miller et al. (1999)\\nSchwarz: Schwarz (1978)\\nSchölkopf: Chen et al. (2005),\\nSchölkopf and Smola (2001)\\nSchütze: Manning and Schütze\\n(1999), Schütze (1998), Schütze\\net al. (1995), Schütze and\\nPedersen (1995), Schütze and\\nSilverstein (1997)\\nSebastiani: Sebastiani (2002)\\nSeo: Ko et al. (2004)\\nShaked: Burges et al. (2005)\\nShanmugasundaram: Amer-Yahia\\net al. (2006), Amer-Yahia et al.\\n(2005)\\nShawe-Taylor: Cristianini and\\nShawe-Taylor (2000), Lodhi et al.\\n(2002), Shawe-Taylor and\\nCristianini (2004)\\nShih: Rennie et al. (2003), Sproat et al.\\n(1996)\\nShkapenyuk: Shkapenyuk and Suel\\n(2002)\\nSiegel: Siegel and Castellan (1988)\\nSifry: Sifry (2007)\\nSigelman: McKeown et al. (2002)\\nSigurbjörnsson: Kamps et al. (2004),\\nKamps et al. (2006),\\nSigurbjörnsson et al. (2004),\\nTrotman and Sigurbjörnsson\\n(2004)\\nSilverstein: Schütze and Silverstein\\n(1997), Silverstein et al. (1999)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n533\\nSilvestri: Silvestri (2007), Silvestri\\net al. (2004)\\nSimon: Zha et al. (2001)\\nSindhwani: Sindhwani and Keerthi\\n(2006)\\nSinger: Allwein et al. (2000), Cohen\\net al. (1998), Cohen and Singer\\n(1999), Crammer and Singer\\n(2001), Schapire and Singer\\n(2000), Schapire et al. (1998)\\nSinghal: Buckley et al. (1995),\\nSchapire et al. (1998), Singhal\\net al. (1996a), Singhal et al.\\n(1997), Singhal et al. (1995),\\nSinghal et al. (1996b)\\nSingitham: Singitham et al. (2004)\\nSivakumar: Kumar et al. (2000)\\nSlonim: Tishby and Slonim (2000)\\nSmeulders: Smeulders et al. (2000)\\nSmith: Creecy et al. (1992)\\nSmola: Schölkopf and Smola (2001)\\nSmyth: Rosen-Zvi et al. (2004)\\nSneath: Sneath and Sokal (1973)\\nSnedecor: Snedecor and Cochran\\n(1989)\\nSnell: Grinstead and Snell (1997),\\nKemeny and Snell (1976)\\nSnyder-Duch: Lombard et al. (2002)\\nSoffer: Carmel et al. (2001), Carmel\\net al. (2003), Mass et al. (2003)\\nSokal: Sneath and Sokal (1973)\\nSomogyi: Somogyi (1990)\\nSong: Song et al. (2005)\\nSornil: Sornil (2001)\\nSozio: Chierichetti et al. (2007)\\nSpink: Spink and Cole (2005), Spink\\net al. (2000)\\nSpitters: Kraaij and Spitters (2003)\\nSproat: Sproat and Emerson (2003),\\nSproat et al. (1996), Sproat (1992)\\nSrinivasan: Coden et al. (2002)\\nStata: Broder et al. (2000)\\nStein: Stein and zu Eissen (2004),\\nStein et al. (2003)\\nSteinbach: Steinbach et al. (2000)\\nSteyvers: Rosen-Zvi et al. (2004)\\nStork: Duda et al. (2000)\\nStrang: Strang (1986)\\nStrehl: Strehl (2002)\\nStrohman: Strohman and Croft (2007)\\nStuiver: Moffat and Stuiver (1996)\\nStutz: Cheeseman and Stutz (1996)\\nSuel: Long and Suel (2003),\\nShkapenyuk and Suel (2002),\\nZhang et al. (2007)\\nSwanson: Swanson (1988)\\nSzlávik: Fuhr et al. (2005)\\nTague-Sutcliffe: Tague-Sutcliffe and\\nBlustein (1995)\\nTan: Tan and Cheng (2007)\\nTannier: Tannier and Geva (2005)\\nTao: Tao et al. (2006)\\nTarasov: Kozlov et al. (1979)\\nTaube: Taube and Wooster (1958)\\nTaylor: Robertson et al. (2004), Taylor\\net al. (2006)\\nTeevan: Rennie et al. (2003)\\nTeh: Teh et al. (2006)\\nTheiler: Perkins et al. (2003)\\nTheobald: Theobald et al. (2008),\\nTheobald et al. (2005)\\nThomas: Cover and Thomas (1991)\\nTiberi: Chierichetti et al. (2007)\\nTibshirani: Hastie et al. (2001),\\nTibshirani et al. (2001)\\nTipping: Zaragoza et al. (2003)\\nTishby: Tishby and Slonim (2000)\\nToda: Toda and Kataoka (2005)\\nTokunaga: Iwayama and Tokunaga\\n(1995)\\nTomasic: Tomasic and Garcia-Molina\\n(1993)\\nTombros: Betsi et al. (2006), Lalmas\\nand Tombros (2007), Tombros\\nand Sanderson (1998), Tombros\\net al. (2002)\\nTomkins: Broder et al. (2000), Kumar\\net al. (1999), Kumar et al. (2000)\\nTomlinson: Tomlinson (2003)\\nTong: Tong and Koller (2001)\\nToutanova: Toutanova and Moore\\n(2002)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n534\\nAuthor Index\\nTreeratpituk: Treeratpituk and Callan\\n(2006)\\nTrenkle: Cavnar and Trenkle (1994)\\nTrotman: Fuhr et al. (2007), O’Keefe\\nand Trotman (2004), Trotman\\n(2003), Trotman and Geva (2006),\\nTrotman et al. (2007), Trotman\\net al. (2006), Trotman and\\nSigurbjörnsson (2004)\\nTsaparas: Borodin et al. (2001)\\nTsegay: Turpin et al. (2007)\\nTseng: Tseng et al. (2005)\\nTsikrika: Betsi et al. (2006)\\nTsioutsiouliklis: Glover et al. (2002b)\\nTsochantaridis: Riezler et al. (2007),\\nTsochantaridis et al. (2005)\\nTudhope: Clarke et al. (2000)\\nTukey: Cutting et al. (1992)\\nTurpin: Hersh et al. (2000a), Hersh\\net al. (2001), Hersh et al. (2000b),\\nTurpin and Hersh (2001), Turpin\\nand Hersh (2002), Turpin et al.\\n(2007)\\nTurtle: Turtle (1994), Turtle and Croft\\n(1989), Turtle and Croft (1991),\\nTurtle and Flood (1995)\\nUchimoto: Murata et al. (2000)\\nUllman: Garcia-Molina et al. (1999),\\nHopcroft et al. (2000)\\nUlusoy: Altingövde et al. (2007)\\nUngar: Popescul and Ungar (2000)\\nUpfal: Chierichetti et al. (2007),\\nKumar et al. (2000)\\nUtiyama: Murata et al. (2000)\\nVaithyanathan: Vaithyanathan and\\nDom (2000)\\nVamplew: Johnson et al. (2006)\\nVapnik: Vapnik (1998)\\nVasserman: Riezler et al. (2007)\\nVassilvitskii: Arthur and Vassilvitskii\\n(2006)\\nVempala: Kannan et al. (2000)\\nVenkatasubramanian: Bharat et al.\\n(1998)\\nVenturini: Ferragina and Venturini\\n(2007)\\nVeta: Kannan et al. (2000)\\nVigna: Boldi et al. (2002), Boldi et al.\\n(2005), Boldi and Vigna (2004a),\\nBoldi and Vigna (2004b), Boldi\\nand Vigna (2005)\\nVilla: Tombros et al. (2002)\\nVittaut: Vittaut and Gallinari (2006)\\nViña: Cacheda et al. (2003)\\nVoorhees: Buckley and Voorhees\\n(2000), Voorhees (1985a),\\nVoorhees (1985b), Voorhees\\n(2000), Voorhees and Harman\\n(2005)\\nVries: List et al. (2005)\\nWagner: Wagner and Fischer (1974)\\nWalker: Spärck Jones et al. (2000)\\nWalther: Tibshirani et al. (2001)\\nWaltz: Creecy et al. (1992)\\nWan: Liu et al. (2005)\\nWang: Qin et al. (2007), Tao et al.\\n(2006)\\nWard Jr.: Ward Jr. (1963)\\nWatkins: Lodhi et al. (2002), Weston\\nand Watkins (1999)\\nWei: Wei and Croft (2006)\\nWeigend: Weigend et al. (1999)\\nWeikum: Amer-Yahia et al. (2005),\\nChaudhuri et al. (2006),\\nKammenhuber et al. (2006),\\nTheobald et al. (2008), Theobald\\net al. (2005)\\nWeinstein: Hayes and Weinstein\\n(1990)\\nWeiss: Apté et al. (1994), Ng et al.\\n(2001a), Osi´nski and Weiss (2005)\\nWen: Song et al. (2005)\\nWesterveld: Kraaij et al. (2002)\\nWeston: Weston and Watkins (1999)\\nWidom: Garcia-Molina et al. (1999),\\nJeh and Widom (2003)\\nWiener: Broder et al. (2000), Weigend\\net al. (1999)\\nWiering: van Zwol et al. (2006)\\nWilkinson: Zobel et al. (1995)\\nWillett: El-Hamdouchi and Willett\\n(1986)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nAuthor Index\\n535\\nWilliams: Bahle et al. (2002), Garcia\\net al. (2004), Heinz et al. (2002),\\nLance and Williams (1967),\\nLester et al. (2006), Scholer et al.\\n(2002), Turpin et al. (2007),\\nWilliams and Zobel (2005),\\nWilliams et al. (2004)\\nWinograd: Page et al. (1998)\\nWitten: Witten and Bell (1990), Witten\\nand Frank (2005), Witten et al.\\n(1999)\\nWißbrock: Stein et al. (2003)\\nWong: Hartigan and Wong (1979),\\nWong et al. (1988)\\nWoodley: Woodley and Geva (2006)\\nWooster: Taube and Wooster (1958)\\nWorring: Smeulders et al. (2000)\\nWu: Gao et al. (2005), Gao et al. (2004)\\nXu: Cao et al. (2006), Ji and Xu (2006),\\nXu and Croft (1996), Xu and\\nCroft (1999)\\nYang: Ault and Yang (2002), Lewis\\net al. (2004), Li and Yang (2003),\\nLiu et al. (2005), Melnik et al.\\n(2001), Yang and Callan (2006),\\nYang (1994), Yang (1999), Yang\\n(2001), Yang and Kisiel (2003),\\nYang and Liu (1999), Yang and\\nPedersen (1997)\\nYao: Wong et al. (1988)\\nYiannis: Scholer et al. (2002)\\nYih: Kołcz and Yih (2007)\\nYilmaz: Aslam and Yilmaz (2005)\\nYoung: Berry and Young (1995),\\nEckart and Young (1936)\\nYu: Hand and Yu (2001)\\nYue: Yue et al. (2007)\\nZamir: Zamir and Etzioni (1999)\\nZaragoza: Robertson et al. (2004),\\nTaylor et al. (2006), Zaragoza\\net al. (2003)\\nZavrel: Zavrel et al. (2000)\\nZeng: Liu et al. (2005)\\nZha: Zha et al. (2001)\\nZhai: Lafferty and Zhai (2001),\\nLafferty and Zhai (2003), Tao\\net al. (2006), Zhai and Lafferty\\n(2001a), Zhai and Lafferty\\n(2001b), Zhai and Lafferty (2002)\\nZhang: Qin et al. (2007), Radev et al.\\n(2001), Zhang et al. (2007), Zhang\\nand Oles (2001)\\nZhao: Zhao and Karypis (2002)\\nZheng: Ng et al. (2001b)\\nZien: Chapelle et al. (2006)\\nZipf: Zipf (1949)\\nZiviani: Badue et al. (2001), de Moura\\net al. (2000), Ribeiro-Neto et al.\\n(1999)\\nZobel: Bahle et al. (2002), Heinz and\\nZobel (2003), Heinz et al. (2002),\\nKaszkiel and Zobel (1997), Lester\\net al. (2005), Lester et al. (2006),\\nMoffat and Zobel (1992), Moffat\\nand Zobel (1996), Moffat and\\nZobel (1998), Persin et al. (1996),\\nScholer et al. (2002), Williams\\nand Zobel (2005), Williams et al.\\n(2004), Zobel (1998), Zobel and\\nDart (1995), Zobel and Dart\\n(1996), Zobel and Moffat (2006),\\nZobel et al. (1995)\\nZukowski: Zukowski et al. (2006)\\nZweig: Broder et al. (1997)\\nZwol: van Zwol et al. (2006)\\ndel Bimbo: del Bimbo (1999)\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nIndex\\nL2 distance, 131\\nχ2 feature selection, 275\\nδ codes, 104\\nγ encoding, 99\\nk nearest neighbor classiﬁcation, 297\\nk-gram index, 54, 60\\n1/0 loss, 221\\n11-point interpolated average\\nprecision, 159\\n20 Newsgroups, 154\\nA/B test, 170\\naccess control lists, 81\\naccumulator, 113, 125\\naccuracy, 155\\nactive learning, 336\\nad hoc retrieval, 5, 253\\nadd-one smoothing, 260\\nadjacency table, 455\\nadversarial information retrieval, 429\\nAkaike Information Criterion, 367\\nalgorithmic search, 430\\nanchor text, 425\\nany-of classiﬁcation, 257, 306\\nauthority score, 474\\nauxiliary index, 78\\naverage-link clustering, 389\\nB-tree, 50\\nbag of words, 117, 267\\nbag-of-words, 269\\nbalanced F measure, 156\\nBayes error rate, 300\\nBayes Optimal Decision Rule, 222\\nBayes risk, 222\\nBayes’ Rule, 220\\nBayesian networks, 234\\nBayesian prior, 226\\nBernoulli model, 263\\nbest-merge persistence, 388\\nbias, 311\\nbias-variance tradeoff, 241, 312, 321\\nbiclustering, 374\\nbigram language model, 240\\nBinary Independence Model, 222\\nbinary tree, 50, 377\\nbiword index, 39, 43\\nblind relevance feedback, see pseudo\\nrelevance feedback\\nblocked sort-based indexing\\nalgorithm, 71\\nblocked storage, 92\\nblog, 195\\nBM25 weights, 232\\nboosting, 286\\nbottom-up clustering, see hierarchical\\nagglomerative clustering\\nbowtie, 426\\nbreak-even, 334\\nbreak-even point, 161\\nBSBI, 71\\nBuckshot algorithm, 399\\nbuffer, 69\\ncaching, 9, 68, 146, 447, 450\\ncapture-recapture method, 435\\ncardinality\\nin clustering, 355\\nCAS topics, 211\\ncase-folding, 30\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n538\\nIndex\\ncategory, 256\\ncentroid, 292, 360\\nin relevance feedback, 181\\ncentroid-based classiﬁcation, 314\\nchain rule, 220\\nchaining\\nin clustering, 385\\nchampion lists, 143\\nclass boundary, 303\\nclassiﬁcation, 253, 344\\nclassiﬁcation function, 256\\nclassiﬁer, 183\\nCLEF, 154\\nclick spam, 431\\nclickstream mining, 170, 188\\nclickthrough log analysis, 170\\nclique, 384\\ncluster, 74, 349\\nin relevance feedback, 184\\ncluster hypothesis, 350\\ncluster-based classiﬁcation, 314\\ncluster-internal labeling, 396\\nCO topics, 211\\nco-clustering, 374\\ncollection, 4\\ncollection frequency, 27\\ncombination similarity, 378, 384, 393\\ncomplete-link clustering, 382\\ncomplete-linkage clustering, see\\ncomplete-link clustering\\ncomponent coverage, 212\\ncompound-splitter, 25\\ncompounds, 25\\nconcept drift, 269, 283, 286, 336\\nconditional independence\\nassumption, 224, 266\\nconfusion matrix, 307\\nconnected component, 384\\nconnectivity queries, 455\\nconnectivity server, 455\\ncontent management system, 84\\ncontext\\nXML, 199\\ncontext resemblance, 208\\ncontiguity hypothesis, 289\\ncontinuation bit, 96\\ncorpus, 4\\ncosine similarity, 121, 372\\nCPC, 430\\nCPM, 430\\nCranﬁeld, 153\\ncross-entropy, 251\\ncross-language information retrieval,\\n154, 417\\ncumulative gain, 162\\ndata-centric XML, 196, 214\\ndatabase\\nrelational, 1, 195, 214\\ndecision boundary, 292, 303\\ndecision hyperplane, 290, 302\\ndecision trees, 282, 286\\ndendrogram, 378\\ndevelopment set, 283\\ndevelopment test collection, 153\\nDice coefﬁcient, 163\\ndictionary, 6, 7\\ndifferential cluster labeling, 396\\ndigital libraries, 195\\ndistortion, 366\\ndistributed index, 74, 458\\ndistributed indexing, 74\\ndistributed information retrieval, see\\ndistributed crawling, 458\\ndivisive clustering, 395\\nDNS resolution, 450\\nDNS server, 450\\ndocID, 7\\ndocument, 4, 20\\ndocument collection, see collection\\ndocument frequency, 7, 118\\ndocument likelihood model, 250\\ndocument partitioning, 454\\ndocument space, 256\\ndocument vector, 119, 120\\ndocument-at-a-time, 126, 140\\ndocument-partitioned index, 75\\ndot product, 121\\nEast Asian languages, 45\\nedit distance, 58\\neffectiveness, 5, 280\\neigen decomposition, 406\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nIndex\\n539\\neigenvalue, 404\\nEM algorithm, 369\\nemail sorting, 254\\nenterprise resource planning, 84\\nenterprise search, 67\\nentropy, 99, 106, 358\\nequivalence classes, 28\\nErgodic Markov Chain, 467\\nEuclidean distance, 131, 372\\nEuclidean length, 121\\nevidence accumulation, 146\\nexclusive clustering, 355\\nexhaustive clustering, 355\\nexpectation step, 370\\nExpectation-Maximization algorithm,\\n336, 369\\nexpected edge density, 373\\nextended query, 205\\nExtensible Markup Language, 196\\nexternal criterion of quality, 356\\nexternal sorting algorithm, 70\\nF measure, 156, 173\\nas an evaluation measure in\\nclustering, 359\\nfalse negative, 359\\nfalse positive, 359\\nfeature engineering, 338\\nfeature selection, 271\\nﬁeld, 110\\nﬁltering, 253, 314\\nﬁrst story detection, 395, 399\\nﬂat clustering, 350\\nfocused retrieval, 217\\nfree text, 109, 148\\nfree text query, see query, free text,\\n124, 145, 196\\nfrequency-based feature selection, 277\\nFrobenius norm, 410\\nfront coding, 93\\nfunctional margin, 322\\nGAAC, 388\\ngenerative model, 237, 309, 311\\ngeometric margin, 323\\ngold standard, 152\\nGolomb codes, 106\\nGOV2, 154\\ngreedy feature selection, 279\\ngrep, 3\\nground truth, 152\\ngroup-average agglomerative\\nclustering, 388\\ngroup-average clustering, 389\\nHAC, 378\\nhard assignment, 350\\nhard clustering, 350, 355\\nharmonic number, 101\\nHeaps’ law, 88\\nheld-out, 298\\nheld-out data, 283\\nhierarchic clustering, 377\\nhierarchical agglomerative clustering,\\n378\\nhierarchical classiﬁcation, 337, 347\\nhierarchical clustering, 350, 377\\nHierarchical Dirichlet Processes, 418\\nhierarchy\\nin clustering, 377\\nhighlighting, 203\\nHITS, 477\\nHTML, 421\\nhttp, 421\\nhub score, 474\\nhyphens, 24\\ni.i.d., 283, see independent and\\nidentically distributed\\nIde dec-hi, 183\\nidf, 83, 204, 227, 232\\niid, see independent and identically\\ndistributed\\nimpact, 81\\nimplicit relevance feedback, 187\\nin-links, 425, 461\\nincidence matrix, 3, 408\\nindependence, 275\\nindependent and identically\\ndistributed, 283\\nin clustering, 367\\nindex, 3, see permuterm index, see also\\nparametric index, zone index\\nindex construction, 67\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n540\\nIndex\\nindexer, 67\\nindexing, 67\\nsort-based, 7\\nindexing granularity, 21\\nindexing unit, 201\\nINEX, 210\\ninformation gain, 285\\ninformation need, 5, 152\\ninformation retrieval, 1\\ninformational queries, 432\\ninner product, 121\\ninstance-based learning, 300\\ninter-similarity, 381\\ninternal criterion of quality, 356\\ninterpolated precision, 158\\nintersection\\npostings list, 10\\ninverse document frequency, 118, 125\\ninversion, 71, 378, 391\\ninverted ﬁle, see inverted index\\ninverted index, 6\\ninverted list, see postings list\\ninverter, 76\\nIP address, 449\\nJaccard coefﬁcient, 61, 438\\nK-medoids, 365\\nkappa statistic, 165, 174, 373\\nkernel, 332\\nkernel function, 332\\nkernel trick, 331\\nkey-value pairs, 75\\nkeyword-in-context, 171\\nkNN classiﬁcation, 297\\nKruskal’s algorithm, 399\\nKullback-Leibler divergence, 251,\\n317, 372\\nKWIC, see keyword-in-context\\nlabel, 256\\nlabeling, 255\\nlanguage, 237\\nlanguage identiﬁcation, 24, 46\\nlanguage model, 238\\nLaplace smoothing, 260\\nLatent Dirichlet Allocation, 418\\nlatent semantic indexing, 192, 413\\nLDA, 418\\nlearning algorithm, 256\\nlearning error, 310\\nlearning method, 256\\nlemma, 32\\nlemmatization, 32\\nlemmatizer, 33\\nlength-normalization, 121\\nLevenshtein distance, 58\\nlexicalized subtree, 206\\nlexicon, 6\\nlikelihood, 221\\nlikelihood ratio, 239\\nlinear classiﬁer, 301, 343\\nlinear problem, 303\\nlinear separability, 304\\nlink farms, 481\\nlink spam, 429, 461\\nLM, 243\\nlogarithmic merging, 79\\nlossless, 87\\nlossy compression, 87\\nlow-rank approximation, 410\\nLSA, 413\\nLSI as soft clustering, 417\\nmachine translation, 240, 243, 251\\nmachine-learned relevance, 113, 342\\nmacroaveraging, 280\\nMAP, 159, 227, 258\\nmap phase, 75\\nMapReduce, 75\\nmargin, 320\\nmarginal relevance, 167\\nmarginal statistic, 165\\nmaster node, 75\\nmatrix decomposition, 406\\nmaximization step, 370\\nmaximum a posteriori, 227, 265\\nmaximum a posteriori class, 258\\nmaximum likelihood estimate, 226,\\n259\\nmaximum likelihood estimation, 244\\nMean Average Precision, see MAP\\nmedoid, 365\\nmemory capacity, 312\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nIndex\\n541\\nmemory-based learning, 300\\nMercator, 445\\nMercer kernel, 332\\nmerge\\npostings, 10\\nmerge algorithm, 10\\nmetadata, 24, 110, 171, 197, 373, 428\\nmicroaveraging, 280\\nminimum spanning tree, 399, 401\\nminimum variance clustering, 399\\nMLE, see maximum likelihood\\nestimate\\nModApte split, 279, 286\\nmodel complexity, 312, 366\\nmodel-based clustering, 368\\nmonotonicity, 378\\nmulticlass classiﬁcation, 306\\nmulticlass SVM, 347\\nmultilabel classiﬁcation, 306\\nmultimodal class, 296\\nmultinomial classiﬁcation, 306\\nmultinomial distribution, 241\\nmultinomial model, 263, 270\\nmultinomial Naive Bayes, 258\\nmultinomial NB, see multinomial\\nNaive Bayes\\nmultivalue classiﬁcation, 306\\nmultivariate Bernoulli model, 263\\nmutual information, 272, 358\\nNaive Bayes assumption, 224\\nnamed entity tagging, 195, 339\\nNational Institute of Standards and\\nTechnology, 153\\nnatural language processing, xxxiv,\\n33, 171, 217, 249, 372\\nnavigational queries, 432\\nNDCG, 163\\nnested elements, 203\\nNEXI, 200\\nnext word index, 44\\nnibble, 98\\nNLP, see natural language processing\\nNMI, 358\\nnoise document, 303\\nnoise feature, 271\\nnonlinear classiﬁer, 305\\nnonlinear problem, 305\\nnormal vector, 293\\nnormalized discounted cumulative\\ngain, 163\\nnormalized mutual information, 358\\nnovelty detection, 395\\nNTCIR, 154, 174\\nobjective function, 354, 360\\nodds, 221\\nodds ratio, 225\\nOkapi weighting, 232\\none-of classiﬁcation, 257, 284, 306\\noptimal classiﬁer, 270, 310\\noptimal clustering, 393\\noptimal learning method, 310\\nordinal regression, 344\\nout-links, 425\\noutlier, 363\\noverﬁtting, 271, 312\\nPageRank, 464\\npaid inclusion, 428\\nparameter tuning, 153, 314, 315, 348\\nparameter tying, 340\\nparameter-free compression, 100\\nparameterized compression, 106\\nparametric index, 110\\nparametric search, 197\\nparser, 75\\npartition rule, 220\\npartitional clustering, 355\\npassage retrieval, 217\\npatent databases, 195\\nperceptron algorithm, 286, 315\\nperformance, 280\\npermuterm index, 53\\npersonalized PageRank, 471\\nphrase index, 40\\nphrase queries, 39, 47\\nphrase search, 15\\npivoted document length\\nnormalization, 129\\npointwise mutual information, 286\\npolychotomous, 306\\npolytomous classiﬁcation, 306\\npolytope, 298\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n542\\nIndex\\npooling, 164, 174\\npornography ﬁltering, 338\\nPorter stemmer, 33\\npositional independence, 267\\npositional index, 41\\nposterior probability, 220\\nposting, 6, 7, 71, 86\\npostings list, 6\\npower law, 89, 426\\nprecision, 5, 155\\nprecision at k, 161\\nprecision-recall curve, 158\\npreﬁx-free code, 100\\nprincipal direction divisive\\npartitioning, 400\\nprincipal left eigenvector, 465\\nprior probability, 220\\nProbability Ranking Principle, 221\\nprobability vector, 466\\nprototype, 290\\nproximity operator, 14\\nproximity weighting, 145\\npseudo relevance feedback, 187\\npseudocounts, 226\\npull model, 314\\npurity, 356\\npush model, 314\\nQuadratic Programming, 324\\nquery, 5\\nfree text, 14, 16, 117\\nsimple conjunctive, 10\\nquery expansion, 189\\nquery likelihood model, 242\\nquery optimization, 11\\nquery-by-example, 201, 249\\nR-precision, 161, 174\\nRand index, 359\\nadjusted, 373\\nrandom variable, 220\\nrandom variable C, 268\\nrandom variable U, 266\\nrandom variable X, 266\\nrank, 403\\nRanked Boolean retrieval, 112\\nranked retrieval, 81, 107\\nmodel, 14\\nranking SVM, 345\\nrecall, 5, 155\\nreduce phase, 75\\nreduced SVD, 409, 412\\nregression, 344\\nregular expressions, 3, 18\\nregularization, 328\\nrelational database, 195, 214\\nrelative frequency, 226\\nrelevance, 5, 152\\nrelevance feedback, 178\\nresidual sum of squares, 360\\nresults snippets, 146\\nretrieval model\\nBoolean, 4\\nRetrieval Status Value, 225\\nretrieval systems, 81\\nReuters-21578, 154\\nReuters-RCV1, 69, 154\\nRF, 178\\nRobots Exclusion Protocol, 447\\nROC curve, 162\\nRocchio algorithm, 181\\nRocchio classiﬁcation, 292\\nrouting, 253, 314\\nRSS, 360\\nrule of 30, 86\\nrules in text classiﬁcation, 255\\nScatter-Gather, 351\\nschema, 199\\nschema diversity, 204\\nschema heterogeneity, 204\\nsearch advertising, 430\\nsearch engine marketing, 431\\nSearch Engine Optimizers, 429\\nsearch result clustering, 351\\nsearch results, 351\\nsecurity, 81\\nseed, 361\\nseek time, 68\\nsegment ﬁle, 75\\nsemi-supervised learning, 336\\nsemistructured query, 197\\nsemistructured retrieval, 2, 197\\nsensitivity, 162\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\nIndex\\n543\\nsentiment detection, 254\\nsequence model, 267\\nshingling, 438\\nsingle-label classiﬁcation, 306\\nsingle-link clustering, 382\\nsingle-linkage clustering, see\\nsingle-link clustering\\nsingle-pass in-memory indexing, 73\\nsingleton, 378\\nsingleton cluster, 363\\nsingular value decomposition, 407\\nskip list, 36, 46\\nslack variables, 327\\nSMART, 182\\nsmoothing, 127, 226\\nadd α, 226\\nadd 1\\n2, 232\\nadd 1\\n2, 226–229, 262\\nBayesian prior, 226, 228, 245\\nlinear interpolation, 245\\nsnippet, 170\\nsoft assignment, 350\\nsoft clustering, 350, 355, 377\\nsorting\\nin index construction, 7\\nsoundex, 63\\nspam, 338, 427\\nemail, 254\\nweb, 254\\nsparseness, 241, 244, 260\\nspeciﬁcity, 162\\nspectral clustering, 400\\nspeech recognition, 240\\nspelling correction, 147, 240, 242\\nspider, 443\\nspider traps, 433\\nSPIMI, 73\\nsplits, 75\\nsponsored search, 430\\nstanding query, 253\\nstatic quality scores, 138\\nstatic web pages, 424\\nstatistical signiﬁcance, 276\\nstatistical text classiﬁcation, 255\\nsteady-state, 467, 468\\nstemming, 32, 46\\nstochastic matrix, 465\\nstop words, 117\\nstop list, 27\\nstop words, 117\\nstop words, 23, 27, 45, 127\\nstructural SVM, 345\\nstructural SVMs, 330\\nstructural term, 207\\nstructured document retrieval\\nprinciple, 201\\nstructured query, 197\\nstructured retrieval, 195, 197\\nsummarization, 400\\nsummary\\ndynamic, 171\\nstatic, 171\\nsupervised learning, 256\\nsupport vector, 320\\nsupport vector machine, 319, 346\\nmulticlass, 330\\nSVD, 373, 400, 408\\nSVM, see support vector machine\\nsymmetric diagonal decomposition,\\n407, 408\\nsynonymy, 177\\nteleport, 464\\nterm, 3, 19, 22\\nterm frequency, 16, 117\\nterm normalization, 28\\nterm partitioning, 454\\nterm-at-a-time, 125, 140\\nterm-document matrix, 123\\nterm-partitioned index, 74\\ntermID, 69\\ntest data, 256\\ntest set, 256, 283\\ntext categorization, 253\\ntext classiﬁcation, 253\\ntext summarization, 171\\ntext-centric XML, 214\\ntf, see term frequency\\ntf-idf, 119\\ntiered indexes, 143\\ntoken, 19, 22\\ntoken normalization, 28\\ntop docs, 149\\n',\n",
       " 'Online edition (c)\\n2009 Cambridge UP\\n544\\nIndex\\ntop-down clustering, 395\\ntopic, 153, 253\\nin XML retrieval, 211\\ntopic classiﬁcation, 253\\ntopic spotting, 253\\ntopic-speciﬁc PageRank, 471\\ntopical relevance, 212\\ntraining set, 256, 283\\ntransactional query, 433\\ntransductive SVMs, 336\\ntranslation model, 251\\nTREC, 153, 314\\ntrec_eval, 174\\ntruecasing, 30, 46\\ntruncated SVD, 409, 412, 415\\ntwo-class classiﬁer, 279\\ntype, 22\\nunary code, 99\\nunigram language model, 240\\nunion-ﬁnd algorithm, 395, 440\\nuniversal code, 100\\nunsupervised learning, 349\\nURL, 422\\nURL normalization, 447\\nutility measure, 286\\nvariable byte encoding, 96\\nvariance, 311\\nvector space model, 120\\nvertical search engine, 254\\nvocabulary, 6\\nVoronoi tessellation, 297\\nWard’s method, 399\\nweb crawler, 443\\nweight vector, 322\\nweighted zone scoring, 110\\nWikipedia, 211\\nwildcard query, 3, 49, 52\\nwithin-point scatter, 375\\nword segmentation, 25\\nXML, 20, 196\\nXML attribute, 197\\nXML DOM, 197\\nXML DTD, 199\\nXML element, 197\\nXML fragment, 216\\nXML Schema, 199\\nXML tag, 197\\nXPath, 199\\nZipf’s law, 89\\nzone, 110, 337, 339, 340\\nzone index, 110\\nzone search, 197\\n']"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c97af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in corpus: ['Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n1\\n1\\nBoolean retrieval\\nThe meaning of the term information retrieval can be very broad. Just getting\\na credit card out of your wallet so that you can type in the card number\\nis a form of information retrieval. However, as an academic ﬁeld of study,\\ninformation retrieval might be deﬁned thus:\\nINFORMATION\\nRETRIEVAL\\nInformation retrieval (IR) is ﬁnding material (usually documents) of\\nan unstructured nature (usually text) that satisﬁes an information need\\nfrom within large collections (usually stored on computers).\\nAs deﬁned in this way, information retrieval used to be an activity that only\\na few people engaged in: reference librarians, paralegals, and similar pro-\\nfessional searchers. Now the world has changed, and hundreds of millions\\nof people engage in information retrieval every day when they use a web\\nsearch engine or search their email.1 Information retrieval is fast becoming\\nthe dominant form of information access, overtaking traditional database-\\nstyle searching (the sort that is going on when a clerk says to you: “I’m sorry,\\nI can only look up your order if you can give me your Order ID”).\\nIR can also cover other kinds of data and information problems beyond\\nthat speciﬁed in the core deﬁnition above. The term “unstructured data”\\nrefers to data which does not have clear, semantically overt, easy-for-a-computer\\nstructure. It is the opposite of structured data, the canonical example of\\nwhich is a relational database, of the sort companies usually use to main-\\ntain product inventories and personnel records. In reality, almost no data\\nare truly “unstructured”. This is deﬁnitely true of all text data if you count\\nthe latent linguistic structure of human languages. But even accepting that\\nthe intended notion of structure is overt structure, most text has structure,\\nsuch as headings and paragraphs and footnotes, which is commonly repre-\\nsented in documents by explicit markup (such as the coding underlying web\\n1. In modern parlance, the word “search” has tended to replace “(information) retrieval”; the\\nterm “search” is quite ambiguous, but in context we use the two synonymously.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2\\n1\\nBoolean retrieval\\npages). IR is also used to facilitate “semistructured” search such as ﬁnding a\\ndocument where the title contains Java and the body contains threading.\\nThe ﬁeld of information retrieval also covers supporting users in browsing\\nor ﬁltering document collections or further processing a set of retrieved doc-\\numents. Given a set of documents, clustering is the task of coming up with a\\ngood grouping of the documents based on their contents. It is similar to ar-\\nranging books on a bookshelf according to their topic. Given a set of topics,\\nstanding information needs, or other categories (such as suitability of texts\\nfor different age groups), classiﬁcation is the task of deciding which class(es),\\nif any, each of a set of documents belongs to. It is often approached by ﬁrst\\nmanually classifying some documents and then hoping to be able to classify\\nnew documents automatically.\\nInformation retrieval systems can also be distinguished by the scale at\\nwhich they operate, and it is useful to distinguish three prominent scales.\\nIn web search, the system has to provide search over billions of documents\\nstored on millions of computers. Distinctive issues are needing to gather\\ndocuments for indexing, being able to build systems that work efﬁciently\\nat this enormous scale, and handling particular aspects of the web, such as\\nthe exploitation of hypertext and not being fooled by site providers manip-\\nulating page content in an attempt to boost their search engine rankings,\\ngiven the commercial importance of the web. We focus on all these issues\\nin Chapters 19–21. At the other extreme is personal information retrieval. In\\nthe last few years, consumer operating systems have integrated information\\nretrieval (such as Apple’s Mac OS X Spotlight or Windows Vista’s Instant\\nSearch). Email programs usually not only provide search but also text clas-\\nsiﬁcation: they at least provide a spam (junk mail) ﬁlter, and commonly also\\nprovide either manual or automatic means for classifying mail so that it can\\nbe placed directly into particular folders. Distinctive issues here include han-\\ndling the broad range of document types on a typical personal computer,\\nand making the search system maintenance free and sufﬁciently lightweight\\nin terms of startup, processing, and disk space usage that it can run on one\\nmachine without annoying its owner. In between is the space of enterprise,\\ninstitutional, and domain-speciﬁc search, where retrieval might be provided for\\ncollections such as a corporation’s internal documents, a database of patents,\\nor research articles on biochemistry. In this case, the documents will typi-\\ncally be stored on centralized ﬁle systems and one or a handful of dedicated\\nmachines will provide search over the collection. This book contains tech-\\nniques of value over this whole spectrum, but our coverage of some aspects\\nof parallel and distributed search in web-scale search systems is compara-\\ntively light owing to the relatively small published literature on the details\\nof such systems. However, outside of a handful of web search companies, a\\nsoftware developer is most likely to encounter the personal search and en-\\nterprise scenarios.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.1\\nAn example information retrieval problem\\n3\\nIn this chapter we begin with a very simple example of an information\\nretrieval problem, and introduce the idea of a term-document matrix (Sec-\\ntion 1.1) and the central inverted index data structure (Section 1.2). We will\\nthen examine the Boolean retrieval model and how Boolean queries are pro-\\ncessed (Sections 1.3 and 1.4).\\n1.1\\nAn example information retrieval problem\\nA fat book which many people own is Shakespeare’s Collected Works. Sup-\\npose you wanted to determine which plays of Shakespeare contain the words\\nBrutus AND Caesar AND NOT Calpurnia. One way to do that is to start at the\\nbeginning and to read through all the text, noting for each play whether\\nit contains Brutus and Caesar and excluding it from consideration if it con-\\ntains Calpurnia. The simplest form of document retrieval is for a computer\\nto do this sort of linear scan through documents. This process is commonly\\nreferred to as grepping through text, after the Unix command grep, which\\nGREP\\nperforms this process. Grepping through text can be a very effective process,\\nespecially given the speed of modern computers, and often allows useful\\npossibilities for wildcard pattern matching through the use of regular expres-\\nsions. With modern computers, for simple querying of modest collections\\n(the size of Shakespeare’s Collected Works is a bit under one million words\\nof text in total), you really need nothing more.\\nBut for many purposes, you do need more:\\n1. To process large document collections quickly. The amount of online data\\nhas grown at least as quickly as the speed of computers, and we would\\nnow like to be able to search collections that total in the order of billions\\nto trillions of words.\\n2. To allow more ﬂexible matching operations. For example, it is impractical\\nto perform the query Romans NEAR countrymen with grep, where NEAR\\nmight be deﬁned as “within 5 words” or “within the same sentence”.\\n3. To allow ranked retrieval: in many cases you want the best answer to an\\ninformation need among many documents that contain certain words.\\nThe way to avoid linearly scanning the texts for each query is to index the\\nINDEX\\ndocuments in advance. Let us stick with Shakespeare’s Collected Works,\\nand use it to introduce the basics of the Boolean retrieval model. Suppose\\nwe record for each document – here a play of Shakespeare’s – whether it\\ncontains each word out of all the words Shakespeare used (Shakespeare used\\nabout 32,000 different words). The result is a binary term-document incidence\\nINCIDENCE MATRIX\\nmatrix, as in Figure 1.1. Terms are the indexed units (further discussed in\\nTERM\\nSection 2.2); they are usually words, and for the moment you can think of\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4\\n1\\nBoolean retrieval\\nAntony\\nJulius\\nThe\\nHamlet\\nOthello\\nMacbeth\\n...\\nand\\nCaesar\\nTempest\\nCleopatra\\nAntony\\n1\\n1\\n0\\n0\\n0\\n1\\nBrutus\\n1\\n1\\n0\\n1\\n0\\n0\\nCaesar\\n1\\n1\\n0\\n1\\n1\\n1\\nCalpurnia\\n0\\n1\\n0\\n0\\n0\\n0\\nCleopatra\\n1\\n0\\n0\\n0\\n0\\n0\\nmercy\\n1\\n0\\n1\\n1\\n1\\n1\\nworser\\n1\\n0\\n1\\n1\\n1\\n0\\n...\\n◮Figure 1.1\\nA term-document incidence matrix. Matrix element (t, d) is 1 if the\\nplay in column d contains the word in row t, and is 0 otherwise.\\nthem as words, but the information retrieval literature normally speaks of\\nterms because some of them, such as perhaps I-9 or Hong Kong are not usually\\nthought of as words. Now, depending on whether we look at the matrix rows\\nor columns, we can have a vector for each term, which shows the documents\\nit appears in, or a vector for each document, showing the terms that occur in\\nit.2\\nTo answer the query Brutus AND Caesar AND NOT Calpurnia, we take the\\nvectors for Brutus, Caesar and Calpurnia, complement the last, and then do a\\nbitwise AND:\\n110100 AND 110111 AND 101111 = 100100\\nThe answers for this query are thus Antony and Cleopatra and Hamlet (Fig-\\nure 1.2).\\nThe Boolean retrieval model is a model for information retrieval in which we\\nBOOLEAN RETRIEVAL\\nMODEL\\ncan pose any query which is in the form of a Boolean expression of terms,\\nthat is, in which terms are combined with the operators AND, OR, and NOT.\\nThe model views each document as just a set of words.\\nLet us now consider a more realistic scenario, simultaneously using the\\nopportunity to introduce some terminology and notation. Suppose we have\\nN = 1 million documents. By documents we mean whatever units we have\\nDOCUMENT\\ndecided to build a retrieval system over. They might be individual memos\\nor chapters of a book (see Section 2.1.2 (page 20) for further discussion). We\\nwill refer to the group of documents over which we perform retrieval as the\\n(document) collection. It is sometimes also referred to as a corpus (a body of\\nCOLLECTION\\nCORPUS\\ntexts). Suppose each document is about 1000 words long (2–3 book pages). If\\n2. Formally, we take the transpose of the matrix to be able to get the terms as column vectors.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.1\\nAn example information retrieval problem\\n5\\nAntony and Cleopatra, Act III, Scene ii\\nAgrippa [Aside to Domitius Enobarbus]:\\nWhy, Enobarbus,\\nWhen Antony found Julius Caesar dead,\\nHe cried almost to roaring; and he wept\\nWhen at Philippi he found Brutus slain.\\nHamlet, Act III, Scene ii\\nLord Polonius:\\nI did enact Julius Caesar: I was killed i’ the\\nCapitol; Brutus killed me.\\n◮Figure 1.2\\nResults from Shakespeare for the query Brutus AND Caesar AND NOT\\nCalpurnia.\\nwe assume an average of 6 bytes per word including spaces and punctuation,\\nthen this is a document collection about 6 GB in size. Typically, there might\\nbe about M = 500,000 distinct terms in these documents. There is nothing\\nspecial about the numbers we have chosen, and they might vary by an order\\nof magnitude or more, but they give us some idea of the dimensions of the\\nkinds of problems we need to handle. We will discuss and model these size\\nassumptions in Section 5.1 (page 86).\\nOur goal is to develop a system to address the ad hoc retrieval task. This is\\nAD HOC RETRIEVAL\\nthe most standard IR task. In it, a system aims to provide documents from\\nwithin the collection that are relevant to an arbitrary user information need,\\ncommunicated to the system by means of a one-off, user-initiated query. An\\ninformation need is the topic about which the user desires to know more, and\\nINFORMATION NEED\\nis differentiated from a query, which is what the user conveys to the com-\\nQUERY\\nputer in an attempt to communicate the information need. A document is\\nrelevant if it is one that the user perceives as containing information of value\\nRELEVANCE\\nwith respect to their personal information need. Our example above was\\nrather artiﬁcial in that the information need was deﬁned in terms of par-\\nticular words, whereas usually a user is interested in a topic like “pipeline\\nleaks” and would like to ﬁnd relevant documents regardless of whether they\\nprecisely use those words or express the concept with other words such as\\npipeline rupture. To assess the effectiveness of an IR system (i.e., the quality of\\nEFFECTIVENESS\\nits search results), a user will usually want to know two key statistics about\\nthe system’s returned results for a query:\\nPrecision: What fraction of the returned results are relevant to the informa-\\nPRECISION\\ntion need?\\nRecall: What fraction of the relevant documents in the collection were re-\\nRECALL\\nturned by the system?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6\\n1\\nBoolean retrieval\\nDetailed discussion of relevance and evaluation measures including preci-\\nsion and recall is found in Chapter 8.\\nWe now cannot build a term-document matrix in a naive way. A 500K ×\\n1M matrix has half-a-trillion 0’s and 1’s – too many to ﬁt in a computer’s\\nmemory. But the crucial observation is that the matrix is extremely sparse,\\nthat is, it has few non-zero entries. Because each document is 1000 words\\nlong, the matrix has no more than one billion 1’s, so a minimum of 99.8% of\\nthe cells are zero. A much better representation is to record only the things\\nthat do occur, that is, the 1 positions.\\nThis idea is central to the ﬁrst major concept in information retrieval, the\\ninverted index. The name is actually redundant: an index always maps back\\nINVERTED INDEX\\nfrom terms to the parts of a document where they occur. Nevertheless, in-\\nverted index, or sometimes inverted ﬁle, has become the standard term in infor-\\nmation retrieval.3 The basic idea of an inverted index is shown in Figure 1.3.\\nWe keep a dictionary of terms (sometimes also referred to as a vocabulary or\\nDICTIONARY\\nVOCABULARY\\nlexicon; in this book, we use dictionary for the data structure and vocabulary\\nLEXICON\\nfor the set of terms). Then for each term, we have a list that records which\\ndocuments the term occurs in. Each item in the list – which records that a\\nterm appeared in a document (and, later, often, the positions in the docu-\\nment) – is conventionally called a posting.4 The list is then called a postings\\nPOSTING\\nPOSTINGS LIST\\nlist (or inverted list), and all the postings lists taken together are referred to as\\nthe postings. The dictionary in Figure 1.3 has been sorted alphabetically and\\nPOSTINGS\\neach postings list is sorted by document ID. We will see why this is useful in\\nSection 1.3, below, but later we will also consider alternatives to doing this\\n(Section 7.1.5).\\n1.2\\nA ﬁrst take at building an inverted index\\nTo gain the speed beneﬁts of indexing at retrieval time, we have to build the\\nindex in advance. The major steps in this are:\\n1. Collect the documents to be indexed:\\nFriends, Romans, countrymen.\\nSo let it be with Caesar ...\\n2. Tokenize the text, turning each document into a list of tokens:\\nFriends\\nRomans\\ncountrymen\\nSo ...\\n3. Some information retrieval researchers prefer the term inverted ﬁle, but expressions like in-\\ndex construction and index compression are much more common than inverted ﬁle construction and\\ninverted ﬁle compression. For consistency, we use (inverted) index throughout this book.\\n4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherently\\nassociated with a term, via the postings list it is placed on; sometimes we will also talk of a\\n(term, docID) pair as a posting.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.2\\nA ﬁrst take at building an inverted index\\n7\\nBrutus\\n−→\\n1\\n2\\n4\\n11\\n31\\n45\\n173\\n174\\nCaesar\\n−→\\n1\\n2\\n4\\n5\\n6\\n16\\n57\\n132\\n...\\nCalpurnia\\n−→\\n2\\n31\\n54\\n101\\n...\\n|\\n{z\\n}\\n|\\n{z\\n}\\nDictionary\\nPostings\\n◮Figure 1.3\\nThe two parts of an inverted index. The dictionary is commonly kept\\nin memory, with pointers to each postings list, which is stored on disk.\\n3. Do linguistic preprocessing, producing a list of normalized tokens, which\\nare the indexing terms: friend\\nroman\\ncountryman\\nso ...\\n4. Index the documents that each term occurs in by creating an inverted in-\\ndex, consisting of a dictionary and postings.\\nWe will deﬁne and discuss the earlier stages of processing, that is, steps 1–3,\\nin Section 2.2 (page 22). Until then you can think of tokens and normalized\\ntokens as also loosely equivalent to words. Here, we assume that the ﬁrst\\n3 steps have already been done, and we examine building a basic inverted\\nindex by sort-based indexing.\\nWithin a document collection, we assume that each document has a unique\\nserial number, known as the document identiﬁer (docID). During index con-\\nDOCID\\nstruction, we can simply assign successive integers to each new document\\nwhen it is ﬁrst encountered. The input to indexing is a list of normalized\\ntokens for each document, which we can equally think of as a list of pairs of\\nterm and docID, as in Figure 1.4. The core indexing step is sorting this list\\nSORTING\\nso that the terms are alphabetical, giving us the representation in the middle\\ncolumn of Figure 1.4. Multiple occurrences of the same term from the same\\ndocument are then merged.5 Instances of the same term are then grouped,\\nand the result is split into a dictionary and postings, as shown in the right\\ncolumn of Figure 1.4. Since a term generally occurs in a number of docu-\\nments, this data organization already reduces the storage requirements of\\nthe index. The dictionary also records some statistics, such as the number of\\ndocuments which contain each term (the document frequency, which is here\\nDOCUMENT\\nFREQUENCY\\nalso the length of each postings list). This information is not vital for a ba-\\nsic Boolean search engine, but it allows us to improve the efﬁciency of the\\n5. Unix users can note that these steps are similar to use of the sort and then uniq commands.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8\\n1\\nBoolean retrieval\\nDoc 1\\nDoc 2\\nI did enact Julius Caesar: I was killed\\ni’ the Capitol; Brutus killed me.\\nSo let it be with Caesar. The noble Brutus\\nhath told you Caesar was ambitious:\\nterm\\ndocID\\nI\\n1\\ndid\\n1\\nenact\\n1\\njulius\\n1\\ncaesar\\n1\\nI\\n1\\nwas\\n1\\nkilled\\n1\\ni’\\n1\\nthe\\n1\\ncapitol\\n1\\nbrutus\\n1\\nkilled\\n1\\nme\\n1\\nso\\n2\\nlet\\n2\\nit\\n2\\nbe\\n2\\nwith\\n2\\ncaesar\\n2\\nthe\\n2\\nnoble\\n2\\nbrutus\\n2\\nhath\\n2\\ntold\\n2\\nyou\\n2\\ncaesar\\n2\\nwas\\n2\\nambitious\\n2\\n=⇒\\nterm\\ndocID\\nambitious\\n2\\nbe\\n2\\nbrutus\\n1\\nbrutus\\n2\\ncapitol\\n1\\ncaesar\\n1\\ncaesar\\n2\\ncaesar\\n2\\ndid\\n1\\nenact\\n1\\nhath\\n1\\nI\\n1\\nI\\n1\\ni’\\n1\\nit\\n2\\njulius\\n1\\nkilled\\n1\\nkilled\\n1\\nlet\\n2\\nme\\n1\\nnoble\\n2\\nso\\n2\\nthe\\n1\\nthe\\n2\\ntold\\n2\\nyou\\n2\\nwas\\n1\\nwas\\n2\\nwith\\n2\\n=⇒\\nterm\\ndoc. freq.\\n→\\npostings lists\\nambitious\\n1\\n→\\n2\\nbe\\n1\\n→\\n2\\nbrutus\\n2\\n→\\n1 →2\\ncapitol\\n1\\n→\\n1\\ncaesar\\n2\\n→\\n1 →2\\ndid\\n1\\n→\\n1\\nenact\\n1\\n→\\n1\\nhath\\n1\\n→\\n2\\nI\\n1\\n→\\n1\\ni’\\n1\\n→\\n1\\nit\\n1\\n→\\n2\\njulius\\n1\\n→\\n1\\nkilled\\n1\\n→\\n1\\nlet\\n1\\n→\\n2\\nme\\n1\\n→\\n1\\nnoble\\n1\\n→\\n2\\nso\\n1\\n→\\n2\\nthe\\n2\\n→\\n1 →2\\ntold\\n1\\n→\\n2\\nyou\\n1\\n→\\n2\\nwas\\n2\\n→\\n1 →2\\nwith\\n1\\n→\\n2\\n◮Figure 1.4\\nBuilding an index by sorting and grouping. The sequence of terms\\nin each document, tagged by their documentID (left) is sorted alphabetically (mid-\\ndle). Instances of the same term are then grouped by word and then by documentID.\\nThe terms and documentIDs are then separated out (right). The dictionary stores\\nthe terms, and has a pointer to the postings list for each term. It commonly also\\nstores other summary information such as, here, the document frequency of each\\nterm. We use this information for improving query time efﬁciency and, later, for\\nweighting in ranked retrieval models. Each postings list stores the list of documents\\nin which a term occurs, and may store other information such as the term frequency\\n(the frequency of each term in each document) or the position(s) of the term in each\\ndocument.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.2\\nA ﬁrst take at building an inverted index\\n9\\nsearch engine at query time, and it is a statistic later used in many ranked re-\\ntrieval models. The postings are secondarily sorted by docID. This provides\\nthe basis for efﬁcient query processing. This inverted index structure is es-\\nsentially without rivals as the most efﬁcient structure for supporting ad hoc\\ntext search.\\nIn the resulting index, we pay for storage of both the dictionary and the\\npostings lists. The latter are much larger, but the dictionary is commonly\\nkept in memory, while postings lists are normally kept on disk, so the size\\nof each is important, and in Chapter 5 we will examine how each can be\\noptimized for storage and access efﬁciency. What data structure should be\\nused for a postings list? A ﬁxed length array would be wasteful as some\\nwords occur in many documents, and others in very few. For an in-memory\\npostings list, two good alternatives are singly linked lists or variable length\\narrays. Singly linked lists allow cheap insertion of documents into postings\\nlists (following updates, such as when recrawling the web for updated doc-\\numents), and naturally extend to more advanced indexing strategies such as\\nskip lists (Section 2.3), which require additional pointers. Variable length ar-\\nrays win in space requirements by avoiding the overhead for pointers and in\\ntime requirements because their use of contiguous memory increases speed\\non modern processors with memory caches. Extra pointers can in practice be\\nencoded into the lists as offsets. If updates are relatively infrequent, variable\\nlength arrays will be more compact and faster to traverse. We can also use a\\nhybrid scheme with a linked list of ﬁxed length arrays for each term. When\\npostings lists are stored on disk, they are stored (perhaps compressed) as a\\ncontiguous run of postings without explicit pointers (as in Figure 1.3), so as\\nto minimize the size of the postings list and the number of disk seeks to read\\na postings list into memory.\\n?\\nExercise 1.1\\n[⋆]\\nDraw the inverted index that would be built for the following document collection.\\n(See Figure 1.3 for an example.)\\nDoc 1\\nnew home sales top forecasts\\nDoc 2\\nhome sales rise in july\\nDoc 3\\nincrease in home sales in july\\nDoc 4\\njuly new home sales rise\\nExercise 1.2\\n[⋆]\\nConsider these documents:\\nDoc 1\\nbreakthrough drug for schizophrenia\\nDoc 2\\nnew schizophrenia drug\\nDoc 3\\nnew approach for treatment of schizophrenia\\nDoc 4\\nnew hopes for schizophrenia patients\\na. Draw the term-document incidence matrix for this document collection.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10\\n1\\nBoolean retrieval\\nBrutus\\n−→\\n1 →2 →4 →11 →31 →45 →173 →174\\nCalpurnia\\n−→\\n2 →31 →54 →101\\nIntersection\\n=⇒\\n2 →31\\n◮Figure 1.5\\nIntersecting the postings lists for Brutus and Calpurnia from Figure 1.3.\\nb. Draw the inverted index representation for this collection, as in Figure 1.3 (page 7).\\nExercise 1.3\\n[⋆]\\nFor the document collection shown in Exercise 1.2, what are the returned results for\\nthese queries:\\na. schizophrenia AND drug\\nb. for AND NOT(drug OR approach)\\n1.3\\nProcessing Boolean queries\\nHow do we process a query using an inverted index and the basic Boolean\\nretrieval model? Consider processing the simple conjunctive query:\\nSIMPLE CONJUNCTIVE\\nQUERIES\\n(1.1)\\nBrutus AND Calpurnia\\nover the inverted index partially shown in Figure 1.3 (page 7). We:\\n1. Locate Brutus in the Dictionary\\n2. Retrieve its postings\\n3. Locate Calpurnia in the Dictionary\\n4. Retrieve its postings\\n5. Intersect the two postings lists, as shown in Figure 1.5.\\nThe intersection operation is the crucial one: we need to efﬁciently intersect\\nPOSTINGS LIST\\nINTERSECTION\\npostings lists so as to be able to quickly ﬁnd documents that contain both\\nterms. (This operation is sometimes referred to as merging postings lists:\\nPOSTINGS MERGE\\nthis slightly counterintuitive name reﬂects using the term merge algorithm for\\na general family of algorithms that combine multiple sorted lists by inter-\\nleaved advancing of pointers through each; here we are merging the lists\\nwith a logical AND operation.)\\nThere is a simple and effective method of intersecting postings lists using\\nthe merge algorithm (see Figure 1.6): we maintain pointers into both lists\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.3\\nProcessing Boolean queries\\n11\\nINTERSECT(p1, p2)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen ADD(answer, docID(p1))\\n5\\np1 ←next(p1)\\n6\\np2 ←next(p2)\\n7\\nelse if docID(p1) < docID(p2)\\n8\\nthen p1 ←next(p1)\\n9\\nelse p2 ←next(p2)\\n10\\nreturn answer\\n◮Figure 1.6\\nAlgorithm for the intersection of two postings lists p1 and p2.\\nand walk through the two postings lists simultaneously, in time linear in\\nthe total number of postings entries. At each step, we compare the docID\\npointed to by both pointers. If they are the same, we put that docID in the\\nresults list, and advance both pointers. Otherwise we advance the pointer\\npointing to the smaller docID. If the lengths of the postings lists are x and\\ny, the intersection takes O(x + y) operations. Formally, the complexity of\\nquerying is Θ(N), where N is the number of documents in the collection.6\\nOur indexing methods gain us just a constant, not a difference in Θ time\\ncomplexity compared to a linear scan, but in practice the constant is huge.\\nTo use this algorithm, it is crucial that postings be sorted by a single global\\nordering. Using a numeric sort by docID is one simple way to achieve this.\\nWe can extend the intersection operation to process more complicated queries\\nlike:\\n(1.2)\\n(Brutus OR Caesar) AND NOT Calpurnia\\nQuery optimization is the process of selecting how to organize the work of an-\\nQUERY OPTIMIZATION\\nswering a query so that the least total amount of work needs to be done by\\nthe system. A major element of this for Boolean queries is the order in which\\npostings lists are accessed. What is the best order for query processing? Con-\\nsider a query that is an AND of t terms, for instance:\\n(1.3)\\nBrutus AND Caesar AND Calpurnia\\nFor each of the t terms, we need to get its postings, then AND them together.\\nThe standard heuristic is to process terms in order of increasing document\\n6. The notation Θ(·) is used to express an asymptotically tight bound on the complexity of\\nan algorithm. Informally, this is often written as O(·), but this notation really expresses an\\nasymptotic upper bound, which need not be tight (Cormen et al. 1990).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12\\n1\\nBoolean retrieval\\nINTERSECT(⟨t1, . . . , tn⟩)\\n1\\nterms ←SORTBYINCREASINGFREQUENCY(⟨t1, . . . , tn⟩)\\n2\\nresult ←postings( first(terms))\\n3\\nterms ←rest(terms)\\n4\\nwhile terms ̸= NIL and result ̸= NIL\\n5\\ndo result ←INTERSECT(result, postings( first(terms)))\\n6\\nterms ←rest(terms)\\n7\\nreturn result\\n◮Figure 1.7\\nAlgorithm for conjunctive queries that returns the set of documents\\ncontaining each term in the input list of terms.\\nfrequency: if we start by intersecting the two smallest postings lists, then all\\nintermediate results must be no bigger than the smallest postings list, and we\\nare therefore likely to do the least amount of total work. So, for the postings\\nlists in Figure 1.3 (page 7), we execute the above query as:\\n(1.4)\\n(Calpurnia AND Brutus) AND Caesar\\nThis is a ﬁrst justiﬁcation for keeping the frequency of terms in the dictionary:\\nit allows us to make this ordering decision based on in-memory data before\\naccessing any postings list.\\nConsider now the optimization of more general queries, such as:\\n(1.5)\\n(madding OR crowd) AND (ignoble OR strife) AND (killed OR slain)\\nAs before, we will get the frequencies for all terms, and we can then (con-\\nservatively) estimate the size of each OR by the sum of the frequencies of its\\ndisjuncts. We can then process the query in increasing order of the size of\\neach disjunctive term.\\nFor arbitrary Boolean queries, we have to evaluate and temporarily store\\nthe answers for intermediate expressions in a complex expression. However,\\nin many circumstances, either because of the nature of the query language,\\nor just because this is the most common type of query that users submit, a\\nquery is purely conjunctive. In this case, rather than viewing merging post-\\nings lists as a function with two inputs and a distinct output, it is more ef-\\nﬁcient to intersect each retrieved postings list with the current intermediate\\nresult in memory, where we initialize the intermediate result by loading the\\npostings list of the least frequent term. This algorithm is shown in Figure 1.7.\\nThe intersection operation is then asymmetric: the intermediate results list\\nis in memory while the list it is being intersected with is being read from\\ndisk. Moreover the intermediate results list is always at least as short as the\\nother list, and in many cases it is orders of magnitude shorter. The postings\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.3\\nProcessing Boolean queries\\n13\\nintersection can still be done by the algorithm in Figure 1.6, but when the\\ndifference between the list lengths is very large, opportunities to use alter-\\nnative techniques open up. The intersection can be calculated in place by\\ndestructively modifying or marking invalid items in the intermediate results\\nlist. Or the intersection can be done as a sequence of binary searches in the\\nlong postings lists for each posting in the intermediate results list. Another\\npossibility is to store the long postings list as a hashtable, so that membership\\nof an intermediate result item can be calculated in constant rather than linear\\nor log time. However, such alternative techniques are difﬁcult to combine\\nwith postings list compression of the sort discussed in Chapter 5. Moreover,\\nstandard postings list intersection operations remain necessary when both\\nterms of a query are very common.\\n?\\nExercise 1.4\\n[⋆]\\nFor the queries below, can we still run through the intersection in time O(x + y),\\nwhere x and y are the lengths of the postings lists for Brutus and Caesar? If not, what\\ncan we achieve?\\na. Brutus AND NOT Caesar\\nb. Brutus OR NOT Caesar\\nExercise 1.5\\n[⋆]\\nExtend the postings merge algorithm to arbitrary Boolean query formulas. What is\\nits time complexity? For instance, consider:\\nc. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\\nCan we always merge in linear time? Linear in what? Can we do better than this?\\nExercise 1.6\\n[⋆⋆]\\nWe can use distributive laws for AND and OR to rewrite queries.\\na. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using\\nthe distributive laws.\\nb. Would the resulting query be more or less efﬁciently evaluated than the original\\nform of this query?\\nc. Is this result true in general or does it depend on the words and the contents of\\nthe document collection?\\nExercise 1.7\\n[⋆]\\nRecommend a query processing order for\\nd. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)\\ngiven the following postings list sizes:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14\\n1\\nBoolean retrieval\\nTerm\\nPostings size\\neyes\\n213312\\nkaleidoscope\\n87009\\nmarmalade\\n107913\\nskies\\n271658\\ntangerine\\n46653\\ntrees\\n316812\\nExercise 1.8\\n[⋆]\\nIf the query is:\\ne. friends AND romans AND (NOT countrymen)\\nhow could we use the frequency of countrymen in evaluating the best query evaluation\\norder? In particular, propose a way of handling negation in determining the order of\\nquery processing.\\nExercise 1.9\\n[⋆⋆]\\nFor a conjunctive query, is processing postings lists in order of size guaranteed to be\\noptimal? Explain why it is, or give an example where it isn’t.\\nExercise 1.10\\n[⋆⋆]\\nWrite out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an x OR y\\nquery.\\nExercise 1.11\\n[⋆⋆]\\nHow should the Boolean query x AND NOT y be handled? Why is naive evaluation\\nof this query normally very expensive? Write out a postings merge algorithm that\\nevaluates this query efﬁciently.\\n1.4\\nThe extended Boolean model versus ranked retrieval\\nThe Boolean retrieval model contrasts with ranked retrieval models such as the\\nRANKED RETRIEVAL\\nMODEL\\nvector space model (Section 6.3), in which users largely use free text queries,\\nFREE TEXT QUERIES\\nthat is, just typing one or more words rather than using a precise language\\nwith operators for building up query expressions, and the system decides\\nwhich documents best satisfy the query. Despite decades of academic re-\\nsearch on the advantages of ranked retrieval, systems implementing the Boo-\\nlean retrieval model were the main or only search option provided by large\\ncommercial information providers for three decades until the early 1990s (ap-\\nproximately the date of arrival of the World Wide Web). However, these\\nsystems did not have just the basic Boolean operations (AND, OR, and NOT)\\nwhich we have presented so far. A strict Boolean expression over terms with\\nan unordered results set is too limited for many of the information needs\\nthat people have, and these systems implemented extended Boolean retrieval\\nmodels by incorporating additional operators such as term proximity oper-\\nators. A proximity operator is a way of specifying that two terms in a query\\nPROXIMITY OPERATOR\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.4\\nThe extended Boolean model versus ranked retrieval\\n15\\nmust occur close to each other in a document, where closeness may be mea-\\nsured by limiting the allowed number of intervening words or by reference\\nto a structural unit such as a sentence or paragraph.\\n\\x0f\\nExample 1.1: Commercial Boolean searching: Westlaw.\\nWestlaw (http://www.westlaw.com/)\\nis the largest commercial legal search service (in terms of the number of paying sub-\\nscribers), with over half a million subscribers performing millions of searches a day\\nover tens of terabytes of text data. The service was started in 1975. In 2005, Boolean\\nsearch (called “Terms and Connectors” by Westlaw) was still the default, and used\\nby a large percentage of users, although ranked free text querying (called “Natural\\nLanguage” by Westlaw) was added in 1992. Here are some example Boolean queries\\non Westlaw:\\nInformation need: Information on the legal theories involved in preventing the\\ndisclosure of trade secrets by employees formerly employed by a competing\\ncompany. Query: \"trade secret\" /s disclos! /s prevent /s employe!\\nInformation need: Requirements for disabled people to be able to access a work-\\nplace.\\nQuery: disab! /p access! /s work-site work-place (employment /3 place)\\nInformation need: Cases about a host’s responsibility for drunk guests.\\nQuery: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\\nNote the long, precise queries and the use of proximity operators, both uncommon\\nin web search. Submitted queries average about ten words in length. Unlike web\\nsearch conventions, a space between words represents disjunction (the tightest bind-\\ning operator), & is AND and /s, /p, and /k ask for matches in the same sentence,\\nsame paragraph or within k words respectively. Double quotes give a phrase search\\n(consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trail-\\ning wildcard query (see Section 3.2, page 51); thus liab! matches all words starting\\nwith liab. Additionally work-site matches any of worksite, work-site or work site; see\\nSection 2.2.1 (page 22). Typical expert queries are usually carefully deﬁned and incre-\\nmentally developed until they obtain what look to be good results to the user.\\nMany users, particularly professionals, prefer Boolean query models.\\nBoolean\\nqueries are precise: a document either matches the query or it does not. This of-\\nfers the user greater control and transparency over what is retrieved. And some do-\\nmains, such as legal materials, allow an effective means of document ranking within a\\nBoolean model: Westlaw returns documents in reverse chronological order, which is\\nin practice quite effective. In 2007, the majority of law librarians still seem to rec-\\nommend terms and connectors for high recall searches, and the majority of legal\\nusers think they are getting greater control by using them. However, this does not\\nmean that Boolean queries are more effective for professional searchers. Indeed, ex-\\nperimenting on a Westlaw subcollection, Turtle (1994) found that free text queries\\nproduced better results than Boolean queries prepared by Westlaw’s own reference\\nlibrarians for the majority of the information needs in his experiments. A general\\nproblem with Boolean search is that using AND operators tends to produce high pre-\\ncision but low recall searches, while using OR operators gives low precision but high\\nrecall searches, and it is difﬁcult or impossible to ﬁnd a satisfactory middle ground.\\nIn this chapter, we have looked at the structure and construction of a basic\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16\\n1\\nBoolean retrieval\\ninverted index, comprising a dictionary and postings lists. We introduced\\nthe Boolean retrieval model, and examined how to do efﬁcient retrieval via\\nlinear time merges and simple query optimization. In Chapters 2–7 we will\\nconsider in detail richer query models and the sort of augmented index struc-\\ntures that are needed to handle them efﬁciently. Here we just mention a few\\nof the main additional things we would like to be able to do:\\n1. We would like to better determine the set of terms in the dictionary and\\nto provide retrieval that is tolerant to spelling mistakes and inconsistent\\nchoice of words.\\n2. It is often useful to search for compounds or phrases that denote a concept\\nsuch as “operating system”. As the Westlaw examples show, we might also\\nwish to do proximity queries such as Gates NEAR Microsoft. To answer\\nsuch queries, the index has to be augmented to capture the proximities of\\nterms in documents.\\n3. A Boolean model only records term presence or absence, but often we\\nwould like to accumulate evidence, giving more weight to documents that\\nhave a term several times as opposed to ones that contain it only once. To\\nbe able to do this we need term frequency information (the number of times\\nTERM FREQUENCY\\na term occurs in a document) in postings lists.\\n4. Boolean queries just retrieve a set of matching documents, but commonly\\nwe wish to have an effective method to order (or “rank”) the returned\\nresults. This requires having a mechanism for determining a document\\nscore which encapsulates how good a match a document is for a query.\\nWith these additional ideas, we will have seen most of the basic technol-\\nogy that supports ad hoc searching over unstructured information. Ad hoc\\nsearching over documents has recently conquered the world, powering not\\nonly web search engines but the kind of unstructured search that lies behind\\nthe large eCommerce websites. Although the main web search engines differ\\nby emphasizing free text querying, most of the basic issues and technologies\\nof indexing and querying remain the same, as we will see in later chapters.\\nMoreover, over time, web search engines have added at least partial imple-\\nmentations of some of the most popular operators from extended Boolean\\nmodels: phrase search is especially popular and most have a very partial\\nimplementation of Boolean operators. Nevertheless, while these options are\\nliked by expert searchers, they are little used by most people and are not the\\nmain focus in work on trying to improve web search engine performance.\\n?\\nExercise 1.12\\n[⋆]\\nWrite a query using Westlaw syntax which would ﬁnd any of the words professor,\\nteacher, or lecturer in the same sentence as a form of the verb explain.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n1.5\\nReferences and further reading\\n17\\nExercise 1.13\\n[⋆]\\nTry using the Boolean search features on a couple of major web search engines. For\\ninstance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar\\nAND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and\\ntop hits. Do they make sense in terms of Boolean logic? Often they haven’t for major\\nsearch engines. Can you make sense of what is going on? What about if you try\\ndifferent words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR\\nconquer. What bound should the number of results from the ﬁrst two queries place\\non the third query? Is this bound observed?\\n1.5\\nReferences and further reading\\nThe practical pursuit of computerized information retrieval began in the late\\n1940s (Cleverdon 1991, Liddy 2005). A great increase in the production of\\nscientiﬁc literature, much in the form of less formal technical reports rather\\nthan traditional journal articles, coupled with the availability of computers,\\nled to interest in automatic document retrieval. However, in those days, doc-\\nument retrieval was always based on author, title, and keywords; full-text\\nsearch came much later.\\nThe article of Bush (1945) provided lasting inspiration for the new ﬁeld:\\n“Consider a future device for individual use, which is a sort of mech-\\nanized private ﬁle and library. It needs a name, and, to coin one at\\nrandom, ‘memex’ will do. A memex is a device in which an individual\\nstores all his books, records, and communications, and which is mech-\\nanized so that it may be consulted with exceeding speed and ﬂexibility.\\nIt is an enlarged intimate supplement to his memory.”\\nThe term Information Retrieval was coined by Calvin Mooers in 1948/1950\\n(Mooers 1950).\\nIn 1958, much newspaper attention was paid to demonstrations at a con-\\nference (see Taube and Wooster 1958) of IBM “auto-indexing” machines, based\\nprimarily on the work of H. P. Luhn. Commercial interest quickly gravitated\\ntowards Boolean retrieval systems, but the early years saw a heady debate\\nover various disparate technologies for retrieval systems. For example Moo-\\ners (1961) dissented:\\n“It is a common fallacy, underwritten at this date by the investment of\\nseveral million dollars in a variety of retrieval hardware, that the al-\\ngebra of George Boole (1847) is the appropriate formalism for retrieval\\nsystem design. This view is as widely and uncritically accepted as it is\\nwrong.”\\nThe observation of AND vs. OR giving you opposite extremes in a precision/\\nrecall tradeoff, but not the middle ground comes from (Lee and Fox 1988).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18\\n1\\nBoolean retrieval\\nThe book (Witten et al. 1999) is the standard reference for an in-depth com-\\nparison of the space and time efﬁciency of the inverted index versus other\\npossible data structures; a more succinct and up-to-date presentation ap-\\npears in Zobel and Moffat (2006). We further discuss several approaches in\\nChapter 5.\\nFriedl (2006) covers the practical usage of regular expressions for searching.\\nREGULAR EXPRESSIONS\\nThe underlying computer science appears in (Hopcroft et al. 2000).\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n19\\n2\\nThe term vocabulary and postings\\nlists\\nRecall the major steps in inverted index construction:\\n1. Collect the documents to be indexed.\\n2. Tokenize the text.\\n3. Do linguistic preprocessing of tokens.\\n4. Index the documents that each term occurs in.\\nIn this chapter we ﬁrst brieﬂy mention how the basic unit of a document can\\nbe deﬁned and how the character sequence that it comprises is determined\\n(Section 2.1). We then examine in detail some of the substantive linguis-\\ntic issues of tokenization and linguistic preprocessing, which determine the\\nvocabulary of terms which a system uses (Section 2.2). Tokenization is the\\nprocess of chopping character streams into tokens, while linguistic prepro-\\ncessing then deals with building equivalence classes of tokens which are the\\nset of terms that are indexed. Indexing itself is covered in Chapters 1 and 4.\\nThen we return to the implementation of postings lists. In Section 2.3, we\\nexamine an extended postings list data structure that supports faster query-\\ning, while Section 2.4 covers building postings data structures suitable for\\nhandling phrase and proximity queries, of the sort that commonly appear in\\nboth extended Boolean models and on the web.\\n2.1\\nDocument delineation and character sequence decoding\\n2.1.1\\nObtaining the character sequence in a document\\nDigital documents that are the input to an indexing process are typically\\nbytes in a ﬁle or on a web server. The ﬁrst step of processing is to convert this\\nbyte sequence into a linear sequence of characters. For the case of plain En-\\nglish text in ASCII encoding, this is trivial. But often things get much more\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20\\n2\\nThe term vocabulary and postings lists\\ncomplex. The sequence of characters may be encoded by one of various sin-\\ngle byte or multibyte encoding schemes, such as Unicode UTF-8, or various\\nnational or vendor-speciﬁc standards. We need to determine the correct en-\\ncoding. This can be regarded as a machine learning classiﬁcation problem,\\nas discussed in Chapter 13,1 but is often handled by heuristic methods, user\\nselection, or by using provided document metadata. Once the encoding is\\ndetermined, we decode the byte sequence to a character sequence. We might\\nsave the choice of encoding because it gives some evidence about what lan-\\nguage the document is written in.\\nThe characters may have to be decoded out of some binary representation\\nlike Microsoft Word DOC ﬁles and/or a compressed format such as zip ﬁles.\\nAgain, we must determine the document format, and then an appropriate\\ndecoder has to be used. Even for plain text documents, additional decoding\\nmay need to be done. In XML documents (Section 10.1, page 197), charac-\\nter entities, such as &amp;, need to be decoded to give the correct character,\\nnamely & for &amp;. Finally, the textual part of the document may need to\\nbe extracted out of other material that will not be processed. This might be\\nthe desired handling for XML ﬁles, if the markup is going to be ignored; we\\nwould almost certainly want to do this with postscript or PDF ﬁles. We will\\nnot deal further with these issues in this book, and will assume henceforth\\nthat our documents are a list of characters. Commercial products usually\\nneed to support a broad range of document types and encodings, since users\\nwant things to just work with their data as is. Often, they just think of docu-\\nments as text inside applications and are not even aware of how it is encoded\\non disk. This problem is usually solved by licensing a software library that\\nhandles decoding document formats and character encodings.\\nThe idea that text is a linear sequence of characters is also called into ques-\\ntion by some writing systems, such as Arabic, where text takes on some\\ntwo dimensional and mixed order characteristics, as shown in Figures 2.1\\nand 2.2. But, despite some complicated writing system conventions, there\\nis an underlying sequence of sounds being represented and hence an essen-\\ntially linear structure remains, and this is what is represented in the digital\\nrepresentation of Arabic, as shown in Figure 2.1.\\n2.1.2\\nChoosing a document unit\\nThe next phase is to determine what the document unit for indexing is. Thus\\nDOCUMENT UNIT\\nfar we have assumed that documents are ﬁxed units for the purposes of in-\\ndexing. For example, we take each ﬁle in a folder as a document. But there\\n1. A classiﬁer is a function that takes objects of some sort and assigns them to one of a number\\nof distinct classes (see Chapter 13). Usually classiﬁcation is done by machine learning methods\\nsuch as probabilistic models, but it can also be done by hand-written rules.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.1\\nDocument delineation and character sequence decoding\\n21\\nٌ     ك   ِ  ت  ا   ب   ٌ     ⇐   آِ\\x05َ\\x03ب\\n             un b ā  t  i k  \\n/kitābun/ ‘a book’ \\n \\n◮Figure 2.1\\nAn example of a vocalized Modern Standard Arabic word. The writing\\nis from right to left and letters undergo complex mutations as they are combined. The\\nrepresentation of short vowels (here, /i/ and /u/) and the ﬁnal /n/ (nunation) de-\\nparts from strict linearity by being represented as diacritics above and below letters.\\nNevertheless, the represented text is still clearly a linear ordering of characters repre-\\nsenting sounds. Full vocalization, as here, normally appears only in the Koran and\\nchildren’s books. Day-to-day text is unvocalized (short vowels are not represented\\nbut the letter for ¯a would still appear) or partially vocalized, with short vowels in-\\nserted in places where the writer perceives ambiguities. These choices add further\\ncomplexities to indexing.\\n \\x02\\x03\\x04 \\x05\\x06 \\x07\\x08ا\\x04\\x10\\x0f\\x0e\\r ا\\x0c\\x0b\\nا1962\\n \\x15\\x16\\x17 \\n132\\n\\x05\\x19\\x1a\\x07\\x1b\\x0c #\"!\" !  ا\\x1f\\x1e\\x10\\x1dل ا\\n. \\n \\n ←                   → ←   → ←                               START \\n‘Algeria achieved its independence in 1962 after 132 years of French occupation.’ \\n \\n◮Figure 2.2\\nThe conceptual linear order of characters is not necessarily the order\\nthat you see on the page. In languages that are written right-to-left, such as Hebrew\\nand Arabic, it is quite common to also have left-to-right text interspersed, such as\\nnumbers and dollar amounts. With modern Unicode representation concepts, the\\norder of characters in ﬁles matches the conceptual order, and the reversal of displayed\\ncharacters is handled by the rendering system, but this may not be true for documents\\nin older encodings.\\nare many cases in which you might want to do something different. A tra-\\nditional Unix (mbox-format) email ﬁle stores a sequence of email messages\\n(an email folder) in one ﬁle, but you might wish to regard each email mes-\\nsage as a separate document. Many email messages now contain attached\\ndocuments, and you might then want to regard the email message and each\\ncontained attachment as separate documents. If an email message has an\\nattached zip ﬁle, you might want to decode the zip ﬁle and regard each ﬁle\\nit contains as a separate document. Going in the opposite direction, various\\npieces of web software (such as latex2html) take things that you might regard\\nas a single document (e.g., a Powerpoint ﬁle or a LATEX document) and split\\nthem into separate HTML pages for each slide or subsection, stored as sep-\\narate ﬁles. In these cases, you might want to combine multiple ﬁles into a\\nsingle document.\\nMore generally, for very long documents, the issue of indexing granularity\\nINDEXING\\nGRANULARITY\\narises. For a collection of books, it would usually be a bad idea to index an\\n', 'Online edition (c)\\n2009 Cambridge UP\\n22\\n2\\nThe term vocabulary and postings lists\\nentire book as a document. A search for Chinese toys might bring up a book\\nthat mentions China in the ﬁrst chapter and toys in the last chapter, but this\\ndoes not make it relevant to the query. Instead, we may well wish to index\\neach chapter or paragraph as a mini-document. Matches are then more likely\\nto be relevant, and since the documents are smaller it will be much easier for\\nthe user to ﬁnd the relevant passages in the document. But why stop there?\\nWe could treat individual sentences as mini-documents. It becomes clear\\nthat there is a precision/recall tradeoff here. If the units get too small, we\\nare likely to miss important passages because terms were distributed over\\nseveral mini-documents, while if units are too large we tend to get spurious\\nmatches and the relevant information is hard for the user to ﬁnd.\\nThe problems with large document units can be alleviated by use of ex-\\nplicit or implicit proximity search (Sections 2.4.2 and 7.2.2), and the trade-\\noffs in resulting system performance that we are hinting at are discussed\\nin Chapter 8. The issue of index granularity, and in particular a need to\\nsimultaneously index documents at multiple levels of granularity, appears\\nprominently in XML retrieval, and is taken up again in Chapter 10. An IR\\nsystem should be designed to offer choices of granularity. For this choice to\\nbe made well, the person who is deploying the system must have a good\\nunderstanding of the document collection, the users, and their likely infor-\\nmation needs and usage patterns. For now, we will henceforth assume that\\na suitable size document unit has been chosen, together with an appropriate\\nway of dividing or aggregating ﬁles, if needed.\\n2.2\\nDetermining the vocabulary of terms\\n2.2.1\\nTokenization\\nGiven a character sequence and a deﬁned document unit, tokenization is the\\ntask of chopping it up into pieces, called tokens, perhaps at the same time\\nthrowing away certain characters, such as punctuation. Here is an example\\nof tokenization:\\nInput: Friends, Romans, Countrymen, lend me your ears;\\nOutput: Friends\\nRomans\\nCountrymen\\nlend\\nme\\nyour\\nears\\nThese tokens are often loosely referred to as terms or words, but it is some-\\ntimes important to make a type/token distinction. A token is an instance\\nTOKEN\\nof a sequence of characters in some particular document that are grouped\\ntogether as a useful semantic unit for processing. A type is the class of all\\nTYPE\\ntokens containing the same character sequence. A term is a (perhaps nor-\\nTERM\\nmalized) type that is included in the IR system’s dictionary. The set of index\\nterms could be entirely distinct from the tokens, for instance, they could be\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n23\\nsemantic identiﬁers in a taxonomy, but in practice in modern IR systems they\\nare strongly related to the tokens in the document. However, rather than be-\\ning exactly the tokens that appear in the document, they are usually derived\\nfrom them by various normalization processes which are discussed in Sec-\\ntion 2.2.3.2 For example, if the document to be indexed is to sleep perchance to\\ndream, then there are 5 tokens, but only 4 types (since there are 2 instances of\\nto). However, if to is omitted from the index (as a stop word, see Section 2.2.2\\n(page 27)), then there will be only 3 terms: sleep, perchance, and dream.\\nThe major question of the tokenization phase is what are the correct tokens\\nto use? In this example, it looks fairly trivial: you chop on whitespace and\\nthrow away punctuation characters. This is a starting point, but even for\\nEnglish there are a number of tricky cases. For example, what do you do\\nabout the various uses of the apostrophe for possession and contractions?\\nMr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t\\namusing.\\nFor O’Neill, which of the following is the desired tokenization?\\nneill\\noneill\\no’neill\\no’\\nneill\\no\\nneill ?\\nAnd for aren’t, is it:\\naren’t\\narent\\nare\\nn’t\\naren\\nt ?\\nA simple strategy is to just split on all non-alphanumeric characters, but\\nwhile o\\nneill looks okay, aren\\nt looks intuitively bad. For all of them,\\nthe choices determine which Boolean queries will match. A query of neill\\nAND capital will match in three cases but not the other two. In how many\\ncases would a query of o’neill AND capital match? If no preprocessing of a\\nquery is done, then it would match in only one of the ﬁve cases. For either\\n2. That is, as deﬁned here, tokens that are not indexed (stop words) are not terms, and if mul-\\ntiple tokens are collapsed together via normalization, they are indexed as one term, under the\\nnormalized form. However, we later relax this deﬁnition when discussing classiﬁcation and\\nclustering in Chapters 13–18, where there is no index. In these chapters, we drop the require-\\nment of inclusion in the dictionary. A term means a normalized word.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n24\\n2\\nThe term vocabulary and postings lists\\nBoolean or free text queries, you always want to do the exact same tokeniza-\\ntion of document and query words, generally by processing queries with the\\nsame tokenizer. This guarantees that a sequence of characters in a text will\\nalways match the same sequence typed in a query.3\\nThese issues of tokenization are language-speciﬁc. It thus requires the lan-\\nguage of the document to be known. Language identiﬁcation based on clas-\\nLANGUAGE\\nIDENTIFICATION\\nsiﬁers that use short character subsequences as features is highly effective;\\nmost languages have distinctive signature patterns (see page 46 for refer-\\nences).\\nFor most languages and particular domains within them there are unusual\\nspeciﬁc tokens that we wish to recognize as terms, such as the programming\\nlanguages C++ and C#, aircraft names like B-52, or a T.V. show name such\\nas M*A*S*H – which is sufﬁciently integrated into popular culture that you\\nﬁnd usages such as M*A*S*H-style hospitals. Computer technology has in-\\ntroduced new types of character sequences that a tokenizer should probably\\ntokenize as a single token, including email addresses (jblack@mail.yahoo.com),\\nweb URLs (http://stuff.big.com/new/specials.html),numeric IP addresses (142.32.48.231),\\npackage tracking numbers (1Z9999W99845399981), and more. One possible\\nsolution is to omit from indexing tokens such as monetary amounts, num-\\nbers, and URLs, since their presence greatly expands the size of the vocab-\\nulary. However, this comes at a large cost in restricting what people can\\nsearch for. For instance, people might want to search in a bug database for\\nthe line number where an error occurs. Items such as the date of an email,\\nwhich have a clear semantic type, are often indexed separately as document\\nmetadata (see Section 6.1, page 110).\\nIn English, hyphenation is used for various purposes ranging from split-\\nHYPHENS\\nting up vowels in words (co-education) to joining nouns as names (Hewlett-\\nPackard) to a copyediting device to show word grouping (the hold-him-back-\\nand-drag-him-away maneuver). It is easy to feel that the ﬁrst example should be\\nregarded as one token (and is indeed more commonly written as just coedu-\\ncation), the last should be separated into words, and that the middle case is\\nunclear. Handling hyphens automatically can thus be complex: it can either\\nbe done as a classiﬁcation problem, or more commonly by some heuristic\\nrules, such as allowing short hyphenated preﬁxes on words, but not longer\\nhyphenated forms.\\nConceptually, splitting on white space can also split what should be re-\\ngarded as a single token. This occurs most commonly with names (San Fran-\\ncisco, Los Angeles) but also with borrowed foreign phrases (au fait) and com-\\n3. For the free text case, this is straightforward. The Boolean case is more complex: this tok-\\nenization may produce multiple terms from one query word. This can be handled by combining\\nthe terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system\\nto handle the opposite case where the user entered as two terms something that was tokenized\\ntogether in the document processing.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n25\\npounds that are sometimes written as a single word and sometimes space\\nseparated (such as white space vs. whitespace). Other cases with internal spaces\\nthat we might wish to regard as a single token include phone numbers ((800) 234-\\n2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad\\nretrieval results, for example, if a search for York University mainly returns\\ndocuments containing New York University. The problems of hyphens and\\nnon-separating whitespace can even interact. Advertisements for air fares\\nfrequently contain items like San Francisco-Los Angeles, where simply doing\\nwhitespace splitting would give unfortunate results. In such cases, issues of\\ntokenization interact with handling phrase queries (which we discuss in Sec-\\ntion 2.4 (page 39)), particularly if we would like queries for all of lowercase,\\nlower-case and lower case to return the same results. The last two can be han-\\ndled by splitting on hyphens and using a phrase index. Getting the ﬁrst case\\nright would depend on knowing that it is sometimes written as two words\\nand also indexing it in this way. One effective strategy in practice, which\\nis used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis\\n(Example 1.1), is to encourage users to enter hyphens wherever they may be\\npossible, and whenever there is a hyphenated form, the system will general-\\nize the query to cover all three of the one word, hyphenated, and two word\\nforms, so that a query for over-eager will search for over-eager OR “over eager”\\nOR overeager. However, this strategy depends on user training, since if you\\nquery using either of the other two forms, you get no generalization.\\nEach new language presents some new issues. For instance, French has a\\nvariant use of the apostrophe for a reduced deﬁnite article ‘the’ before a word\\nbeginning with a vowel (e.g., l’ensemble) and has some uses of the hyphen\\nwith postposed clitic pronouns in imperatives and questions (e.g., donne-\\nmoi ‘give me’). Getting the ﬁrst case correct will affect the correct indexing\\nof a fair percentage of nouns and adjectives: you would want documents\\nmentioning both l’ensemble and un ensemble to be indexed under ensemble.\\nOther languages make the problem harder in new ways. German writes\\ncompound nouns without spaces (e.g., Computerlinguistik ‘computational lin-\\nCOMPOUNDS\\nguistics’; Lebensversicherungsgesellschaftsangestellter ‘life insurance company\\nemployee’). Retrieval systems for German greatly beneﬁt from the use of a\\ncompound-splitter module, which is usually implemented by seeing if a word\\nCOMPOUND-SPLITTER\\ncan be subdivided into multiple words that appear in a vocabulary. This phe-\\nnomenon reaches its limit case with major East Asian Languages (e.g., Chi-\\nnese, Japanese, Korean, and Thai), where text is written without any spaces\\nbetween words. An example is shown in Figure 2.3. One approach here is to\\nperform word segmentation as prior linguistic processing. Methods of word\\nWORD SEGMENTATION\\nsegmentation vary from having a large vocabulary and taking the longest\\nvocabulary match with some heuristics for unknown words to the use of\\nmachine learning sequence models, such as hidden Markov models or condi-\\ntional random ﬁelds, trained over hand-segmented words (see the references\\n', 'Online edition (c)\\n2009 Cambridge UP\\n26\\n2\\nThe term vocabulary and postings lists\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x04\\n\\x05\\n\\x06\\n\\x07\\n\\x05\\n\\x08\\n\\t\\n\\n\\x0b\\n\\x0c\\n\\r\\n\\x0e\\n\\x0f\\n\\x10\\n\\x11\\n\\x12\\n\\x13\\n\\x14\\n\\x15\\n\\x16\\n\\x17\\n\\x18\\n\\x19\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x05\\n\\x08\\n\\t\\n\\x1a\\n\\x1b\\n\\x1c\\n\\x1d\\n\\x1e\\n\\x1f\\n \\n!\\n\"\\n#\\n$\\n%\\n&\\n\\'\\n\\x18\\n\\x12\\n\\'\\n\\x18\\n(\\n)\\n*\\n\\x19\\n\\x00\\x01\\n\\x02\\n\\x03\\n+\\n,\\n#\\n-\\n\\x08\\n\\r\\n.\\n/\\n\\x12\\n◮Figure 2.3\\nThe standard unsegmented form of Chinese text using the simpliﬁed\\ncharacters of mainland China. There is no whitespace between words, not even be-\\ntween sentences – the apparent space after the Chinese period (◦) is just a typograph-\\nical illusion caused by placing the character on the left side of its square box. The\\nﬁrst sentence is just words in Chinese characters with no spaces between them. The\\nsecond and third sentences include Arabic numerals and punctuation breaking up\\nthe Chinese characters.\\n◮Figure 2.4\\nAmbiguities in Chinese word segmentation. The two characters can\\nbe treated as one word meaning ‘monk’ or as a sequence of two words meaning ‘and’\\nand ‘still’.\\na\\nan\\nand\\nare\\nas\\nat\\nbe\\nby\\nfor\\nfrom\\nhas\\nhe\\nin\\nis\\nit\\nits\\nof\\non\\nthat\\nthe\\nto\\nwas\\nwere\\nwill\\nwith\\n◮Figure 2.5\\nA stop list of 25 semantically non-selective words which are common\\nin Reuters-RCV1.\\nin Section 2.5). Since there are multiple possible segmentations of character\\nsequences (see Figure 2.4), all such methods make mistakes sometimes, and\\nso you are never guaranteed a consistent unique tokenization. The other ap-\\nproach is to abandon word-based indexing and to do all indexing via just\\nshort subsequences of characters (character k-grams), regardless of whether\\nparticular sequences cross word boundaries or not. Three reasons why this\\napproach is appealing are that an individual Chinese character is more like a\\nsyllable than a letter and usually has some semantic content, that most words\\nare short (the commonest length is 2 characters), and that, given the lack of\\nstandardization of word breaking in the writing system, it is not always clear\\nwhere word boundaries should be placed anyway. Even in English, some\\ncases of where to put word boundaries are just orthographic conventions –\\nthink of notwithstanding vs. not to mention or into vs. on to – but people are\\neducated to write the words with consistent use of spaces.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n27\\n2.2.2\\nDropping common terms: stop words\\nSometimes, some extremely common words which would appear to be of\\nlittle value in helping select documents matching a user need are excluded\\nfrom the vocabulary entirely. These words are called stop words. The general\\nSTOP WORDS\\nstrategy for determining a stop list is to sort the terms by collection frequency\\nCOLLECTION\\nFREQUENCY\\n(the total number of times each term appears in the document collection),\\nand then to take the most frequent terms, often hand-ﬁltered for their se-\\nmantic content relative to the domain of the documents being indexed, as\\na stop list, the members of which are then discarded during indexing. An\\nSTOP LIST\\nexample of a stop list is shown in Figure 2.5. Using a stop list signiﬁcantly\\nreduces the number of postings that a system has to store; we will present\\nsome statistics on this in Chapter 5 (see Table 5.1, page 87). And a lot of\\nthe time not indexing stop words does little harm: keyword searches with\\nterms like the and by don’t seem very useful. However, this is not true for\\nphrase searches. The phrase query “President of the United States”, which con-\\ntains two stop words, is more precise than President AND “United States”. The\\nmeaning of ﬂights to London is likely to be lost if the word to is stopped out. A\\nsearch for Vannevar Bush’s article As we may think will be difﬁcult if the ﬁrst\\nthree words are stopped out, and the system searches simply for documents\\ncontaining the word think. Some special query types are disproportionately\\naffected. Some song titles and well known pieces of verse consist entirely of\\nwords that are commonly on stop lists (To be or not to be, Let It Be, I don’t want\\nto be, ...).\\nThe general trend in IR systems over time has been from standard use of\\nquite large stop lists (200–300 terms) to very small stop lists (7–12 terms)\\nto no stop list whatsoever. Web search engines generally do not use stop\\nlists. Some of the design of modern IR systems has focused precisely on\\nhow we can exploit the statistics of language so as to be able to cope with\\ncommon words in better ways. We will show in Section 5.3 (page 95) how\\ngood compression techniques greatly reduce the cost of storing the postings\\nfor common words. Section 6.2.1 (page 117) then discusses how standard\\nterm weighting leads to very common words having little impact on doc-\\nument rankings. Finally, Section 7.1.5 (page 140) shows how an IR system\\nwith impact-sorted indexes can terminate scanning a postings list early when\\nweights get small, and hence common words do not cause a large additional\\nprocessing cost for the average query, even though postings lists for stop\\nwords are very long. So for most modern IR systems, the additional cost of\\nincluding stop words is not that big – neither in terms of index size nor in\\nterms of query processing time.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n28\\n2\\nThe term vocabulary and postings lists\\nQuery term\\nTerms in documents that should be matched\\nWindows\\nWindows\\nwindows\\nWindows, windows, window\\nwindow\\nwindow, windows\\n◮Figure 2.6\\nAn example of how asymmetric expansion of query terms can usefully\\nmodel users’ expectations.\\n2.2.3\\nNormalization (equivalence classing of terms)\\nHaving broken up our documents (and also our query) into tokens, the easy\\ncase is if tokens in the query just match tokens in the token list of the doc-\\nument. However, there are many cases when two character sequences are\\nnot quite the same but you would like a match to occur. For instance, if you\\nsearch for USA, you might hope to also match documents containing U.S.A.\\nToken normalization is the process of canonicalizing tokens so that matches\\nTOKEN\\nNORMALIZATION\\noccur despite superﬁcial differences in the character sequences of the to-\\nkens.4 The most standard way to normalize is to implicitly create equivalence\\nEQUIVALENCE CLASSES\\nclasses, which are normally named after one member of the set. For instance,\\nif the tokens anti-discriminatory and antidiscriminatory are both mapped onto\\nthe term antidiscriminatory, in both the document text and queries, then searches\\nfor one term will retrieve documents that contain either.\\nThe advantage of just using mapping rules that remove characters like hy-\\nphens is that the equivalence classing to be done is implicit, rather than being\\nfully calculated in advance: the terms that happen to become identical as the\\nresult of these rules are the equivalence classes. It is only easy to write rules\\nof this sort that remove characters. Since the equivalence classes are implicit,\\nit is not obvious when you might want to add characters. For instance, it\\nwould be hard to know to turn antidiscriminatory into anti-discriminatory.\\nAn alternative to creating equivalence classes is to maintain relations be-\\ntween unnormalized tokens. This method can be extended to hand-constructed\\nlists of synonyms such as car and automobile, a topic we discuss further in\\nChapter 9. These term relationships can be achieved in two ways. The usual\\nway is to index unnormalized tokens and to maintain a query expansion list\\nof multiple vocabulary entries to consider for a certain query term. A query\\nterm is then effectively a disjunction of several postings lists. The alterna-\\ntive is to perform the expansion during index construction. When the doc-\\nument contains automobile, we index it under car as well (and, usually, also\\nvice-versa). Use of either of these methods is considerably less efﬁcient than\\nequivalence classing, as there are more postings to store and merge. The ﬁrst\\n4. It is also often referred to as term normalization, but we prefer to reserve the name term for the\\noutput of the normalization process.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n29\\nmethod adds a query expansion dictionary and requires more processing at\\nquery time, while the second method requires more space for storing post-\\nings. Traditionally, expanding the space required for the postings lists was\\nseen as more disadvantageous, but with modern storage costs, the increased\\nﬂexibility that comes from distinct postings lists is appealing.\\nThese approaches are more ﬂexible than equivalence classes because the\\nexpansion lists can overlap while not being identical. This means there can\\nbe an asymmetry in expansion. An example of how such an asymmetry can\\nbe exploited is shown in Figure 2.6: if the user enters windows, we wish to\\nallow matches with the capitalized Windows operating system, but this is not\\nplausible if the user enters window, even though it is plausible for this query\\nto also match lowercase windows.\\nThe best amount of equivalence classing or query expansion to do is a\\nfairly open question. Doing some deﬁnitely seems a good idea. But doing a\\nlot can easily have unexpected consequences of broadening queries in unin-\\ntended ways. For instance, equivalence-classing U.S.A. and USA to the latter\\nby deleting periods from tokens might at ﬁrst seem very reasonable, given\\nthe prevalent pattern of optional use of periods in acronyms. However, if I\\nput in as my query term C.A.T., I might be rather upset if it matches every\\nappearance of the word cat in documents.5\\nBelow we present some of the forms of normalization that are commonly\\nemployed and how they are implemented. In many cases they seem helpful,\\nbut they can also do harm. In fact, you can worry about many details of\\nequivalence classing, but it often turns out that providing processing is done\\nconsistently to the query and to documents, the ﬁne details may not have\\nmuch aggregate effect on performance.\\nAccents and diacritics.\\nDiacritics on characters in English have a fairly\\nmarginal status, and we might well want cliché and cliche to match, or naive\\nand naïve. This can be done by normalizing tokens to remove diacritics. In\\nmany other languages, diacritics are a regular part of the writing system and\\ndistinguish different sounds. Occasionally words are distinguished only by\\ntheir accents. For instance, in Spanish, peña is ‘a cliff’, while pena is ‘sorrow’.\\nNevertheless, the important question is usually not prescriptive or linguistic\\nbut is a question of how users are likely to write queries for these words. In\\nmany cases, users will enter queries for words without diacritics, whether\\nfor reasons of speed, laziness, limited software, or habits born of the days\\nwhen it was hard to use non-ASCII text on many computer systems. In these\\ncases, it might be best to equate all words to a form without diacritics.\\n5. At the time we wrote this chapter (Aug. 2005), this was actually the case on Google: the top\\nresult for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n30\\n2\\nThe term vocabulary and postings lists\\nCapitalization/case-folding.\\nA common strategy is to do case-folding by re-\\nCASE-FOLDING\\nducing all letters to lower case. Often this is a good idea: it will allow in-\\nstances of Automobile at the beginning of a sentence to match with a query of\\nautomobile. It will also help on a web search engine when most of your users\\ntype in ferrari when they are interested in a Ferrari car. On the other hand,\\nsuch case folding can equate words that might better be kept apart. Many\\nproper nouns are derived from common nouns and so are distinguished only\\nby case, including companies (General Motors, The Associated Press), govern-\\nment organizations (the Fed vs. fed) and person names (Bush, Black). We al-\\nready mentioned an example of unintended query expansion with acronyms,\\nwhich involved not only acronym normalization (C.A.T. →CAT) but also\\ncase-folding (CAT →cat).\\nFor English, an alternative to making every token lowercase is to just make\\nsome tokens lowercase. The simplest heuristic is to convert to lowercase\\nwords at the beginning of a sentence and all words occurring in a title that is\\nall uppercase or in which most or all words are capitalized. These words are\\nusually ordinary words that have been capitalized. Mid-sentence capitalized\\nwords are left as capitalized (which is usually correct). This will mostly avoid\\ncase-folding in cases where distinctions should be kept apart. The same task\\ncan be done more accurately by a machine learning sequence model which\\nuses more features to make the decision of when to case-fold. This is known\\nas truecasing. However, trying to get capitalization right in this way probably\\nTRUECASING\\ndoesn’t help if your users usually use lowercase regardless of the correct case\\nof words. Thus, lowercasing everything often remains the most practical\\nsolution.\\nOther issues in English.\\nOther possible normalizations are quite idiosyn-\\ncratic and particular to English.\\nFor instance, you might wish to equate\\nne’er and never or the British spelling colour and the American spelling color.\\nDates, times and similar items come in multiple formats, presenting addi-\\ntional challenges. You might wish to collapse together 3/12/91 and Mar. 12,\\n1991. However, correct processing here is complicated by the fact that in the\\nU.S., 3/12/91 is Mar. 12, 1991, whereas in Europe it is 3 Dec 1991.\\nOther languages.\\nEnglish has maintained a dominant position on the WWW;\\napproximately 60% of web pages are in English (Gerrand 2007). But that still\\nleaves 40% of the web, and the non-English portion might be expected to\\ngrow over time, since less than one third of Internet users and less than 10%\\nof the world’s population primarily speak English. And there are signs of\\nchange: Sifry (2007) reports that only about one third of blog posts are in\\nEnglish.\\nOther languages again present distinctive issues in equivalence classing.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n31\\n\\x00\\x01\\n\\x02\\n\\x03\\n\\x04\\n\\x05\\n\\x06\\n\\x07\\n\\x08\\n\\x06\\n\\t\\n\\n\\x0b\\n\\x0c\\n\\r\\n\\x0e\\n\\x0f\\n\\x10\\n\\x01\\n\\x11\\n\\x12\\n\\x13\\n\\x14\\n\\x15\\n\\x16\\n\\x17\\n\\x18\\n\\x19\\n\\x07\\n\\x1a\\n\\x1b\\n\\x1c\\n\\x1d\\n\\x1e\\n\\x1f\\n\\x1f\\n \\n!\\n\"\\n \\n!\\n#\\n$\\n\\x0c\\n%\\n\\x01\\n\\x0c\\n&\\n\\'\\n(\\n)\\n\\t\\n*\\n+\\n,\\n-\\n.\\n/\\n0\\n)\\n\\x10\\n\\r\\n1\\n\\x0c\\n2\\n3\\n4\\n5\\n6\\n7\\n&\\n+\\n8\\n9\\n\\n:\\n;\\n:\\n<\\n\\x07\\n=\\n>\\n\\t\\n?\\n@\\nA\\nB\\nC\\n\\x15\\n-\\nD\\nE\\n6\\n8\\n9\\n\\n:\\n;\\n:\\n<\\n)\\nF\\nG\\n*\\nH\\nI\\n\\t\\n*\\n:\\n\\x1c\\nJ\\n)\\nK\\n+\\nL\\nM\\nN\\n?\\nO\\nP\\n\\x1c\\nQ\\nR\\nS\\n\\x01\\nT\\n\\x07\\nU\\nV\\nV\\nW\\nX\\nY\\n&\\nZ\\n[\\nN\\n?\\n)\\n\\x1b\\n+\\n\\\\\\n]\\n;\\n^\\n_\\n+\\n\\x12\\n`\\n4\\na\\n+\\nb\\n;\\nc\\n\\x07\\nd\\ne\\n*\\nf\\nV\\ng\\nh\\nV\\n-\\n?\\ni\\nN\\nj\\nk\\nl\\nm\\nn\\n\\x13\\n:\\nA\\no\\n\\x06\\n\\x08\\n\\x06\\np\\nN\\n5\\n+\\nq\\nV\\nr\\ns\\nt\\nu\\n&\\nv\\nw\\nx\\n)\\nQ\\ny\\nz\\n{\\nh\\n|\\n&\\n}\\n\\x06\\n\\x15\\n~\\n\\x7f\\nM\\n?\\n@\\nA\\n◮Figure 2.7\\nJapanese makes use of multiple intermingled writing systems and,\\nlike Chinese, does not segment words. The text is mainly Chinese characters with\\nthe hiragana syllabary for inﬂectional endings and function words. The part in latin\\nletters is actually a Japanese expression, but has been taken up as the name of an\\nenvironmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His\\nname is written using the katakana syllabary in the middle of the ﬁrst line. The ﬁrst\\nfour characters of the ﬁnal line express a monetary amount that we would want to\\nmatch with ¥500,000 (500,000 Japanese yen).\\nThe French word for the has distinctive forms based not only on the gender\\n(masculine or feminine) and number of the following noun, but also depend-\\ning on whether the following word begins with a vowel: le, la, l’, les. We may\\nwell wish to equivalence class these various forms of the. German has a con-\\nvention whereby vowels with an umlaut can be rendered instead as a two\\nvowel digraph. We would want to treat Schütze and Schuetze as equivalent.\\nJapanese is a well-known difﬁcult writing system, as illustrated in Fig-\\nure 2.7. Modern Japanese is standardly an intermingling of multiple alpha-\\nbets, principally Chinese characters, two syllabaries (hiragana and katakana)\\nand western characters (Latin letters, Arabic numerals, and various sym-\\nbols). While there are strong conventions and standardization through the\\neducation system over the choice of writing system, in many cases the same\\nword can be written with multiple writing systems. For example, a word\\nmay be written in katakana for emphasis (somewhat like italics). Or a word\\nmay sometimes be written in hiragana and sometimes in Chinese charac-\\nters. Successful retrieval thus requires complex equivalence classing across\\nthe writing systems. In particular, an end user might commonly present a\\nquery entirely in hiragana, because it is easier to type, just as Western end\\nusers commonly use all lowercase.\\nDocument collections being indexed can include documents from many\\ndifferent languages. Or a single document can easily contain text from mul-\\ntiple languages. For instance, a French email might quote clauses from a\\ncontract document written in English. Most commonly, the language is de-\\ntected and language-particular tokenization and normalization rules are ap-\\nplied at a predetermined granularity, such as whole documents or individual\\nparagraphs, but this still will not correctly deal with cases where language\\nchanges occur for brief quotations. When document collections contain mul-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n32\\n2\\nThe term vocabulary and postings lists\\ntiple languages, a single index may have to contain terms of several lan-\\nguages. One option is to run a language identiﬁcation classiﬁer on docu-\\nments and then to tag terms in the vocabulary for their language. Or this\\ntagging can simply be omitted, since it is relatively rare for the exact same\\ncharacter sequence to be a word in different languages.\\nWhen dealing with foreign or complex words, particularly foreign names,\\nthe spelling may be unclear or there may be variant transliteration standards\\ngiving different spellings (for example, Chebyshev and Tchebycheff or Beijing\\nand Peking). One way of dealing with this is to use heuristics to equiva-\\nlence class or expand terms with phonetic equivalents. The traditional and\\nbest known such algorithm is the Soundex algorithm, which we cover in\\nSection 3.4 (page 63).\\n2.2.4\\nStemming and lemmatization\\nFor grammatical reasons, documents are going to use different forms of a\\nword, such as organize, organizes, and organizing. Additionally, there are fami-\\nlies of derivationally related words with similar meanings, such as democracy,\\ndemocratic, and democratization. In many situations, it seems as if it would be\\nuseful for a search for one of these words to return documents that contain\\nanother word in the set.\\nThe goal of both stemming and lemmatization is to reduce inﬂectional\\nforms and sometimes derivationally related forms of a word to a common\\nbase form. For instance:\\nam, are, is ⇒be\\ncar, cars, car’s, cars’ ⇒car\\nThe result of this mapping of text will be something like:\\nthe boy’s cars are different colors ⇒\\nthe boy car be differ color\\nHowever, the two words differ in their ﬂavor. Stemming usually refers to\\nSTEMMING\\na crude heuristic process that chops off the ends of words in the hope of\\nachieving this goal correctly most of the time, and often includes the re-\\nmoval of derivational afﬁxes. Lemmatization usually refers to doing things\\nLEMMATIZATION\\nproperly with the use of a vocabulary and morphological analysis of words,\\nnormally aiming to remove inﬂectional endings only and to return the base\\nor dictionary form of a word, which is known as the lemma. If confronted\\nLEMMA\\nwith the token saw, stemming might return just s, whereas lemmatization\\nwould attempt to return either see or saw depending on whether the use of\\nthe token was as a verb or a noun. The two may also differ in that stemming\\nmost commonly collapses derivationally related words, whereas lemmatiza-\\ntion commonly only collapses the different inﬂectional forms of a lemma.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n33\\nLinguistic processing for stemming or lemmatization is often done by an ad-\\nditional plug-in component to the indexing process, and a number of such\\ncomponents exist, both commercial and open-source.\\nThe most common algorithm for stemming English, and one that has re-\\npeatedly been shown to be empirically very effective, is Porter’s algorithm\\nPORTER STEMMER\\n(Porter 1980). The entire algorithm is too long and intricate to present here,\\nbut we will indicate its general nature. Porter’s algorithm consists of 5 phases\\nof word reductions, applied sequentially. Within each phase there are var-\\nious conventions to select rules, such as selecting the rule from each rule\\ngroup that applies to the longest sufﬁx. In the ﬁrst phase, this convention is\\nused with the following rule group:\\n(2.1)\\nRule\\nExample\\nSSES\\n→\\nSS\\ncaresses\\n→\\ncaress\\nIES\\n→\\nI\\nponies\\n→\\nponi\\nSS\\n→\\nSS\\ncaress\\n→\\ncaress\\nS\\n→\\ncats\\n→\\ncat\\nMany of the later rules use a concept of the measure of a word, which loosely\\nchecks the number of syllables to see whether a word is long enough that it\\nis reasonable to regard the matching portion of a rule as a sufﬁx rather than\\nas part of the stem of a word. For example, the rule:\\n(m > 1)\\nEMENT\\n→\\nwould map replacement to replac, but not cement to c. The ofﬁcial site for the\\nPorter Stemmer is:\\nhttp://www.tartarus.org/˜martin/PorterStemmer/\\nOther stemmers exist, including the older, one-pass Lovins stemmer (Lovins\\n1968), and newer entrants like the Paice/Husk stemmer (Paice 1990); see:\\nhttp://www.cs.waikato.ac.nz/˜eibe/stemmers/\\nhttp://www.comp.lancs.ac.uk/computing/research/stemming/\\nFigure 2.8 presents an informal comparison of the different behaviors of these\\nstemmers. Stemmers use language-speciﬁc rules, but they require less know-\\nledge than a lemmatizer, which needs a complete vocabulary and morpho-\\nlogical analysis to correctly lemmatize words. Particular domains may also\\nrequire special stemming rules. However, the exact stemmed form does not\\nmatter, only the equivalence classes it forms.\\nRather than using a stemmer, you can use a lemmatizer, a tool from Nat-\\nLEMMATIZER\\nural Language Processing which does full morphological analysis to accu-\\nrately identify the lemma for each word. Doing full morphological analysis\\nproduces at most very modest beneﬁts for retrieval. It is hard to say more,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n34\\n2\\nThe term vocabulary and postings lists\\nSample text: Such an analysis can reveal features that are not easily visible\\nfrom the variations in the individual genes and can lead to a picture of\\nexpression that is more biologically transparent and accessible to\\ninterpretation\\nLovins stemmer: such an analys can reve featur that ar not eas vis from th\\nvari in th individu gen and can lead to a pictur of expres that is mor\\nbiolog transpar and acces to interpres\\nPorter stemmer: such an analysi can reveal featur that ar not easili visibl\\nfrom the variat in the individu gene and can lead to a pictur of express\\nthat is more biolog transpar and access to interpret\\nPaice stemmer: such an analys can rev feat that are not easy vis from the\\nvary in the individ gen and can lead to a pict of express that is mor\\nbiolog transp and access to interpret\\n◮Figure 2.8\\nA comparison of three stemming algorithms on a sample text.\\nbecause either form of normalization tends not to improve English informa-\\ntion retrieval performance in aggregate – at least not by very much. While\\nit helps a lot for some queries, it equally hurts performance a lot for others.\\nStemming increases recall while harming precision. As an example of what\\ncan go wrong, note that the Porter stemmer stems all of the following words:\\noperate operating operates operation operative operatives operational\\nto oper. However, since operate in its various forms is a common verb, we\\nwould expect to lose considerable precision on queries such as the following\\nwith Porter stemming:\\noperational AND research\\noperating AND system\\noperative AND dentistry\\nFor a case like this, moving to using a lemmatizer would not completely ﬁx\\nthe problem because particular inﬂectional forms are used in particular col-\\nlocations: a sentence with the words operate and system is not a good match\\nfor the query operating AND system. Getting better value from term normaliza-\\ntion depends more on pragmatic issues of word use than on formal issues of\\nlinguistic morphology.\\nThe situation is different for languages with much more morphology (such\\nas Spanish, German, and Finnish). Results in the European CLEF evaluations\\nhave repeatedly shown quite large gains from the use of stemmers (and com-\\npound splitting for languages like German); see the references in Section 2.5.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.2\\nDetermining the vocabulary of terms\\n35\\n?\\nExercise 2.1\\n[⋆]\\nAre the following statements true or false?\\na. In a Boolean retrieval system, stemming never lowers precision.\\nb. In a Boolean retrieval system, stemming never lowers recall.\\nc. Stemming increases the size of the vocabulary.\\nd. Stemming should be invoked at indexing time but not while processing a query.\\nExercise 2.2\\n[⋆]\\nSuggest what normalized form should be used for these words (including the word\\nitself as a possibility):\\na. ’Cos\\nb. Shi’ite\\nc. cont’d\\nd. Hawai’i\\ne. O’Rourke\\nExercise 2.3\\n[⋆]\\nThe following pairs of words are stemmed to the same form by the Porter stemmer.\\nWhich pairs would you argue shouldn’t be conﬂated. Give your reasoning.\\na. abandon/abandonment\\nb. absorbency/absorbent\\nc. marketing/markets\\nd. university/universe\\ne. volume/volumes\\nExercise 2.4\\n[⋆]\\nFor the Porter stemmer rule group shown in (2.1):\\na. What is the purpose of including an identity rule such as SS →SS?\\nb. Applying just this rule group, what will the following words be stemmed to?\\ncircus\\ncanaries\\nboss\\nc. What rule should be added to correctly stem pony?\\nd. The stemming for ponies and pony might seem strange. Does it have a deleterious\\neffect on retrieval? Why or why not?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n36\\n2\\nThe term vocabulary and postings lists\\n◮Figure 2.9\\nPostings lists with skip pointers. The postings intersection can use a\\nskip pointer when the end point is still less than the item on the other list.\\n2.3\\nFaster postings list intersection via skip pointers\\nIn the remainder of this chapter, we will discuss extensions to postings list\\ndata structures and ways to increase the efﬁciency of using postings lists. Re-\\ncall the basic postings list intersection operation from Section 1.3 (page 10):\\nwe walk through the two postings lists simultaneously, in time linear in the\\ntotal number of postings entries. If the list lengths are m and n, the intersec-\\ntion takes O(m + n) operations. Can we do better than this? That is, empiri-\\ncally, can we usually process postings list intersection in sublinear time? We\\ncan, if the index isn’t changing too fast.\\nOne way to do this is to use a skip list by augmenting postings lists with\\nSKIP LIST\\nskip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are\\neffectively shortcuts that allow us to avoid processing parts of the postings\\nlist that will not ﬁgure in the search results. The two questions are then where\\nto place skip pointers and how to do efﬁcient merging using skip pointers.\\nConsider ﬁrst efﬁcient merging, with Figure 2.9 as an example. Suppose\\nwe’ve stepped through the lists in the ﬁgure until we have matched 8 on\\neach list and moved it to the results list. We advance both pointers, giving us\\n16 on the upper list and 41 on the lower list. The smallest item is then the\\nelement 16 on the top list. Rather than simply advancing the upper pointer,\\nwe ﬁrst check the skip list pointer and note that 28 is also less than 41. Hence\\nwe can follow the skip list pointer, and then we advance the upper pointer\\nto 28 . We thus avoid stepping to 19 and 23 on the upper list. A number\\nof variant versions of postings list intersection with skip pointers is possible\\ndepending on when exactly you check the skip pointer. One version is shown\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.3\\nFaster postings list intersection via skip pointers\\n37\\nINTERSECTWITHSKIPS(p1, p2)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen ADD(answer, docID(p1))\\n5\\np1 ←next(p1)\\n6\\np2 ←next(p2)\\n7\\nelse if docID(p1) < docID(p2)\\n8\\nthen if hasSkip(p1) and (docID(skip(p1)) ≤docID(p2))\\n9\\nthen while hasSkip(p1) and (docID(skip(p1)) ≤docID(p2))\\n10\\ndo p1 ←skip(p1)\\n11\\nelse p1 ←next(p1)\\n12\\nelse if hasSkip(p2) and (docID(skip(p2)) ≤docID(p1))\\n13\\nthen while hasSkip(p2) and (docID(skip(p2)) ≤docID(p1))\\n14\\ndo p2 ←skip(p2)\\n15\\nelse p2 ←next(p2)\\n16\\nreturn answer\\n◮Figure 2.10\\nPostings lists intersection with skip pointers.\\nin Figure 2.10. Skip pointers will only be available for the original postings\\nlists. For an intermediate result in a complex query, the call hasSkip(p) will\\nalways return false. Finally, note that the presence of skip pointers only helps\\nfor AND queries, not for OR queries.\\nWhere do we place skips? There is a tradeoff. More skips means shorter\\nskip spans, and that we are more likely to skip. But it also means lots of\\ncomparisons to skip pointers, and lots of space storing skip pointers. Fewer\\nskips means few pointer comparisons, but then long skip spans which means\\nthat there will be fewer opportunities to skip. A simple heuristic for placing\\nskips, which has been found to work well in practice, is that for a postings\\nlist of length P, use\\n√\\nP evenly-spaced skip pointers. This heuristic can be\\nimproved upon; it ignores any details of the distribution of query terms.\\nBuilding effective skip pointers is easy if an index is relatively static; it\\nis harder if a postings list keeps changing because of updates. A malicious\\ndeletion strategy can render skip lists ineffective.\\nChoosing the optimal encoding for an inverted index is an ever-changing\\ngame for the system builder, because it is strongly dependent on underly-\\ning computer technologies and their relative speeds and sizes. Traditionally,\\nCPUs were slow, and so highly compressed techniques were not optimal.\\nNow CPUs are fast and disk is slow, so reducing disk postings list size dom-\\ninates. However, if you’re running a search engine with everything in mem-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n38\\n2\\nThe term vocabulary and postings lists\\nory then the equation changes again. We discuss the impact of hardware\\nparameters on index construction time in Section 4.1 (page 68) and the im-\\npact of index size on system speed in Chapter 5.\\n?\\nExercise 2.5\\n[⋆]\\nWhy are skip pointers not useful for queries of the form x OR y?\\nExercise 2.6\\n[⋆]\\nWe have a two-word query. For one term the postings list consists of the following 16\\nentries:\\n[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\\nand for the other it is the one entry postings list:\\n[47].\\nWork out how many comparisons would be done to intersect the two postings lists\\nwith the following two strategies. Brieﬂy justify your answers:\\na. Using standard postings lists\\nb. Using postings lists stored with skip pointers, with a skip length of\\n√\\nP, as sug-\\ngested in Section 2.3.\\nExercise 2.7\\n[⋆]\\nConsider a postings intersection between this postings list, with skip pointers:\\n3\\n5\\n9\\n15\\n24\\n39\\n60\\n68\\n75\\n81\\n84\\n89\\n92\\n96\\n97 100 115\\nand the following intermediate result postings list (which hence has no skip pointers):\\n3\\n5\\n89\\n95\\n97\\n99\\n100\\n101\\nTrace through the postings intersection algorithm in Figure 2.10 (page 37).\\na. How often is a skip pointer followed (i.e., p1 is advanced to skip(p1))?\\nb. How many postings comparisons will be made by this algorithm while intersect-\\ning the two lists?\\nc. How many postings comparisons would be made if the postings lists are inter-\\nsected without the use of skip pointers?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n39\\n2.4\\nPositional postings and phrase queries\\nMany complex or technical concepts and many organization and product\\nnames are multiword compounds or phrases. We would like to be able to\\npose a query such as Stanford University by treating it as a phrase so that a\\nsentence in a document like The inventor Stanford Ovshinsky never went to uni-\\nversity. is not a match. Most recent search engines support a double quotes\\nsyntax (“stanford university”) for phrase queries, which has proven to be very\\nPHRASE QUERIES\\neasily understood and successfully used by users. As many as 10% of web\\nqueries are phrase queries, and many more are implicit phrase queries (such\\nas person names), entered without use of double quotes. To be able to sup-\\nport such queries, it is no longer sufﬁcient for postings lists to be simply lists\\nof documents that contain individual terms. In this section we consider two\\napproaches to supporting phrase queries and their combination. A search\\nengine should not only support phrase queries, but implement them efﬁ-\\nciently. A related but distinct concept is term proximity weighting, where a\\ndocument is preferred to the extent that the query terms appear close to each\\nother in the text. This technique is covered in Section 7.2.2 (page 144) in the\\ncontext of ranked retrieval.\\n2.4.1\\nBiword indexes\\nOne approach to handling phrases is to consider every pair of consecutive\\nterms in a document as a phrase. For example, the text Friends, Romans,\\nCountrymen would generate the biwords:\\nBIWORD INDEX\\nfriends romans\\nromans countrymen\\nIn this model, we treat each of these biwords as a vocabulary term. Being\\nable to process two-word phrase queries is immediate. Longer phrases can\\nbe processed by breaking them down. The query stanford university palo alto\\ncan be broken into the Boolean query on biwords:\\n“stanford university” AND “university palo” AND “palo alto”\\nThis query could be expected to work fairly well in practice, but there can\\nand will be occasional false positives. Without examining the documents,\\nwe cannot verify that the documents matching the above Boolean query do\\nactually contain the original 4 word phrase.\\nAmong possible queries, nouns and noun phrases have a special status in\\ndescribing the concepts people are interested in searching for. But related\\nnouns can often be divided from each other by various function words, in\\nphrases such as the abolition of slavery or renegotiation of the constitution. These\\nneeds can be incorporated into the biword indexing model in the following\\n', 'Online edition (c)\\n2009 Cambridge UP\\n40\\n2\\nThe term vocabulary and postings lists\\nway. First, we tokenize the text and perform part-of-speech-tagging.6 We\\ncan then group terms into nouns, including proper nouns, (N) and function\\nwords, including articles and prepositions, (X), among other classes. Now\\ndeem any string of terms of the form NX*N to be an extended biword. Each\\nsuch extended biword is made a term in the vocabulary. For example:\\nrenegotiation\\nof\\nthe\\nconstitution\\nN\\nX\\nX\\nN\\nTo process a query using such an extended biword index, we need to also\\nparse it into N’s and X’s, and then segment the query into extended biwords,\\nwhich can be looked up in the index.\\nThis algorithm does not always work in an intuitively optimal manner\\nwhen parsing longer queries into Boolean queries. Using the above algo-\\nrithm, the query\\ncost overruns on a power plant\\nis parsed into\\n“cost overruns” AND “overruns power” AND “power plant”\\nwhereas it might seem a better query to omit the middle biword. Better\\nresults can be obtained by using more precise part-of-speech patterns that\\ndeﬁne which extended biwords should be indexed.\\nThe concept of a biword index can be extended to longer sequences of\\nwords, and if the index includes variable length word sequences, it is gen-\\nerally referred to as a phrase index. Indeed, searches for a single term are\\nPHRASE INDEX\\nnot naturally handled in a biword index (you would need to scan the dic-\\ntionary for all biwords containing the term), and so we also need to have an\\nindex of single-word terms. While there is always a chance of false positive\\nmatches, the chance of a false positive match on indexed phrases of length 3\\nor more becomes very small indeed. But on the other hand, storing longer\\nphrases has the potential to greatly expand the vocabulary size. Maintain-\\ning exhaustive phrase indexes for phrases of length greater than two is a\\ndaunting prospect, and even use of an exhaustive biword dictionary greatly\\nexpands the size of the vocabulary. However, towards the end of this sec-\\ntion we discuss the utility of the strategy of using a partial phrase index in a\\ncompound indexing scheme.\\n6. Part of speech taggers classify words as nouns, verbs, etc. – or, in practice, often as ﬁner-\\ngrained classes like “plural proper noun”. Many fairly accurate (c. 96% per-tag accuracy) part-\\nof-speech taggers now exist, usually trained by machine learning methods on hand-tagged text.\\nSee, for instance, Manning and Schütze (1999, ch. 10).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n41\\nto, 993427:\\n⟨1, 6: ⟨7, 18, 33, 72, 86, 231⟩;\\n2, 5: ⟨1, 17, 74, 222, 255⟩;\\n4, 5: ⟨8, 16, 190, 429, 433⟩;\\n5, 2: ⟨363, 367⟩;\\n7, 3: ⟨13, 23, 191⟩; ... ⟩\\nbe, 178239:\\n⟨1, 2: ⟨17, 25⟩;\\n4, 5: ⟨17, 191, 291, 430, 434⟩;\\n5, 3: ⟨14, 19, 101⟩; ... ⟩\\n◮Figure 2.11\\nPositional index example. The word to has a document frequency\\n993,477, and occurs 6 times in document 1 at positions 7, 18, 33, etc.\\n2.4.2\\nPositional indexes\\nFor the reasons given, a biword index is not the standard solution. Rather,\\na positional index is most commonly employed. Here, for each term in the\\nPOSITIONAL INDEX\\nvocabulary, we store postings of the form docID: ⟨position1, position2, ... ⟩,\\nas shown in Figure 2.11, where each position is a token index in the docu-\\nment. Each posting will also usually record the term frequency, for reasons\\ndiscussed in Chapter 6.\\nTo process a phrase query, you still need to access the inverted index en-\\ntries for each distinct term. As before, you would start with the least frequent\\nterm and then work to further restrict the list of possible candidates. In the\\nmerge operation, the same general technique is used as before, but rather\\nthan simply checking that both terms are in a document, you also need to\\ncheck that their positions of appearance in the document are compatible with\\nthe phrase query being evaluated. This requires working out offsets between\\nthe words.\\n\\x0f\\nExample 2.1: Satisfying phrase queries.\\nSuppose the postings lists for to and\\nbe are as in Figure 2.11, and the query is “to be or not to be”. The postings lists to access\\nare: to, be, or, not. We will examine intersecting the postings lists for to and be. We\\nﬁrst look for documents that contain both terms. Then, we look for places in the lists\\nwhere there is an occurrence of be with a token index one higher than a position of to,\\nand then we look for another occurrence of each word with token index 4 higher than\\nthe ﬁrst occurrence. In the above lists, the pattern of occurrences that is a possible\\nmatch is:\\nto: ⟨. . . ; 4:⟨. . . ,429,433⟩; . . . ⟩\\nbe: ⟨. . . ; 4:⟨. . . ,430,434⟩; . . . ⟩\\n', 'Online edition (c)\\n2009 Cambridge UP\\n42\\n2\\nThe term vocabulary and postings lists\\nPOSITIONALINTERSECT(p1, p2, k)\\n1\\nanswer ←⟨⟩\\n2\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n3\\ndo if docID(p1) = docID(p2)\\n4\\nthen l ←⟨⟩\\n5\\npp1 ←positions(p1)\\n6\\npp2 ←positions(p2)\\n7\\nwhile pp1 ̸= NIL\\n8\\ndo while pp2 ̸= NIL\\n9\\ndo if |pos(pp1) −pos(pp2)| ≤k\\n10\\nthen ADD(l, pos(pp2))\\n11\\nelse if pos(pp2) > pos(pp1)\\n12\\nthen break\\n13\\npp2 ←next(pp2)\\n14\\nwhile l ̸= ⟨⟩and |l[0] −pos(pp1)| > k\\n15\\ndo DELETE(l[0])\\n16\\nfor each ps ∈l\\n17\\ndo ADD(answer, ⟨docID(p1), pos(pp1), ps⟩)\\n18\\npp1 ←next(pp1)\\n19\\np1 ←next(p1)\\n20\\np2 ←next(p2)\\n21\\nelse if docID(p1) < docID(p2)\\n22\\nthen p1 ←next(p1)\\n23\\nelse p2 ←next(p2)\\n24\\nreturn answer\\n◮Figure 2.12\\nAn algorithm for proximity intersection of postings lists p1 and p2.\\nThe algorithm ﬁnds places where the two terms appear within k words of each other\\nand returns a list of triples giving docID and the term position in p1 and p2.\\nThe same general method is applied for within k word proximity searches,\\nof the sort we saw in Example 1.1 (page 15):\\nemployment /3 place\\nHere, /k means “within k words of (on either side)”. Clearly, positional in-\\ndexes can be used for such queries; biword indexes cannot. We show in\\nFigure 2.12 an algorithm for satisfying within k word proximity searches; it\\nis further discussed in Exercise 2.12.\\nPositional index size.\\nAdopting a positional index expands required post-\\nings storage signiﬁcantly, even if we compress position values/offsets as we\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.4\\nPositional postings and phrase queries\\n43\\nwill discuss in Section 5.3 (page 95). Indeed, moving to a positional index\\nalso changes the asymptotic complexity of a postings intersection operation,\\nbecause the number of items to check is now bounded not by the number of\\ndocuments but by the total number of tokens in the document collection T.\\nThat is, the complexity of a Boolean query is Θ(T) rather than Θ(N). How-\\never, most applications have little choice but to accept this, since most users\\nnow expect to have the functionality of phrase and proximity searches.\\nLet’s examine the space implications of having a positional index. A post-\\ning now needs an entry for each occurrence of a term. The index size thus\\ndepends on the average document size. The average web page has less than\\n1000 terms, but documents like SEC stock ﬁlings, books, and even some epic\\npoems easily reach 100,000 terms. Consider a term with frequency 1 in 1000\\nterms on average. The result is that large documents cause an increase of two\\norders of magnitude in the space required to store the postings list:\\nExpected\\nExpected entries\\nDocument size\\npostings\\nin positional posting\\n1000\\n1\\n1\\n100,000\\n1\\n100\\nWhile the exact numbers depend on the type of documents and the language\\nbeing indexed, some rough rules of thumb are to expect a positional index to\\nbe 2 to 4 times as large as a non-positional index, and to expect a compressed\\npositional index to be about one third to one half the size of the raw text\\n(after removal of markup, etc.) of the original uncompressed documents.\\nSpeciﬁc numbers for an example collection are given in Table 5.1 (page 87)\\nand Table 5.6 (page 103).\\n2.4.3\\nCombination schemes\\nThe strategies of biword indexes and positional indexes can be fruitfully\\ncombined. If users commonly query on particular phrases, such as Michael\\nJackson, it is quite inefﬁcient to keep merging positional postings lists. A\\ncombination strategy uses a phrase index, or just a biword index, for certain\\nqueries and uses a positional index for other phrase queries. Good queries\\nto include in the phrase index are ones known to be common based on re-\\ncent querying behavior. But this is not the only criterion: the most expensive\\nphrase queries to evaluate are ones where the individual words are com-\\nmon but the desired phrase is comparatively rare. Adding Britney Spears as\\na phrase index entry may only give a speedup factor to that query of about\\n3, since most documents that mention either word are valid results, whereas\\nadding The Who as a phrase index entry may speed up that query by a factor\\nof 1000. Hence, having the latter is more desirable, even if it is a relatively\\nless common query.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n44\\n2\\nThe term vocabulary and postings lists\\nWilliams et al. (2004) evaluate an even more sophisticated scheme which\\nemploys indexes of both these sorts and additionally a partial next word\\nindex as a halfway house between the ﬁrst two strategies. For each term, a\\nnext word index records terms that follow it in a document. They conclude\\nNEXT WORD INDEX\\nthat such a strategy allows a typical mixture of web phrase queries to be\\ncompleted in one quarter of the time taken by use of a positional index alone,\\nwhile taking up 26% more space than use of a positional index alone.\\n?\\nExercise 2.8\\n[⋆]\\nAssume a biword index. Give an example of a document which will be returned\\nfor a query of New York University but is actually a false positive which should not be\\nreturned.\\nExercise 2.9\\n[⋆]\\nShown below is a portion of a positional index in the format: term: doc1: ⟨position1,\\nposition2, . . . ⟩; doc2: ⟨position1, position2, . . . ⟩; etc.\\nangels: 2: ⟨36,174,252,651⟩; 4: ⟨12,22,102,432⟩; 7: ⟨17⟩;\\nfools: 2: ⟨1,17,74,222⟩; 4: ⟨8,78,108,458⟩; 7: ⟨3,13,23,193⟩;\\nfear: 2: ⟨87,704,722,901⟩; 4: ⟨13,43,113,433⟩; 7: ⟨18,328,528⟩;\\nin: 2: ⟨3,37,76,444,851⟩; 4: ⟨10,20,110,470,500⟩; 7: ⟨5,15,25,195⟩;\\nrush: 2: ⟨2,66,194,321,702⟩; 4: ⟨9,69,149,429,569⟩; 7: ⟨4,14,404⟩;\\nto: 2: ⟨47,86,234,999⟩; 4: ⟨14,24,774,944⟩; 7: ⟨199,319,599,709⟩;\\ntread: 2: ⟨57,94,333⟩; 4: ⟨15,35,155⟩; 7: ⟨20,320⟩;\\nwhere: 2: ⟨67,124,393,1001⟩; 4: ⟨11,41,101,421,431⟩; 7: ⟨16,36,736⟩;\\nWhich document(s) if any match each of the following queries, where each expression\\nwithin quotes is a phrase query?\\na. “fools rush in”\\nb. “fools rush in” AND “angels fear to tread”\\nExercise 2.10\\n[⋆]\\nConsider the following fragment of a positional index with the format:\\nword: document: ⟨position, position, . . .⟩; document: ⟨position, . . .⟩\\n. . .\\nGates: 1: ⟨3⟩; 2: ⟨6⟩; 3: ⟨2,17⟩; 4: ⟨1⟩;\\nIBM: 4: ⟨3⟩; 7: ⟨14⟩;\\nMicrosoft: 1: ⟨1⟩; 2: ⟨1,21⟩; 3: ⟨3⟩; 5: ⟨16,22,51⟩;\\nThe /k operator, word1 /k word2 ﬁnds occurrences of word1 within k words of word2 (on\\neither side), where k is a positive integer argument. Thus k = 1 demands that word1\\nbe adjacent to word2.\\na. Describe the set of documents that satisfy the query Gates /2 Microsoft.\\nb. Describe each set of values for k for which the query Gates /k Microsoft returns a\\ndifferent set of documents as the answer.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.5\\nReferences and further reading\\n45\\nExercise 2.11\\n[⋆⋆]\\nConsider the general procedure for merging two positional postings lists for a given\\ndocument, to determine the document positions where a document satisﬁes a /k\\nclause (in general there can be multiple positions at which each term occurs in a sin-\\ngle document). We begin with a pointer to the position of occurrence of each term\\nand move each pointer along the list of occurrences in the document, checking as we\\ndo so whether we have a hit for /k. Each move of either pointer counts as a step. Let\\nL denote the total number of occurrences of the two terms in the document. What is\\nthe big-O complexity of the merge procedure, if we wish to have postings including\\npositions in the result?\\nExercise 2.12\\n[⋆⋆]\\nConsider the adaptation of the basic algorithm for intersection of two postings lists\\n(Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\\nqueries. A naive algorithm for this operation could be O(PLmax2), where P is the\\nsum of the lengths of the postings lists (i.e., the sum of document frequencies) and\\nLmax is the maximum length of a document (in tokens).\\na. Go through this algorithm carefully and explain how it works.\\nb. What is the complexity of this algorithm? Justify your answer carefully.\\nc. For certain queries and data distributions, would another algorithm be more efﬁ-\\ncient? What complexity does it have?\\nExercise 2.13\\n[⋆⋆]\\nSuppose we wish to use a postings intersection procedure to determine simply the\\nlist of documents that satisfy a /k clause, rather than returning the list of positions,\\nas in Figure 2.12 (page 42). For simplicity, assume k ≥2. Let L denote the total\\nnumber of occurrences of the two terms in the document collection (i.e., the sum of\\ntheir collection frequencies). Which of the following is true? Justify your answer.\\na. The merge can be accomplished in a number of steps linear in L and independent\\nof k, and we can ensure that each pointer moves only to the right.\\nb. The merge can be accomplished in a number of steps linear in L and independent\\nof k, but a pointer may be forced to move non-monotonically (i.e., to sometimes\\nback up)\\nc. The merge can require kL steps in some cases.\\nExercise 2.14\\n[⋆⋆]\\nHow could an IR system combine use of a positional index and use of stop words?\\nWhat is the potential problem, and how could it be handled?\\n2.5\\nReferences and further reading\\nExhaustive discussion of the character-level processing of East Asian lan-\\nEAST ASIAN\\nLANGUAGES\\nguages can be found in Lunde (1998). Character bigram indexes are perhaps\\nthe most standard approach to indexing Chinese, although some systems use\\nword segmentation. Due to differences in the language and writing system,\\nword segmentation is most usual for Japanese (Luk and Kwok 2002, Kishida\\n', 'Online edition (c)\\n2009 Cambridge UP\\n46\\n2\\nThe term vocabulary and postings lists\\net al. 2005). The structure of a character k-gram index over unsegmented text\\ndiffers from that in Section 3.2.2 (page 54): there the k-gram dictionary points\\nto postings lists of entries in the regular dictionary, whereas here it points\\ndirectly to document postings lists. For further discussion of Chinese word\\nsegmentation, see Sproat et al. (1996), Sproat and Emerson (2003), Tseng et al.\\n(2005), and Gao et al. (2005).\\nLita et al. (2003) present a method for truecasing. Natural language pro-\\ncessing work on computational morphology is presented in (Sproat 1992,\\nBeesley and Karttunen 2003).\\nLanguage identiﬁcation was perhaps ﬁrst explored in cryptography; for\\nexample, Konheim (1981) presents a character-level k-gram language identi-\\nﬁcation algorithm. While other methods such as looking for particular dis-\\ntinctive function words and letter combinations have been used, with the\\nadvent of widespread digital text, many people have explored the charac-\\nter n-gram technique, and found it to be highly successful (Beesley 1998,\\nDunning 1994, Cavnar and Trenkle 1994). Written language identiﬁcation\\nis regarded as a fairly easy problem, while spoken language identiﬁcation\\nremains more difﬁcult; see Hughes et al. (2006) for a recent survey.\\nExperiments on and discussion of the positive and negative impact of\\nstemming in English can be found in the following works: Salton (1989), Har-\\nman (1991), Krovetz (1995), Hull (1996). Hollink et al. (2004) provide detailed\\nresults for the effectiveness of language-speciﬁc methods on 8 European lan-\\nguages. In terms of percent change in mean average precision (see page 159)\\nover a baseline system, diacritic removal gains up to 23% (being especially\\nhelpful for Finnish, French, and Swedish). Stemming helped markedly for\\nFinnish (30% improvement) and Spanish (10% improvement), but for most\\nlanguages, including English, the gain from stemming was in the range 0–\\n5%, and results from a lemmatizer were poorer still. Compound splitting\\ngained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather\\nthan language-particular methods, indexing character k-grams (as we sug-\\ngested for Chinese) could often give as good or better results: using within-\\nword character 4-grams rather than words gave gains of 37% in Finnish, 27%\\nin Swedish, and 20% in German, while even being slightly positive for other\\nlanguages, such as Dutch, Spanish, and English. Tomlinson (2003) presents\\nbroadly similar results.\\nBar-Ilan and Gutman (2005) suggest that, at the\\ntime of their study (2003), the major commercial web search engines suffered\\nfrom lacking decent language-particular processing; for example, a query on\\nwww.google.fr for l’électricité did not separate off the article l’ but only matched\\npages with precisely this string of article+noun.\\nThe classic presentation of skip pointers for IR can be found in Moffat and\\nSKIP LIST\\nZobel (1996). Extended techniques are discussed in Boldi and Vigna (2005).\\nThe main paper in the algorithms literature is Pugh (1990), which uses mul-\\ntilevel skip pointers to give expected O(log P) list access (the same expected\\n', 'Online edition (c)\\n2009 Cambridge UP\\n2.5\\nReferences and further reading\\n47\\nefﬁciency as using a tree data structure) with less implementational complex-\\nity. In practice, the effectiveness of using skip pointers depends on various\\nsystem parameters. Moffat and Zobel (1996) report conjunctive queries run-\\nning about ﬁve times faster with the use of skip pointers, but Bahle et al.\\n(2002, p. 217) report that, with modern CPUs, using skip lists instead slows\\ndown search because it expands the size of the postings list (i.e., disk I/O\\ndominates performance). In contrast, Strohman and Croft (2007) again show\\ngood performance gains from skipping, in a system architecture designed to\\noptimize for the large memory spaces and multiple cores of recent CPUs.\\nJohnson et al. (2006) report that 11.7% of all queries in two 2002 web query\\nlogs contained phrase queries, though Kammenhuber et al. (2006) report\\nonly 3% phrase queries for a different data set. Silverstein et al. (1999) note\\nthat many queries without explicit phrase operators are actually implicit\\nphrase searches.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n49\\n3\\nDictionaries and tolerant\\nretrieval\\nIn Chapters 1 and 2 we developed the ideas underlying inverted indexes\\nfor handling Boolean and proximity queries. Here, we develop techniques\\nthat are robust to typographical errors in the query, as well as alternative\\nspellings.\\nIn Section 3.1 we develop data structures that help the search\\nfor terms in the vocabulary in an inverted index. In Section 3.2 we study\\nthe idea of a wildcard query: a query such as *a*e*i*o*u*, which seeks doc-\\nWILDCARD QUERY\\numents containing any term that includes all the ﬁve vowels in sequence.\\nThe * symbol indicates any (possibly empty) string of characters. Users pose\\nsuch queries to a search engine when they are uncertain about how to spell\\na query term, or seek documents containing variants of a query term; for in-\\nstance, the query automat* would seek documents containing any of the terms\\nautomatic, automation and automated.\\nWe then turn to other forms of imprecisely posed queries, focusing on\\nspelling errors in Section 3.3. Users make spelling errors either by accident,\\nor because the term they are searching for (e.g., Herman) has no unambiguous\\nspelling in the collection. We detail a number of techniques for correcting\\nspelling errors in queries, one term at a time as well as for an entire string\\nof query terms. Finally, in Section 3.4 we study a method for seeking vo-\\ncabulary terms that are phonetically close to the query term(s). This can be\\nespecially useful in cases like the Herman example, where the user may not\\nknow how a proper name is spelled in documents in the collection.\\nBecause we will develop many variants of inverted indexes in this chapter,\\nwe will use sometimes the phrase standard inverted index to mean the inverted\\nindex developed in Chapters 1 and 2, in which each vocabulary term has a\\npostings list with the documents in the collection.\\n3.1\\nSearch structures for dictionaries\\nGiven an inverted index and a query, our ﬁrst task is to determine whether\\neach query term exists in the vocabulary and if so, identify the pointer to the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n50\\n3\\nDictionaries and tolerant retrieval\\ncorresponding postings. This vocabulary lookup operation uses a classical\\ndata structure called the dictionary and has two broad classes of solutions:\\nhashing, and search trees. In the literature of data structures, the entries in\\nthe vocabulary (in our case, terms) are often referred to as keys. The choice\\nof solution (hashing, or search trees) is governed by a number of questions:\\n(1) How many keys are we likely to have? (2) Is the number likely to remain\\nstatic, or change a lot – and in the case of changes, are we likely to only have\\nnew keys inserted, or to also have some keys in the dictionary be deleted? (3)\\nWhat are the relative frequencies with which various keys will be accessed?\\nHashing has been used for dictionary lookup in some search engines. Each\\nvocabulary term (key) is hashed into an integer over a large enough space\\nthat hash collisions are unlikely; collisions if any are resolved by auxiliary\\nstructures that can demand care to maintain.1 At query time, we hash each\\nquery term separately and following a pointer to the corresponding post-\\nings, taking into account any logic for resolving hash collisions. There is no\\neasy way to ﬁnd minor variants of a query term (such as the accented and\\nnon-accented versions of a word like resume), since these could be hashed to\\nvery different integers. In particular, we cannot seek (for instance) all terms\\nbeginning with the preﬁx automat, an operation that we will require below\\nin Section 3.2. Finally, in a setting (such as the Web) where the size of the\\nvocabulary keeps growing, a hash function designed for current needs may\\nnot sufﬁce in a few years’ time.\\nSearch trees overcome many of these issues – for instance, they permit us\\nto enumerate all vocabulary terms beginning with automat. The best-known\\nsearch tree is the binary tree, in which each internal node has two children.\\nBINARY TREE\\nThe search for a term begins at the root of the tree. Each internal node (in-\\ncluding the root) represents a binary test, based on whose outcome the search\\nproceeds to one of the two sub-trees below that node. Figure 3.1 gives an ex-\\nample of a binary search tree used for a dictionary. Efﬁcient search (with a\\nnumber of comparisons that is O(log M)) hinges on the tree being balanced:\\nthe numbers of terms under the two sub-trees of any node are either equal\\nor differ by one. The principal issue here is that of rebalancing: as terms are\\ninserted into or deleted from the binary search tree, it needs to be rebalanced\\nso that the balance property is maintained.\\nTo mitigate rebalancing, one approach is to allow the number of sub-trees\\nunder an internal node to vary in a ﬁxed interval. A search tree commonly\\nused for a dictionary is the B-tree – a search tree in which every internal node\\nB-TREE\\nhas a number of children in the interval [a, b], where a and b are appropriate\\npositive integers; Figure 3.2 shows an example with a = 2 and b = 4. Each\\nbranch under an internal node again represents a test for a range of char-\\n1. So-called perfect hash functions are designed to preclude collisions, but are rather more com-\\nplicated both to implement and to compute.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n51\\n◮Figure 3.1\\nA binary search tree. In this example the branch at the root partitions\\nvocabulary terms into two subtrees, those whose ﬁrst letter is between a and m, and\\nthe rest.\\nacter sequences, as in the binary tree example of Figure 3.1. A B-tree may\\nbe viewed as “collapsing” multiple levels of the binary tree into one; this\\nis especially advantageous when some of the dictionary is disk-resident, in\\nwhich case this collapsing serves the function of pre-fetching imminent bi-\\nnary tests. In such cases, the integers a and b are determined by the sizes of\\ndisk blocks. Section 3.5 contains pointers to further background on search\\ntrees and B-trees.\\nIt should be noted that unlike hashing, search trees demand that the char-\\nacters used in the document collection have a prescribed ordering; for in-\\nstance, the 26 letters of the English alphabet are always listed in the speciﬁc\\norder A through Z. Some Asian languages such as Chinese do not always\\nhave a unique ordering, although by now all languages (including Chinese\\nand Japanese) have adopted a standard ordering system for their character\\nsets.\\n3.2\\nWildcard queries\\nWildcard queries are used in any of the following situations: (1) the user\\nis uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which\\n', 'Online edition (c)\\n2009 Cambridge UP\\n52\\n3\\nDictionaries and tolerant retrieval\\n◮Figure 3.2\\nA B-tree. In this example every internal node has between 2 and 4\\nchildren.\\nleads to the wildcard query S*dney); (2) the user is aware of multiple vari-\\nants of spelling a term and (consciously) seeks documents containing any of\\nthe variants (e.g., color vs. colour); (3) the user seeks documents containing\\nvariants of a term that would be caught by stemming, but is unsure whether\\nthe search engine performs stemming (e.g., judicial vs. judiciary, leading to the\\nwildcard query judicia*); (4) the user is uncertain of the correct rendition of a\\nforeign word or phrase (e.g., the query Universit* Stuttgart).\\nA query such as mon* is known as a trailing wildcard query, because the *\\nWILDCARD QUERY\\nsymbol occurs only once, at the end of the search string. A search tree on\\nthe dictionary is a convenient way of handling trailing wildcard queries: we\\nwalk down the tree following the symbols m, o and n in turn, at which point\\nwe can enumerate the set W of terms in the dictionary with the preﬁx mon.\\nFinally, we use |W| lookups on the standard inverted index to retrieve all\\ndocuments containing any term in W.\\nBut what about wildcard queries in which the * symbol is not constrained\\nto be at the end of the search string? Before handling this general case, we\\nmention a slight generalization of trailing wildcard queries. First, consider\\nleading wildcard queries, or queries of the form *mon. Consider a reverse B-tree\\non the dictionary – one in which each root-to-leaf path of the B-tree corre-\\nsponds to a term in the dictionary written backwards: thus, the term lemon\\nwould, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down\\nthe reverse B-tree then enumerates all terms R in the vocabulary with a given\\npreﬁx.\\nIn fact, using a regular B-tree together with a reverse B-tree, we can handle\\nan even more general case: wildcard queries in which there is a single * sym-\\nbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set\\nW of dictionary terms beginning with the preﬁx se, then the reverse B-tree to\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n53\\nenumerate the set R of terms ending with the sufﬁx mon. Next, we take the\\nintersection W ∩R of these two sets, to arrive at the set of terms that begin\\nwith the preﬁx se and end with the sufﬁx mon. Finally, we use the standard\\ninverted index to retrieve all documents containing any terms in this inter-\\nsection. We can thus handle wildcard queries that contain a single * symbol\\nusing two B-trees, the normal B-tree and a reverse B-tree.\\n3.2.1\\nGeneral wildcard queries\\nWe now study two techniques for handling general wildcard queries. Both\\ntechniques share a common strategy: express the given wildcard query qw as\\na Boolean query Q on a specially constructed index, such that the answer to\\nQ is a superset of the set of vocabulary terms matching qw. Then, we check\\neach term in the answer to Q against qw, discarding those vocabulary terms\\nthat do not match qw. At this point we have the vocabulary terms matching\\nqw and can resort to the standard inverted index.\\nPermuterm indexes\\nOur ﬁrst special index for general wildcard queries is the permuterm index,\\nPERMUTERM INDEX\\na form of inverted index. First, we introduce a special symbol $ into our\\ncharacter set, to mark the end of a term. Thus, the term hello is shown here as\\nthe augmented term hello$. Next, we construct a permuterm index, in which\\nthe various rotations of each term (augmented with $) all link to the original\\nvocabulary term. Figure 3.3 gives an example of such a permuterm index\\nentry for the term hello.\\nWe refer to the set of rotated terms in the permuterm index as the per-\\nmuterm vocabulary.\\nHow does this index help us with wildcard queries? Consider the wildcard\\nquery m*n. The key is to rotate such a wildcard query so that the * symbol\\nappears at the end of the string – thus the rotated wildcard query becomes\\nn$m*. Next, we look up this string in the permuterm index, where seeking\\nn$m* (via a search tree) leads to rotations of (among others) the terms man\\nand moron.\\nNow that the permuterm index enables us to identify the original vocab-\\nulary terms matching a wildcard query, we look up these terms in the stan-\\ndard inverted index to retrieve matching documents. We can thus handle\\nany wildcard query with a single * symbol. But what about a query such as\\nﬁ*mo*er? In this case we ﬁrst enumerate the terms in the dictionary that are\\nin the permuterm index of er$ﬁ*. Not all such dictionary terms will have\\nthe string mo in the middle - we ﬁlter these out by exhaustive enumera-\\ntion, checking each candidate to see if it contains mo. In this example, the\\nterm ﬁshmonger would survive this ﬁltering but ﬁlibuster would not. We then\\n', 'Online edition (c)\\n2009 Cambridge UP\\n54\\n3\\nDictionaries and tolerant retrieval\\n◮Figure 3.3\\nA portion of a permuterm index.\\nrun the surviving terms through the standard inverted index for document\\nretrieval. One disadvantage of the permuterm index is that its dictionary\\nbecomes quite large, including as it does all rotations of each term.\\nNotice the close interplay between the B-tree and the permuterm index\\nabove. Indeed, it suggests that the structure should perhaps be viewed as\\na permuterm B-tree. However, we follow traditional terminology here in\\ndescribing the permuterm index as distinct from the B-tree that allows us to\\nselect the rotations with a given preﬁx.\\n3.2.2\\nk-gram indexes for wildcard queries\\nWhereas the permuterm index is simple, it can lead to a considerable blowup\\nfrom the number of rotations per term; for a dictionary of English terms, this\\ncan represent an almost ten-fold space increase. We now present a second\\ntechnique, known as the k-gram index, for processing wildcard queries. We\\nwill also use k-gram indexes in Section 3.3.4. A k-gram is a sequence of k\\ncharacters. Thus cas, ast and stl are all 3-grams occurring in the term castle.\\nWe use a special character $ to denote the beginning or end of a term, so the\\nfull set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$.\\nIn a k-gram index, the dictionary contains all k-grams that occur in any term\\nk-GRAM INDEX\\nin the vocabulary. Each postings list points from a k-gram to all vocabulary\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.2\\nWildcard queries\\n55\\netr\\nbeetroot\\nmetric\\npetrify\\nretrieval\\n-\\n-\\n-\\n-\\n◮Figure 3.4\\nExample of a postings list in a 3-gram index. Here the 3-gram etr is\\nillustrated. Matching vocabulary terms are lexicographically ordered in the postings.\\nterms containing that k-gram. For instance, the 3-gram etr would point to vo-\\ncabulary terms such as metric and retrieval. An example is given in Figure 3.4.\\nHow does such an index help us with wildcard queries? Consider the\\nwildcard query re*ve. We are seeking documents containing any term that\\nbegins with re and ends with ve. Accordingly, we run the Boolean query $re\\nAND ve$. This is looked up in the 3-gram index and yields a list of matching\\nterms such as relive, remove and retrieve. Each of these matching terms is then\\nlooked up in the standard inverted index to yield documents matching the\\nquery.\\nThere is however a difﬁculty with the use of k-gram indexes, that demands\\none further step of processing. Consider using the 3-gram index described\\nabove for the query red*. Following the process described above, we ﬁrst\\nissue the Boolean query $re AND red to the 3-gram index. This leads to a\\nmatch on terms such as retired, which contain the conjunction of the two 3-\\ngrams $re and red, yet do not match the original wildcard query red*.\\nTo cope with this, we introduce a post-ﬁltering step, in which the terms enu-\\nmerated by the Boolean query on the 3-gram index are checked individually\\nagainst the original query red*. This is a simple string-matching operation\\nand weeds out terms such as retired that do not match the original query.\\nTerms that survive are then searched in the standard inverted index as usual.\\nWe have seen that a wildcard query can result in multiple terms being\\nenumerated, each of which becomes a single-term query on the standard in-\\nverted index. Search engines do allow the combination of wildcard queries\\nusing Boolean operators, for example, re*d AND fe*ri. What is the appropriate\\nsemantics for such a query? Since each wildcard query turns into a disjunc-\\ntion of single-term queries, the appropriate interpretation of this example\\nis that we have a conjunction of disjunctions: we seek all documents that\\ncontain any term matching re*d and any term matching fe*ri.\\nEven without Boolean combinations of wildcard queries, the processing of\\na wildcard query can be quite expensive, because of the added lookup in the\\nspecial index, ﬁltering and ﬁnally the standard inverted index. A search en-\\ngine may support such rich functionality, but most commonly, the capability\\nis hidden behind an interface (say an “Advanced Query” interface) that most\\n', 'Online edition (c)\\n2009 Cambridge UP\\n56\\n3\\nDictionaries and tolerant retrieval\\nusers never use. Exposing such functionality in the search interface often en-\\ncourages users to invoke it even when they do not require it (say, by typing\\na preﬁx of their query followed by a *), increasing the processing load on the\\nsearch engine.\\n?\\nExercise 3.1\\nIn the permuterm index, each permuterm vocabulary term points to the original vo-\\ncabulary term(s) from which it was derived. How many original vocabulary terms\\ncan there be in the postings list of a permuterm vocabulary term?\\nExercise 3.2\\nWrite down the entries in the permuterm index dictionary that are generated by the\\nterm mama.\\nExercise 3.3\\nIf you wanted to search for s*ng in a permuterm wildcard index, what key(s) would\\none do the lookup on?\\nExercise 3.4\\nRefer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the\\npostings are lexicographically ordered. Why is this ordering useful?\\nExercise 3.5\\nConsider again the query ﬁ*mo*er from Section 3.2.1. What Boolean query on a bigram\\nindex would be generated for this query? Can you think of a term that matches the\\npermuterm query in Section 3.2.1, but does not satisfy this Boolean query?\\nExercise 3.6\\nGive an example of a sentence that falsely matches the wildcard query mon*h if the\\nsearch were to simply use a conjunction of bigrams.\\n3.3\\nSpelling correction\\nWe next look at the problem of correcting spelling errors in queries. For in-\\nstance, we may wish to retrieve documents containing the term carrot when\\nthe user types the query carot. Google reports (http://www.google.com/jobs/britney.html)\\nthat the following are all treated as misspellings of the query britney spears:\\nbritian spears, britney’s spears, brandy spears and prittany spears. We look at two\\nsteps to solving this problem: the ﬁrst based on edit distance and the second\\nbased on k-gram overlap. Before getting into the algorithmic details of these\\nmethods, we ﬁrst review how search engines provide spell-correction as part\\nof a user experience.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n57\\n3.3.1\\nImplementing spelling correction\\nThere are two basic principles underlying most spelling correction algorithms.\\n1. Of various alternative correct spellings for a mis-spelled query, choose\\nthe “nearest” one. This demands that we have a notion of nearness or\\nproximity between a pair of queries. We will develop these proximity\\nmeasures in Section 3.3.3.\\n2. When two correctly spelled queries are tied (or nearly tied), select the one\\nthat is more common. For instance, grunt and grant both seem equally\\nplausible as corrections for grnt. Then, the algorithm should choose the\\nmore common of grunt and grant as the correction. The simplest notion\\nof more common is to consider the number of occurrences of the term\\nin the collection; thus if grunt occurs more often than grant, it would be\\nthe chosen correction. A different notion of more common is employed\\nin many search engines, especially on the web. The idea is to use the\\ncorrection that is most common among queries typed in by other users.\\nThe idea here is that if grunt is typed as a query more often than grant, then\\nit is more likely that the user who typed grnt intended to type the query\\ngrunt.\\nBeginning in Section 3.3.3 we describe notions of proximity between queries,\\nas well as their efﬁcient computation. Spelling correction algorithms build on\\nthese computations of proximity; their functionality is then exposed to users\\nin one of several ways:\\n1. On the query carot always retrieve documents containing carot as well as\\nany “spell-corrected” version of carot, including carrot and tarot.\\n2. As in (1) above, but only when the query term carot is not in the dictionary.\\n3. As in (1) above, but only when the original query returned fewer than a\\npreset number of documents (say fewer than ﬁve documents).\\n4. When the original query returns fewer than a preset number of docu-\\nments, the search interface presents a spelling suggestion to the end user:\\nthis suggestion consists of the spell-corrected query term(s). Thus, the\\nsearch engine might respond to the user: “Did you mean carrot?”\\n3.3.2\\nForms of spelling correction\\nWe focus on two speciﬁc forms of spelling correction that we refer to as\\nisolated-term correction and context-sensitive correction. In isolated-term cor-\\nrection, we attempt to correct a single query term at a time – even when we\\n', 'Online edition (c)\\n2009 Cambridge UP\\n58\\n3\\nDictionaries and tolerant retrieval\\nhave a multiple-term query. The carot example demonstrates this type of cor-\\nrection. Such isolated-term correction would fail to detect, for instance, that\\nthe query ﬂew form Heathrow contains a mis-spelling of the term from – because\\neach term in the query is correctly spelled in isolation.\\nWe begin by examining two techniques for addressing isolated-term cor-\\nrection: edit distance, and k-gram overlap.\\nWe then proceed to context-\\nsensitive correction.\\n3.3.3\\nEdit distance\\nGiven two character strings s1 and s2, the edit distance between them is the\\nEDIT DISTANCE\\nminimum number of edit operations required to transform s1 into s2. Most\\ncommonly, the edit operations allowed for this purpose are: (i) insert a char-\\nacter into a string; (ii) delete a character from a string and (iii) replace a char-\\nacter of a string by another character; for these operations, edit distance is\\nsometimes known as Levenshtein distance. For example, the edit distance be-\\nLEVENSHTEIN\\nDISTANCE\\ntween cat and dog is 3. In fact, the notion of edit distance can be generalized\\nto allowing different weights for different kinds of edit operations, for in-\\nstance a higher weight may be placed on replacing the character s by the\\ncharacter p, than on replacing it by the character a (the latter being closer to s\\non the keyboard). Setting weights in this way depending on the likelihood of\\nletters substituting for each other is very effective in practice (see Section 3.4\\nfor the separate issue of phonetic similarity). However, the remainder of our\\ntreatment here will focus on the case in which all edit operations have the\\nsame weight.\\nIt is well-known how to compute the (weighted) edit distance between\\ntwo strings in time O(|s1| × |s2|), where |si| denotes the length of a string si.\\nThe idea is to use the dynamic programming algorithm in Figure 3.5, where\\nthe characters in s1 and s2 are given in array form. The algorithm ﬁlls the\\n(integer) entries in a matrix m whose two dimensions equal the lengths of\\nthe two strings whose edit distances is being computed; the (i, j) entry of the\\nmatrix will hold (after the algorithm is executed) the edit distance between\\nthe strings consisting of the ﬁrst i characters of s1 and the ﬁrst j characters\\nof s2. The central dynamic programming step is depicted in Lines 8-10 of\\nFigure 3.5, where the three quantities whose minimum is taken correspond\\nto substituting a character in s1, inserting a character in s1 and inserting a\\ncharacter in s2.\\nFigure 3.6 shows an example Levenshtein distance computation of Fig-\\nure 3.5. The typical cell [i, j] has four entries formatted as a 2 × 2 cell. The\\nlower right entry in each cell is the min of the other three, corresponding to\\nthe main dynamic programming step in Figure 3.5. The other three entries\\nare the three entries m[i −1, j −1] + 0 or 1 depending on whether s1[i] =\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n59\\nEDITDISTANCE(s1, s2)\\n1\\nint m[i, j] = 0\\n2\\nfor i ←1 to |s1|\\n3\\ndo m[i, 0] = i\\n4\\nfor j ←1 to |s2|\\n5\\ndo m[0, j] = j\\n6\\nfor i ←1 to |s1|\\n7\\ndo for j ←1 to |s2|\\n8\\ndo m[i, j] = min{m[i −1, j −1] + if (s1[i] = s2[j]) then 0 else 1ﬁ,\\n9\\nm[i −1, j] + 1,\\n10\\nm[i, j −1] + 1}\\n11\\nreturn m[|s1|, |s2|]\\n◮Figure 3.5\\nDynamic programming algorithm for computing the edit distance be-\\ntween strings s1 and s2.\\nf\\na\\ns\\nt\\n0\\n1\\n1\\n2\\n2\\n3\\n3\\n4\\n4\\nc\\n1\\n1\\n1\\n2\\n2\\n1\\n2\\n3\\n2\\n2\\n3\\n4\\n3\\n3\\n4\\n5\\n4\\n4\\na\\n2\\n2\\n2\\n2\\n3\\n2\\n1\\n3\\n3\\n1\\n3\\n4\\n2\\n2\\n4\\n5\\n3\\n3\\nt\\n3\\n3\\n3\\n3\\n4\\n3\\n3\\n2\\n4\\n2\\n2\\n3\\n3\\n2\\n2\\n4\\n3\\n2\\ns\\n4\\n4\\n4\\n4\\n5\\n4\\n4\\n3\\n5\\n3\\n2\\n3\\n4\\n2\\n3\\n3\\n3\\n3\\n◮Figure 3.6\\nExample Levenshtein distance computation. The 2 × 2 cell in the [i, j]\\nentry of the table shows the three numbers whose minimum yields the fourth. The\\ncells in italics determine the edit distance in this example.\\ns2[j], m[i −1, j] + 1 and m[i, j −1] + 1. The cells with numbers in italics depict\\nthe path by which we determine the Levenshtein distance.\\nThe spelling correction problem however demands more than computing\\nedit distance: given a set S of strings (corresponding to terms in the vocab-\\nulary) and a query string q, we seek the string(s) in V of least edit distance\\nfrom q. We may view this as a decoding problem, in which the codewords\\n(the strings in V) are prescribed in advance. The obvious way of doing this\\nis to compute the edit distance from q to each string in V, before selecting the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n60\\n3\\nDictionaries and tolerant retrieval\\nstring(s) of minimum edit distance. This exhaustive search is inordinately\\nexpensive. Accordingly, a number of heuristics are used in practice to efﬁ-\\nciently retrieve vocabulary terms likely to have low edit distance to the query\\nterm(s).\\nThe simplest such heuristic is to restrict the search to dictionary terms be-\\nginning with the same letter as the query string; the hope would be that\\nspelling errors do not occur in the ﬁrst character of the query. A more sophis-\\nticated variant of this heuristic is to use a version of the permuterm index,\\nin which we omit the end-of-word symbol $. Consider the set of all rota-\\ntions of the query string q. For each rotation r from this set, we traverse the\\nB-tree into the permuterm index, thereby retrieving all dictionary terms that\\nhave a rotation beginning with r. For instance, if q is mase and we consider\\nthe rotation r = sema, we would retrieve dictionary terms such as semantic\\nand semaphore that do not have a small edit distance to q. Unfortunately, we\\nwould miss more pertinent dictionary terms such as mare and mane. To ad-\\ndress this, we reﬁne this rotation scheme: for each rotation, we omit a sufﬁx\\nof ℓcharacters before performing the B-tree traversal. This ensures that each\\nterm in the set R of terms retrieved from the dictionary includes a “long”\\nsubstring in common with q. The value of ℓcould depend on the length of q.\\nAlternatively, we may set it to a ﬁxed constant such as 2.\\n3.3.4\\nk-gram indexes for spelling correction\\nTo further limit the set of vocabulary terms for which we compute edit dis-\\ntances to the query term, we now show how to invoke the k-gram index of\\nSection 3.2.2 (page 54) to assist with retrieving vocabulary terms with low\\nedit distance to the query q. Once we retrieve such terms, we can then ﬁnd\\nthe ones of least edit distance from q.\\nIn fact, we will use the k-gram index to retrieve vocabulary terms that\\nhave many k-grams in common with the query. We will argue that for rea-\\nsonable deﬁnitions of “many k-grams in common,” the retrieval process is\\nessentially that of a single scan through the postings for the k-grams in the\\nquery string q.\\nThe 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the post-\\nings for the three bigrams in the query bord. Suppose we wanted to retrieve\\nvocabulary terms that contained at least two of these three bigrams. A single\\nscan of the postings (much as in Chapter 1) would let us enumerate all such\\nterms; in the example of Figure 3.7 we would enumerate aboard, boardroom\\nand border.\\nThis straightforward application of the linear scan intersection of postings\\nimmediately reveals the shortcoming of simply requiring matched vocabu-\\nlary terms to contain a ﬁxed number of k-grams from the query q: terms\\nlike boardroom, an implausible “correction” of bord, get enumerated. Conse-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.3\\nSpelling correction\\n61\\nrd\\naboard\\nardent\\nboardroom\\nborder\\nor\\nborder\\nlord\\nmorbid\\nsordid\\nbo\\naboard\\nabout\\nboardroom\\nborder\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n◮Figure 3.7\\nMatching at least two of the three 2-grams in the query bord.\\nquently, we require more nuanced measures of the overlap in k-grams be-\\ntween a vocabulary term and q. The linear scan intersection can be adapted\\nwhen the measure of overlap is the Jaccard coefﬁcient for measuring the over-\\nJACCARD COEFFICIENT\\nlap between two sets A and B, deﬁned to be |A ∩B|/|A ∪B|. The two sets we\\nconsider are the set of k-grams in the query q, and the set of k-grams in a vo-\\ncabulary term. As the scan proceeds, we proceed from one vocabulary term\\nt to the next, computing on the ﬂy the Jaccard coefﬁcient between q and t. If\\nthe coefﬁcient exceeds a preset threshold, we add t to the output; if not, we\\nmove on to the next term in the postings. To compute the Jaccard coefﬁcient,\\nwe need the set of k-grams in q and t.\\nSince we are scanning the postings for all k-grams in q, we immediately\\nhave these k-grams on hand. What about the k-grams of t? In principle,\\nwe could enumerate these on the ﬂy from t; in practice this is not only slow\\nbut potentially infeasible since, in all likelihood, the postings entries them-\\nselves do not contain the complete string t but rather some encoding of t. The\\ncrucial observation is that to compute the Jaccard coefﬁcient, we only need\\nthe length of the string t. To see this, recall the example of Figure 3.7 and\\nconsider the point when the postings scan for query q = bord reaches term\\nt = boardroom. We know that two bigrams match. If the postings stored the\\n(pre-computed) number of bigrams in boardroom (namely, 8), we have all the\\ninformation we require to compute the Jaccard coefﬁcient to be 2/(8 + 3 −2);\\nthe numerator is obtained from the number of postings hits (2, from bo and\\nrd) while the denominator is the sum of the number of bigrams in bord and\\nboardroom, less the number of postings hits.\\nWe could replace the Jaccard coefﬁcient by other measures that allow ef-\\nﬁcient on the ﬂy computation during postings scans. How do we use these\\n', 'Online edition (c)\\n2009 Cambridge UP\\n62\\n3\\nDictionaries and tolerant retrieval\\nfor spelling correction? One method that has some empirical support is to\\nﬁrst use the k-gram index to enumerate a set of candidate vocabulary terms\\nthat are potential corrections of q. We then compute the edit distance from q\\nto each term in this set, selecting terms from the set with small edit distance\\nto q.\\n3.3.5\\nContext sensitive spelling correction\\nIsolated-term correction would fail to correct typographical errors such as\\nﬂew form Heathrow, where all three query terms are correctly spelled. When\\na phrase such as this retrieves few documents, a search engine may like to\\noffer the corrected query ﬂew from Heathrow. The simplest way to do this is to\\nenumerate corrections of each of the three query terms (using the methods\\nleading up to Section 3.3.4) even though each query term is correctly spelled,\\nthen try substitutions of each correction in the phrase. For the example ﬂew\\nform Heathrow, we enumerate such phrases as ﬂed form Heathrow and ﬂew fore\\nHeathrow. For each such substitute phrase, the search engine runs the query\\nand determines the number of matching results.\\nThis enumeration can be expensive if we ﬁnd many corrections of the in-\\ndividual terms, since we could encounter a large number of combinations of\\nalternatives. Several heuristics are used to trim this space. In the example\\nabove, as we expand the alternatives for ﬂew and form, we retain only the\\nmost frequent combinations in the collection or in the query logs, which con-\\ntain previous queries by users. For instance, we would retain ﬂew from as an\\nalternative to try and extend to a three-term corrected query, but perhaps not\\nﬂed fore or ﬂea form. In this example, the biword ﬂed fore is likely to be rare\\ncompared to the biword ﬂew from. Then, we only attempt to extend the list of\\ntop biwords (such as ﬂew from), to corrections of Heathrow. As an alternative\\nto using the biword statistics in the collection, we may use the logs of queries\\nissued by users; these could of course include queries with spelling errors.\\n?\\nExercise 3.7\\nIf |si| denotes the length of string si, show that the edit distance between s1 and s2 is\\nnever more than max{|s1|, |s2|}.\\nExercise 3.8\\nCompute the edit distance between paris and alice. Write down the 5 × 5 array of\\ndistances between all preﬁxes as computed by the algorithm in Figure 3.5.\\nExercise 3.9\\nWrite pseudocode showing the details of computing on the ﬂy the Jaccard coefﬁcient\\nwhile scanning the postings of the k-gram index, as mentioned on page 61.\\nExercise 3.10\\nCompute the Jaccard coefﬁcients between the query bord and each of the terms in\\nFigure 3.7 that contain the bigram or.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.4\\nPhonetic correction\\n63\\nExercise 3.11\\nConsider the four-term query catched in the rye and suppose that each of the query\\nterms has ﬁve alternative terms suggested by isolated-term correction. How many\\npossible corrected phrases must we consider if we do not trim the space of corrected\\nphrases, but instead try all six variants for each of the terms?\\nExercise 3.12\\nFor each of the preﬁxes of the query — catched, catched in and catched in the — we have\\na number of substitute preﬁxes arising from each term and its alternatives. Suppose\\nthat we were to retain only the top 10 of these substitute preﬁxes, as measured by\\nits number of occurrences in the collection. We eliminate the rest from consideration\\nfor extension to longer preﬁxes: thus, if batched in is not one of the 10 most common\\n2-term queries in the collection, we do not consider any extension of batched in as pos-\\nsibly leading to a correction of catched in the rye. How many of the possible substitute\\npreﬁxes are we eliminating at each phase?\\nExercise 3.13\\nAre we guaranteed that retaining and extending only the 10 commonest substitute\\npreﬁxes of catched in will lead to one of the 10 commonest substitute preﬁxes of catched\\nin the?\\n3.4\\nPhonetic correction\\nOur ﬁnal technique for tolerant retrieval has to do with phonetic correction:\\nmisspellings that arise because the user types a query that sounds like the tar-\\nget term. Such algorithms are especially applicable to searches on the names\\nof people. The main idea here is to generate, for each term, a “phonetic hash”\\nso that similar-sounding terms hash to the same value. The idea owes its\\norigins to work in international police departments from the early 20th cen-\\ntury, seeking to match names for wanted criminals despite the names being\\nspelled differently in different countries. It is mainly used to correct phonetic\\nmisspellings in proper nouns.\\nAlgorithms for such phonetic hashing are commonly collectively known as\\nsoundex algorithms. However, there is an original soundex algorithm, with\\nSOUNDEX\\nvarious variants, built on the following scheme:\\n1. Turn every term to be indexed into a 4-character reduced form. Build an\\ninverted index from these reduced forms to the original terms; call this\\nthe soundex index.\\n2. Do the same with query terms.\\n3. When the query calls for a soundex match, search this soundex index.\\nThe variations in different soundex algorithms have to do with the conver-\\nsion of terms to 4-character forms. A commonly used conversion results in\\na 4-character code, with the ﬁrst character being a letter of the alphabet and\\nthe other three being digits between 0 and 9.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n64\\n3\\nDictionaries and tolerant retrieval\\n1. Retain the ﬁrst letter of the term.\\n2. Change all occurrences of the following letters to ’0’ (zero): ’A’, E’, ’I’, ’O’,\\n’U’, ’H’, ’W’, ’Y’.\\n3. Change letters to digits as follows:\\nB, F, P, V to 1.\\nC, G, J, K, Q, S, X, Z to 2.\\nD,T to 3.\\nL to 4.\\nM, N to 5.\\nR to 6.\\n4. Repeatedly remove one out of each pair of consecutive identical digits.\\n5. Remove all zeros from the resulting string. Pad the resulting string with\\ntrailing zeros and return the ﬁrst four positions, which will consist of a\\nletter followed by three digits.\\nFor an example of a soundex map, Hermann maps to H655. Given a query\\n(say herman), we compute its soundex code and then retrieve all vocabulary\\nterms matching this soundex code from the soundex index, before running\\nthe resulting query on the standard inverted index.\\nThis algorithm rests on a few observations: (1) vowels are viewed as inter-\\nchangeable, in transcribing names; (2) consonants with similar sounds (e.g.,\\nD and T) are put in equivalence classes. This leads to related names often\\nhaving the same soundex codes. While these rules work for many cases,\\nespecially European languages, such rules tend to be writing system depen-\\ndent. For example, Chinese names can be written in Wade-Giles or Pinyin\\ntranscription. While soundex works for some of the differences in the two\\ntranscriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2,\\nit fails in other cases, for example Wade-Giles j and Pinyin r are mapped\\ndifferently.\\n?\\nExercise 3.14\\nFind two differently spelled proper nouns whose soundex codes are the same.\\nExercise 3.15\\nFind two phonetically similar proper nouns whose soundex codes are different.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n3.5\\nReferences and further reading\\n65\\n3.5\\nReferences and further reading\\nKnuth (1997) is a comprehensive source for information on search trees, in-\\ncluding B-trees and their use in searching through dictionaries.\\nGarﬁeld (1976) gives one of the ﬁrst complete descriptions of the permuterm\\nindex. Ferragina and Venturini (2007) give an approach to addressing the\\nspace blowup in permuterm indexes.\\nOne of the earliest formal treatments of spelling correction was due to\\nDamerau (1964). The notion of edit distance that we have used is due to Lev-\\nenshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer\\n(1974). Peterson (1980) and Kukich (1992) developed variants of methods\\nbased on edit distances, culminating in a detailed empirical study of sev-\\neral methods by Zobel and Dart (1995), which shows that k-gram indexing\\nis very effective for ﬁnding candidate mismatches, but should be combined\\nwith a more ﬁne-grained technique such as edit distance to determine the\\nmost likely misspellings. Gusﬁeld (1997) is a standard reference on string\\nalgorithms such as edit distance.\\nProbabilistic models (“noisy channel” models) for spelling correction were\\npioneered by Kernighan et al. (1990) and further developed by Brill and\\nMoore (2000) and Toutanova and Moore (2002). In these models, the mis-\\nspelled query is viewed as a probabilistic corruption of a correct query. They\\nhave a similar mathematical basis to the language model methods presented\\nin Chapter 12, and also provide ways of incorporating phonetic similarity,\\ncloseness on the keyboard, and data from the actual spelling mistakes of\\nusers. Many would regard them as the state-of-the-art approach. Cucerzan\\nand Brill (2004) show how this work can be extended to learning spelling\\ncorrection models based on query reformulations in search engine logs.\\nThe soundex algorithm is attributed to Margaret K. Odell and Robert C.\\nRusselli (from U.S. patents granted in 1918 and 1922); the version described\\nhere draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate var-\\nious phonetic matching algorithms, ﬁnding that a variant of the soundex\\nalgorithm performs poorly for general spelling correction, but that other al-\\ngorithms based on the phonetic similarity of term pronunciations perform\\nwell.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n67\\n4\\nIndex construction\\nIn this chapter, we look at how to construct an inverted index. We call this\\nprocess index construction or indexing; the process or machine that performs it\\nINDEXING\\nthe indexer. The design of indexing algorithms is governed by hardware con-\\nINDEXER\\nstraints. We therefore begin this chapter with a review of the basics of com-\\nputer hardware that are relevant for indexing. We then introduce blocked\\nsort-based indexing (Section 4.2), an efﬁcient single-machine algorithm de-\\nsigned for static collections that can be viewed as a more scalable version of\\nthe basic sort-based indexing algorithm we introduced in Chapter 1. Sec-\\ntion 4.3 describes single-pass in-memory indexing, an algorithm that has\\neven better scaling properties because it does not hold the vocabulary in\\nmemory. For very large collections like the web, indexing has to be dis-\\ntributed over computer clusters with hundreds or thousands of machines.\\nWe discuss this in Section 4.4. Collections with frequent changes require dy-\\nnamic indexing introduced in Section 4.5 so that changes in the collection are\\nimmediately reﬂected in the index. Finally, we cover some complicating is-\\nsues that can arise in indexing – such as security and indexes for ranked\\nretrieval – in Section 4.6.\\nIndex construction interacts with several topics covered in other chapters.\\nThe indexer needs raw text, but documents are encoded in many ways (see\\nChapter 2). Indexers compress and decompress intermediate ﬁles and the\\nﬁnal index (see Chapter 5). In web search, documents are not on a local\\nﬁle system, but have to be spidered or crawled (see Chapter 20). In enter-\\nprise search, most documents are encapsulated in varied content manage-\\nment systems, email applications, and databases. We give some examples\\nin Section 4.7. Although most of these applications can be accessed via http,\\nnative Application Programming Interfaces (APIs) are usually more efﬁcient.\\nThe reader should be aware that building the subsystem that feeds raw text\\nto the indexing process can in itself be a challenging problem.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n68\\n4\\nIndex construction\\n◮Table 4.1\\nTypical system parameters in 2007. The seek time is the time needed\\nto position the disk head in a new position. The transfer time per byte is the rate of\\ntransfer from disk to memory when the head is in the right position.\\nSymbol\\nStatistic\\nValue\\ns\\naverage seek time\\n5 ms = 5 × 10−3 s\\nb\\ntransfer time per byte\\n0.02 µs = 2 × 10−8 s\\nprocessor’s clock rate\\n109 s−1\\np\\nlowlevel operation\\n(e.g., compare & swap a word)\\n0.01 µs = 10−8 s\\nsize of main memory\\nseveral GB\\nsize of disk space\\n1 TB or more\\n4.1\\nHardware basics\\nWhen building an information retrieval (IR) system, many decisions are based\\non the characteristics of the computer hardware on which the system runs.\\nWe therefore begin this chapter with a brief review of computer hardware.\\nPerformance characteristics typical of systems in 2007 are shown in Table 4.1.\\nA list of hardware basics that we need in this book to motivate IR system\\ndesign follows.\\n• Access to data in memory is much faster than access to data on disk. It\\ntakes a few clock cycles (perhaps 5 × 10−9 seconds) to access a byte in\\nmemory, but much longer to transfer it from disk (about 2 × 10−8 sec-\\nonds). Consequently, we want to keep as much data as possible in mem-\\nory, especially those data that we need to access frequently. We call the\\ntechnique of keeping frequently used disk data in main memory caching.\\nCACHING\\n• When doing a disk read or write, it takes a while for the disk head to\\nmove to the part of the disk where the data are located. This time is called\\nthe seek time and it averages 5 ms for typical disks. No data are being\\nSEEK TIME\\ntransferred during the seek. To maximize data transfer rates, chunks of\\ndata that will be read together should therefore be stored contiguously on\\ndisk. For example, using the numbers in Table 4.1 it may take as little as\\n0.2 seconds to transfer 10 megabytes (MB) from disk to memory if it is\\nstored as one chunk, but up to 0.2 + 100 × (5 × 10−3) = 0.7 seconds if it\\nis stored in 100 noncontiguous chunks because we need to move the disk\\nhead up to 100 times.\\n• Operating systems generally read and write entire blocks. Thus, reading\\na single byte from disk can take as much time as reading the entire block.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.2\\nBlocked sort-based indexing\\n69\\nBlock sizes of 8, 16, 32, and 64 kilobytes (KB) are common. We call the part\\nof main memory where a block being read or written is stored a buffer.\\nBUFFER\\n• Data transfers from disk to memory are handled by the system bus, not by\\nthe processor. This means that the processor is available to process data\\nduring disk I/O. We can exploit this fact to speed up data transfers by\\nstoring compressed data on disk. Assuming an efﬁcient decompression\\nalgorithm, the total time of reading and then decompressing compressed\\ndata is usually less than reading uncompressed data.\\n• Servers used in IR systems typically have several gigabytes (GB) of main\\nmemory, sometimes tens of GB. Available disk space is several orders of\\nmagnitude larger.\\n4.2\\nBlocked sort-based indexing\\nThe basic steps in constructing a nonpositional index are depicted in Fig-\\nure 1.4 (page 8). We ﬁrst make a pass through the collection assembling all\\nterm–docID pairs. We then sort the pairs with the term as the dominant key\\nand docID as the secondary key. Finally, we organize the docIDs for each\\nterm into a postings list and compute statistics like term and document fre-\\nquency. For small collections, all this can be done in memory. In this chapter,\\nwe describe methods for large collections that require the use of secondary\\nstorage.\\nTo make index construction more efﬁcient, we represent terms as termIDs\\n(instead of strings as we did in Figure 1.4), where each termID is a unique\\nTERMID\\nserial number. We can build the mapping from terms to termIDs on the ﬂy\\nwhile we are processing the collection; or, in a two-pass approach, we com-\\npile the vocabulary in the ﬁrst pass and construct the inverted index in the\\nsecond pass. The index construction algorithms described in this chapter all\\ndo a single pass through the data. Section 4.7 gives references to multipass\\nalgorithms that are preferable in certain applications, for example, when disk\\nspace is scarce.\\nWe work with the Reuters-RCV1 collection as our model collection in this\\nREUTERS-RCV1\\nchapter, a collection with roughly 1 GB of text. It consists of about 800,000\\ndocuments that were sent over the Reuters newswire during a 1-year pe-\\nriod between August 20, 1996, and August 19, 1997. A typical document is\\nshown in Figure 4.1, but note that we ignore multimedia information like\\nimages in this book and are only concerned with text. Reuters-RCV1 covers\\na wide range of international topics, including politics, business, sports, and\\n(as in this example) science. Some key statistics of the collection are shown\\nin Table 4.2.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n70\\n4\\nIndex construction\\n◮Table 4.2\\nCollection statistics for Reuters-RCV1. Values are rounded for the com-\\nputations in this book. The unrounded values are: 806,791 documents, 222 tokens\\nper document, 391,523 (distinct) terms, 6.04 bytes per token with spaces and punc-\\ntuation, 4.5 bytes per token without spaces and punctuation, 7.5 bytes per term, and\\n96,969,056 tokens. The numbers in this table correspond to the third line (“case fold-\\ning”) in Table 5.1 (page 87).\\nSymbol\\nStatistic\\nValue\\nN\\ndocuments\\n800,000\\nLave\\navg. # tokens per document\\n200\\nM\\nterms\\n400,000\\navg. # bytes per token (incl. spaces/punct.)\\n6\\navg. # bytes per token (without spaces/punct.)\\n4.5\\navg. # bytes per term\\n7.5\\nT\\ntokens\\n100,000,000\\nREUTERS\\nExtreme conditions create rare Antarctic clouds\\nYou are here:  Home > News > Science > Article\\nGo to a Section:      U.S.     International      Business      Markets\\nPolitics\\nEntertainment\\nTechnology\\nTue Aug 1, 2006 3:20am ET\\nEmail This Article | Print This Article | Reprints\\nSYDNEY (Reuters) - Rare, mother-of-pearl colored clouds\\ncaused by extreme weather conditions above Antarctica are a\\npossible indication of global warming, Australian scientists said on\\nTuesday.\\nKnown as nacreous clouds, the spectacular formations showing delicate\\nwisps of colors were photographed in the sky over an Australian\\nmeteorological base at Mawson Station on July 25.\\nSports\\nOddly Enough\\n[-] Text [+]\\n◮Figure 4.1\\nDocument from the Reuters newswire.\\nReuters-RCV1 has 100 million tokens. Collecting all termID–docID pairs of\\nthe collection using 4 bytes each for termID and docID therefore requires 0.8\\nGB of storage. Typical collections today are often one or two orders of mag-\\nnitude larger than Reuters-RCV1. You can easily see how such collections\\noverwhelm even large computers if we try to sort their termID–docID pairs\\nin memory. If the size of the intermediate ﬁles during index construction is\\nwithin a small factor of available memory, then the compression techniques\\nintroduced in Chapter 5 can help; however, the postings ﬁle of many large\\ncollections cannot ﬁt into memory even after compression.\\nWith main memory insufﬁcient, we need to use an external sorting algo-\\nEXTERNAL SORTING\\nALGORITHM\\nrithm, that is, one that uses disk. For acceptable speed, the central require-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.2\\nBlocked sort-based indexing\\n71\\nBSBINDEXCONSTRUCTION()\\n1\\nn ←0\\n2\\nwhile (all documents have not been processed)\\n3\\ndo n ←n + 1\\n4\\nblock ←PARSENEXTBLOCK()\\n5\\nBSBI-INVERT(block)\\n6\\nWRITEBLOCKTODISK(block, fn)\\n7\\nMERGEBLOCKS( f1, . . . , fn; f merged)\\n◮Figure 4.2\\nBlocked sort-based indexing. The algorithm stores inverted blocks in\\nﬁles f1, . . . , fn and the merged index in f merged.\\nment of such an algorithm is that it minimize the number of random disk\\nseeks during sorting – sequential disk reads are far faster than seeks as we\\nexplained in Section 4.1. One solution is the blocked sort-based indexing algo-\\nBLOCKED SORT-BASED\\nINDEXING ALGORITHM\\nrithm or BSBI in Figure 4.2. BSBI (i) segments the collection into parts of equal\\nsize, (ii) sorts the termID–docID pairs of each part in memory, (iii) stores in-\\ntermediate sorted results on disk, and (iv) merges all intermediate results\\ninto the ﬁnal index.\\nThe algorithm parses documents into termID–docID pairs and accumu-\\nlates the pairs in memory until a block of a ﬁxed size is full (PARSENEXTBLOCK\\nin Figure 4.2). We choose the block size to ﬁt comfortably into memory to\\npermit a fast in-memory sort. The block is then inverted and written to disk.\\nInversion involves two steps. First, we sort the termID–docID pairs. Next,\\nINVERSION\\nwe collect all termID–docID pairs with the same termID into a postings list,\\nwhere a posting is simply a docID. The result, an inverted index for the block\\nPOSTING\\nwe have just read, is then written to disk. Applying this to Reuters-RCV1 and\\nassuming we can ﬁt 10 million termID–docID pairs into memory, we end up\\nwith ten blocks, each an inverted index of one part of the collection.\\nIn the ﬁnal step, the algorithm simultaneously merges the ten blocks into\\none large merged index. An example with two blocks is shown in Figure 4.3,\\nwhere we use di to denote the ith document of the collection. To do the merg-\\ning, we open all block ﬁles simultaneously, and maintain small read buffers\\nfor the ten blocks we are reading and a write buffer for the ﬁnal merged in-\\ndex we are writing. In each iteration, we select the lowest termID that has\\nnot been processed yet using a priority queue or a similar data structure. All\\npostings lists for this termID are read and merged, and the merged list is\\nwritten back to disk. Each read buffer is reﬁlled from its ﬁle when necessary.\\nHow expensive is BSBI? Its time complexity is Θ(T log T) because the step\\nwith the highest time complexity is sorting and T is an upper bound for the\\nnumber of items we must sort (i.e., the number of termID–docID pairs). But\\n', 'Online edition (c)\\n2009 Cambridge UP\\n72\\n4\\nIndex construction\\nbrutus\\nd1,d3\\ncaesar\\nd1,d2,d4\\nnoble\\nd5\\nwith\\nd1,d2,d3,d5\\nbrutus\\nd6,d7\\ncaesar\\nd8,d9\\njulius\\nd10\\nkilled\\nd8\\npostings lists\\nto be merged\\nbrutus\\nd1,d3,d6,d7\\ncaesar\\nd1,d2,d4,d8,d9\\njulius\\nd10\\nkilled\\nd8\\nnoble\\nd5\\nwith\\nd1,d2,d3,d5\\nmerged\\npostings lists\\ndisk\\n◮Figure 4.3\\nMerging in blocked sort-based indexing. Two blocks (“postings lists to\\nbe merged”) are loaded from disk into memory, merged in memory (“merged post-\\nings lists”) and written back to disk. We show terms instead of termIDs for better\\nreadability.\\nthe actual indexing time is usually dominated by the time it takes to parse the\\ndocuments (PARSENEXTBLOCK) and to do the ﬁnal merge (MERGEBLOCKS).\\nExercise 4.6 asks you to compute the total index construction time for RCV1\\nthat includes these steps as well as inverting the blocks and writing them to\\ndisk.\\nNotice that Reuters-RCV1 is not particularly large in an age when one or\\nmore GB of memory are standard on personal computers. With appropriate\\ncompression (Chapter 5), we could have created an inverted index for RCV1\\nin memory on a not overly beefy server. The techniques we have described\\nare needed, however, for collections that are several orders of magnitude\\nlarger.\\n?\\nExercise 4.1\\nIf we need T log2 T comparisons (where T is the number of termID–docID pairs) and\\ntwo disk seeks for each comparison, how much time would index construction for\\nReuters-RCV1 take if we used disk instead of memory for storage and an unopti-\\nmized sorting algorithm (i.e., not an external sorting algorithm)? Use the system\\nparameters in Table 4.1.\\nExercise 4.2\\n[⋆]\\nHow would you create the dictionary in blocked sort-based indexing on the ﬂy to\\navoid an extra pass through the data?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.3\\nSingle-pass in-memory indexing\\n73\\nSPIMI-INVERT(token_stream)\\n1\\noutput_file = NEWFILE()\\n2\\ndictionary = NEWHASH()\\n3\\nwhile (free memory available)\\n4\\ndo token ←next(token_stream)\\n5\\nif term(token) /∈dictionary\\n6\\nthen postings_list = ADDTODICTIONARY(dictionary, term(token))\\n7\\nelse postings_list = GETPOSTINGSLIST(dictionary, term(token))\\n8\\nif f ull(postings_list)\\n9\\nthen postings_list = DOUBLEPOSTINGSLIST(dictionary, term(token))\\n10\\nADDTOPOSTINGSLIST(postings_list, docID(token))\\n11\\nsorted_terms ←SORTTERMS(dictionary)\\n12\\nWRITEBLOCKTODISK(sorted_terms, dictionary, output_file)\\n13\\nreturn output_file\\n◮Figure 4.4\\nInversion of a block in single-pass in-memory indexing\\n4.3\\nSingle-pass in-memory indexing\\nBlocked sort-based indexing has excellent scaling properties, but it needs\\na data structure for mapping terms to termIDs. For very large collections,\\nthis data structure does not ﬁt into memory. A more scalable alternative is\\nsingle-pass in-memory indexing or SPIMI. SPIMI uses terms instead of termIDs,\\nSINGLE-PASS\\nIN-MEMORY INDEXING\\nwrites each block’s dictionary to disk, and then starts a new dictionary for the\\nnext block. SPIMI can index collections of any size as long as there is enough\\ndisk space available.\\nThe SPIMI algorithm is shown in Figure 4.4. The part of the algorithm that\\nparses documents and turns them into a stream of term–docID pairs, which\\nwe call tokens here, has been omitted. SPIMI-INVERT is called repeatedly on\\nthe token stream until the entire collection has been processed.\\nTokens are processed one by one (line 4) during each successive call of\\nSPIMI-INVERT. When a term occurs for the ﬁrst time, it is added to the\\ndictionary (best implemented as a hash), and a new postings list is created\\n(line 6). The call in line 7 returns this postings list for subsequent occurrences\\nof the term.\\nA difference between BSBI and SPIMI is that SPIMI adds a posting di-\\nrectly to its postings list (line 10). Instead of ﬁrst collecting all termID–docID\\npairs and then sorting them (as we did in BSBI), each postings list is dynamic\\n(i.e., its size is adjusted as it grows) and it is immediately available to collect\\npostings. This has two advantages: It is faster because there is no sorting\\nrequired, and it saves memory because we keep track of the term a postings\\n', 'Online edition (c)\\n2009 Cambridge UP\\n74\\n4\\nIndex construction\\nlist belongs to, so the termIDs of postings need not be stored. As a result, the\\nblocks that individual calls of SPIMI-INVERT can process are much larger\\nand the index construction process as a whole is more efﬁcient.\\nBecause we do not know how large the postings list of a term will be when\\nwe ﬁrst encounter it, we allocate space for a short postings list initially and\\ndouble the space each time it is full (lines 8–9). This means that some mem-\\nory is wasted, which counteracts the memory savings from the omission of\\ntermIDs in intermediate data structures. However, the overall memory re-\\nquirements for the dynamically constructed index of a block in SPIMI are\\nstill lower than in BSBI.\\nWhen memory has been exhausted, we write the index of the block (which\\nconsists of the dictionary and the postings lists) to disk (line 12). We have to\\nsort the terms (line 11) before doing this because we want to write postings\\nlists in lexicographic order to facilitate the ﬁnal merging step. If each block’s\\npostings lists were written in unsorted order, merging blocks could not be\\naccomplished by a simple linear scan through each block.\\nEach call of SPIMI-INVERT writes a block to disk, just as in BSBI. The last\\nstep of SPIMI (corresponding to line 7 in Figure 4.2; not shown in Figure 4.4)\\nis then to merge the blocks into the ﬁnal inverted index.\\nIn addition to constructing a new dictionary structure for each block and\\neliminating the expensive sorting step, SPIMI has a third important compo-\\nnent: compression. Both the postings and the dictionary terms can be stored\\ncompactly on disk if we employ compression. Compression increases the ef-\\nﬁciency of the algorithm further because we can process even larger blocks,\\nand because the individual blocks require less space on disk. We refer readers\\nto the literature for this aspect of the algorithm (Section 4.7).\\nThe time complexity of SPIMI is Θ(T) because no sorting of tokens is re-\\nquired and all operations are at most linear in the size of the collection.\\n4.4\\nDistributed indexing\\nCollections are often so large that we cannot perform index construction efﬁ-\\nciently on a single machine. This is particularly true of the World Wide Web\\nfor which we need large computer clusters1 to construct any reasonably sized\\nweb index. Web search engines, therefore, use distributed indexing algorithms\\nfor index construction. The result of the construction process is a distributed\\nindex that is partitioned across several machines – either according to term\\nor according to document. In this section, we describe distributed indexing\\nfor a term-partitioned index. Most large search engines prefer a document-\\n1. A cluster in this chapter is a group of tightly coupled computers that work together closely.\\nThis sense of the word is different from the use of cluster as a group of documents that are\\nsemantically similar in Chapters 16–18.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.4\\nDistributed indexing\\n75\\npartitioned index (which can be easily generated from a term-partitioned\\nindex). We discuss this topic further in Section 20.3 (page 454).\\nThe distributed index construction method we describe in this section is an\\napplication of MapReduce, a general architecture for distributed computing.\\nMAPREDUCE\\nMapReduce is designed for large computer clusters. The point of a cluster is\\nto solve large computing problems on cheap commodity machines or nodes\\nthat are built from standard parts (processor, memory, disk) as opposed to on\\na supercomputer with specialized hardware. Although hundreds or thou-\\nsands of machines are available in such clusters, individual machines can\\nfail at any time. One requirement for robust distributed indexing is, there-\\nfore, that we divide the work up into chunks that we can easily assign and\\n– in case of failure – reassign. A master node directs the process of assigning\\nMASTER NODE\\nand reassigning tasks to individual worker nodes.\\nThe map and reduce phases of MapReduce split up the computing job\\ninto chunks that standard machines can process in a short time. The various\\nsteps of MapReduce are shown in Figure 4.5 and an example on a collection\\nconsisting of two documents is shown in Figure 4.6. First, the input data,\\nin our case a collection of web pages, are split into n splits where the size of\\nSPLITS\\nthe split is chosen to ensure that the work can be distributed evenly (chunks\\nshould not be too large) and efﬁciently (the total number of chunks we need\\nto manage should not be too large); 16 or 64 MB are good sizes in distributed\\nindexing. Splits are not preassigned to machines, but are instead assigned\\nby the master node on an ongoing basis: As a machine ﬁnishes processing\\none split, it is assigned the next one. If a machine dies or becomes a laggard\\ndue to hardware problems, the split it is working on is simply reassigned to\\nanother machine.\\nIn general, MapReduce breaks a large computing problem into smaller\\nparts by recasting it in terms of manipulation of key-value pairs. For index-\\nKEY-VALUE PAIRS\\ning, a key-value pair has the form (termID,docID). In distributed indexing,\\nthe mapping from terms to termIDs is also distributed and therefore more\\ncomplex than in single-machine indexing. A simple solution is to maintain\\na (perhaps precomputed) mapping for frequent terms that is copied to all\\nnodes and to use terms directly (instead of termIDs) for infrequent terms.\\nWe do not address this problem here and assume that all nodes share a con-\\nsistent term →termID mapping.\\nThe map phase of MapReduce consists of mapping splits of the input data\\nMAP PHASE\\nto key-value pairs. This is the same parsing task we also encountered in BSBI\\nand SPIMI, and we therefore call the machines that execute the map phase\\nparsers. Each parser writes its output to local intermediate ﬁles, the segment\\nPARSER\\nSEGMENT FILE\\nﬁles (shown as a-f g-p q-z in Figure 4.5).\\nFor the reduce phase, we want all values for a given key to be stored close\\nREDUCE PHASE\\ntogether, so that they can be read and processed quickly. This is achieved by\\n', 'Online edition (c)\\n2009 Cambridge UP\\n76\\n4\\nIndex construction\\nmaster\\nassign\\nmap\\nphase\\nreduce\\nphase\\nassign\\nparser\\nsplits\\nparser\\nparser\\ninverter\\npostings\\ninverter\\ninverter\\na-f\\ng-p\\nq-z\\na-f g-p q-z\\na-f g-p q-z\\na-f\\nsegment\\nfiles\\ng-p q-z\\n◮Figure 4.5\\nAn example of distributed indexing with MapReduce. Adapted from\\nDean and Ghemawat (2004).\\npartitioning the keys into j term partitions and having the parsers write key-\\nvalue pairs for each term partition into a separate segment ﬁle. In Figure 4.5,\\nthe term partitions are according to ﬁrst letter: a–f, g–p, q–z, and j = 3. (We\\nchose these key ranges for ease of exposition. In general, key ranges need not\\ncorrespond to contiguous terms or termIDs.) The term partitions are deﬁned\\nby the person who operates the indexing system (Exercise 4.10). The parsers\\nthen write corresponding segment ﬁles, one for each term partition. Each\\nterm partition thus corresponds to r segments ﬁles, where r is the number\\nof parsers. For instance, Figure 4.5 shows three a–f segment ﬁles of the a–f\\npartition, corresponding to the three parsers shown in the ﬁgure.\\nCollecting all values (here: docIDs) for a given key (here: termID) into one\\nlist is the task of the inverters in the reduce phase. The master assigns each\\nINVERTER\\nterm partition to a different inverter – and, as in the case of parsers, reas-\\nsigns term partitions in case of failing or slow inverters. Each term partition\\n(corresponding to r segment ﬁles, one on each parser) is processed by one in-\\nverter. We assume here that segment ﬁles are of a size that a single machine\\ncan handle (Exercise 4.9). Finally, the list of values is sorted for each key and\\nwritten to the ﬁnal sorted postings list (“postings” in the ﬁgure). (Note that\\npostings in Figure 4.6 include term frequencies, whereas each posting in the\\nother sections of this chapter is simply a docID without term frequency in-\\nformation.) The data ﬂow is shown for a–f in Figure 4.5. This completes the\\nconstruction of the inverted index.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.4\\nDistributed indexing\\n77\\nSchema of map and reduce functions\\nmap: input\\nlist(k,v)\\nreduce: (k,list(v))\\noutput\\nInstantiation of the schema for index construction\\nmap: web collection\\nlist(termID, docID)\\nreduce: ( termID ,\\n1 list(docID) ,  termID  ,\\n2 list(docID) , . . . )\\n(postings list\\nlist\\n1, postings\\n2, ...)\\nExample for index construction\\nmap: d2 : C died. d1 : C came, C c’ed.\\n( C, d2 ,  died,d2 ,  C,d1 ,  came,d1 ,  C,d1 ,  〈c’ed,d1〉)\\nreduce: ( C,(d2,d1,d1) , died,(d2) , came,(d1) , c’ed,(d1) )\\n(〈C,(d1:2,d2:1)〉, 〈died,(d2:1)〉, 〈came,(d1:1)〉, 〈c’ed,(d1:1)〉 )\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〉\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n〈\\n→\\n→\\n→\\n→\\n→\\n→\\n◮Figure 4.6\\nMap and reduce functions in MapReduce. In general, the map func-\\ntion produces a list of key-value pairs. All values for a key are collected into one\\nlist in the reduce phase. This list is then processed further. The instantiations of the\\ntwo functions and an example are shown for index construction. Because the map\\nphase processes documents in a distributed fashion, termID–docID pairs need not be\\nordered correctly initially as in this example. The example shows terms instead of\\ntermIDs for better readability. We abbreviate Caesar as C and conquered as c’ed.\\nParsers and inverters are not separate sets of machines. The master iden-\\ntiﬁes idle machines and assigns tasks to them. The same machine can be a\\nparser in the map phase and an inverter in the reduce phase. And there are\\noften other jobs that run in parallel with index construction, so in between\\nbeing a parser and an inverter a machine might do some crawling or another\\nunrelated task.\\nTo minimize write times before inverters reduce the data, each parser writes\\nits segment ﬁles to its local disk. In the reduce phase, the master communi-\\ncates to an inverter the locations of the relevant segment ﬁles (e.g., of the r\\nsegment ﬁles of the a–f partition). Each segment ﬁle only requires one se-\\nquential read because all data relevant to a particular inverter were written\\nto a single segment ﬁle by the parser. This setup minimizes the amount of\\nnetwork trafﬁc needed during indexing.\\nFigure 4.6 shows the general schema of the MapReduce functions. In-\\nput and output are often lists of key-value pairs themselves, so that several\\nMapReduce jobs can run in sequence. In fact, this was the design of the\\nGoogle indexing system in 2004. What we describe in this section corre-\\nsponds to only one of ﬁve to ten MapReduce operations in that indexing\\nsystem. Another MapReduce operation transforms the term-partitioned in-\\ndex we just created into a document-partitioned one.\\nMapReduce offers a robust and conceptually simple framework for imple-\\nmenting index construction in a distributed environment. By providing a\\nsemiautomatic method for splitting index construction into smaller tasks, it\\ncan scale to almost arbitrarily large collections, given computer clusters of\\n', 'Online edition (c)\\n2009 Cambridge UP\\n78\\n4\\nIndex construction\\nsufﬁcient size.\\n?\\nExercise 4.3\\nFor n = 15 splits, r = 10 segments, and j = 3 term partitions, how long would\\ndistributed index creation take for Reuters-RCV1 in a MapReduce architecture? Base\\nyour assumptions about cluster machines on Table 4.1.\\n4.5\\nDynamic indexing\\nThus far, we have assumed that the document collection is static. This is ﬁne\\nfor collections that change infrequently or never (e.g., the Bible or Shake-\\nspeare). But most collections are modiﬁed frequently with documents being\\nadded, deleted, and updated. This means that new terms need to be added\\nto the dictionary, and postings lists need to be updated for existing terms.\\nThe simplest way to achieve this is to periodically reconstruct the index\\nfrom scratch. This is a good solution if the number of changes over time is\\nsmall and a delay in making new documents searchable is acceptable – and\\nif enough resources are available to construct a new index while the old one\\nis still available for querying.\\nIf there is a requirement that new documents be included quickly, one solu-\\ntion is to maintain two indexes: a large main index and a small auxiliary index\\nAUXILIARY INDEX\\nthat stores new documents. The auxiliary index is kept in memory. Searches\\nare run across both indexes and results merged. Deletions are stored in an in-\\nvalidation bit vector. We can then ﬁlter out deleted documents before return-\\ning the search result. Documents are updated by deleting and reinserting\\nthem.\\nEach time the auxiliary index becomes too large, we merge it into the main\\nindex. The cost of this merging operation depends on how we store the index\\nin the ﬁle system. If we store each postings list as a separate ﬁle, then the\\nmerge simply consists of extending each postings list of the main index by\\nthe corresponding postings list of the auxiliary index. In this scheme, the\\nreason for keeping the auxiliary index is to reduce the number of disk seeks\\nrequired over time. Updating each document separately requires up to Mave\\ndisk seeks, where Mave is the average size of the vocabulary of documents in\\nthe collection. With an auxiliary index, we only put additional load on the\\ndisk when we merge auxiliary and main indexes.\\nUnfortunately, the one-ﬁle-per-postings-list scheme is infeasible because\\nmost ﬁle systems cannot efﬁciently handle very large numbers of ﬁles. The\\nsimplest alternative is to store the index as one large ﬁle, that is, as a concate-\\nnation of all postings lists. In reality, we often choose a compromise between\\nthe two extremes (Section 4.7). To simplify the discussion, we choose the\\nsimple option of storing the index as one large ﬁle here.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.5\\nDynamic indexing\\n79\\nLMERGEADDTOKEN(indexes, Z0, token)\\n1\\nZ0 ←MERGE(Z0, {token})\\n2\\nif |Z0| = n\\n3\\nthen for i ←0 to ∞\\n4\\ndo if Ii ∈indexes\\n5\\nthen Zi+1 ←MERGE(Ii, Zi)\\n6\\n(Zi+1 is a temporary index on disk.)\\n7\\nindexes ←indexes −{Ii}\\n8\\nelse Ii ←Zi\\n(Zi becomes the permanent index Ii.)\\n9\\nindexes ←indexes ∪{Ii}\\n10\\nBREAK\\n11\\nZ0 ←∅\\nLOGARITHMICMERGE()\\n1\\nZ0 ←∅\\n(Z0 is the in-memory index.)\\n2\\nindexes ←∅\\n3\\nwhile true\\n4\\ndo LMERGEADDTOKEN(indexes, Z0, GETNEXTTOKEN())\\n◮Figure 4.7\\nLogarithmic merging. Each token (termID,docID) is initially added to\\nin-memory index Z0 by LMERGEADDTOKEN. LOGARITHMICMERGE initializes Z0\\nand indexes.\\nIn this scheme, we process each posting ⌊T/n⌋times because we touch it\\nduring each of ⌊T/n⌋merges where n is the size of the auxiliary index and T\\nthe total number of postings. Thus, the overall time complexity is Θ(T2/n).\\n(We neglect the representation of terms here and consider only the docIDs.\\nFor the purpose of time complexity, a postings list is simply a list of docIDs.)\\nWe can do better than Θ(T2/n) by introducing log2(T/n) indexes I0, I1,\\nI2, ...of size 20 × n, 21 × n, 22 × n .... Postings percolate up this sequence of\\nindexes and are processed only once on each level. This scheme is called log-\\nLOGARITHMIC\\nMERGING\\narithmic merging (Figure 4.7). As before, up to n postings are accumulated in\\nan in-memory auxiliary index, which we call Z0. When the limit n is reached,\\nthe 20 × n postings in Z0 are transferred to a new index I0 that is created on\\ndisk. The next time Z0 is full, it is merged with I0 to create an index Z1 of size\\n21× n. Then Z1 is either stored as I1 (if there isn’t already an I1) or merged\\nwith I1 into Z2 (if I1 exists); and so on. We service search requests by query-\\ning in-memory Z0 and all currently valid indexes Ii on disk and merging the\\nresults. Readers familiar with the binomial heap data structure2 will recog-\\n2. See, for example, (Cormen et al. 1990, Chapter 19).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n80\\n4\\nIndex construction\\nnize its similarity with the structure of the inverted indexes in logarithmic\\nmerging.\\nOverall index construction time is Θ(T log(T/n)) because each posting\\nis processed only once on each of the log(T/n) levels. We trade this efﬁ-\\nciency gain for a slow down of query processing; we now need to merge\\nresults from log(T/n) indexes as opposed to just two (the main and auxil-\\niary indexes). As in the auxiliary index scheme, we still need to merge very\\nlarge indexes occasionally (which slows down the search system during the\\nmerge), but this happens less frequently and the indexes involved in a merge\\non average are smaller.\\nHaving multiple indexes complicates the maintenance of collection-wide\\nstatistics. For example, it affects the spelling correction algorithm in Sec-\\ntion 3.3 (page 56) that selects the corrected alternative with the most hits.\\nWith multiple indexes and an invalidation bit vector, the correct number of\\nhits for a term is no longer a simple lookup. In fact, all aspects of an IR\\nsystem – index maintenance, query processing, distribution, and so on – are\\nmore complex in logarithmic merging.\\nBecause of this complexity of dynamic indexing, some large search engines\\nadopt a reconstruction-from-scratch strategy. They do not construct indexes\\ndynamically. Instead, a new index is built from scratch periodically. Query\\nprocessing is then switched from the new index and the old index is deleted.\\n?\\nExercise 4.4\\nFor n = 2 and 1 ≤T ≤30, perform a step-by-step simulation of the algorithm in\\nFigure 4.7. Create a table that shows, for each point in time at which T = 2 ∗k tokens\\nhave been processed (1 ≤k ≤15), which of the three indexes I0, . . . , I3 are in use. The\\nﬁrst three lines of the table are given below.\\nI3\\nI2\\nI1\\nI0\\n2\\n0\\n0\\n0\\n0\\n4\\n0\\n0\\n0\\n1\\n6\\n0\\n0\\n1\\n0\\n4.6\\nOther types of indexes\\nThis chapter only describes construction of nonpositional indexes. Except\\nfor the much larger data volume we need to accommodate, the main differ-\\nence for positional indexes is that (termID, docID, (position1, position2, ...))\\ntriples, instead of (termID, docID) pairs have to be processed and that tokens\\nand postings contain positional information in addition to docIDs. With this\\nchange, the algorithms discussed here can all be applied to positional in-\\ndexes.\\nIn the indexes we have considered so far, postings lists are ordered with\\nrespect to docID. As we see in Chapter 5, this is advantageous for compres-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.6\\nOther types of indexes\\n81\\nusers\\ndocuments\\n0/1\\ndoc\\ne.\\n, 1 otherwis\\n0 if user can’t read\\n◮Figure 4.8\\nA user-document matrix for access control lists. Element (i, j) is 1 if\\nuser i has access to document j and 0 otherwise. During query processing, a user’s\\naccess postings list is intersected with the results list returned by the text part of the\\nindex.\\nsion – instead of docIDs we can compress smaller gaps between IDs, thus\\nreducing space requirements for the index. However, this structure for the\\nindex is not optimal when we build ranked (Chapters 6 and 7) – as opposed to\\nRANKED\\nBoolean – retrieval systems. In ranked retrieval, postings are often ordered ac-\\nRETRIEVAL SYSTEMS\\ncording to weight or impact, with the highest-weighted postings occurring\\nﬁrst. With this organization, scanning of long postings lists during query\\nprocessing can usually be terminated early when weights have become so\\nsmall that any further documents can be predicted to be of low similarity\\nto the query (see Chapter 6). In a docID-sorted index, new documents are\\nalways inserted at the end of postings lists. In an impact-sorted index (Sec-\\ntion 7.1.5, page 140), the insertion can occur anywhere, thus complicating the\\nupdate of the inverted index.\\nSecurityis an important consideration for retrieval systems in corporations.\\nSECURITY\\nA low-level employee should not be able to ﬁnd the salary roster of the cor-\\nporation, but authorized managers need to be able to search for it. Users’\\nresults lists must not contain documents they are barred from opening; the\\nvery existence of a document can be sensitive information.\\nUser authorization is often mediated through access control lists or ACLs.\\nACCESS CONTROL LISTS\\nACLs can be dealt with in an information retrieval system by representing\\neach document as the set of users that can access them (Figure 4.8) and then\\ninverting the resulting user-document matrix. The inverted ACL index has,\\nfor each user, a “postings list” of documents they can access – the user’s ac-\\ncess list. Search results are then intersected with this list. However, such\\nan index is difﬁcult to maintain when access permissions change – we dis-\\ncussed these difﬁculties in the context of incremental indexing for regular\\npostings lists in Section 4.5. It also requires the processing of very long post-\\nings lists for users with access to large document subsets. User membership\\nis therefore often veriﬁed by retrieving access information directly from the\\nﬁle system at query time – even though this slows down retrieval.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n82\\n4\\nIndex construction\\n◮Table 4.3\\nThe ﬁve steps in constructing an index for Reuters-RCV1 in blocked\\nsort-based indexing. Line numbers refer to Figure 4.2.\\nStep\\nTime\\n1\\nreading of collection (line 4)\\n2\\n10 initial sorts of 107 records each (line 5)\\n3\\nwriting of 10 blocks (line 6)\\n4\\ntotal disk transfer time for merging (line 7)\\n5\\ntime of actual merging (line 7)\\ntotal\\n◮Table 4.4\\nCollection statistics for a large collection.\\nSymbol\\nStatistic\\nValue\\nN\\n# documents\\n1,000,000,000\\nLave\\n# tokens per document\\n1000\\nM\\n# distinct terms\\n44,000,000\\nWe discussed indexes for storing and retrieving terms (as opposed to doc-\\numents) in Chapter 3.\\n?\\nExercise 4.5\\nCan spelling correction compromise document-level security? Consider the case where\\na spelling correction is based on documents to which the user does not have access.\\n?\\nExercise 4.6\\nTotal index construction time in blocked sort-based indexing is broken down in Ta-\\nble 4.3. Fill out the time column of the table for Reuters-RCV1 assuming a system\\nwith the parameters given in Table 4.1.\\nExercise 4.7\\nRepeat Exercise 4.6 for the larger collection in Table 4.4. Choose a block size that is\\nrealistic for current technology (remember that a block should easily ﬁt into main\\nmemory). How many blocks do you need?\\nExercise 4.8\\nAssume that we have a collection of modest size whose index can be constructed with\\nthe simple in-memory indexing algorithm in Figure 1.4 (page 8). For this collection,\\ncompare memory, disk and time requirements of the simple algorithm in Figure 1.4\\nand blocked sort-based indexing.\\nExercise 4.9\\nAssume that machines in MapReduce have 100 GB of disk space each. Assume fur-\\nther that the postings list of the term the has a size of 200 GB. Then the MapReduce\\nalgorithm as described cannot be run to construct the index. How would you modify\\nMapReduce so that it can handle this case?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n4.7\\nReferences and further reading\\n83\\nExercise 4.10\\nFor optimal load balancing, the inverters in MapReduce must get segmented postings\\nﬁles of similar sizes. For a new collection, the distribution of key-value pairs may not\\nbe known in advance. How would you solve this problem?\\nExercise 4.11\\nApply MapReduce to the problem of counting how often each term occurs in a set of\\nﬁles. Specify map and reduce operations for this task. Write down an example along\\nthe lines of Figure 4.6.\\nExercise 4.12\\nWe claimed (on page 80) that an auxiliary index can impair the quality of collec-\\ntion statistics. An example is the term weighting method idf, which is deﬁned as\\nlog(N/dfi) where N is the total number of documents and dfi is the number of docu-\\nments that term i occurs in (Section 6.2.1, page 117). Show that even a small auxiliary\\nindex can cause signiﬁcant error in idf when it is computed on the main index only.\\nConsider a rare term that suddenly occurs frequently (e.g., Flossie as in Tropical Storm\\nFlossie).\\n4.7\\nReferences and further reading\\nWitten et al. (1999, Chapter 5) present an extensive treatment of the subject of\\nindex construction and additional indexing algorithms with different trade-\\noffs of memory, disk space, and time. In general, blocked sort-based indexing\\ndoes well on all three counts. However, if conserving memory or disk space\\nis the main criterion, then other algorithms may be a better choice. See Wit-\\nten et al. (1999), Tables 5.4 and 5.5; BSBI is closest to “sort-based multiway\\nmerge,” but the two algorithms differ in dictionary structure and use of com-\\npression.\\nMoffat and Bell (1995) show how to construct an index “in situ,” that\\nis, with disk space usage close to what is needed for the ﬁnal index and\\nwith a minimum of additional temporary ﬁles (cf. also Harman and Candela\\n(1990)). They give Lesk (1988) and Somogyi (1990) credit for being among\\nthe ﬁrst to employ sorting for index construction.\\nThe SPIMI method in Section 4.3 is from (Heinz and Zobel 2003). We have\\nsimpliﬁed several aspects of the algorithm, including compression and the\\nfact that each term’s data structure also contains, in addition to the postings\\nlist, its document frequency and house keeping information. We recommend\\nHeinz and Zobel (2003) and Zobel and Moffat (2006) as up-do-date, in-depth\\ntreatments of index construction. Other algorithms with good scaling prop-\\nerties with respect to vocabulary size require several passes through the data,\\ne.g., FAST-INV (Fox and Lee 1991, Harman et al. 1992).\\nThe MapReduce architecture was introduced by Dean and Ghemawat (2004).\\nAn open source implementation of MapReduce is available at http://lucene.apache.org/hadoop/.\\nRibeiro-Neto et al. (1999) and Melnik et al. (2001) describe other approaches\\n', 'Online edition (c)\\n2009 Cambridge UP\\n84\\n4\\nIndex construction\\nto distributed indexing. Introductory chapters on distributed IR are (Baeza-\\nYates and Ribeiro-Neto 1999, Chapter 9) and (Grossman and Frieder 2004,\\nChapter 8). See also Callan (2000).\\nLester et al. (2005) and Büttcher and Clarke (2005a) analyze the proper-\\nties of logarithmic merging and compare it with other construction methods.\\nOne of the ﬁrst uses of this method was in Lucene (http://lucene.apache.org).\\nOther dynamic indexing methods are discussed by Büttcher et al. (2006) and\\nLester et al. (2006). The latter paper also discusses the strategy of replacing\\nthe old index by one built from scratch.\\nHeinz et al. (2002) compare data structures for accumulating the vocabu-\\nlary in memory. Büttcher and Clarke (2005b) discuss security models for a\\ncommon inverted index for multiple users. A detailed characterization of the\\nReuters-RCV1 collection can be found in (Lewis et al. 2004). NIST distributes\\nthe collection (see http://trec.nist.gov/data/reuters/reuters.html).\\nGarcia-Molina et al. (1999, Chapter 2) review computer hardware relevant\\nto system design in depth.\\nAn effective indexer for enterprise search needs to be able to communicate\\nefﬁciently with a number of applications that hold text data in corporations,\\nincluding Microsoft Outlook, IBM’s Lotus software, databases like Oracle\\nand MySQL, content management systems like Open Text, and enterprise\\nresource planning software like SAP.\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n85\\n5\\nIndex compression\\nChapter 1 introduced the dictionary and the inverted index as the central\\ndata structures in information retrieval (IR). In this chapter, we employ a\\nnumber of compression techniques for dictionary and inverted index that\\nare essential for efﬁcient IR systems.\\nOne beneﬁt of compression is immediately clear. We need less disk space.\\nAs we will see, compression ratios of 1:4 are easy to achieve, potentially cut-\\nting the cost of storing the index by 75%.\\nThere are two more subtle beneﬁts of compression. The ﬁrst is increased\\nuse of caching. Search systems use some parts of the dictionary and the index\\nmuch more than others. For example, if we cache the postings list of a fre-\\nquently used query term t, then the computations necessary for responding\\nto the one-term query t can be entirely done in memory. With compression,\\nwe can ﬁt a lot more information into main memory. Instead of having to\\nexpend a disk seek when processing a query with t, we instead access its\\npostings list in memory and decompress it. As we will see below, there are\\nsimple and efﬁcient decompression methods, so that the penalty of having to\\ndecompress the postings list is small. As a result, we are able to decrease the\\nresponse time of the IR system substantially. Because memory is a more ex-\\npensive resource than disk space, increased speed owing to caching – rather\\nthan decreased space requirements – is often the prime motivator for com-\\npression.\\nThe second more subtle advantage of compression is faster transfer of data\\nfrom disk to memory. Efﬁcient decompression algorithms run so fast on\\nmodern hardware that the total time of transferring a compressed chunk of\\ndata from disk and then decompressing it is usually less than transferring\\nthe same chunk of data in uncompressed form. For instance, we can reduce\\ninput/output (I/O) time by loading a much smaller compressed postings\\nlist, even when you add on the cost of decompression. So, in most cases,\\nthe retrieval system runs faster on compressed postings lists than on uncom-\\npressed postings lists.\\nIf the main goal of compression is to conserve disk space, then the speed\\n', 'Online edition (c)\\n2009 Cambridge UP\\n86\\n5\\nIndex compression\\nof compression algorithms is of no concern. But for improved cache uti-\\nlization and faster disk-to-memory transfer, decompression speeds must be\\nhigh. The compression algorithms we discuss in this chapter are highly efﬁ-\\ncient and can therefore serve all three purposes of index compression.\\nIn this chapter, we deﬁne a posting as a docID in a postings list. For exam-\\nPOSTING\\nple, the postings list (6; 20, 45, 100), where 6 is the termID of the list’s term,\\ncontains three postings. As discussed in Section 2.4.2 (page 41), postings in\\nmost search systems also contain frequency and position information; but we\\nwill only consider simple docID postings here. See Section 5.4 for references\\non compressing frequencies and positions.\\nThis chapter ﬁrst gives a statistical characterization of the distribution of\\nthe entities we want to compress – terms and postings in large collections\\n(Section 5.1). We then look at compression of the dictionary, using the dictionary-\\nas-a-string method and blocked storage (Section 5.2). Section 5.3 describes\\ntwo techniques for compressing the postings ﬁle, variable byte encoding and\\nγ encoding.\\n5.1\\nStatistical properties of terms in information retrieval\\nAs in the last chapter, we use Reuters-RCV1 as our model collection (see Ta-\\nble 4.2, page 70). We give some term and postings statistics for the collection\\nin Table 5.1. “∆%” indicates the reduction in size from the previous line.\\n“T%” is the cumulative reduction from unﬁltered.\\nThe table shows the number of terms for different levels of preprocessing\\n(column 2). The number of terms is the main factor in determining the size\\nof the dictionary. The number of nonpositional postings (column 3) is an\\nindicator of the expected size of the nonpositional index of the collection.\\nThe expected size of a positional index is related to the number of positions\\nit must encode (column 4).\\nIn general, the statistics in Table 5.1 show that preprocessing affects the size\\nof the dictionary and the number of nonpositional postings greatly. Stem-\\nming and case folding reduce the number of (distinct) terms by 17% each\\nand the number of nonpositional postings by 4% and 3%, respectively. The\\ntreatment of the most frequent words is also important. The rule of 30 states\\nRULE OF 30\\nthat the 30 most common words account for 30% of the tokens in written text\\n(31% in the table). Eliminating the 150 most common words from indexing\\n(as stop words; cf. Section 2.2.2, page 27) cuts 25% to 30% of the nonpositional\\npostings. But, although a stop list of 150 words reduces the number of post-\\nings by a quarter or more, this size reduction does not carry over to the size\\nof the compressed index. As we will see later in this chapter, the postings\\nlists of frequent words require only a few bits per posting after compression.\\nThe deltas in the table are in a range typical of large collections. Note,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.1\\nStatistical properties of terms in information retrieval\\n87\\n◮Table 5.1\\nThe effect of preprocessing on the number of terms, nonpositional post-\\nings, and tokens for Reuters-RCV1. “∆%” indicates the reduction in size from the pre-\\nvious line, except that “30 stop words” and “150 stop words” both use “case folding”\\nas their reference line. “T%” is the cumulative (“total”) reduction from unﬁltered. We\\nperformed stemming with the Porter stemmer (Chapter 2, page 33).\\ntokens (= number of positio\\n(distinct) terms\\nnonpositional postings\\nentries in postings)\\nnumber\\n∆%\\nT%\\nnumber\\n∆%\\nT%\\nnumber\\n∆%\\nT%\\nunﬁltered\\n484,494\\n109,971,179\\n197,879,290\\nno numbers\\n473,723\\n−2\\n−2\\n100,680,242\\n−8\\n−8\\n179,158,204\\n−9\\n−9\\ncase folding\\n391,523\\n−17\\n−19\\n96,969,056\\n−3\\n−12\\n179,158,204\\n−0\\n−9\\n30 stop words\\n391,493\\n−0\\n−19\\n83,390,443\\n−14\\n−24\\n121,857,825\\n−31\\n−38\\n150 stop words\\n391,373\\n−0\\n−19\\n67,001,847\\n−30\\n−39\\n94,516,599\\n−47\\n−52\\nstemming\\n322,383\\n−17\\n−33\\n63,812,300\\n−4\\n−42\\n94,516,599\\n−0\\n−52\\nhowever, that the percentage reductions can be very different for some text\\ncollections. For example, for a collection of web pages with a high proportion\\nof French text, a lemmatizer for French reduces vocabulary size much more\\nthan the Porter stemmer does for an English-only collection because French\\nis a morphologically richer language than English.\\nThe compression techniques we describe in the remainder of this chapter\\nare lossless, that is, all information is preserved. Better compression ratios\\nLOSSLESS\\ncan be achieved with lossy compression, which discards some information.\\nLOSSY COMPRESSION\\nCase folding, stemming, and stop word elimination are forms of lossy com-\\npression. Similarly, the vector space model (Chapter 6) and dimensionality\\nreduction techniques like latent semantic indexing (Chapter 18) create com-\\npact representations from which we cannot fully restore the original collec-\\ntion. Lossy compression makes sense when the “lost” information is unlikely\\never to be used by the search system. For example, web search is character-\\nized by a large number of documents, short queries, and users who only look\\nat the ﬁrst few pages of results. As a consequence, we can discard postings of\\ndocuments that would only be used for hits far down the list. Thus, there are\\nretrieval scenarios where lossy methods can be used for compression without\\nany reduction in effectiveness.\\nBefore introducing techniques for compressing the dictionary, we want to\\nestimate the number of distinct terms M in a collection. It is sometimes said\\nthat languages have a vocabulary of a certain size. The second edition of\\nthe Oxford English Dictionary (OED) deﬁnes more than 600,000 words. But\\nthe vocabulary of most large collections is much larger than the OED. The\\nOED does not include most names of people, locations, products, or scientiﬁc\\n', 'Online edition (c)\\n2009 Cambridge UP\\n88\\n5\\nIndex compression\\n0\\n2\\n4\\n6\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nlog10 T\\nlog10 M\\n◮Figure 5.1\\nHeaps’ law.\\nVocabulary size M as a function of collection size T\\n(number of tokens) for Reuters-RCV1. For these data, the dashed line log10 M =\\n0.49 ∗log10 T + 1.64 is the best least-squares ﬁt. Thus, k = 101.64 ≈44 and b = 0.49.\\nentities like genes. These names need to be included in the inverted index,\\nso our users can search for them.\\n5.1.1\\nHeaps’ law: Estimating the number of terms\\nA better way of getting a handle on M is Heaps’ law, which estimates vocab-\\nHEAPS’ LAW\\nulary size as a function of collection size:\\nM = kTb\\n(5.1)\\nwhere T is the number of tokens in the collection. Typical values for the\\nparameters k and b are: 30 ≤k ≤100 and b ≈0.5. The motivation for\\nHeaps’ law is that the simplest possible relationship between collection size\\nand vocabulary size is linear in log–log space and the assumption of linearity\\nis usually born out in practice as shown in Figure 5.1 for Reuters-RCV1. In\\nthis case, the ﬁt is excellent for T > 105 = 100,000, for the parameter values\\nb = 0.49 and k = 44. For example, for the ﬁrst 1,000,020 tokens Heaps’ law\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.1\\nStatistical properties of terms in information retrieval\\n89\\npredicts 38,323 terms:\\n44 × 1,000,0200.49 ≈38,323.\\nThe actual number is 38,365 terms, very close to the prediction.\\nThe parameter k is quite variable because vocabulary growth depends a\\nlot on the nature of the collection and how it is processed. Case-folding and\\nstemming reduce the growth rate of the vocabulary, whereas including num-\\nbers and spelling errors increase it. Regardless of the values of the param-\\neters for a particular collection, Heaps’ law suggests that (i) the dictionary\\nsize continues to increase with more documents in the collection, rather than\\na maximum vocabulary size being reached, and (ii) the size of the dictionary\\nis quite large for large collections. These two hypotheses have been empir-\\nically shown to be true of large text collections (Section 5.4). So dictionary\\ncompression is important for an effective information retrieval system.\\n5.1.2\\nZipf’s law: Modeling the distribution of terms\\nWe also want to understand how terms are distributed across documents.\\nThis helps us to characterize the properties of the algorithms for compressing\\npostings lists in Section 5.3.\\nA commonly used model of the distribution of terms in a collection is Zipf’s\\nZIPF’S LAW\\nlaw. It states that, if t1 is the most common term in the collection, t2 is the\\nnext most common, and so on, then the collection frequency cfi of the ith\\nmost common term is proportional to 1/i:\\ncfi ∝1\\ni .\\n(5.2)\\nSo if the most frequent term occurs cf1 times, then the second most frequent\\nterm has half as many occurrences, the third most frequent term a third as\\nmany occurrences, and so on. The intuition is that frequency decreases very\\nrapidly with rank. Equation (5.2) is one of the simplest ways of formalizing\\nsuch a rapid decrease and it has been found to be a reasonably good model.\\nEquivalently, we can write Zipf’s law as cfi = cik or as log cfi = log c +\\nk log i where k = −1 and c is a constant to be deﬁned in Section 5.3.2. It\\nis therefore a power law with exponent k = −1. See Chapter 19, page 426,\\nPOWER LAW\\nfor another power law, a law characterizing the distribution of links on web\\npages.\\nThe log–log graph in Figure 5.2 plots the collection frequency of a term as\\na function of its rank for Reuters-RCV1. A line with slope –1, corresponding\\nto the Zipf function log cfi = log c −log i, is also shown. The ﬁt of the data\\nto the law is not particularly good, but good enough to serve as a model for\\nterm distributions in our calculations in Section 5.3.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n90\\n5\\nIndex compression\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nlog10 rank\\n7\\nlog10 cf\\n◮Figure 5.2\\nZipf’s law for Reuters-RCV1.\\nFrequency is plotted as a function of\\nfrequency rank for the terms in the collection. The line is the distribution predicted\\nby Zipf’s law (weighted least-squares ﬁt; intercept is 6.95).\\n?\\nExercise 5.1\\n[⋆]\\nAssuming one machine word per posting, what is the size of the uncompressed (non-\\npositional) index for different tokenizations based on Table 5.1? How do these num-\\nbers compare with Table 5.6?\\n5.2\\nDictionary compression\\nThis section presents a series of dictionary data structures that achieve in-\\ncreasingly higher compression ratios. The dictionary is small compared with\\nthe postings ﬁle as suggested by Table 5.1. So why compress it if it is respon-\\nsible for only a small percentage of the overall space requirements of the IR\\nsystem?\\nOne of the primary factors in determining the response time of an IR sys-\\ntem is the number of disk seeks necessary to process a query. If parts of the\\ndictionary are on disk, then many more disk seeks are necessary in query\\nevaluation. Thus, the main goal of compressing the dictionary is to ﬁt it in\\nmain memory, or at least a large portion of it, to support high query through-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.2\\nDictionary compression\\n91\\nterm\\ndocument\\nfrequency\\npointer\\nto\\npostings list\\na\\n656,265\\n−→\\naachen\\n65\\n−→\\n...\\n...\\n...\\nzulu\\n221\\n−→\\nspace needed:\\n20 bytes\\n4 bytes\\n4 bytes\\n◮Figure 5.3\\nStoring the dictionary as an array of ﬁxed-width entries.\\nput. Although dictionaries of very large collections ﬁt into the memory of a\\nstandard desktop machine, this is not true of many other application scenar-\\nios. For example, an enterprise search server for a large corporation may\\nhave to index a multiterabyte collection with a comparatively large vocab-\\nulary because of the presence of documents in many different languages.\\nWe also want to be able to design search systems for limited hardware such\\nas mobile phones and onboard computers. Other reasons for wanting to\\nconserve memory are fast startup time and having to share resources with\\nother applications. The search system on your PC must get along with the\\nmemory-hogging word processing suite you are using at the same time.\\n5.2.1\\nDictionary as a string\\nThe simplest data structure for the dictionary is to sort the vocabulary lex-\\nicographically and store it in an array of ﬁxed-width entries as shown in\\nFigure 5.3. We allocate 20 bytes for the term itself (because few terms have\\nmore than twenty characters in English), 4 bytes for its document frequency,\\nand 4 bytes for the pointer to its postings list. Four-byte pointers resolve a\\n4 gigabytes (GB) address space. For large collections like the web, we need\\nto allocate more bytes per pointer. We look up terms in the array by binary\\nsearch.\\nFor Reuters-RCV1, we need M × (20 + 4 + 4) = 400,000 × 28 =\\n11.2megabytes (MB) for storing the dictionary in this scheme.\\nUsing ﬁxed-width entries for terms is clearly wasteful. The average length\\nof a term in English is about eight characters (Table 4.2, page 70), so on av-\\nerage we are wasting twelve characters in the ﬁxed-width scheme. Also,\\nwe have no way of storing terms with more than twenty characters like\\nhydrochloroﬂuorocarbons and supercalifragilisticexpialidocious. We can overcome\\nthese shortcomings by storing the dictionary terms as one long string of char-\\nacters, as shown in Figure 5.4. The pointer to the next term is also used to\\ndemarcate the end of the current term. As before, we locate terms in the data\\nstructure by way of binary search in the (now smaller) table. This scheme\\nsaves us 60% compared to ﬁxed-width storage – 12 bytes on average of the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n92\\n5\\nIndex compression\\n. . . s y s t i l e s y z y g e t i c s y z y g i a l s y z y g y s z a i b e l y i t e s z e c i n s z o n o .  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n...\\n4 bytes\\npostings ptr.\\n \\n \\n \\n \\n \\n...\\n4 bytes\\nterm ptr.\\n3 bytes\\n. . .\\n→\\n→\\n→\\n→\\n→\\n◮Figure 5.4\\nDictionary-as-a-string storage. Pointers mark the end of the preceding\\nterm and the beginning of the next. For example, the ﬁrst three terms in this example\\nare systile, syzygetic, and syzygial.\\n20 bytes we allocated for terms before. However, we now also need to store\\nterm pointers. The term pointers resolve 400,000 × 8 = 3.2 × 106 positions,\\nso they need to be log2 3.2 × 106 ≈22 bits or 3 bytes long.\\nIn this new scheme, we need 400,000 × (4 + 4 + 3 + 8) = 7.6 MB for the\\nReuters-RCV1 dictionary: 4 bytes each for frequency and postings pointer, 3\\nbytes for the term pointer, and 8 bytes on average for the term. So we have\\nreduced the space requirements by one third from 11.2 to 7.6 MB.\\n5.2.2\\nBlocked storage\\nWe can further compress the dictionary by grouping terms in the string into\\nblocks of size k and keeping a term pointer only for the ﬁrst term of each\\nblock (Figure 5.5). We store the length of the term in the string as an ad-\\nditional byte at the beginning of the term. We thus eliminate k −1 term\\npointers, but need an additional k bytes for storing the length of each term.\\nFor k = 4, we save (k −1) × 3 = 9 bytes for term pointers, but need an ad-\\nditional k = 4 bytes for term lengths. So the total space requirements for the\\ndictionary of Reuters-RCV1 are reduced by 5 bytes per four-term block, or a\\ntotal of 400,000 × 1/4 × 5 = 0.5 MB, bringing us down to 7.1 MB.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.2\\nDictionary compression\\n93\\n. . . 7 s y s t i l e 9 s y z y g e t i c 8 s y z y g i a l 6 s y z y g y11s z a i b e l y i t e 6 s z e c i n .  .  .\\nfreq.\\n9\\n92\\n5\\n71\\n12\\n. . .\\npostings ptr.\\n \\n \\n \\n \\n \\n. . .\\nterm ptr.\\n. . .\\n→\\n→\\n→\\n→\\n→\\n◮Figure 5.5\\nBlocked storage with four terms per block. The ﬁrst block consists of\\nsystile, syzygetic, syzygial, and syzygy with lengths of seven, nine, eight, and six charac-\\nters, respectively. Each term is preceded by a byte encoding its length that indicates\\nhow many bytes to skip to reach subsequent terms.\\nBy increasing the block size k, we get better compression. However, there\\nis a tradeoff between compression and the speed of term lookup. For the\\neight-term dictionary in Figure 5.6, steps in binary search are shown as dou-\\nble lines and steps in list search as simple lines. We search for terms in the un-\\ncompressed dictionary by binary search (a). In the compressed dictionary, we\\nﬁrst locate the term’s block by binary search and then its position within the\\nlist by linear search through the block (b). Searching the uncompressed dic-\\ntionary in (a) takes on average (0 + 1 + 2 + 3 + 2 + 1 + 2 + 2)/8 ≈1.6 steps,\\nassuming each term is equally likely to come up in a query. For example,\\nﬁnding the two terms, aid and box, takes three and two steps, respectively.\\nWith blocks of size k = 4 in (b), we need (0 + 1 + 2 + 3 + 4 + 1 + 2 + 3)/8 = 2\\nsteps on average, ≈25% more. For example, ﬁnding den takes one binary\\nsearch step and two steps through the block. By increasing k, we can get\\nthe size of the compressed dictionary arbitrarily close to the minimum of\\n400,000 × (4 + 4 + 1 + 8) = 6.8 MB, but term lookup becomes prohibitively\\nslow for large values of k.\\nOne source of redundancy in the dictionary we have not exploited yet is\\nthe fact that consecutive entries in an alphabetically sorted list share common\\npreﬁxes. This observation leads to front coding (Figure 5.7). A common preﬁx\\nFRONT CODING\\n', 'Online edition (c)\\n2009 Cambridge UP\\n94\\n5\\nIndex compression\\n(a)\\naid\\nbox\\nden\\nex\\njob\\nox\\npit\\nwin\\n(b)\\naid\\nbox\\nden\\nex\\njob\\nox\\npit\\nwin\\n◮Figure 5.6\\nSearch of the uncompressed dictionary (a) and a dictionary com-\\npressed by blocking with k = 4 (b).\\nOne block in blocked compression (k = 4) ...\\n8automata8automate9automatic10automation\\n⇓\\n...further compressed with front coding.\\n8automat∗a1⋄e2 ⋄ic3⋄ion\\n◮Figure 5.7\\nFront coding. A sequence of terms with identical preﬁx (“automat”) is\\nencoded by marking the end of the preﬁx with ∗and replacing it with ⋄in subsequent\\nterms. As before, the ﬁrst byte of each entry encodes the number of characters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n95\\n◮Table 5.2\\nDictionary compression for Reuters-RCV1.\\ndata structure\\nsize in MB\\ndictionary, ﬁxed-width\\n11.2\\ndictionary, term pointers into string\\n7.6\\n∼, with blocking, k = 4\\n7.1\\n∼, with blocking & front coding\\n5.9\\nis identiﬁed for a subsequence of the term list and then referred to with a\\nspecial character. In the case of Reuters, front coding saves another 1.2 MB,\\nas we found in an experiment.\\nOther schemes with even greater compression rely on minimal perfect\\nhashing, that is, a hash function that maps M terms onto [1, . . . , M] without\\ncollisions. However, we cannot adapt perfect hashes incrementally because\\neach new term causes a collision and therefore requires the creation of a new\\nperfect hash function. Therefore, they cannot be used in a dynamic environ-\\nment.\\nEven with the best compression scheme, it may not be feasible to store\\nthe entire dictionary in main memory for very large text collections and for\\nhardware with limited memory. If we have to partition the dictionary onto\\npages that are stored on disk, then we can index the ﬁrst term of each page\\nusing a B-tree. For processing most queries, the search system has to go to\\ndisk anyway to fetch the postings. One additional seek for retrieving the\\nterm’s dictionary page from disk is a signiﬁcant, but tolerable increase in the\\ntime it takes to process a query.\\nTable 5.2 summarizes the compression achieved by the four dictionary\\ndata structures.\\n?\\nExercise 5.2\\nEstimate the space usage of the Reuters-RCV1 dictionary with blocks of size k = 8\\nand k = 16 in blocked dictionary storage.\\nExercise 5.3\\nEstimate the time needed for term lookup in the compressed dictionary of Reuters-\\nRCV1 with block sizes of k = 4 (Figure 5.6, b), k = 8, and k = 16. What is the\\nslowdown compared with k = 1 (Figure 5.6, a)?\\n5.3\\nPostings ﬁle compression\\nRecall from Table 4.2 (page 70) that Reuters-RCV1 has 800,000 documents,\\n200 tokens per document, six characters per token, and 100,000,000 post-\\nings where we deﬁne a posting in this chapter as a docID in a postings\\nlist, that is, excluding frequency and position information. These numbers\\n', 'Online edition (c)\\n2009 Cambridge UP\\n96\\n5\\nIndex compression\\n◮Table 5.3\\nEncoding gaps instead of document IDs. For example, we store gaps\\n107, 5, 43, . . . , instead of docIDs 283154, 283159, 283202, . . . for computer. The ﬁrst\\ndocID is left unchanged (only shown for arachnocentric).\\nencoding\\npostings list\\nthe\\ndocIDs\\n...\\n283042\\n283043\\n283044\\n283045\\ngaps\\n1\\n1\\n1\\ncomputer\\ndocIDs\\n...\\n283047\\n283154\\n283159\\n283202\\ngaps\\n107\\n5\\n43\\narachnocentric\\ndocIDs\\n252000\\n500100\\ngaps\\n252000\\n248100\\ncorrespond to line 3 (“case folding”) in Table 5.1. Document identiﬁers are\\nlog2 800,000 ≈20 bits long. Thus, the size of the collection is about 800,000 ×\\n200 × 6 bytes = 960 MB and the size of the uncompressed postings ﬁle is\\n100,000,000 × 20/8 = 250 MB.\\nTo devise a more efﬁcient representation of the postings ﬁle, one that uses\\nfewer than 20 bits per document, we observe that the postings for frequent\\nterms are close together. Imagine going through the documents of a collec-\\ntion one by one and looking for a frequent term like computer. We will ﬁnd\\na document containing computer, then we skip a few documents that do not\\ncontain it, then there is again a document with the term and so on (see Ta-\\nble 5.3). The key idea is that the gaps between postings are short, requiring a\\nlot less space than 20 bits to store. In fact, gaps for the most frequent terms\\nsuch as the and for are mostly equal to 1. But the gaps for a rare term that\\noccurs only once or twice in a collection (e.g., arachnocentric in Table 5.3) have\\nthe same order of magnitude as the docIDs and need 20 bits. For an econom-\\nical representation of this distribution of gaps, we need a variable encoding\\nmethod that uses fewer bits for short gaps.\\nTo encode small numbers in less space than large numbers, we look at two\\ntypes of methods: bytewise compression and bitwise compression. As the\\nnames suggest, these methods attempt to encode gaps with the minimum\\nnumber of bytes and bits, respectively.\\n5.3.1\\nVariable byte codes\\nVariable byte (VB) encoding uses an integral number of bytes to encode a gap.\\nVARIABLE BYTE\\nENCODING\\nThe last 7 bits of a byte are “payload” and encode part of the gap. The ﬁrst\\nbit of the byte is a continuation bit.It is set to 1 for the last byte of the encoded\\nCONTINUATION BIT\\ngap and to 0 otherwise. To decode a variable byte code, we read a sequence\\nof bytes with continuation bit 0 terminated by a byte with continuation bit 1.\\nWe then extract and concatenate the 7-bit parts. Figure 5.8 gives pseudocode\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n97\\nVBENCODENUMBER(n)\\n1\\nbytes ←⟨⟩\\n2\\nwhile true\\n3\\ndo PREPEND(bytes, n mod 128)\\n4\\nif n < 128\\n5\\nthen BREAK\\n6\\nn ←n div 128\\n7\\nbytes[LENGTH(bytes)] += 128\\n8\\nreturn bytes\\nVBENCODE(numbers)\\n1\\nbytestream ←⟨⟩\\n2\\nfor each n ∈numbers\\n3\\ndo bytes ←VBENCODENUMBER(n)\\n4\\nbytestream ←EXTEND(bytestream, bytes)\\n5\\nreturn bytestream\\nVBDECODE(bytestream)\\n1\\nnumbers ←⟨⟩\\n2\\nn ←0\\n3\\nfor i ←1 to LENGTH(bytestream)\\n4\\ndo if bytestream[i] < 128\\n5\\nthen n ←128 × n + bytestream[i]\\n6\\nelse n ←128 × n + (bytestream[i] −128)\\n7\\nAPPEND(numbers, n)\\n8\\nn ←0\\n9\\nreturn numbers\\n◮Figure 5.8\\nVB encoding and decoding.\\nThe functions div and mod compute\\ninteger division and remainder after integer division, respectively. PREPEND adds an\\nelement to the beginning of a list, for example, PREPEND(⟨1,2⟩, 3) = ⟨3, 1, 2⟩. EXTEND\\nextends a list, for example, EXTEND(⟨1,2⟩, ⟨3, 4⟩) = ⟨1, 2, 3, 4⟩.\\n◮Table 5.4\\nVB encoding.\\nGaps are encoded using an integral number of bytes.\\nThe ﬁrst bit, the continuation bit, of each byte indicates whether the code ends with\\nthis byte (1) or not (0).\\ndocIDs\\n824\\n829\\n215406\\ngaps\\n5\\n214577\\nVB code\\n00000110 10111000\\n10000101\\n00001101 00001100 10110001\\n', 'Online edition (c)\\n2009 Cambridge UP\\n98\\n5\\nIndex compression\\n◮Table 5.5\\nSome examples of unary and γ codes. Unary codes are only shown for\\nthe smaller numbers. Commas in γ codes are for readability only and are not part of\\nthe actual codes.\\nnumber\\nunary code\\nlength\\noffset\\nγ code\\n0\\n0\\n1\\n10\\n0\\n0\\n2\\n110\\n10\\n0\\n10,0\\n3\\n1110\\n10\\n1\\n10,1\\n4\\n11110\\n110\\n00\\n110,00\\n9\\n1111111110\\n1110\\n001\\n1110,001\\n13\\n1110\\n101\\n1110,101\\n24\\n11110\\n1000\\n11110,1000\\n511\\n111111110\\n11111111\\n111111110,11111111\\n1025\\n11111111110\\n0000000001\\n11111111110,0000000001\\nfor VB encoding and decoding and Table 5.4 an example of a VB-encoded\\npostings list. 1\\nWith VB compression, the size of the compressed index for Reuters-RCV1\\nis 116 MB as we veriﬁed in an experiment. This is a more than 50% reduction\\nof the size of the uncompressed index (see Table 5.6).\\nThe idea of VB encoding can also be applied to larger or smaller units than\\nbytes: 32-bit words, 16-bit words, and 4-bit words or nibbles. Larger words\\nNIBBLE\\nfurther decrease the amount of bit manipulation necessary at the cost of less\\neffective (or no) compression. Word sizes smaller than bytes get even better\\ncompression ratios at the cost of more bit manipulation. In general, bytes\\noffer a good compromise between compression ratio and speed of decom-\\npression.\\nFor most IR systems variable byte codes offer an excellent tradeoff between\\ntime and space. They are also simple to implement – most of the alternatives\\nreferred to in Section 5.4 are more complex. But if disk space is a scarce\\nresource, we can achieve better compression ratios by using bit-level encod-\\nings, in particular two closely related encodings: γ codes, which we will turn\\nto next, and δ codes (Exercise 5.9).\\n$\\n5.3.2\\nγ codes\\nVB codes use an adaptive number of bytes depending on the size of the gap.\\nBit-level codes adapt the length of the code on the ﬁner grained bit level. The\\n1. Note that the origin is 0 in the table. Because we never need to encode a docID or a gap of\\n0, in practice the origin is usually 1, so that 10000000 encodes 1, 10000101 encodes 6 (not 5 as in\\nthe table), and so on.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n99\\nsimplest bit-level code is unary code. The unary code of n is a string of n 1s\\nUNARY CODE\\nfollowed by a 0 (see the ﬁrst two columns of Table 5.5). Obviously, this is not\\na very efﬁcient code, but it will come in handy in a moment.\\nHow efﬁcient can a code be in principle? Assuming the 2n gaps G with\\n1 ≤G ≤2n are all equally likely, the optimal encoding uses n bits for each\\nG. So some gaps (G = 2n in this case) cannot be encoded with fewer than\\nlog2 G bits. Our goal is to get as close to this lower bound as possible.\\nA method that is within a factor of optimal is γ encoding. γ codes im-\\nγ ENCODING\\nplement variable-length encoding by splitting the representation of a gap G\\ninto a pair of length and offset. Offset is G in binary, but with the leading 1\\nremoved.2 For example, for 13 (binary 1101) offset is 101. Length encodes the\\nlength of offset in unary code. For 13, the length of offset is 3 bits, which is 1110\\nin unary. The γ code of 13 is therefore 1110101, the concatenation of length\\n1110 and offset 101. The right hand column of Table 5.5 gives additional\\nexamples of γ codes.\\nA γ code is decoded by ﬁrst reading the unary code up to the 0 that ter-\\nminates it, for example, the four bits 1110 when decoding 1110101. Now we\\nknow how long the offset is: 3 bits. The offset 101 can then be read correctly\\nand the 1 that was chopped off in encoding is prepended: 101 →1101 = 13.\\nThe length of offset is ⌊log2 G⌋bits and the length of length is ⌊log2 G⌋+ 1\\nbits, so the length of the entire code is 2 × ⌊log2 G⌋+ 1 bits. γ codes are\\nalways of odd length and they are within a factor of 2 of what we claimed\\nto be the optimal encoding length log2 G. We derived this optimum from\\nthe assumption that the 2n gaps between 1 and 2n are equiprobable. But this\\nneed not be the case. In general, we do not know the probability distribution\\nover gaps a priori.\\nThe characteristic of a discrete probability distribution3 P that determines\\nits coding properties (including whether a code is optimal) is its entropy H(P),\\nENTROPY\\nwhich is deﬁned as follows:\\nH(P) = −∑\\nx∈X\\nP(x) log2 P(x)\\nwhere X is the set of all possible numbers we need to be able to encode\\n(and therefore ∑x∈X P(x) = 1.0). Entropy is a measure of uncertainty as\\nshown in Figure 5.9 for a probability distribution P over two possible out-\\ncomes, namely, X = {x1, x2}. Entropy is maximized (H(P) = 1) for P(x1) =\\nP(x2) = 0.5 when uncertainty about which xi will appear next is largest; and\\n2. We assume here that G has no leading 0s. If there are any, they are removed before deleting\\nthe leading 1.\\n3. Readers who want to review basic concepts of probability theory may want to consult Rice\\n(2006) or Ross (2006). Note that we are interested in probability distributions over integers (gaps,\\nfrequencies, etc.), but that the coding properties of a probability distribution are independent of\\nwhether the outcomes are integers or something else.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n100\\n5\\nIndex compression\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nP(x1)\\nH(P)\\n◮Figure 5.9\\nEntropy H(P) as a function of P(x1) for a sample space with two\\noutcomes x1 and x2.\\nminimized (H(P) = 0) for P(x1) = 1, P(x2) = 0 and for P(x1) = 0, P(x2) = 1\\nwhen there is absolute certainty.\\nIt can be shown that the lower bound for the expected length E(L) of a\\ncode L is H(P) if certain conditions hold (see the references). It can further\\nbe shown that for 1 < H(P) < ∞, γ encoding is within a factor of 3 of this\\noptimal encoding, approaching 2 for large H(P):\\nE(Lγ)\\nH(P) ≤2 +\\n1\\nH(P) ≤3.\\nWhat is remarkable about this result is that it holds for any probability distri-\\nbution P. So without knowing anything about the properties of the distribu-\\ntion of gaps, we can apply γ codes and be certain that they are within a factor\\nof ≈2 of the optimal code for distributions of large entropy. A code like γ\\ncode with the property of being within a factor of optimal for an arbitrary\\ndistribution P is called universal.\\nUNIVERSAL CODE\\nIn addition to universality, γ codes have two other properties that are use-\\nful for index compression. First, they are preﬁx free, namely, no γ code is the\\nPREFIX FREE\\npreﬁx of another. This means that there is always a unique decoding of a\\nsequence of γ codes – and we do not need delimiters between them, which\\nwould decrease the efﬁciency of the code. The second property is that γ\\ncodes are parameter free. For many other efﬁcient codes, we have to ﬁt the\\nPARAMETER FREE\\nparameters of a model (e.g., the binomial distribution) to the distribution\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n101\\nof gaps in the index. This complicates the implementation of compression\\nand decompression. For instance, the parameters need to be stored and re-\\ntrieved. And in dynamic indexing, the distribution of gaps can change, so\\nthat the original parameters are no longer appropriate. These problems are\\navoided with a parameter-free code.\\nHow much compression of the inverted index do γ codes achieve? To\\nanswer this question we use Zipf’s law, the term distribution model intro-\\nduced in Section 5.1.2. According to Zipf’s law, the collection frequency cfi\\nis proportional to the inverse of the rank i, that is, there is a constant c′ such\\nthat:\\ncfi = c′\\ni .\\n(5.3)\\nWe can choose a different constant c such that the fractions c/i are relative\\nfrequencies and sum to 1 (that is, c/i = cfi/T):\\n1 =\\nM\\n∑\\ni=1\\nc\\ni = c\\nM\\n∑\\ni=1\\n1\\ni\\n=\\nc HM\\n(5.4)\\nc =\\n1\\nHM\\n(5.5)\\nwhere M is the number of distinct terms and HM is the Mth harmonic num-\\nber. 4 Reuters-RCV1 has M = 400,000 distinct terms and HM ≈ln M, so we\\nhave\\nc =\\n1\\nHM\\n≈\\n1\\nln M =\\n1\\nln 400,000 ≈1\\n13.\\nThus the ith term has a relative frequency of roughly 1/(13i), and the ex-\\npected average number of occurrences of term i in a document of length L\\nis:\\nLc\\ni ≈200 × 1\\n13\\ni\\n≈15\\ni\\nwhere we interpret the relative frequency as a term occurrence probability.\\nRecall that 200 is the average number of tokens per document in Reuters-\\nRCV1 (Table 4.2).\\nNow we have derived term statistics that characterize the distribution of\\nterms in the collection and, by extension, the distribution of gaps in the post-\\nings lists. From these statistics, we can calculate the space requirements for\\nan inverted index compressed with γ encoding. We ﬁrst stratify the vocab-\\nulary into blocks of size Lc = 15. On average, term i occurs 15/i times per\\n4. Note that, unfortunately, the conventional symbol for both entropy and harmonic number is\\nH. Context should make clear which is meant in this chapter.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n102\\n5\\nIndex compression\\nN documents\\nLc most\\nfrequent\\nN gaps of 1 each\\nterms\\nLc next most\\nfrequent\\nN/2 gaps of 2 each\\nterms\\nLc next most\\nfrequent\\nN/3 gaps of 3 each\\nterms\\n...\\n...\\n◮Figure 5.10\\nStratiﬁcation of terms for estimating the size of a γ encoded inverted\\nindex.\\ndocument. So the average number of occurrences f per document is 1 ≤f for\\nterms in the ﬁrst block, corresponding to a total number of N gaps per term.\\nThe average is 1\\n2 ≤f < 1 for terms in the second block, corresponding to\\nN/2 gaps per term, and 1\\n3 ≤f < 1\\n2 for terms in the third block, correspond-\\ning to N/3 gaps per term, and so on. (We take the lower bound because it\\nsimpliﬁes subsequent calculations. As we will see, the ﬁnal estimate is too\\npessimistic, even with this assumption.) We will make the somewhat unre-\\nalistic assumption that all gaps for a given term have the same size as shown\\nin Figure 5.10. Assuming such a uniform distribution of gaps, we then have\\ngaps of size 1 in block 1, gaps of size 2 in block 2, and so on.\\nEncoding the N/j gaps of size j with γ codes, the number of bits needed\\nfor the postings list of a term in the jth block (corresponding to one row in\\nthe ﬁgure) is:\\nbits-per-row\\n=\\nN\\nj × (2 × ⌊log2 j⌋+ 1)\\n≈\\n2N log2 j\\nj\\n.\\nTo encode the entire block, we need (Lc) · (2N log2 j)/j bits. There are M/(Lc)\\nblocks, so the postings ﬁle as a whole will take up:\\nM\\nLc\\n∑\\nj=1\\n2NLc log2 j\\nj\\n.\\n(5.6)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.3\\nPostings ﬁle compression\\n103\\n◮Table 5.6\\nIndex and dictionary compression for Reuters-RCV1. The compression\\nratio depends on the proportion of actual text in the collection. Reuters-RCV1 con-\\ntains a large amount of XML markup. Using the two best compression schemes, γ\\nencoding and blocking with front coding, the ratio compressed index to collection\\nsize is therefore especially small for Reuters-RCV1: (101 + 5.9)/3600 ≈0.03.\\ndata structure\\nsize in MB\\ndictionary, ﬁxed-width\\n11.2\\ndictionary, term pointers into string\\n7.6\\n∼, with blocking, k = 4\\n7.1\\n∼, with blocking & front coding\\n5.9\\ncollection (text, xml markup etc)\\n3600.0\\ncollection (text)\\n960.0\\nterm incidence matrix\\n40,000.0\\npostings, uncompressed (32-bit words)\\n400.0\\npostings, uncompressed (20 bits)\\n250.0\\npostings, variable byte encoded\\n116.0\\npostings, γ encoded\\n101.0\\nFor Reuters-RCV1, M\\nLc ≈400,000/15 ≈27,000 and\\n27,000\\n∑\\nj=1\\n2 × 106 × 15 log2 j\\nj\\n≈224 MB.\\n(5.7)\\nSo the postings ﬁle of the compressed inverted index for our 960 MB collec-\\ntion has a size of 224 MB, one fourth the size of the original collection.\\nWhen we run γ compression on Reuters-RCV1, the actual size of the com-\\npressed index is even lower: 101 MB, a bit more than one tenth of the size of\\nthe collection. The reason for the discrepancy between predicted and actual\\nvalue is that (i) Zipf’s law is not a very good approximation of the actual dis-\\ntribution of term frequencies for Reuters-RCV1 and (ii) gaps are not uniform.\\nThe Zipf model predicts an index size of 251 MB for the unrounded numbers\\nfrom Table 4.2. If term frequencies are generated from the Zipf model and\\na compressed index is created for these artiﬁcial terms, then the compressed\\nsize is 254 MB. So to the extent that the assumptions about the distribution\\nof term frequencies are accurate, the predictions of the model are correct.\\nTable 5.6 summarizes the compression techniques covered in this chapter.\\nThe term incidence matrix (Figure 1.1, page 4) for Reuters-RCV1 has size\\n400,000 × 800,000 = 40 × 8 × 109 bits or 40 GB.\\nγ codes achieve great compression ratios – about 15% better than vari-\\nable byte codes for Reuters-RCV1. But they are expensive to decode. This is\\nbecause many bit-level operations – shifts and masks – are necessary to de-\\ncode a sequence of γ codes as the boundaries between codes will usually be\\n', 'Online edition (c)\\n2009 Cambridge UP\\n104\\n5\\nIndex compression\\nsomewhere in the middle of a machine word. As a result, query processing is\\nmore expensive for γ codes than for variable byte codes. Whether we choose\\nvariable byte or γ encoding depends on the characteristics of an application,\\nfor example, on the relative weights we give to conserving disk space versus\\nmaximizing query response time.\\nThe compression ratio for the index in Table 5.6 is about 25%: 400 MB (un-\\ncompressed, each posting stored as a 32-bit word) versus 101 MB (γ) and 116\\nMB (VB). This shows that both γ and VB codes meet the objectives we stated\\nin the beginning of the chapter. Index compression substantially improves\\ntime and space efﬁciency of indexes by reducing the amount of disk space\\nneeded, increasing the amount of information that can be kept in the cache,\\nand speeding up data transfers from disk to memory.\\n?\\nExercise 5.4\\n[⋆]\\nCompute variable byte codes for the numbers in Tables 5.3 and 5.5.\\nExercise 5.5\\n[⋆]\\nCompute variable byte and γ codes for the postings list ⟨777, 17743, 294068, 31251336⟩.\\nUse gaps instead of docIDs where possible. Write binary codes in 8-bit blocks.\\nExercise 5.6\\nConsider the postings list ⟨4, 10, 11, 12, 15, 62, 63, 265, 268, 270, 400⟩with a correspond-\\ning list of gaps ⟨4, 6, 1, 1, 3, 47, 1, 202, 3, 2, 130⟩. Assume that the length of the postings\\nlist is stored separately, so the system knows when a postings list is complete. Us-\\ning variable byte encoding: (i) What is the largest gap you can encode in 1 byte? (ii)\\nWhat is the largest gap you can encode in 2 bytes? (iii) How many bytes will the\\nabove postings list require under this encoding? (Count only space for encoding the\\nsequence of numbers.)\\nExercise 5.7\\nA little trick is to notice that a gap cannot be of length 0 and that the stuff left to encode\\nafter shifting cannot be 0. Based on these observations: (i) Suggest a modiﬁcation to\\nvariable byte encoding that allows you to encode slightly larger gaps in the same\\namount of space. (ii) What is the largest gap you can encode in 1 byte? (iii) What\\nis the largest gap you can encode in 2 bytes? (iv) How many bytes will the postings\\nlist in Exercise 5.6 require under this encoding? (Count only space for encoding the\\nsequence of numbers.)\\nExercise 5.8\\n[⋆]\\nFrom the following sequence of γ-coded gaps, reconstruct ﬁrst the gap sequence and\\nthen the postings sequence: 1110001110101011111101101111011.\\nExercise 5.9\\nγ codes are relatively inefﬁcient for large numbers (e.g., 1025 in Table 5.5) as they\\nencode the length of the offset in inefﬁcient unary code. δ codes differ from γ codes\\nδ CODES\\nin that they encode the ﬁrst part of the code (length) in γ code instead of unary code.\\nThe encoding of offset is the same. For example, the δ code of 7 is 10,0,11 (again, we\\nadd commas for readability). 10,0 is the γ code for length (2 in this case) and the\\nencoding of offset (11) is unchanged. (i) Compute the δ codes for the other numbers\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.4\\nReferences and further reading\\n105\\n◮Table 5.7\\nTwo gap sequences to be merged in blocked sort-based indexing\\nγ encoded gap sequence of run 1\\n1110110111111001011111111110100011111001\\nγ encoded gap sequence of run 2\\n11111010000111111000100011111110010000011111010101\\nin Table 5.5. For what range of numbers is the δ code shorter than the γ code? (ii) γ\\ncode beats variable byte code in Table 5.6 because the index contains stop words and\\nthus many small gaps. Show that variable byte code is more compact if larger gaps\\ndominate. (iii) Compare the compression ratios of δ code and variable byte code for\\na distribution of gaps dominated by large gaps.\\nExercise 5.10\\nGo through the above calculation of index size and explicitly state all the approxima-\\ntions that were made to arrive at Equation (5.6).\\nExercise 5.11\\nFor a collection of your choosing, determine the number of documents and terms and\\nthe average length of a document. (i) How large is the inverted index predicted to be\\nby Equation (5.6)? (ii) Implement an indexer that creates a γ-compressed inverted\\nindex for the collection. How large is the actual index? (iii) Implement an indexer\\nthat uses variable byte encoding. How large is the variable byte encoded index?\\nExercise 5.12\\nTo be able to hold as many postings as possible in main memory, it is a good idea to\\ncompress intermediate index ﬁles during index construction. (i) This makes merging\\nruns in blocked sort-based indexing more complicated. As an example, work out the\\nγ-encoded merged sequence of the gaps in Table 5.7. (ii) Index construction is more\\nspace efﬁcient when using compression. Would you also expect it to be faster?\\nExercise 5.13\\n(i) Show that the size of the vocabulary is ﬁnite according to Zipf’s law and inﬁnite\\naccording to Heaps’ law. (ii) Can we derive Heaps’ law from Zipf’s law?\\n5.4\\nReferences and further reading\\nHeaps’ law was discovered by Heaps (1978). See also Baeza-Yates and Ribeiro-\\nNeto (1999). A detailed study of vocabulary growth in large collections is\\n(Williams and Zobel 2005). Zipf’s law is due to Zipf (1949). Witten and Bell\\n(1990) investigate the quality of the ﬁt obtained by the law. Other term distri-\\nbution models, including K mixture and two-poisson model, are discussed\\nby Manning and Schütze (1999, Chapter 15). Carmel et al. (2001), Büttcher\\nand Clarke (2006), Blanco and Barreiro (2007), and Ntoulas and Cho (2007)\\nshow that lossy compression can achieve good compression with no or no\\nsigniﬁcant decrease in retrieval effectiveness.\\nDictionary compression is covered in detail by Witten et al. (1999, Chap-\\nter 4), which is recommended as additional reading.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n106\\n5\\nIndex compression\\nSubsection 5.3.1 is based on (Scholer et al. 2002). The authors ﬁnd that\\nvariable byte codes process queries two times faster than either bit-level\\ncompressed indexes or uncompressed indexes with a 30% penalty in com-\\npression ratio compared with the best bit-level compression method. They\\nalso show that compressed indexes can be superior to uncompressed indexes\\nnot only in disk usage, but also in query processing speed. Compared with\\nVB codes, “variable nibble” codes showed 5% to 10% better compression\\nand up to one third worse effectiveness in one experiment (Anh and Moffat\\n2005). Trotman (2003) also recommends using VB codes unless disk space is\\nat a premium. In recent work, Anh and Moffat (2005; 2006a) and Zukowski\\net al. (2006) have constructed word-aligned binary codes that are both faster\\nin decompression and at least as efﬁcient as VB codes. Zhang et al. (2007) in-\\nvestigate the increased effectiveness of caching when a number of different\\ncompression techniques for postings lists are used on modern hardware.\\nδ codes (Exercise 5.9) and γ codes were introduced by Elias (1975), who\\nproved that both codes are universal. In addition, δ codes are asymptotically\\noptimal for H(P) →∞. δ codes perform better than γ codes if large num-\\nbers (greater than 15) dominate. A good introduction to information theory,\\nincluding the concept of entropy, is (Cover and Thomas 1991). While Elias\\ncodes are only asymptotically optimal, arithmetic codes (Witten et al. 1999,\\nSection 2.4) can be constructed to be arbitrarily close to the optimum H(P)\\nfor any P.\\nSeveral additional index compression techniques are covered by Witten et\\nal. (1999; Sections 3.3 and 3.4 and Chapter 5). They recommend using param-\\nPARAMETERIZED CODE\\neterized codes for index compression, codes that explicitly model the probabil-\\nity distribution of gaps for each term. For example, they show that Golomb\\nGOLOMB CODES\\ncodes achieve better compression ratios than γ codes for large collections.\\nMoffat and Zobel (1992) compare several parameterized methods, including\\nLLRUN (Fraenkel and Klein 1985).\\nThe distribution of gaps in a postings list depends on the assignment of\\ndocIDs to documents. A number of researchers have looked into assign-\\ning docIDs in a way that is conducive to the efﬁcient compression of gap\\nsequences (Moffat and Stuiver 1996; Blandford and Blelloch 2002; Silvestri\\net al. 2004; Blanco and Barreiro 2006; Silvestri 2007). These techniques assign\\ndocIDs in a small range to documents in a cluster where a cluster can consist\\nof all documents in a given time period, on a particular web site, or sharing\\nanother property. As a result, when a sequence of documents from a clus-\\nter occurs in a postings list, their gaps are small and can be more effectively\\ncompressed.\\nDifferent considerations apply to the compression of term frequencies and\\nword positions than to the compression of docIDs in postings lists. See Scho-\\nler et al. (2002) and Zobel and Moffat (2006). Zobel and Moffat (2006) is\\nrecommended in general as an in-depth and up-to-date tutorial on inverted\\n', 'Online edition (c)\\n2009 Cambridge UP\\n5.4\\nReferences and further reading\\n107\\nindexes, including index compression.\\nThis chapter only looks at index compression for Boolean retrieval. For\\nranked retrieval (Chapter 6), it is advantageous to order postings according\\nto term frequency instead of docID. During query processing, the scanning\\nof many postings lists can then be terminated early because smaller weights\\ndo not change the ranking of the highest ranked k documents found so far. It\\nis not a good idea to precompute and store weights in the index (as opposed\\nto frequencies) because they cannot be compressed as well as integers (see\\nSection 7.1.5, page 140).\\nDocument compression can also be important in an efﬁcient information re-\\ntrieval system. de Moura et al. (2000) and Brisaboa et al. (2007) describe\\ncompression schemes that allow direct searching of terms and phrases in the\\ncompressed text, which is infeasible with standard text compression utilities\\nlike gzip and compress.\\n?\\nExercise 5.14\\n[⋆]\\nWe have deﬁned unary codes as being “10”: sequences of 1s terminated by a 0. In-\\nterchanging the roles of 0s and 1s yields an equivalent “01” unary code. When this\\n01 unary code is used, the construction of a γ code can be stated as follows: (1) Write\\nG down in binary using b = ⌊log2 j⌋+ 1 bits. (2) Prepend (b −1) 0s. (i) Encode the\\nnumbers in Table 5.5 in this alternative γ code. (ii) Show that this method produces\\na well-deﬁned alternative γ code in the sense that it has the same length and can be\\nuniquely decoded.\\nExercise 5.15\\n[⋆⋆⋆]\\nUnary code is not a universal code in the sense deﬁned above. However, there exists\\na distribution over gaps for which unary code is optimal. Which distribution is this?\\nExercise 5.16\\nGive some examples of terms that violate the assumption that gaps all have the same\\nsize (which we made when estimating the space requirements of a γ-encoded index).\\nWhat are general characteristics of these terms?\\nExercise 5.17\\nConsider a term whose postings list has size n, say, n = 10,000. Compare the size of\\nthe γ-compressed gap-encoded postings list if the distribution of the term is uniform\\n(i.e., all gaps have the same size) versus its size when the distribution is not uniform.\\nWhich compressed postings list is smaller?\\nExercise 5.18\\nWork out the sum in Equation (5.7) and show it adds up to about 251 MB. Use the\\nnumbers in Table 4.2, but do not round Lc, c, and the number of vocabulary blocks.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n109\\n6\\nScoring, term weighting and the\\nvector space model\\nThus far we have dealt with indexes that support Boolean queries: a docu-\\nment either matches or does not match a query. In the case of large document\\ncollections, the resulting number of matching documents can far exceed the\\nnumber a human user could possibly sift through. Accordingly, it is essen-\\ntial for a search engine to rank-order the documents matching a query. To do\\nthis, the search engine computes, for each matching document, a score with\\nrespect to the query at hand. In this chapter we initiate the study of assigning\\na score to a (query, document) pair. This chapter consists of three main ideas.\\n1. We introduce parametric and zone indexes in Section 6.1, which serve\\ntwo purposes. First, they allow us to index and retrieve documents by\\nmetadata such as the language in which a document is written. Second,\\nthey give us a simple means for scoring (and thereby ranking) documents\\nin response to a query.\\n2. Next, in Section 6.2 we develop the idea of weighting the importance of a\\nterm in a document, based on the statistics of occurrence of the term.\\n3. In Section 6.3 we show that by viewing each document as a vector of such\\nweights, we can compute a score between a query and each document.\\nThis view is known as vector space scoring.\\nSection 6.4 develops several variants of term-weighting for the vector space\\nmodel. Chapter 7 develops computational aspects of vector space scoring,\\nand related topics.\\nAs we develop these ideas, the notion of a query will assume multiple\\nnuances. In Section 6.1 we consider queries in which speciﬁc query terms\\noccur in speciﬁed regions of a matching document. Beginning Section 6.2 we\\nwill in fact relax the requirement of matching speciﬁc regions of a document;\\ninstead, we will look at so-called free text queries that simply consist of query\\nterms with no speciﬁcation on their relative order, importance or where in a\\ndocument they should be found. The bulk of our study of scoring will be in\\nthis latter notion of a query being such a set of terms.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n110\\n6\\nScoring, term weighting and the vector space model\\n6.1\\nParametric and zone indexes\\nWe have thus far viewed a document as a sequence of terms. In fact, most\\ndocuments have additional structure. Digital documents generally encode,\\nin machine-recognizable form, certain metadata associated with each docu-\\nMETADATA\\nment. By metadata, we mean speciﬁc forms of data about a document, such\\nas its author(s), title and date of publication. This metadata would generally\\ninclude ﬁelds such as the date of creation and the format of the document, as\\nFIELD\\nwell the author and possibly the title of the document. The possible values\\nof a ﬁeld should be thought of as ﬁnite – for instance, the set of all dates of\\nauthorship.\\nConsider queries of the form “ﬁnd documents authored by William Shake-\\nspeare in 1601, containing the phrase alas poor Yorick”. Query processing then\\nconsists as usual of postings intersections, except that we may merge post-\\nings from standard inverted as well as parametric indexes. There is one para-\\nPARAMETRIC INDEX\\nmetric index for each ﬁeld (say, date of creation); it allows us to select only\\nthe documents matching a date speciﬁed in the query. Figure 6.1 illustrates\\nthe user’s view of such a parametric search. Some of the ﬁelds may assume\\nordered values, such as dates; in the example query above, the year 1601 is\\none such ﬁeld value. The search engine may support querying ranges on\\nsuch ordered values; to this end, a structure like a B-tree may be used for the\\nﬁeld’s dictionary.\\nZones are similar to ﬁelds, except the contents of a zone can be arbitrary\\nZONE\\nfree text. Whereas a ﬁeld may take on a relatively small set of values, a zone\\ncan be thought of as an arbitrary, unbounded amount of text. For instance,\\ndocument titles and abstracts are generally treated as zones. We may build a\\nseparate inverted index for each zone of a document, to support queries such\\nas “ﬁnd documents with merchant in the title and william in the author list and\\nthe phrase gentle rain in the body”. This has the effect of building an index\\nthat looks like Figure 6.2. Whereas the dictionary for a parametric index\\ncomes from a ﬁxed vocabulary (the set of languages, or the set of dates), the\\ndictionary for a zone index must structure whatever vocabulary stems from\\nthe text of that zone.\\nIn fact, we can reduce the size of the dictionary by encoding the zone in\\nwhich a term occurs in the postings. In Figure 6.3 for instance, we show how\\noccurrences of william in the title and author zones of various documents are\\nencoded. Such an encoding is useful when the size of the dictionary is a\\nconcern (because we require the dictionary to ﬁt in main memory). But there\\nis another important reason why the encoding of Figure 6.3 is useful: the\\nefﬁcient computation of scores using a technique we will call weighted zone\\nWEIGHTED ZONE\\nSCORING\\nscoring.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n111\\n◮Figure 6.1\\nParametric search. In this example we have a collection with ﬁelds al-\\nlowing us to select publications by zones such as Author and ﬁelds such as Language.\\nwilliam.author\\n2\\n3\\n5\\n8\\nwilliam.title\\n2\\n4\\n8\\n16\\nwilliam.abstract\\n11\\n121\\n1441\\n1729\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n◮Figure 6.2\\nBasic zone index ; zones are encoded as extensions of dictionary en-\\ntries.\\nwilliam\\n2.author,2.title\\n3.author\\n4.title\\n5.author\\n-\\n-\\n-\\n-\\n◮Figure 6.3\\nZone index in which the zone is encoded in the postings rather than\\nthe dictionary.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n112\\n6\\nScoring, term weighting and the vector space model\\n6.1.1\\nWeighted zone scoring\\nThus far in Section 6.1 we have focused on retrieving documents based on\\nBoolean queries on ﬁelds and zones. We now turn to a second application of\\nzones and ﬁelds.\\nGiven a Boolean query q and a document d, weighted zone scoring assigns\\nto the pair (q, d) a score in the interval [0, 1], by computing a linear combina-\\ntion of zone scores, where each zone of the document contributes a Boolean\\nvalue. More speciﬁcally, consider a set of documents each of which has ℓ\\nzones. Let g1, . . . , gℓ∈[0, 1] such that ∑ℓ\\ni=1 gi = 1. For 1 ≤i ≤ℓ, let si be the\\nBoolean score denoting a match (or absence thereof) between q and the ith\\nzone. For instance, the Boolean score from a zone could be 1 if all the query\\nterm(s) occur in that zone, and zero otherwise; indeed, it could be any Boo-\\nlean function that maps the presence of query terms in a zone to 0, 1. Then,\\nthe weighted zone score is deﬁned to be\\nℓ\\n∑\\ni=1\\ngisi.\\n(6.1)\\nWeighted zone scoring is sometimes referred to also as ranked Boolean re-\\nRANKED BOOLEAN\\nRETRIEVAL\\ntrieval.\\n\\x0f\\nExample 6.1:\\nConsider the query shakespeare in a collection in which each doc-\\nument has three zones: author, title and body. The Boolean score function for a zone\\ntakes on the value 1 if the query term shakespeare is present in the zone, and zero\\notherwise. Weighted zone scoring in such a collection would require three weights\\ng1, g2 and g3, respectively corresponding to the author, title and body zones. Suppose\\nwe set g1 = 0.2, g2 = 0.3 and g3 = 0.5 (so that the three weights add up to 1); this cor-\\nresponds to an application in which a match in the author zone is least important to\\nthe overall score, the title zone somewhat more, and the body contributes even more.\\nThus if the term shakespeare were to appear in the title and body zones but not the\\nauthor zone of a document, the score of this document would be 0.8.\\nHow do we implement the computation of weighted zone scores? A sim-\\nple approach would be to compute the score for each document in turn,\\nadding in all the contributions from the various zones. However, we now\\nshow how we may compute weighted zone scores directly from inverted in-\\ndexes. The algorithm of Figure 6.4 treats the case when the query q is a two-\\nterm query consisting of query terms q1 and q2, and the Boolean function is\\nAND: 1 if both query terms are present in a zone and 0 otherwise. Following\\nthe description of the algorithm, we describe the extension to more complex\\nqueries and Boolean functions.\\nThe reader may have noticed the close similarity between this algorithm\\nand that in Figure 1.6. Indeed, they represent the same postings traversal,\\nexcept that instead of merely adding a document to the set of results for\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n113\\nZONESCORE(q1, q2)\\n1\\nﬂoat scores[N] = [0]\\n2\\nconstant g[ℓ]\\n3\\np1 ←postings(q1)\\n4\\np2 ←postings(q2)\\n5\\n// scores[] is an array with a score entry for each document, initialized to zero.\\n6\\n//p1 and p2 are initialized to point to the beginning of their respective postings.\\n7\\n//Assume g[] is initialized to the respective zone weights.\\n8\\nwhile p1 ̸= NIL and p2 ̸= NIL\\n9\\ndo if docID(p1) = docID(p2)\\n10\\nthen scores[docID(p1)] ←WEIGHTEDZONE(p1, p2, g)\\n11\\np1 ←next(p1)\\n12\\np2 ←next(p2)\\n13\\nelse if docID(p1) < docID(p2)\\n14\\nthen p1 ←next(p1)\\n15\\nelse p2 ←next(p2)\\n16\\nreturn scores\\n◮Figure 6.4\\nAlgorithm for computing the weighted zone score from two postings\\nlists. Function WEIGHTEDZONE (not shown here) is assumed to compute the inner\\nloop of Equation 6.1.\\na Boolean AND query, we now compute a score for each such document.\\nSome literature refers to the array scores[] above as a set of accumulators. The\\nACCUMULATOR\\nreason for this will be clear as we consider more complex Boolean functions\\nthan the AND; thus we may assign a non-zero score to a document even if it\\ndoes not contain all query terms.\\n6.1.2\\nLearning weights\\nHow do we determine the weights gi for weighted zone scoring? These\\nweights could be speciﬁed by an expert (or, in principle, the user); but in-\\ncreasingly, these weights are “learned” using training examples that have\\nbeen judged editorially. This latter methodology falls under a general class\\nof approaches to scoring and ranking in information retrieval, known as\\nmachine-learned relevance. We provide a brief introduction to this topic here\\nMACHINE-LEARNED\\nRELEVANCE\\nbecause weighted zone scoring presents a clean setting for introducing it; a\\ncomplete development demands an understanding of machine learning and\\nis deferred to Chapter 15.\\n1. We are provided with a set of training examples, each of which is a tu-\\nple consisting of a query q and a document d, together with a relevance\\n', 'Online edition (c)\\n2009 Cambridge UP\\n114\\n6\\nScoring, term weighting and the vector space model\\njudgment for d on q. In the simplest form, each relevance judgments is ei-\\nther Relevant or Non-relevant. More sophisticated implementations of the\\nmethodology make use of more nuanced judgments.\\n2. The weights gi are then “learned” from these examples, in order that the\\nlearned scores approximate the relevance judgments in the training exam-\\nples.\\nFor weighted zone scoring, the process may be viewed as learning a lin-\\near function of the Boolean match scores contributed by the various zones.\\nThe expensive component of this methodology is the labor-intensive assem-\\nbly of user-generated relevance judgments from which to learn the weights,\\nespecially in a collection that changes frequently (such as the Web). We now\\ndetail a simple example that illustrates how we can reduce the problem of\\nlearning the weights gi to a simple optimization problem.\\nWe now consider a simple case of weighted zone scoring, where each doc-\\nument has a title zone and a body zone. Given a query q and a document d, we\\nuse the given Boolean match function to compute Boolean variables sT(d, q)\\nand sB(d, q), depending on whether the title (respectively, body) zone of d\\nmatches query q. For instance, the algorithm in Figure 6.4 uses an AND of\\nthe query terms for this Boolean function. We will compute a score between\\n0 and 1 for each (document, query) pair using sT(d, q) and sB(d, q) by using\\na constant g ∈[0, 1], as follows:\\nscore(d, q) = g · sT(d, q) + (1 −g)sB(d, q).\\n(6.2)\\nWe now describe how to determine the constant g from a set of training ex-\\namples, each of which is a triple of the form Φj = (dj, qj, r(dj, qj)). In each\\ntraining example, a given training document dj and a given training query qj\\nare assessed by a human editor who delivers a relevance judgment r(dj, qj)\\nthat is either Relevant or Non-relevant. This is illustrated in Figure 6.5, where\\nseven training examples are shown.\\nFor each training example Φj we have Boolean values sT(dj, qj) and sB(dj, qj)\\nthat we use to compute a score from (6.2)\\nscore(dj, qj) = g · sT(dj, qj) + (1 −g)sB(dj, qj).\\n(6.3)\\nWe now compare this computed score to the human relevance judgment for\\nthe same document-query pair (dj, qj); to this end, we will quantize each\\nRelevant judgment as a 1 and each Non-relevant judgment as a 0. Suppose\\nthat we deﬁne the error of the scoring function with weight g as\\nε(g, Φj) = (r(dj, qj) −score(dj, qj))2,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.1\\nParametric and zone indexes\\n115\\nExample\\nDocID\\nQuery\\nsT\\nsB\\nJudgment\\nΦ1\\n37\\nlinux\\n1\\n1\\nRelevant\\nΦ2\\n37\\npenguin\\n0\\n1\\nNon-relevant\\nΦ3\\n238\\nsystem\\n0\\n1\\nRelevant\\nΦ4\\n238\\npenguin\\n0\\n0\\nNon-relevant\\nΦ5\\n1741\\nkernel\\n1\\n1\\nRelevant\\nΦ6\\n2094\\ndriver\\n0\\n1\\nRelevant\\nΦ7\\n3191\\ndriver\\n1\\n0\\nNon-relevant\\n◮Figure 6.5\\nAn illustration of training examples.\\nsT\\nsB\\nScore\\n0\\n0\\n0\\n0\\n1\\n1 −g\\n1\\n0\\ng\\n1\\n1\\n1\\n◮Figure 6.6\\nThe four possible combinations of sT and sB.\\nwhere we have quantized the editorial relevance judgment r(dj, qj) to 0 or 1.\\nThen, the total error of a set of training examples is given by\\n∑\\nj\\nε(g, Φj).\\n(6.4)\\nThe problem of learning the constant g from the given training examples\\nthen reduces to picking the value of g that minimizes the total error in (6.4).\\nPicking the best value of g in (6.4) in the formulation of Section 6.1.3 re-\\nduces to the problem of minimizing a quadratic function of g over the inter-\\nval [0, 1]. This reduction is detailed in Section 6.1.3.\\n$\\n6.1.3\\nThe optimal weight g\\nWe begin by noting that for any training example Φj for which sT(dj, qj) = 0\\nand sB(dj, qj) = 1, the score computed by Equation (6.2) is 1 −g. In similar\\nfashion, we may write down the score computed by Equation (6.2) for the\\nthree other possible combinations of sT(dj, qj) and sB(dj, qj); this is summa-\\nrized in Figure 6.6.\\nLet n01r (respectively, n01n) denote the number of training examples for\\nwhich sT(dj, qj) = 0 and sB(dj, qj) = 1 and the editorial judgment is Relevant\\n(respectively, Non-relevant). Then the contribution to the total error in Equa-\\ntion (6.4) from training examples for which sT(dj, qj) = 0 and sB(dj, qj) = 1\\n', 'Online edition (c)\\n2009 Cambridge UP\\n116\\n6\\nScoring, term weighting and the vector space model\\nis\\n[1 −(1 −g)]2n01r + [0 −(1 −g)]2n01n.\\nBy writing in similar fashion the error contributions from training examples\\nof the other three combinations of values for sT(dj, qj) and sB(dj, qj) (and\\nextending the notation in the obvious manner), the total error corresponding\\nto Equation (6.4) is\\n(n01r + n10n)g2 + (n10r + n01n)(1 −g)2 + n00r + n11n.\\n(6.5)\\nBy differentiating Equation (6.5) with respect to g and setting the result to\\nzero, it follows that the optimal value of g is\\nn10r + n01n\\nn10r + n10n + n01r + n01n\\n.\\n(6.6)\\n?\\nExercise 6.1\\nWhen using weighted zone scoring, is it necessary for all zones to use the same Boo-\\nlean match function?\\nExercise 6.2\\nIn Example 6.1 above with weights g1 = 0.2, g2 = 0.31 and g3 = 0.49, what are all the\\ndistinct score values a document may get?\\nExercise 6.3\\nRewrite the algorithm in Figure 6.4 to the case of more than two query terms.\\nExercise 6.4\\nWrite pseudocode for the function WeightedZone for the case of two postings lists in\\nFigure 6.4.\\nExercise 6.5\\nApply Equation 6.6 to the sample training set in Figure 6.5 to estimate the best value\\nof g for this sample.\\nExercise 6.6\\nFor the value of g estimated in Exercise 6.5, compute the weighted zone score for each\\n(query, document) example. How do these scores relate to the relevance judgments\\nin Figure 6.5 (quantized to 0/1)?\\nExercise 6.7\\nWhy does the expression for g in (6.6) not involve training examples in which sT(dt, qt)\\nand sB(dt, qt) have the same value?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.2\\nTerm frequency and weighting\\n117\\n6.2\\nTerm frequency and weighting\\nThus far, scoring has hinged on whether or not a query term is present in\\na zone within a document. We take the next logical step: a document or\\nzone that mentions a query term more often has more to do with that query\\nand therefore should receive a higher score. To motivate this, we recall the\\nnotion of a free text query introduced in Section 1.4: a query in which the\\nterms of the query are typed freeform into the search interface, without any\\nconnecting search operators (such as Boolean operators). This query style,\\nwhich is extremely popular on the web, views the query as simply a set of\\nwords. A plausible scoring mechanism then is to compute a score that is the\\nsum, over the query terms, of the match scores between each query term and\\nthe document.\\nTowards this end, we assign to each term in a document a weight for that\\nterm, that depends on the number of occurrences of the term in the doc-\\nument. We would like to compute a score between a query term t and a\\ndocument d, based on the weight of t in d. The simplest approach is to assign\\nthe weight to be equal to the number of occurrences of term t in document d.\\nThis weighting scheme is referred to as term frequency and is denoted tft,d,\\nTERM FREQUENCY\\nwith the subscripts denoting the term and the document in order.\\nFor a document d, the set of weights determined by the tf weights above\\n(or indeed any weighting function that maps the number of occurrences of t\\nin d to a positive real value) may be viewed as a quantitative digest of that\\ndocument. In this view of a document, known in the literature as the bag\\nBAG OF WORDS\\nof words model, the exact ordering of the terms in a document is ignored but\\nthe number of occurrences of each term is material (in contrast to Boolean\\nretrieval). We only retain information on the number of occurrences of each\\nterm. Thus, the document “Mary is quicker than John” is, in this view, iden-\\ntical to the document “John is quicker than Mary”. Nevertheless, it seems\\nintuitive that two documents with similar bag of words representations are\\nsimilar in content. We will develop this intuition further in Section 6.3.\\nBefore doing so we ﬁrst study the question: are all words in a document\\nequally important? Clearly not; in Section 2.2.2 (page 27) we looked at the\\nidea of stop words – words that we decide not to index at all, and therefore do\\nnot contribute in any way to retrieval and scoring.\\n6.2.1\\nInverse document frequency\\nRaw term frequency as above suffers from a critical problem: all terms are\\nconsidered equally important when it comes to assessing relevancy on a\\nquery. In fact certain terms have little or no discriminating power in de-\\ntermining relevance. For instance, a collection of documents on the auto\\nindustry is likely to have the term auto in almost every document. To this\\n', 'Online edition (c)\\n2009 Cambridge UP\\n118\\n6\\nScoring, term weighting and the vector space model\\nWord\\ncf\\ndf\\ntry\\n10422\\n8760\\ninsurance\\n10440\\n3997\\n◮Figure 6.7\\nCollection frequency (cf) and document frequency (df) behave differ-\\nently, as in this example from the Reuters collection.\\nend, we introduce a mechanism for attenuating the effect of terms that occur\\ntoo often in the collection to be meaningful for relevance determination. An\\nimmediate idea is to scale down the term weights of terms with high collec-\\ntion frequency, deﬁned to be the total number of occurrences of a term in the\\ncollection. The idea would be to reduce the tf weight of a term by a factor\\nthat grows with its collection frequency.\\nInstead, it is more commonplace to use for this purpose the document fre-\\nDOCUMENT\\nFREQUENCY\\nquency dft, deﬁned to be the number of documents in the collection that con-\\ntain a term t. This is because in trying to discriminate between documents\\nfor the purpose of scoring it is better to use a document-level statistic (such\\nas the number of documents containing a term) than to use a collection-wide\\nstatistic for the term. The reason to prefer df to cf is illustrated in Figure 6.7,\\nwhere a simple example shows that collection frequency (cf) and document\\nfrequency (df) can behave rather differently. In particular, the cf values for\\nboth try and insurance are roughly equal, but their df values differ signiﬁ-\\ncantly. Intuitively, we want the few documents that contain insurance to get\\na higher boost for a query on insurance than the many documents containing\\ntry get from a query on try.\\nHow is the document frequency df of a term used to scale its weight? De-\\nnoting as usual the total number of documents in a collection by N, we deﬁne\\nthe inverse document frequency (idf) of a term t as follows:\\nINVERSE DOCUMENT\\nFREQUENCY\\nidft = log N\\ndft\\n.\\n(6.7)\\nThus the idf of a rare term is high, whereas the idf of a frequent term is\\nlikely to be low. Figure 6.8 gives an example of idf’s in the Reuters collection\\nof 806,791 documents; in this example logarithms are to the base 10. In fact,\\nas we will see in Exercise 6.12, the precise base of the logarithm is not material\\nto ranking. We will give on page 227 a justiﬁcation of the particular form in\\nEquation (6.7).\\n6.2.2\\nTf-idf weighting\\nWe now combine the deﬁnitions of term frequency and inverse document\\nfrequency, to produce a composite weight for each term in each document.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.2\\nTerm frequency and weighting\\n119\\nterm\\ndft\\nidft\\ncar\\n18,165\\n1.65\\nauto\\n6723\\n2.08\\ninsurance\\n19,241\\n1.62\\nbest\\n25,235\\n1.5\\n◮Figure 6.8\\nExample of idf values.\\nHere we give the idf’s of terms with various\\nfrequencies in the Reuters collection of 806,791 documents.\\nThe tf-idf weighting scheme assigns to term t a weight in document d given\\nTF-IDF\\nby\\ntf-idft,d = tft,d × idft.\\n(6.8)\\nIn other words, tf-idft,d assigns to term t a weight in document d that is\\n1. highest when t occurs many times within a small number of documents\\n(thus lending high discriminating power to those documents);\\n2. lower when the term occurs fewer times in a document, or occurs in many\\ndocuments (thus offering a less pronounced relevance signal);\\n3. lowest when the term occurs in virtually all documents.\\nAt this point, we may view each document as a vector with one component\\nDOCUMENT VECTOR\\ncorresponding to each term in the dictionary, together with a weight for each\\ncomponent that is given by (6.8). For dictionary terms that do not occur in\\na document, this weight is zero. This vector form will prove to be crucial to\\nscoring and ranking; we will develop these ideas in Section 6.3. As a ﬁrst\\nstep, we introduce the overlap score measure: the score of a document d is the\\nsum, over all query terms, of the number of times each of the query terms\\noccurs in d. We can reﬁne this idea so that we add up not the number of\\noccurrences of each query term t in d, but instead the tf-idf weight of each\\nterm in d.\\nScore(q, d) = ∑\\nt∈q\\ntf-idft,d.\\n(6.9)\\nIn Section 6.3 we will develop a more rigorous form of Equation (6.9).\\n?\\nExercise 6.8\\nWhy is the idf of a term always ﬁnite?\\nExercise 6.9\\nWhat is the idf of a term that occurs in every document? Compare this with the use\\nof stop word lists.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n120\\n6\\nScoring, term weighting and the vector space model\\nDoc1\\nDoc2\\nDoc3\\ncar\\n27\\n4\\n24\\nauto\\n3\\n33\\n0\\ninsurance\\n0\\n33\\n29\\nbest\\n14\\n0\\n17\\n◮Figure 6.9\\nTable of tf values for Exercise 6.10.\\nExercise 6.10\\nConsider the table of term frequencies for 3 documents denoted Doc1, Doc2, Doc3 in\\nFigure 6.9. Compute the tf-idf weights for the terms car, auto, insurance, best, for each\\ndocument, using the idf values from Figure 6.8.\\nExercise 6.11\\nCan the tf-idf weight of a term in a document exceed 1?\\nExercise 6.12\\nHow does the base of the logarithm in (6.7) affect the score calculation in (6.9)? How\\ndoes the base of the logarithm affect the relative scores of two documents on a given\\nquery?\\nExercise 6.13\\nIf the logarithm in (6.7) is computed base 2, suggest a simple approximation to the idf\\nof a term.\\n6.3\\nThe vector space model for scoring\\nIn Section 6.2 (page 117) we developed the notion of a document vector that\\ncaptures the relative importance of the terms in a document. The representa-\\ntion of a set of documents as vectors in a common vector space is known as\\nthe vector space model and is fundamental to a host of information retrieval op-\\nVECTOR SPACE MODEL\\nerations ranging from scoring documents on a query, document classiﬁcation\\nand document clustering. We ﬁrst develop the basic ideas underlying vector\\nspace scoring; a pivotal step in this development is the view (Section 6.3.2)\\nof queries as vectors in the same vector space as the document collection.\\n6.3.1\\nDot products\\nWe denote by ⃗V(d) the vector derived from document d, with one com-\\nponent in the vector for each dictionary term. Unless otherwise speciﬁed,\\nthe reader may assume that the components are computed using the tf-idf\\nweighting scheme, although the particular weighting scheme is immaterial\\nto the discussion that follows. The set of documents in a collection then may\\nbe viewed as a set of vectors in a vector space, in which there is one axis for\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n121\\n0\\n1\\n0\\n1\\njealous\\ngossip\\n⃗v(q)\\n⃗v(d1)\\n⃗v(d2)\\n⃗v(d3)\\nθ\\n◮Figure 6.10\\nCosine similarity illustrated. sim(d1, d2) = cos θ.\\neach term. This representation loses the relative ordering of the terms in each\\ndocument; recall our example from Section 6.2 (page 117), where we pointed\\nout that the documents Mary is quicker than John and John is quicker than Mary\\nare identical in such a bag of words representation.\\nHow do we quantify the similarity between two documents in this vector\\nspace? A ﬁrst attempt might consider the magnitude of the vector difference\\nbetween two document vectors. This measure suffers from a drawback: two\\ndocuments with very similar content can have a signiﬁcant vector difference\\nsimply because one is much longer than the other. Thus the relative distribu-\\ntions of terms may be identical in the two documents, but the absolute term\\nfrequencies of one may be far larger.\\nTo compensate for the effect of document length, the standard way of\\nquantifying the similarity between two documents d1 and d2 is to compute\\nthe cosine similarity of their vector representations ⃗V(d1) and ⃗V(d2)\\nCOSINE SIMILARITY\\nsim(d1, d2) =\\n⃗V(d1) · ⃗V(d2)\\n|⃗V(d1)||⃗V(d2)|\\n,\\n(6.10)\\nwhere the numerator represents the dot product (also known as the inner prod-\\nDOT PRODUCT\\nuct) of the vectors ⃗V(d1) and ⃗V(d2), while the denominator is the product of\\ntheir Euclidean lengths. The dot product ⃗x · ⃗y of two vectors is deﬁned as\\nEUCLIDEAN LENGTH\\n∑M\\ni=1 xiyi. Let ⃗V(d) denote the document vector for d, with M components\\n⃗V1(d) . . . ⃗VM(d). The Euclidean length of d is deﬁned to be\\nq\\n∑M\\ni=1 ⃗V2\\ni (d).\\nThe effect of the denominator of Equation (6.10) is thus to length-normalize\\nLENGTH-\\nNORMALIZATION\\nthe vectors ⃗V(d1) and ⃗V(d2) to unit vectors ⃗v(d1) = ⃗V(d1)/|⃗V(d1)| and\\n', 'Online edition (c)\\n2009 Cambridge UP\\n122\\n6\\nScoring, term weighting and the vector space model\\nDoc1\\nDoc2\\nDoc3\\ncar\\n0.88\\n0.09\\n0.58\\nauto\\n0.10\\n0.71\\n0\\ninsurance\\n0\\n0.71\\n0.70\\nbest\\n0.46\\n0\\n0.41\\n◮Figure 6.11\\nEuclidean normalized tf values for documents in Figure 6.9.\\nterm\\nSaS\\nPaP\\nWH\\naffection\\n115\\n58\\n20\\njealous\\n10\\n7\\n11\\ngossip\\n2\\n0\\n6\\n◮Figure 6.12\\nTerm frequencies in three novels. The novels are Austen’s Sense and\\nSensibility, Pride and Prejudice and Brontë’s Wuthering Heights.\\n⃗v(d2) = ⃗V(d2)/|⃗V(d2)|. We can then rewrite (6.10) as\\nsim(d1, d2) = ⃗v(d1) ·⃗v(d2).\\n(6.11)\\n\\x0f\\nExample 6.2:\\nConsider the documents in Figure 6.9. We now apply Euclidean\\nnormalization to the tf values from the table, for each of the three documents in the\\ntable. The quantity\\nq\\n∑M\\ni=1 ⃗V2\\ni (d) has the values 30.56, 46.84 and 41.30 respectively\\nfor Doc1, Doc2 and Doc3. The resulting Euclidean normalized tf values for these\\ndocuments are shown in Figure 6.11.\\nThus, (6.11) can be viewed as the dot product of the normalized versions of\\nthe two document vectors. This measure is the cosine of the angle θ between\\nthe two vectors, shown in Figure 6.10. What use is the similarity measure\\nsim(d1, d2)? Given a document d (potentially one of the di in the collection),\\nconsider searching for the documents in the collection most similar to d. Such\\na search is useful in a system where a user may identify a document and\\nseek others like it – a feature available in the results lists of search engines\\nas a more like this feature. We reduce the problem of ﬁnding the document(s)\\nmost similar to d to that of ﬁnding the di with the highest dot products (sim\\nvalues)⃗v(d) ·⃗v(di). We could do this by computing the dot products between\\n⃗v(d) and each of ⃗v(d1), . . . ,⃗v(dN), then picking off the highest resulting sim\\nvalues.\\n\\x0f\\nExample 6.3:\\nFigure 6.12 shows the number of occurrences of three terms (affection,\\njealous and gossip) in each of the following three novels: Jane Austen’s Sense and Sensi-\\nbility (SaS) and Pride and Prejudice (PaP) and Emily Brontë’s Wuthering Heights (WH).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n123\\nterm\\nSaS\\nPaP\\nWH\\naffection\\n0.996\\n0.993\\n0.847\\njealous\\n0.087\\n0.120\\n0.466\\ngossip\\n0.017\\n0\\n0.254\\n◮Figure 6.13\\nTerm vectors for the three novels of Figure 6.12. These are based on\\nraw term frequency only and are normalized as if these were the only terms in the\\ncollection. (Since affection and jealous occur in all three documents, their tf-idf weight\\nwould be 0 in most formulations.)\\nOf course, there are many other terms occurring in each of these novels. In this ex-\\nample we represent each of these novels as a unit vector in three dimensions, corre-\\nsponding to these three terms (only); we use raw term frequencies here, with no idf\\nmultiplier. The resulting weights are as shown in Figure 6.13.\\nNow consider the cosine similarities between pairs of the resulting three-dimensional\\nvectors. A simple computation shows that sim(⃗v(SAS), ⃗v(PAP)) is 0.999, whereas\\nsim(⃗v(SAS), ⃗v(WH)) is 0.888; thus, the two books authored by Austen (SaS and PaP)\\nare considerably closer to each other than to Brontë’s Wuthering Heights. In fact, the\\nsimilarity between the ﬁrst two is almost perfect (when restricted to the three terms\\nwe consider). Here we have considered tf weights, but we could of course use other\\nterm weight functions.\\nViewing a collection of N documents as a collection of vectors leads to a\\nnatural view of a collection as a term-document matrix: this is an M × N matrix\\nTERM-DOCUMENT\\nMATRIX\\nwhose rows represent the M terms (dimensions) of the N columns, each of\\nwhich corresponds to a document. As always, the terms being indexed could\\nbe stemmed before indexing; for instance, jealous and jealousy would under\\nstemming be considered as a single dimension. This matrix view will prove\\nto be useful in Chapter 18.\\n6.3.2\\nQueries as vectors\\nThere is a far more compelling reason to represent documents as vectors:\\nwe can also view a query as a vector. Consider the query q = jealous gossip.\\nThis query turns into the unit vector ⃗v(q) = (0, 0.707, 0.707) on the three\\ncoordinates of Figures 6.12 and 6.13. The key idea now: to assign to each\\ndocument d a score equal to the dot product\\n⃗v(q) ·⃗v(d).\\nIn the example of Figure 6.13, Wuthering Heights is the top-scoring docu-\\nment for this query with a score of 0.509, with Pride and Prejudice a distant\\nsecond with a score of 0.085, and Sense and Sensibility last with a score of\\n0.074. This simple example is somewhat misleading: the number of dimen-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n124\\n6\\nScoring, term weighting and the vector space model\\nsions in practice will be far larger than three: it will equal the vocabulary size\\nM.\\nTo summarize, by viewing a query as a “bag of words”, we are able to\\ntreat it as a very short document. As a consequence, we can use the cosine\\nsimilarity between the query vector and a document vector as a measure of\\nthe score of the document for that query. The resulting scores can then be\\nused to select the top-scoring documents for a query. Thus we have\\nscore(q, d) =\\n⃗V(q) · ⃗V(d)\\n|⃗V(q)||⃗V(d)|\\n.\\n(6.12)\\nA document may have a high cosine score for a query even if it does not\\ncontain all query terms. Note that the preceding discussion does not hinge\\non any speciﬁc weighting of terms in the document vector, although for the\\npresent we may think of them as either tf or tf-idf weights. In fact, a number\\nof weighting schemes are possible for query as well as document vectors, as\\nillustrated in Example 6.4 and developed further in Section 6.4.\\nComputing the cosine similarities between the query vector and each doc-\\nument vector in the collection, sorting the resulting scores and selecting the\\ntop K documents can be expensive — a single similarity computation can\\nentail a dot product in tens of thousands of dimensions, demanding tens of\\nthousands of arithmetic operations. In Section 7.1 we study how to use an in-\\nverted index for this purpose, followed by a series of heuristics for improving\\non this.\\n\\x0f\\nExample 6.4:\\nWe now consider the query best car insurance on a ﬁctitious collection\\nwith N = 1,000,000 documents where the document frequencies of auto, best, car and\\ninsurance are respectively 5000, 50000, 10000 and 1000.\\nterm\\nquery\\ndocument\\nproduct\\ntf\\ndf\\nidf\\nwt,q\\ntf\\nwf\\nwt,d\\nauto\\n0\\n5000\\n2.3\\n0\\n1\\n1\\n0.41\\n0\\nbest\\n1\\n50000\\n1.3\\n1.3\\n0\\n0\\n0\\n0\\ncar\\n1\\n10000\\n2.0\\n2.0\\n1\\n1\\n0.41\\n0.82\\ninsurance\\n1\\n1000\\n3.0\\n3.0\\n2\\n2\\n0.82\\n2.46\\nIn this example the weight of a term in the query is simply the idf (and zero for a\\nterm not in the query, such as auto); this is reﬂected in the column header wt,q (the en-\\ntry for auto is zero because the query does not contain the termauto). For documents,\\nwe use tf weighting with no use of idf but with Euclidean normalization. The former\\nis shown under the column headed wf, while the latter is shown under the column\\nheaded wt,d. Invoking (6.9) now gives a net score of 0 + 0 + 0.82 + 2.46 = 3.28.\\n6.3.3\\nComputing vector scores\\nIn a typical setting we have a collection of documents each represented by a\\nvector, a free text query represented by a vector, and a positive integer K. We\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.3\\nThe vector space model for scoring\\n125\\nCOSINESCORE(q)\\n1\\nﬂoat Scores[N] = 0\\n2\\nInitialize Length[N]\\n3\\nfor each query term t\\n4\\ndo calculate wt,q and fetch postings list for t\\n5\\nfor each pair(d, tft,d) in postings list\\n6\\ndo Scores[d] += wft,d × wt,q\\n7\\nRead the array Length[d]\\n8\\nfor each d\\n9\\ndo Scores[d] = Scores[d]/Length[d]\\n10\\nreturn Top K components of Scores[]\\n◮Figure 6.14\\nThe basic algorithm for computing vector space scores.\\nseek the K documents of the collection with the highest vector space scores on\\nthe given query. We now initiate the study of determining the K documents\\nwith the highest vector space scores for a query. Typically, we seek these\\nK top documents in ordered by decreasing score; for instance many search\\nengines use K = 10 to retrieve and rank-order the ﬁrst page of the ten best\\nresults. Here we give the basic algorithm for this computation; we develop a\\nfuller treatment of efﬁcient techniques and approximations in Chapter 7.\\nFigure 6.14 gives the basic algorithm for computing vector space scores.\\nThe array Length holds the lengths (normalization factors) for each of the N\\ndocuments, whereas the array Scores holds the scores for each of the docu-\\nments. When the scores are ﬁnally computed in Step 9, all that remains in\\nStep 10 is to pick off the K documents with the highest scores.\\nThe outermost loop beginning Step 3 repeats the updating of Scores, iter-\\nating over each query term t in turn. In Step 5 we calculate the weight in\\nthe query vector for term t. Steps 6-8 update the score of each document by\\nadding in the contribution from term t. This process of adding in contribu-\\ntions one query term at a time is sometimes known as term-at-a-time scoring\\nTERM-AT-A-TIME\\nor accumulation, and the N elements of the array Scores are therefore known\\nas accumulators. For this purpose, it would appear necessary to store, with\\nACCUMULATOR\\neach postings entry, the weight wft,d of term t in document d (we have thus\\nfar used either tf or tf-idf for this weight, but leave open the possibility of\\nother functions to be developed in Section 6.4). In fact this is wasteful, since\\nstoring this weight may require a ﬂoating point number. Two ideas help alle-\\nviate this space problem. First, if we are using inverse document frequency,\\nwe need not precompute idft; it sufﬁces to store N/dft at the head of the\\npostings for t. Second, we store the term frequency tft,d for each postings en-\\ntry. Finally, Step 12 extracts the top K scores – this requires a priority queue\\n', 'Online edition (c)\\n2009 Cambridge UP\\n126\\n6\\nScoring, term weighting and the vector space model\\ndata structure, often implemented using a heap. Such a heap takes no more\\nthan 2N comparisons to construct, following which each of the K top scores\\ncan be extracted from the heap at a cost of O(log N) comparisons.\\nNote that the general algorithm of Figure 6.14 does not prescribe a speciﬁc\\nimplementation of how we traverse the postings lists of the various query\\nterms; we may traverse them one term at a time as in the loop beginning\\nat Step 3, or we could in fact traverse them concurrently as in Figure 1.6. In\\nsuch a concurrent postings traversal we compute the scores of one document\\nat a time, so that it is sometimes called document-at-a-time scoring. We will\\nDOCUMENT-AT-A-TIME\\nsay more about this in Section 7.1.5.\\n?\\nExercise 6.14\\nIf we were to stem jealous and jealousy to a common stem before setting up the vector\\nspace, detail how the deﬁnitions of tf and idf should be modiﬁed.\\nExercise 6.15\\nRecall the tf-idf weights computed in Exercise 6.10. Compute the Euclidean nor-\\nmalized document vectors for each of the documents, where each vector has four\\ncomponents, one for each of the four terms.\\nExercise 6.16\\nVerify that the sum of the squares of the components of each of the document vectors\\nin Exercise 6.15 is 1 (to within rounding error). Why is this the case?\\nExercise 6.17\\nWith term weights as computed in Exercise 6.15, rank the three documents by com-\\nputed score for the query car insurance, for each of the following cases of term weight-\\ning in the query:\\n1. The weight of a term is 1 if present in the query, 0 otherwise.\\n2. Euclidean normalized idf.\\n6.4\\nVariant tf-idf functions\\nFor assigning a weight for each term in each document, a number of alterna-\\ntives to tf and tf-idf have been considered. We discuss some of the principal\\nones here; a more complete development is deferred to Chapter 11. We will\\nsummarize these alternatives in Section 6.4.3 (page 128).\\n6.4.1\\nSublinear tf scaling\\nIt seems unlikely that twenty occurrences of a term in a document truly carry\\ntwenty times the signiﬁcance of a single occurrence. Accordingly, there has\\nbeen considerable research into variants of term frequency that go beyond\\ncounting the number of occurrences of a term. A common modiﬁcation is\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n127\\nto use instead the logarithm of the term frequency, which assigns a weight\\ngiven by\\nwft,d =\\n\\x1a\\n1 + log tft,d\\nif tft,d > 0\\n0\\notherwise .\\n(6.13)\\nIn this form, we may replace tf by some other function wf as in (6.13), to\\nobtain:\\nwf-idft,d = wft,d × idft.\\n(6.14)\\nEquation (6.9) can then be modiﬁed by replacing tf-idf by wf-idf as deﬁned\\nin (6.14).\\n6.4.2\\nMaximum tf normalization\\nOne well-studied technique is to normalize the tf weights of all terms occur-\\nring in a document by the maximum tf in that document. For each document\\nd, let tfmax(d) = maxτ∈d tfτ,d, where τ ranges over all terms in d. Then, we\\ncompute a normalized term frequency for each term t in document d by\\nntft,d = a + (1 −a)\\ntft,d\\ntfmax(d),\\n(6.15)\\nwhere a is a value between 0 and 1 and is generally set to 0.4, although some\\nearly work used the value 0.5. The term a in (6.15) is a smoothing term whose\\nSMOOTHING\\nrole is to damp the contribution of the second term – which may be viewed as\\na scaling down of tf by the largest tf value in d. We will encounter smoothing\\nfurther in Chapter 13 when discussing classiﬁcation; the basic idea is to avoid\\na large swing in ntft,d from modest changes in tft,d (say from 1 to 2). The main\\nidea of maximum tf normalization is to mitigate the following anomaly: we\\nobserve higher term frequencies in longer documents, merely because longer\\ndocuments tend to repeat the same words over and over again. To appreciate\\nthis, consider the following extreme example: supposed we were to take a\\ndocument d and create a new document d′ by simply appending a copy of d\\nto itself. While d′ should be no more relevant to any query than d is, the use\\nof (6.9) would assign it twice as high a score as d. Replacing tf-idft,d in (6.9) by\\nntf-idft,d eliminates the anomaly in this example. Maximum tf normalization\\ndoes suffer from the following issues:\\n1. The method is unstable in the following sense: a change in the stop word\\nlist can dramatically alter term weightings (and therefore ranking). Thus,\\nit is hard to tune.\\n2. A document may contain an outlier term with an unusually large num-\\nber of occurrences of that term, not representative of the content of that\\ndocument.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n128\\n6\\nScoring, term weighting and the vector space model\\nTerm frequency\\nDocument frequency\\nNormalization\\nn (natural)\\ntft,d\\nn (no)\\n1\\nn (none)\\n1\\nl (logarithm)\\n1 + log(tft,d)\\nt (idf)\\nlog N\\ndft\\nc (cosine)\\n1\\n√\\nw2\\n1+w2\\n2+...+w2\\nM\\na (augmented)\\n0.5 +\\n0.5×tft,d\\nmaxt(tft,d)\\np (prob idf)\\nmax{0, log N−dft\\ndft }\\nu (pivoted\\nunique)\\n1/u (Section 6.4.4)\\nb (boolean)\\n\\x1a1\\nif tft,d > 0\\n0\\notherwise\\nb (byte size)\\n1/CharLengthα, α < 1\\nL (log ave)\\n1+log(tft,d)\\n1+log(avet∈d(tft,d))\\n◮Figure 6.15\\nSMART notation for tf-idf variants. Here CharLength is the number\\nof characters in the document.\\n3. More generally, a document in which the most frequent term appears\\nroughly as often as many other terms should be treated differently from\\none with a more skewed distribution.\\n6.4.3\\nDocument and query weighting schemes\\nEquation (6.12) is fundamental to information retrieval systems that use any\\nform of vector space scoring. Variations from one vector space scoring method\\nto another hinge on the speciﬁc choices of weights in the vectors ⃗V(d) and\\n⃗V(q). Figure 6.15 lists some of the principal weighting schemes in use for\\neach of ⃗V(d) and ⃗V(q), together with a mnemonic for representing a spe-\\nciﬁc combination of weights; this system of mnemonics is sometimes called\\nSMART notation, following the authors of an early text retrieval system. The\\nmnemonic for representing a combination of weights takes the form ddd.qqq\\nwhere the ﬁrst triplet gives the term weighting of the document vector, while\\nthe second triplet gives the weighting in the query vector. The ﬁrst letter in\\neach triplet speciﬁes the term frequency component of the weighting, the\\nsecond the document frequency component, and the third the form of nor-\\nmalization used. It is quite common to apply different normalization func-\\ntions to ⃗V(d) and ⃗V(q). For example, a very standard weighting scheme\\nis lnc.ltc, where the document vector has log-weighted term frequency, no\\nidf (for both effectiveness and efﬁciency reasons), and cosine normalization,\\nwhile the query vector uses log-weighted term frequency, idf weighting, and\\ncosine normalization.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n129\\n$\\n6.4.4\\nPivoted normalized document length\\nIn Section 6.3.1 we normalized each document vector by the Euclidean length\\nof the vector, so that all document vectors turned into unit vectors. In doing\\nso, we eliminated all information on the length of the original document;\\nthis masks some subtleties about longer documents. First, longer documents\\nwill – as a result of containing more terms – have higher tf values. Second,\\nlonger documents contain more distinct terms. These factors can conspire to\\nraise the scores of longer documents, which (at least for some information\\nneeds) is unnatural. Longer documents can broadly be lumped into two cat-\\negories: (1) verbose documents that essentially repeat the same content – in\\nthese, the length of the document does not alter the relative weights of dif-\\nferent terms; (2) documents covering multiple different topics, in which the\\nsearch terms probably match small segments of the document but not all of\\nit – in this case, the relative weights of terms are quite different from a single\\nshort document that matches the query terms. Compensating for this phe-\\nnomenon is a form of document length normalization that is independent of\\nterm and document frequencies. To this end, we introduce a form of normal-\\nizing the vector representations of documents in the collection, so that the\\nresulting “normalized” documents are not necessarily of unit length. Then,\\nwhen we compute the dot product score between a (unit) query vector and\\nsuch a normalized document, the score is skewed to account for the effect\\nof document length on relevance. This form of compensation for document\\nlength is known as pivoted document length normalization.\\nPIVOTED DOCUMENT\\nLENGTH\\nNORMALIZATION\\nConsider a document collection together with an ensemble of queries for\\nthat collection. Suppose that we were given, for each query q and for each\\ndocument d, a Boolean judgment of whether or not d is relevant to the query\\nq; in Chapter 8 we will see how to procure such a set of relevance judgments\\nfor a query ensemble and a document collection. Given this set of relevance\\njudgments, we may compute a probability of relevance as a function of docu-\\nment length, averaged over all queries in the ensemble. The resulting plot\\nmay look like the curve drawn in thick lines in Figure 6.16. To compute this\\ncurve, we bucket documents by length and compute the fraction of relevant\\ndocuments in each bucket, then plot this fraction against the median docu-\\nment length of each bucket. (Thus even though the “curve” in Figure 6.16\\nappears to be continuous, it is in fact a histogram of discrete buckets of doc-\\nument length.)\\nOn the other hand, the curve in thin lines shows what might happen with\\nthe same documents and query ensemble if we were to use relevance as pre-\\nscribed by cosine normalization Equation (6.12) – thus, cosine normalization\\nhas a tendency to distort the computed relevance vis-à-vis the true relevance,\\nat the expense of longer documents. The thin and thick curves crossover at a\\npoint p corresponding to document length ℓp, which we refer to as the pivot\\n', 'Online edition (c)\\n2009 Cambridge UP\\n130\\n6\\nScoring, term weighting and the vector space model\\nDocument length\\nRelevance\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n\\x11\\n\\x08\\x08\\x08\\x08\\n\\x08\\x08\\x08\\x08\\nℓp\\np\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x08\\x08\\x08\\x08\\n\\x08\\x08\\x08\\x08\\n-\\n6\\n◮Figure 6.16\\nPivoted document length normalization.\\nlength; dashed lines mark this point on the x−and y−axes. The idea of\\npivoted document length normalization would then be to “rotate” the co-\\nsine normalization curve counter-clockwise about p so that it more closely\\nmatches thick line representing the relevance vs. document length curve.\\nAs mentioned at the beginning of this section, we do so by using in Equa-\\ntion (6.12) a normalization factor for each document vector ⃗V(d) that is not\\nthe Euclidean length of that vector, but instead one that is larger than the Eu-\\nclidean length for documents of length less than ℓp, and smaller for longer\\ndocuments.\\nTo this end, we ﬁrst note that the normalizing term for ⃗V(d) in the de-\\nnominator of Equation (6.12) is its Euclidean length, denoted |⃗V(d)|. In the\\nsimplest implementation of pivoted document length normalization, we use\\na normalization factor in the denominator that is linear in |⃗V(d)|, but one\\nof slope < 1 as in Figure 6.17. In this ﬁgure, the x−axis represents |⃗V(d)|,\\nwhile the y−axis represents possible normalization factors we can use. The\\nthin line y = x depicts the use of cosine normalization. Notice the following\\naspects of the thick line representing pivoted length normalization:\\n1. It is linear in the document length and has the form\\na|⃗V(d)| + (1 −a)piv,\\n(6.16)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.4\\nVariant tf-idf functions\\n131\\n|⃗V(d)|\\nPivoted normalization\\ny = x; Cosine\\nPivoted\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00piv\\n\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\x11\\n-\\n6\\n◮Figure 6.17\\nImplementing pivoted document length normalization by linear scal-\\ning.\\nwhere piv is the cosine normalization value at which the two curves in-\\ntersect.\\n2. Its slope is a < 1 and (3) it crosses the y = x line at piv.\\nIt has been argued that in practice, Equation (6.16) is well approximated by\\naud + (1 −a)piv,\\nwhere ud is the number of unique terms in document d.\\nOf course, pivoted document length normalization is not appropriate for\\nall applications. For instance, in a collection of answers to frequently asked\\nquestions (say, at a customer service website), relevance may have little to\\ndo with document length. In other cases the dependency may be more com-\\nplex than can be accounted for by a simple linear pivoted normalization. In\\nsuch cases, document length can be used as a feature in the machine learning\\nbased scoring approach of Section 6.1.2.\\n?\\nExercise 6.18\\nOne measure of the similarity of two vectors is the Euclidean distance (or L2 distance)\\nEUCLIDEAN DISTANCE\\nbetween them:\\n|⃗x −⃗y| =\\nv\\nu\\nu\\nt\\nM\\n∑\\ni=1\\n(xi −yi)2\\n', 'Online edition (c)\\n2009 Cambridge UP\\n132\\n6\\nScoring, term weighting and the vector space model\\nquery\\ndocument\\nword\\ntf\\nwf\\ndf\\nidf\\nqi = wf-idf\\ntf\\nwf\\ndi = normalized wf\\nqi · di\\ndigital\\n10,000\\nvideo\\n100,000\\ncameras\\n50,000\\n◮Table 6.1\\nCosine computation for Exercise 6.19.\\nGiven a query q and documents d1, d2, . . ., we may rank the documents di in order\\nof increasing Euclidean distance from q. Show that if q and the di are all normalized\\nto unit vectors, then the rank ordering produced by Euclidean distance is identical to\\nthat produced by cosine similarities.\\nExercise 6.19\\nCompute the vector space similarity between the query “digital cameras” and the\\ndocument “digital cameras and video cameras” by ﬁlling out the empty columns in\\nTable 6.1. Assume N = 10,000,000, logarithmic term weighting (wf columns) for\\nquery and document, idf weighting for the query only and cosine normalization for\\nthe document only. Treat and as a stop word. Enter term counts in the tf columns.\\nWhat is the ﬁnal similarity score?\\nExercise 6.20\\nShow that for the query affection, the relative ordering of the scores of the three doc-\\numents in Figure 6.13 is the reverse of the ordering of the scores for the query jealous\\ngossip.\\nExercise 6.21\\nIn turning a query into a unit vector in Figure 6.13, we assigned equal weights to each\\nof the query terms. What other principled approaches are plausible?\\nExercise 6.22\\nConsider the case of a query term that is not in the set of M indexed terms; thus our\\nstandard construction of the query vector results in ⃗V(q) not being in the vector space\\ncreated from the collection. How would one adapt the vector space representation to\\nhandle this case?\\nExercise 6.23\\nRefer to the tf and idf values for four terms and three documents in Exercise 6.10.\\nCompute the two top scoring documents on the query best car insurance for each of\\nthe following weighing schemes: (i) nnn.atc; (ii) ntc.atc.\\nExercise 6.24\\nSuppose that the word coyote does not occur in the collection used in Exercises 6.10\\nand 6.23. How would one compute ntc.atc scores for the query coyote insurance?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n6.5\\nReferences and further reading\\n133\\n6.5\\nReferences and further reading\\nChapter 7 develops the computational aspects of vector space scoring. Luhn\\n(1957; 1958) describes some of the earliest reported applications of term weight-\\ning. His paper dwells on the importance of medium frequency terms (terms\\nthat are neither too commonplace nor too rare) and may be thought of as an-\\nticipating tf-idf and related weighting schemes. Spärck Jones (1972) builds\\non this intuition through detailed experiments showing the use of inverse\\ndocument frequency in term weighting. A series of extensions and theoret-\\nical justiﬁcations of idf are due to Salton and Buckley (1987) Robertson and\\nJones (1976), Croft and Harper (1979) and Papineni (2001). Robertson main-\\ntains a web page (http://www.soi.city.ac.uk/˜ser/idf.html) containing the history\\nof idf, including soft copies of early papers that predated electronic versions\\nof journal article. Singhal et al. (1996a) develop pivoted document length\\nnormalization. Probabilistic language models (Chapter 11) develop weight-\\ning techniques that are more nuanced than tf-idf; the reader will ﬁnd this\\ndevelopment in Section 11.4.3.\\nWe observed that by assigning a weight for each term in a document, a\\ndocument may be viewed as a vector of term weights, one for each term in\\nthe collection. The SMART information retrieval system at Cornell (Salton\\n1971b) due to Salton and colleagues was perhaps the ﬁrst to view a doc-\\nument as a vector of weights. The basic computation of cosine scores as\\ndescribed in Section 6.3.3 is due to Zobel and Moffat (2006). The two query\\nevaluation strategies term-at-a-time and document-at-a-time are discussed\\nby Turtle and Flood (1995).\\nThe SMART notation for tf-idf term weighting schemes in Figure 6.15 is\\npresented in (Salton and Buckley 1988, Singhal et al. 1995; 1996b). Not all\\nversions of the notation are consistent; we most closely follow (Singhal et al.\\n1996b). A more detailed and exhaustive notation was developed in Moffat\\nand Zobel (1998), considering a larger palette of schemes for term and doc-\\nument frequency weighting. Beyond the notation, Moffat and Zobel (1998)\\nsought to set up a space of feasible weighting functions through which hill-\\nclimbing approaches could be used to begin with weighting schemes that\\nperformed well, then make local improvements to identify the best combi-\\nnations. However, they report that such hill-climbing methods failed to lead\\nto any conclusions on the best weighting schemes.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n135\\n7\\nComputing scores in a complete\\nsearch system\\nChapter 6 developed the theory underlying term weighting in documents\\nfor the purposes of scoring, leading up to vector space models and the basic\\ncosine scoring algorithm of Section 6.3.3 (page 124). In this chapter we be-\\ngin in Section 7.1 with heuristics for speeding up this computation; many of\\nthese heuristics achieve their speed at the risk of not ﬁnding quite the top K\\ndocuments matching the query. Some of these heuristics generalize beyond\\ncosine scoring. With Section 7.1 in place, we have essentially all the compo-\\nnents needed for a complete search engine. We therefore take a step back\\nfrom cosine scoring, to the more general problem of computing scores in a\\nsearch engine. In Section 7.2 we outline a complete search engine, includ-\\ning indexes and structures to support not only cosine scoring but also more\\ngeneral ranking factors such as query term proximity. We describe how all\\nof the various pieces ﬁt together in Section 7.2.4. We conclude this chapter\\nwith Section 7.3, where we discuss how the vector space model for free text\\nqueries interacts with common query operators.\\n7.1\\nEfﬁcient scoring and ranking\\nWe begin by recapping the algorithm of Figure 6.14. For a query such as q =\\njealous gossip, two observations are immediate:\\n1. The unit vector ⃗v(q) has only two non-zero components.\\n2. In the absence of any weighting for query terms, these non-zero compo-\\nnents are equal – in this case, both equal 0.707.\\nFor the purpose of ranking the documents matching this query, we are\\nreally interested in the relative (rather than absolute) scores of the documents\\nin the collection. To this end, it sufﬁces to compute the cosine similarity from\\neach document unit vector ⃗v(d) to ⃗V(q) (in which all non-zero components\\nof the query vector are set to 1), rather than to the unit vector ⃗v(q). For any\\n', 'Online edition (c)\\n2009 Cambridge UP\\n136\\n7\\nComputing scores in a complete search system\\nFASTCOSINESCORE(q)\\n1\\nﬂoat Scores[N] = 0\\n2\\nfor each d\\n3\\ndo Initialize Length[d] to the length of doc d\\n4\\nfor each query term t\\n5\\ndo calculate wt,q and fetch postings list for t\\n6\\nfor each pair(d, tft,d) in postings list\\n7\\ndo add wft,d to Scores[d]\\n8\\nRead the array Length[d]\\n9\\nfor each d\\n10\\ndo Divide Scores[d] by Length[d]\\n11\\nreturn Top K components of Scores[]\\n◮Figure 7.1\\nA faster algorithm for vector space scores.\\ntwo documents d1, d2\\n⃗V(q) ·⃗v(d1) > ⃗V(q) ·⃗v(d2) ⇔⃗v(q) ·⃗v(d1) > ⃗v(q) ·⃗v(d2).\\n(7.1)\\nFor any document d, the cosine similarity ⃗V(q) ·⃗v(d) is the weighted sum,\\nover all terms in the query q, of the weights of those terms in d. This in turn\\ncan be computed by a postings intersection exactly as in the algorithm of\\nFigure 6.14, with line 8 altered since we take wt,q to be 1 so that the multiply-\\nadd in that step becomes just an addition; the result is shown in Figure 7.1.\\nWe walk through the postings in the inverted index for the terms in q, accu-\\nmulating the total score for each document – very much as in processing a\\nBoolean query, except we assign a positive score to each document that ap-\\npears in any of the postings being traversed. As mentioned in Section 6.3.3\\nwe maintain an idf value for each dictionary term and a tf value for each\\npostings entry. This scheme computes a score for every document in the\\npostings of any of the query terms; the total number of such documents may\\nbe considerably smaller than N.\\nGiven these scores, the ﬁnal step before presenting results to a user is to\\npick out the K highest-scoring documents. While one could sort the complete\\nset of scores, a better approach is to use a heap to retrieve only the top K\\ndocuments in order. Where J is the number of documents with non-zero\\ncosine scores, constructing such a heap can be performed in 2J comparison\\nsteps, following which each of the K highest scoring documents can be “read\\noff” the heap with log J comparison steps.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n137\\n7.1.1\\nInexact top K document retrieval\\nThus far, we have focused on retrieving precisely the K highest-scoring doc-\\numents for a query. We now consider schemes by which we produce K doc-\\numents that are likely to be among the K highest scoring documents for a\\nquery. In doing so, we hope to dramatically lower the cost of computing\\nthe K documents we output, without materially altering the user’s perceived\\nrelevance of the top K results. Consequently, in most applications it sufﬁces\\nto retrieve K documents whose scores are very close to those of the K best.\\nIn the sections that follow we detail schemes that retrieve K such documents\\nwhile potentially avoiding computing scores for most of the N documents in\\nthe collection.\\nSuch inexact top-K retrieval is not necessarily, from the user’s perspective,\\na bad thing. The top K documents by the cosine measure are in any case not\\nnecessarily the K best for the query: cosine similarity is only a proxy for the\\nuser’s perceived relevance. In Sections 7.1.2–7.1.6 below, we give heuristics\\nusing which we are likely to retrieve K documents with cosine scores close\\nto those of the top K documents. The principal cost in computing the out-\\nput stems from computing cosine similarities between the query and a large\\nnumber of documents. Having a large number of documents in contention\\nalso increases the selection cost in the ﬁnal stage of culling the top K docu-\\nments from a heap. We now consider a series of ideas designed to eliminate\\na large number of documents without computing their cosine scores. The\\nheuristics have the following two-step scheme:\\n1. Find a set A of documents that are contenders, where K < |A| ≪N. A\\ndoes not necessarily contain the K top-scoring documents for the query,\\nbut is likely to have many documents with scores near those of the top K.\\n2. Return the K top-scoring documents in A.\\nFrom the descriptions of these ideas it will be clear that many of them require\\nparameters to be tuned to the collection and application at hand; pointers\\nto experience in setting these parameters may be found at the end of this\\nchapter. It should also be noted that most of these heuristics are well-suited\\nto free text queries, but not for Boolean or phrase queries.\\n7.1.2\\nIndex elimination\\nFor a multi-term query q, it is clear we only consider documents containing at\\nleast one of the query terms. We can take this a step further using additional\\nheuristics:\\n1. We only consider documents containing terms whose idf exceeds a preset\\nthreshold. Thus, in the postings traversal, we only traverse the postings\\n', 'Online edition (c)\\n2009 Cambridge UP\\n138\\n7\\nComputing scores in a complete search system\\nfor terms with high idf. This has a fairly signiﬁcant beneﬁt: the post-\\nings lists of low-idf terms are generally long; with these removed from\\ncontention, the set of documents for which we compute cosines is greatly\\nreduced. One way of viewing this heuristic: low-idf terms are treated as\\nstop words and do not contribute to scoring. For instance, on the query\\ncatcher in the rye, we only traverse the postings for catcher and rye. The\\ncutoff threshold can of course be adapted in a query-dependent manner.\\n2. We only consider documents that contain many (and as a special case,\\nall) of the query terms. This can be accomplished during the postings\\ntraversal; we only compute scores for documents containing all (or many)\\nof the query terms. A danger of this scheme is that by requiring all (or\\neven many) query terms to be present in a document before considering\\nit for cosine computation, we may end up with fewer than K candidate\\ndocuments in the output. This issue will discussed further in Section 7.2.1.\\n7.1.3\\nChampion lists\\nThe idea of champion lists (sometimes also called fancy lists or top docs) is to\\nprecompute, for each term t in the dictionary, the set of the r documents\\nwith the highest weights for t; the value of r is chosen in advance. For tf-\\nidf weighting, these would be the r documents with the highest tf values for\\nterm t. We call this set of r documents the champion list for term t.\\nNow, given a query q we create a set A as follows: we take the union\\nof the champion lists for each of the terms comprising q. We now restrict\\ncosine computation to only the documents in A. A critical parameter in this\\nscheme is the value r, which is highly application dependent. Intuitively, r\\nshould be large compared with K, especially if we use any form of the index\\nelimination described in Section 7.1.2. One issue here is that the value r is set\\nat the time of index construction, whereas K is application dependent and\\nmay not be available until the query is received; as a result we may (as in the\\ncase of index elimination) ﬁnd ourselves with a set A that has fewer than K\\ndocuments. There is no reason to have the same value of r for all terms in the\\ndictionary; it could for instance be set to be higher for rarer terms.\\n7.1.4\\nStatic quality scores and ordering\\nWe now further develop the idea of champion lists, in the somewhat more\\ngeneral setting of static quality scores. In many search engines, we have avail-\\nSTATIC QUALITY\\nSCORES\\nable a measure of quality g(d) for each document d that is query-independent\\nand thus static. This quality measure may be viewed as a number between\\nzero and one. For instance, in the context of news stories on the web, g(d)\\nmay be derived from the number of favorable reviews of the story by web\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n139\\n◮Figure 7.2\\nA static quality-ordered index. In this example we assume that Doc1,\\nDoc2 and Doc3 respectively have static quality scores g(1) = 0.25, g(2) = 0.5, g(3) =\\n1.\\nsurfers. Section 4.6 (page 80) provides further discussion on this topic, as\\ndoes Chapter 21 in the context of web search.\\nThe net score for a document d is some combination of g(d) together with\\nthe query-dependent score induced (say) by (6.12). The precise combination\\nmay be determined by the learning methods of Section 6.1.2, to be developed\\nfurther in Section 15.4.1; but for the purposes of our exposition here, let us\\nconsider a simple sum:\\nnet-score(q, d) = g(d) +\\n⃗V(q) · ⃗V(d)\\n|⃗V(q)||⃗V(d)|\\n.\\n(7.2)\\nIn this simple form, the static quality g(d) and the query-dependent score\\nfrom (6.10) have equal contributions, assuming each is between 0 and 1.\\nOther relative weightings are possible; the effectiveness of our heuristics will\\ndepend on the speciﬁc relative weighting.\\nFirst, consider ordering the documents in the postings list for each term by\\ndecreasing value of g(d). This allows us to perform the postings intersection\\nalgorithm of Figure 1.6. In order to perform the intersection by a single pass\\nthrough the postings of each query term, the algorithm of Figure 1.6 relied on\\nthe postings being ordered by document IDs. But in fact, we only required\\nthat all postings be ordered by a single common ordering; here we rely on the\\ng(d) values to provide this common ordering. This is illustrated in Figure 7.2,\\nwhere the postings are ordered in decreasing order of g(d).\\nThe ﬁrst idea is a direct extension of champion lists: for a well-chosen\\nvalue r, we maintain for each term t a global champion list of the r documents\\n', 'Online edition (c)\\n2009 Cambridge UP\\n140\\n7\\nComputing scores in a complete search system\\nwith the highest values for g(d) + tf-idft,d. The list itself is, like all the post-\\nings lists considered so far, sorted by a common order (either by document\\nIDs or by static quality). Then at query time, we only compute the net scores\\n(7.2) for documents in the union of these global champion lists. Intuitively,\\nthis has the effect of focusing on documents likely to have large net scores.\\nWe conclude the discussion of global champion lists with one further idea.\\nWe maintain for each term t two postings lists consisting of disjoint sets of\\ndocuments, each sorted by g(d) values. The ﬁrst list, which we call high,\\ncontains the m documents with the highest tf values for t. The second list,\\nwhich we call low, contains all other documents containing t. When process-\\ning a query, we ﬁrst scan only the high lists of the query terms, computing\\nnet scores for any document on the high lists of all (or more than a certain\\nnumber of) query terms. If we obtain scores for K documents in the process,\\nwe terminate. If not, we continue the scanning into the low lists, scoring doc-\\numents in these postings lists. This idea is developed further in Section 7.2.1.\\n7.1.5\\nImpact ordering\\nIn all the postings lists described thus far, we order the documents con-\\nsistently by some common ordering: typically by document ID but in Sec-\\ntion 7.1.4 by static quality scores. As noted at the end of Section 6.3.3, such a\\ncommon ordering supports the concurrent traversal of all of the query terms’\\npostings lists, computing the score for each document as we encounter it.\\nComputing scores in this manner is sometimes referred to as document-at-a-\\ntime scoring. We will now introduce a technique for inexact top-K retrieval\\nin which the postings are not all ordered by a common ordering, thereby\\nprecluding such a concurrent traversal. We will therefore require scores to\\nbe “accumulated” one term at a time as in the scheme of Figure 6.14, so that\\nwe have term-at-a-time scoring.\\nThe idea is to order the documents d in the postings list of term t by\\ndecreasing order of tft,d. Thus, the ordering of documents will vary from\\none postings list to another, and we cannot compute scores by a concurrent\\ntraversal of the postings lists of all query terms. Given postings lists ordered\\nby decreasing order of tft,d, two ideas have been found to signiﬁcantly lower\\nthe number of documents for which we accumulate scores: (1) when travers-\\ning the postings list for a query term t, we stop after considering a preﬁx\\nof the postings list – either after a ﬁxed number of documents r have been\\nseen, or after the value of tft,d has dropped below a threshold; (2) when ac-\\ncumulating scores in the outer loop of Figure 6.14, we consider the query\\nterms in decreasing order of idf, so that the query terms likely to contribute\\nthe most to the ﬁnal scores are considered ﬁrst. This latter idea too can be\\nadaptive at the time of processing a query: as we get to query terms with\\nlower idf, we can determine whether to proceed based on the changes in\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.1\\nEfﬁcient scoring and ranking\\n141\\ndocument scores from processing the previous query term. If these changes\\nare minimal, we may omit accumulation from the remaining query terms, or\\nalternatively process shorter preﬁxes of their postings lists.\\nThese ideas form a common generalization of the methods introduced in\\nSections 7.1.2–7.1.4. We may also implement a version of static ordering in\\nwhich each postings list is ordered by an additive combination of static and\\nquery-dependent scores. We would again lose the consistency of ordering\\nacross postings, thereby having to process query terms one at time accumu-\\nlating scores for all documents as we go along. Depending on the particular\\nscoring function, the postings list for a document may be ordered by other\\nquantities than term frequency; under this more general setting, this idea is\\nknown as impact ordering.\\n7.1.6\\nCluster pruning\\nIn cluster pruning we have a preprocessing step during which we cluster the\\ndocument vectors. Then at query time, we consider only documents in a\\nsmall number of clusters as candidates for which we compute cosine scores.\\nSpeciﬁcally, the preprocessing step is as follows:\\n1. Pick\\n√\\nN documents at random from the collection. Call these leaders.\\n2. For each document that is not a leader, we compute its nearest leader.\\nWe refer to documents that are not leaders as followers. Intuitively, in the par-\\ntition of the followers induced by the use of\\n√\\nN randomly chosen leaders,\\nthe expected number of followers for each leader is ≈N/\\n√\\nN =\\n√\\nN. Next,\\nquery processing proceeds as follows:\\n1. Given a query q, ﬁnd the leader L that is closest to q. This entails comput-\\ning cosine similarities from q to each of the\\n√\\nN leaders.\\n2. The candidate set A consists of L together with its followers. We compute\\nthe cosine scores for all documents in this candidate set.\\nThe use of randomly chosen leaders for clustering is fast and likely to re-\\nﬂect the distribution of the document vectors in the vector space: a region\\nof the vector space that is dense in documents is likely to produce multi-\\nple leaders and thus a ﬁner partition into sub-regions. This illustrated in\\nFigure 7.3.\\nVariations of cluster pruning introduce additional parameters b1 and b2,\\nboth of which are positive integers. In the pre-processing step we attach\\neach follower to its b1 closest leaders, rather than a single closest leader. At\\nquery time we consider the b2 leaders closest to the query q. Clearly, the basic\\nscheme above corresponds to the case b1 = b2 = 1. Further, increasing b1 or\\n', 'Online edition (c)\\n2009 Cambridge UP\\n142\\n7\\nComputing scores in a complete search system\\n◮Figure 7.3\\nCluster pruning.\\nb2 increases the likelihood of ﬁnding K documents that are more likely to be\\nin the set of true top-scoring K documents, at the expense of more compu-\\ntation. We reiterate this approach when describing clustering in Chapter 16\\n(page 354).\\n?\\nExercise 7.1\\nWe suggested above (Figure 7.2) that the postings for static quality ordering be in\\ndecreasing order of g(d). Why do we use the decreasing rather than the increasing\\norder?\\nExercise 7.2\\nWhen discussing champion lists, we simply used the r documents with the largest tf\\nvalues to create the champion list for t. But when considering global champion lists,\\nwe used idf as well, identifying documents with the largest values of g(d) + tf-idft,d.\\nWhy do we differentiate between these two cases?\\nExercise 7.3\\nIf we were to only have one-term queries, explain why the use of global champion\\nlists with r = K sufﬁces for identifying the K highest scoring documents. What is a\\nsimple modiﬁcation to this idea if we were to only have s-term queries for any ﬁxed\\ninteger s > 1?\\nExercise 7.4\\nExplain how the common global ordering by g(d) values in all high and low lists\\nhelps make the score computation efﬁcient.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.2\\nComponents of an information retrieval system\\n143\\nExercise 7.5\\nConsider again the data of Exercise 6.23 with nnn.atc for the query-dependent scor-\\ning. Suppose that we were given static quality scores of 1 for Doc1 and 2 for Doc2.\\nDetermine under Equation (7.2) what ranges of static quality score for Doc3 result in\\nit being the ﬁrst, second or third result for the query best car insurance.\\nExercise 7.6\\nSketch the frequency-ordered postings for the data in Figure 6.9.\\nExercise 7.7\\nLet the static quality scores for Doc1, Doc2 and Doc3 in Figure 6.11 be respectively\\n0.25, 0.5 and 1. Sketch the postings for impact ordering when each postings list is\\nordered by the sum of the static quality score and the Euclidean normalized tf values\\nin Figure 6.11.\\nExercise 7.8\\nThe nearest-neighbor problem in the plane is the following: given a set of N data\\npoints on the plane, we preprocess them into some data structure such that, given\\na query point Q, we seek the point in N that is closest to Q in Euclidean distance.\\nClearly cluster pruning can be used as an approach to the nearest-neighbor problem\\nin the plane, if we wished to avoid computing the distance from Q to every one of\\nthe query points. Devise a simple example on the plane so that with two leaders, the\\nanswer returned by cluster pruning is incorrect (it is not the data point closest to Q).\\n7.2\\nComponents of an information retrieval system\\nIn this section we combine the ideas developed so far to describe a rudimen-\\ntary search system that retrieves and scores documents. We ﬁrst develop\\nfurther ideas for scoring, beyond vector spaces. Following this, we will put\\ntogether all of these elements to outline a complete system. Because we con-\\nsider a complete system, we do not restrict ourselves to vector space retrieval\\nin this section. Indeed, our complete system will have provisions for vector\\nspace as well as other query operators and forms of retrieval. In Section 7.3\\nwe will return to how vector space queries interact with other query opera-\\ntors.\\n7.2.1\\nTiered indexes\\nWe mentioned in Section 7.1.2 that when using heuristics such as index elim-\\nination for inexact top-K retrieval, we may occasionally ﬁnd ourselves with\\na set A of contenders that has fewer than K documents. A common solution\\nto this issue is the user of tiered indexes, which may be viewed as a gener-\\nTIERED INDEXES\\nalization of champion lists. We illustrate this idea in Figure 7.4, where we\\nrepresent the documents and terms of Figure 6.9. In this example we set a tf\\nthreshold of 20 for tier 1 and 10 for tier 2, meaning that the tier 1 index only\\nhas postings entries with tf values exceeding 20, while the tier 2 index only\\n', 'Online edition (c)\\n2009 Cambridge UP\\n144\\n7\\nComputing scores in a complete search system\\n◮Figure 7.4\\nTiered indexes. If we fail to get K results from tier 1, query processing\\n“falls back” to tier 2, and so on. Within each tier, postings are ordered by document\\nID.\\nhas postings entries with tf values exceeding 10. In this example we have\\nchosen to order the postings entries within a tier by document ID.\\n7.2.2\\nQuery-term proximity\\nEspecially for free text queries on the web (Chapter 19), users prefer a doc-\\nument in which most or all of the query terms appear close to each other,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.2\\nComponents of an information retrieval system\\n145\\nbecause this is evidence that the document has text focused on their query\\nintent. Consider a query with two or more query terms, t1, t2, . . . , tk. Let ω\\nbe the width of the smallest window in a document d that contains all the\\nquery terms, measured in the number of words in the window. For instance,\\nif the document were to simply consist of the sentence The quality of mercy\\nis not strained, the smallest window for the query strained mercy would be 4.\\nIntuitively, the smaller that ω is, the better that d matches the query. In cases\\nwhere the document does not contain all of the query terms, we can set ω\\nto be some enormous number. We could also consider variants in which\\nonly words that are not stop words are considered in computing ω. Such\\nproximity-weighted scoring functions are a departure from pure cosine sim-\\nilarity and closer to the “soft conjunctive” semantics that Google and other\\nweb search engines evidently use.\\nHow can we design such a proximity-weighted scoring function to depend\\nPROXIMITY WEIGHTING\\non ω? The simplest answer relies on a “hand coding” technique we introduce\\nbelow in Section 7.2.3. A more scalable approach goes back to Section 6.1.2 –\\nwe treat the integer ω as yet another feature in the scoring function, whose\\nimportance is assigned by machine learning, as will be developed further in\\nSection 15.4.1.\\n7.2.3\\nDesigning parsing and scoring functions\\nCommon search interfaces, particularly for consumer-facing search applica-\\ntions on the web, tend to mask query operators from the end user. The intent\\nis to hide the complexity of these operators from the largely non-technical au-\\ndience for such applications, inviting free text queries. Given such interfaces,\\nhow should a search equipped with indexes for various retrieval operators\\ntreat a query such as rising interest rates? More generally, given the various fac-\\ntors we have studied that could affect the score of a document, how should\\nwe combine these features?\\nThe answer of course depends on the user population, the query distri-\\nbution and the collection of documents. Typically, a query parser is used to\\ntranslate the user-speciﬁed keywords into a query with various operators\\nthat is executed against the underlying indexes. Sometimes, this execution\\ncan entail multiple queries against the underlying indexes; for example, the\\nquery parser may issue a stream of queries:\\n1. Run the user-generated query string as a phrase query. Rank them by\\nvector space scoring using as query the vector consisting of the 3 terms\\nrising interest rates.\\n2. If fewer than ten documents contain the phrase rising interest rates, run the\\ntwo 2-term phrase queries rising interest and interest rates; rank these using\\nvector space scoring, as well.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n146\\n7\\nComputing scores in a complete search system\\n3. If we still have fewer than ten results, run the vector space query consist-\\ning of the three individual query terms.\\nEach of these steps (if invoked) may yield a list of scored documents, for\\neach of which we compute a score. This score must combine contributions\\nfrom vector space scoring, static quality, proximity weighting and potentially\\nother factors – particularly since a document may appear in the lists from\\nmultiple steps. This demands an aggregate scoring function that accumulates\\nEVIDENCE\\nACCUMULATION\\nevidence of a document’s relevance from multiple sources. How do we devise\\na query parser and how do we devise the aggregate scoring function?\\nThe answer depends on the setting. In many enterprise settings we have\\napplication builders who make use of a toolkit of available scoring opera-\\ntors, along with a query parsing layer, with which to manually conﬁgure\\nthe scoring function as well as the query parser. Such application builders\\nmake use of the available zones, metadata and knowledge of typical doc-\\numents and queries to tune the parsing and scoring. In collections whose\\ncharacteristics change infrequently (in an enterprise application, signiﬁcant\\nchanges in collection and query characteristics typically happen with infre-\\nquent events such as the introduction of new document formats or document\\nmanagement systems, or a merger with another company). Web search on\\nthe other hand is faced with a constantly changing document collection with\\nnew characteristics being introduced all the time. It is also a setting in which\\nthe number of scoring factors can run into the hundreds, making hand-tuned\\nscoring a difﬁcult exercise. To address this, it is becoming increasingly com-\\nmon to use machine-learned scoring, extending the ideas we introduced in\\nSection 6.1.2, as will be discussed further in Section 15.4.1.\\n7.2.4\\nPutting it all together\\nWe have now studied all the components necessary for a basic search system\\nthat supports free text queries as well as Boolean, zone and ﬁeld queries. We\\nbrieﬂy review how the various pieces ﬁt together into an overall system; this\\nis depicted in Figure 7.5.\\nIn this ﬁgure, documents stream in from the left for parsing and linguis-\\ntic processing (language and format detection, tokenization and stemming).\\nThe resulting stream of tokens feeds into two modules. First, we retain a\\ncopy of each parsed document in a document cache. This will enable us\\nto generate results snippets: snippets of text accompanying each document\\nin the results list for a query. This snippet tries to give a succinct explana-\\ntion to the user of why the document matches the query. The automatic\\ngeneration of such snippets is the subject of Section 8.7.\\nA second copy\\nof the tokens is fed to a bank of indexers that create a bank of indexes in-\\ncluding zone and ﬁeld indexes that store the metadata for each document,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.3\\nVector space scoring and query operator interaction\\n147\\n◮Figure 7.5\\nA complete search system. Data paths are shown primarily for a free\\ntext query.\\n(tiered) positional indexes, indexes for spelling correction and other tolerant\\nretrieval, and structures for accelerating inexact top-K retrieval. A free text\\nuser query (top center) is sent down to the indexes both directly and through\\na module for generating spelling-correction candidates. As noted in Chap-\\nter 3 the latter may optionally be invoked only when the original query fails\\nto retrieve enough results. Retrieved documents (dark arrow) are passed\\nto a scoring module that computes scores based on machine-learned rank-\\ning (MLR), a technique that builds on Section 6.1.2 (to be further developed\\nin Section 15.4.1) for scoring and ranking documents. Finally, these ranked\\ndocuments are rendered as a results page.\\n?\\nExercise 7.9\\nExplain how the postings intersection algorithm ﬁrst introduced in Section 1.3 can be\\nadapted to ﬁnd the smallest integer ω that contains all query terms.\\nExercise 7.10\\nAdapt this procedure to work when not all query terms are present in a document.\\n7.3\\nVector space scoring and query operator interaction\\nWe introduced the vector space model as a paradigm for free text queries.\\nWe conclude this chapter by discussing how the vector space scoring model\\n', 'Online edition (c)\\n2009 Cambridge UP\\n148\\n7\\nComputing scores in a complete search system\\nrelates to the query operators we have studied in earlier chapters. The re-\\nlationship should be viewed at two levels: in terms of the expressiveness\\nof queries that a sophisticated user may pose, and in terms of the index that\\nsupports the evaluation of the various retrieval methods. In building a search\\nengine, we may opt to support multiple query operators for an end user. In\\ndoing so we need to understand what components of the index can be shared\\nfor executing various query operators, as well as how to handle user queries\\nthat mix various query operators.\\nVector space scoring supports so-called free text retrieval, in which a query\\nis speciﬁed as a set of words without any query operators connecting them. It\\nallows documents matching the query to be scored and thus ranked, unlike\\nthe Boolean, wildcard and phrase queries studied earlier. Classically, the\\ninterpretation of such free text queries was that at least one of the query terms\\nbe present in any retrieved document. However more recently, web search\\nengines such as Google have popularized the notion that a set of terms typed\\ninto their query boxes (thus on the face of it, a free text query) carries the\\nsemantics of a conjunctive query that only retrieves documents containing\\nall or most query terms.\\nBoolean retrieval\\nClearly a vector space index can be used to answer Boolean queries, as long\\nas the weight of a term t in the document vector for d is non-zero when-\\never t occurs in d. The reverse is not true, since a Boolean index does not by\\ndefault maintain term weight information. There is no easy way of combin-\\ning vector space and Boolean queries from a user’s standpoint: vector space\\nqueries are fundamentally a form of evidence accumulation, where the pres-\\nence of more query terms in a document adds to the score of a document.\\nBoolean retrieval on the other hand, requires a user to specify a formula\\nfor selecting documents through the presence (or absence) of speciﬁc com-\\nbinations of keywords, without inducing any relative ordering among them.\\nMathematically, it is in fact possible to invoke so-called p-norms to combine\\nBoolean and vector space queries, but we know of no system that makes use\\nof this fact.\\nWildcard queries\\nWildcard and vector space queries require different indexes, except at the\\nbasic level that both can be implemented using postings and a dictionary\\n(e.g., a dictionary of trigrams for wildcard queries). If a search engine allows\\na user to specify a wildcard operator as part of a free text query (for instance,\\nthe query rom* restaurant), we may interpret the wildcard component of the\\nquery as spawning multiple terms in the vector space (in this example, rome\\n', 'Online edition (c)\\n2009 Cambridge UP\\n7.4\\nReferences and further reading\\n149\\nand roman would be two such terms) all of which are added to the query\\nvector. The vector space query is then executed as usual, with matching\\ndocuments being scored and ranked; thus a document containing both rome\\nand roma is likely to be scored higher than another containing only one of\\nthem. The exact score ordering will of course depend on the relative weights\\nof each term in matching documents.\\nPhrase queries\\nThe representation of documents as vectors is fundamentally lossy: the rel-\\native order of terms in a document is lost in the encoding of a document as\\na vector. Even if we were to try and somehow treat every biword as a term\\n(and thus an axis in the vector space), the weights on different axes not in-\\ndependent: for instance the phrase German shepherd gets encoded in the axis\\ngerman shepherd, but immediately has a non-zero weight on the axes german\\nand shepherd. Further, notions such as idf would have to be extended to such\\nbiwords. Thus an index built for vector space retrieval cannot, in general, be\\nused for phrase queries. Moreover, there is no way of demanding a vector\\nspace score for a phrase query — we only know the relative weights of each\\nterm in a document.\\nOn the query german shepherd, we could use vector space retrieval to iden-\\ntify documents heavy in these two terms, with no way of prescribing that\\nthey occur consecutively. Phrase retrieval, on the other hand, tells us of the\\nexistence of the phrase german shepherd in a document, without any indi-\\ncation of the relative frequency or weight of this phrase. While these two\\nretrieval paradigms (phrase and vector space) consequently have different\\nimplementations in terms of indexes and retrieval algorithms, they can in\\nsome cases be combined usefully, as in the three-step example of query pars-\\ning in Section 7.2.3.\\n7.4\\nReferences and further reading\\nHeuristics for fast query processing with early termination are described by\\nAnh et al. (2001), Garcia et al. (2004), Anh and Moffat (2006b), Persin et al.\\n(1996). Cluster pruning is investigated by Singitham et al. (2004) and by\\nChierichetti et al. (2007); see also Section 16.6 (page 372). Champion lists are\\ndescribed in Persin (1994) and (under the name top docs) in Brown (1995),\\nTOP DOCS\\nand further developed in Brin and Page (1998), Long and Suel (2003). While\\nthese heuristics are well-suited to free text queries that can be viewed as vec-\\ntors, they complicate phrase queries; see Anh and Moffat (2006c) for an index\\nstructure that supports both weighted and Boolean/phrase searches. Carmel\\net al. (2001) Clarke et al. (2000) and Song et al. (2005) treat the use of query\\n', 'Online edition (c)\\n2009 Cambridge UP\\n150\\n7\\nComputing scores in a complete search system\\nterm proximity in assessing relevance. Pioneering work on learning of rank-\\ning functions was done by Fuhr (1989), Fuhr and Pfeifer (1994), Cooper et al.\\n(1994), Bartell (1994), Bartell et al. (1998) and by Cohen et al. (1998).\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n151\\n8\\nEvaluation in information\\nretrieval\\nWe have seen in the preceding chapters many alternatives in designing an IR\\nsystem. How do we know which of these techniques are effective in which\\napplications? Should we use stop lists? Should we stem? Should we use in-\\nverse document frequency weighting? Information retrieval has developed\\nas a highly empirical discipline, requiring careful and thorough evaluation to\\ndemonstrate the superior performance of novel techniques on representative\\ndocument collections.\\nIn this chapter we begin with a discussion of measuring the effectiveness\\nof IR systems (Section 8.1) and the test collections that are most often used\\nfor this purpose (Section 8.2). We then present the straightforward notion of\\nrelevant and nonrelevant documents and the formal evaluation methodol-\\nogy that has been developed for evaluating unranked retrieval results (Sec-\\ntion 8.3). This includes explaining the kinds of evaluation measures that\\nare standardly used for document retrieval and related tasks like text clas-\\nsiﬁcation and why they are appropriate. We then extend these notions and\\ndevelop further measures for evaluating ranked retrieval results (Section 8.4)\\nand discuss developing reliable and informative test collections (Section 8.5).\\nWe then step back to introduce the notion of user utility, and how it is ap-\\nproximated by the use of document relevance (Section 8.6). The key utility\\nmeasure is user happiness. Speed of response and the size of the index are\\nfactors in user happiness. It seems reasonable to assume that relevance of\\nresults is the most important factor: blindingly fast, useless answers do not\\nmake a user happy. However, user perceptions do not always coincide with\\nsystem designers’ notions of quality. For example, user happiness commonly\\ndepends very strongly on user interface design issues, including the layout,\\nclarity, and responsiveness of the user interface, which are independent of\\nthe quality of the results returned. We touch on other measures of the qual-\\nity of a system, in particular the generation of high-quality result summary\\nsnippets, which strongly inﬂuence user utility, but are not measured in the\\nbasic relevance ranking paradigm (Section 8.7).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n152\\n8\\nEvaluation in information retrieval\\n8.1\\nInformation retrieval system evaluation\\nTo measure ad hoc information retrieval effectiveness in the standard way,\\nwe need a test collection consisting of three things:\\n1. A document collection\\n2. A test suite of information needs, expressible as queries\\n3. A set of relevance judgments, standardly a binary assessment of either\\nrelevant or nonrelevant for each query-document pair.\\nThe standard approach to information retrieval system evaluation revolves\\naround the notion of relevant and nonrelevant documents. With respect to a\\nRELEVANCE\\nuser information need, a document in the test collection is given a binary\\nclassiﬁcation as either relevant or nonrelevant. This decision is referred to as\\nthe gold standard or ground truth judgment of relevance. The test document\\nGOLD STANDARD\\nGROUND TRUTH\\ncollection and suite of information needs have to be of a reasonable size:\\nyou need to average performance over fairly large test sets, as results are\\nhighly variable over different documents and information needs. As a rule\\nof thumb, 50 information needs has usually been found to be a sufﬁcient\\nminimum.\\nRelevance is assessed relative to an information need, not a query. For\\nINFORMATION NEED\\nexample, an information need might be:\\nInformation on whether drinking red wine is more effective at reduc-\\ning your risk of heart attacks than white wine.\\nThis might be translated into a query such as:\\nwine AND red AND white AND heart AND attack AND effective\\nA document is relevant if it addresses the stated information need, not be-\\ncause it just happens to contain all the words in the query. This distinction is\\noften misunderstood in practice, because the information need is not overt.\\nBut, nevertheless, an information need is present. If a user types python into a\\nweb search engine, they might be wanting to know where they can purchase\\na pet python. Or they might be wanting information on the programming\\nlanguage Python. From a one word query, it is very difﬁcult for a system to\\nknow what the information need is. But, nevertheless, the user has one, and\\ncan judge the returned results on the basis of their relevance to it. To evalu-\\nate a system, we require an overt expression of an information need, which\\ncan be used for judging returned documents as relevant or nonrelevant. At\\nthis point, we make a simpliﬁcation: relevance can reasonably be thought\\nof as a scale, with some documents highly relevant and others marginally\\nso. But for the moment, we will use just a binary decision of relevance. We\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.2\\nStandard test collections\\n153\\ndiscuss the reasons for using binary relevance judgments and alternatives in\\nSection 8.5.1.\\nMany systems contain various weights (often known as parameters) that\\ncan be adjusted to tune system performance. It is wrong to report results on\\na test collection which were obtained by tuning these parameters to maxi-\\nmize performance on that collection. That is because such tuning overstates\\nthe expected performance of the system, because the weights will be set to\\nmaximize performance on one particular set of queries rather than for a ran-\\ndom sample of queries. In such cases, the correct procedure is to have one\\nor more development test collections, and to tune the parameters on the devel-\\nDEVELOPMENT TEST\\nCOLLECTION\\nopment test collection. The tester then runs the system with those weights\\non the test collection and reports the results on that collection as an unbiased\\nestimate of performance.\\n8.2\\nStandard test collections\\nHere is a list of the most standard test collections and evaluation series. We\\nfocus particularly on test collections for ad hoc information retrieval system\\nevaluation, but also mention a couple of similar test collections for text clas-\\nsiﬁcation.\\nThe Cranﬁeld collection. This was the pioneering test collection in allowing\\nCRANFIELD\\nprecise quantitative measures of information retrieval effectiveness, but\\nis nowadays too small for anything but the most elementary pilot experi-\\nments. Collected in the United Kingdom starting in the late 1950s, it con-\\ntains 1398 abstracts of aerodynamics journal articles, a set of 225 queries,\\nand exhaustive relevance judgments of all (query, document) pairs.\\nText Retrieval Conference (TREC). The U.S. National Institute of Standards\\nTREC\\nand Technology (NIST) has run a large IR test bed evaluation series since\\n1992. Within this framework, there have been many tracks over a range\\nof different test collections, but the best known test collections are the\\nones used for the TREC Ad Hoc track during the ﬁrst 8 TREC evaluations\\nbetween 1992 and 1999. In total, these test collections comprise 6 CDs\\ncontaining 1.89 million documents (mainly, but not exclusively, newswire\\narticles) and relevance judgments for 450 information needs, which are\\ncalled topics and speciﬁed in detailed text passages. Individual test col-\\nlections are deﬁned over different subsets of this data. The early TRECs\\neach consisted of 50 information needs, evaluated over different but over-\\nlapping sets of documents. TRECs 6–8 provide 150 information needs\\nover about 528,000 newswire and Foreign Broadcast Information Service\\narticles. This is probably the best subcollection to use in future work, be-\\ncause it is the largest and the topics are more consistent. Because the test\\n', 'Online edition (c)\\n2009 Cambridge UP\\n154\\n8\\nEvaluation in information retrieval\\ndocument collections are so large, there are no exhaustive relevance judg-\\nments. Rather, NIST assessors’ relevance judgments are available only for\\nthe documents that were among the top k returned for some system which\\nwas entered in the TREC evaluation for which the information need was\\ndeveloped.\\nIn more recent years, NIST has done evaluations on larger document col-\\nlections, including the 25 million page GOV2 web page collection. From\\nGOV2\\nthe beginning, the NIST test document collections were orders of magni-\\ntude larger than anything available to researchers previously and GOV2\\nis now the largest Web collection easily available for research purposes.\\nNevertheless, the size of GOV2 is still more than 2 orders of magnitude\\nsmaller than the current size of the document collections indexed by the\\nlarge web search companies.\\nNII Test Collections for IR Systems (NTCIR). The NTCIR project has built\\nNTCIR\\nvarious test collections of similar sizes to the TREC collections, focus-\\ning on East Asian language and cross-language information retrieval, where\\nCROSS-LANGUAGE\\nINFORMATION\\nRETRIEVAL\\nqueries are made in one language over a document collection containing\\ndocuments in one or more other languages. See: http://research.nii.ac.jp/ntcir/data/data-\\nen.html\\nCross Language Evaluation Forum (CLEF). This evaluation series has con-\\nCLEF\\ncentrated on European languages and cross-language information retrieval.\\nSee: http://www.clef-campaign.org/\\nReuters-21578 and Reuters-RCV1. For text classiﬁcation, the most used test\\nREUTERS\\ncollection has been the Reuters-21578 collection of 21578 newswire arti-\\ncles; see Chapter 13, page 279. More recently, Reuters released the much\\nlarger Reuters Corpus Volume 1 (RCV1), consisting of 806,791 documents;\\nsee Chapter 4, page 69. Its scale and rich annotation makes it a better basis\\nfor future research.\\n20 Newsgroups. This is another widely used text classiﬁcation collection,\\n20 NEWSGROUPS\\ncollected by Ken Lang. It consists of 1000 articles from each of 20 Usenet\\nnewsgroups (the newsgroup name being regarded as the category). After\\nthe removal of duplicate articles, as it is usually used, it contains 18941\\narticles.\\n8.3\\nEvaluation of unranked retrieval sets\\nGiven these ingredients, how is system effectiveness measured? The two\\nmost frequent and basic measures for information retrieval effectiveness are\\nprecision and recall. These are ﬁrst deﬁned for the simple case where an\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.3\\nEvaluation of unranked retrieval sets\\n155\\nIR system returns a set of documents for a query. We will see later how to\\nextend these notions to ranked retrieval situations.\\nPrecision (P) is the fraction of retrieved documents that are relevant\\nPRECISION\\nPrecision = #(relevant items retrieved)\\n#(retrieved items)\\n= P(relevant|retrieved)\\n(8.1)\\nRecall (R) is the fraction of relevant documents that are retrieved\\nRECALL\\nRecall = #(relevant items retrieved)\\n#(relevant items)\\n= P(retrieved|relevant)\\n(8.2)\\nThese notions can be made clear by examining the following contingency\\ntable:\\n(8.3)\\nRelevant\\nNonrelevant\\nRetrieved\\ntrue positives (tp)\\nfalse positives (fp)\\nNot retrieved\\nfalse negatives (fn)\\ntrue negatives (tn)\\nThen:\\nP\\n=\\ntp/(tp + f p)\\n(8.4)\\nR\\n=\\ntp/(tp + f n)\\nAn obvious alternative that may occur to the reader is to judge an infor-\\nmation retrieval system by its accuracy, that is, the fraction of its classiﬁca-\\nACCURACY\\ntions that are correct. In terms of the contingency table above, accuracy =\\n(tp + tn)/(tp + f p + f n + tn). This seems plausible, since there are two ac-\\ntual classes, relevant and nonrelevant, and an information retrieval system\\ncan be thought of as a two-class classiﬁer which attempts to label them as\\nsuch (it retrieves the subset of documents which it believes to be relevant).\\nThis is precisely the effectiveness measure often used for evaluating machine\\nlearning classiﬁcation problems.\\nThere is a good reason why accuracy is not an appropriate measure for\\ninformation retrieval problems. In almost all circumstances, the data is ex-\\ntremely skewed: normally over 99.9% of the documents are in the nonrele-\\nvant category. A system tuned to maximize accuracy can appear to perform\\nwell by simply deeming all documents nonrelevant to all queries. Even if the\\nsystem is quite good, trying to label some documents as relevant will almost\\nalways lead to a high rate of false positives. However, labeling all documents\\nas nonrelevant is completely unsatisfying to an information retrieval system\\nuser. Users are always going to want to see some documents, and can be\\n', 'Online edition (c)\\n2009 Cambridge UP\\n156\\n8\\nEvaluation in information retrieval\\nassumed to have a certain tolerance for seeing some false positives provid-\\ning that they get some useful information. The measures of precision and\\nrecall concentrate the evaluation on the return of true positives, asking what\\npercentage of the relevant documents have been found and how many false\\npositives have also been returned.\\nThe advantage of having the two numbers for precision and recall is that\\none is more important than the other in many circumstances. Typical web\\nsurfers would like every result on the ﬁrst page to be relevant (high preci-\\nsion) but have not the slightest interest in knowing let alone looking at every\\ndocument that is relevant. In contrast, various professional searchers such as\\nparalegals and intelligence analysts are very concerned with trying to get as\\nhigh recall as possible, and will tolerate fairly low precision results in order to\\nget it. Individuals searching their hard disks are also often interested in high\\nrecall searches. Nevertheless, the two quantities clearly trade off against one\\nanother: you can always get a recall of 1 (but very low precision) by retriev-\\ning all documents for all queries! Recall is a non-decreasing function of the\\nnumber of documents retrieved. On the other hand, in a good system, preci-\\nsion usually decreases as the number of documents retrieved is increased. In\\ngeneral we want to get some amount of recall while tolerating only a certain\\npercentage of false positives.\\nA single measure that trades off precision versus recall is the F measure,\\nF MEASURE\\nwhich is the weighted harmonic mean of precision and recall:\\nF =\\n1\\nα 1\\nP + (1 −α) 1\\nR\\n= (β2 + 1)PR\\nβ2P + R\\nwhere\\nβ2 = 1 −α\\nα\\n(8.5)\\nwhere α ∈[0, 1] and thus β2 ∈[0, ∞]. The default balanced F measure equally\\nweights precision and recall, which means making α = 1/2 or β = 1. It is\\ncommonly written as F1, which is short for Fβ=1, even though the formula-\\ntion in terms of α more transparently exhibits the F measure as a weighted\\nharmonic mean. When using β = 1, the formula on the right simpliﬁes to:\\nFβ=1 = 2PR\\nP + R\\n(8.6)\\nHowever, using an even weighting is not the only choice. Values of β < 1\\nemphasize precision, while values of β > 1 emphasize recall. For example, a\\nvalue of β = 3 or β = 5 might be used if recall is to be emphasized. Recall,\\nprecision, and the F measure are inherently measures between 0 and 1, but\\nthey are also very commonly written as percentages, on a scale between 0\\nand 100.\\nWhy do we use a harmonic mean rather than the simpler average (arith-\\nmetic mean)? Recall that we can always get 100% recall by just returning all\\ndocuments, and therefore we can always get a 50% arithmetic mean by the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.3\\nEvaluation of unranked retrieval sets\\n157\\n0\\n2\\n0\\n4\\n0\\n6\\n0\\n8\\n0\\n1\\n0\\n0\\n0\\n2\\n0\\n4\\n0\\n6\\n0\\n8\\n0\\n1\\n0\\n0\\nP\\nr\\ne\\nc\\ni\\ns\\ni\\no\\nn\\n(\\nR\\ne\\nc\\na\\nl\\nl\\nf\\ni\\nx\\ne\\nd\\na\\nt\\n7\\n0\\n%\\n)\\nM\\ni\\nn\\ni\\nm\\nu\\nm\\nM\\na\\nx\\ni\\nm\\nu\\nm\\nA\\nr\\ni\\nt\\nh\\nm\\ne\\nt\\ni\\nc\\nG\\ne\\no\\nm\\ne\\nt\\nr\\ni\\nc\\nH\\na\\nr\\nm\\no\\nn\\ni\\nc\\n◮Figure 8.1\\nGraph comparing the harmonic mean to other means.\\nThe graph\\nshows a slice through the calculation of various means of precision and recall for\\nthe ﬁxed recall value of 70%. The harmonic mean is always less than either the arith-\\nmetic or geometric mean, and often quite close to the minimum of the two numbers.\\nWhen the precision is also 70%, all the measures coincide.\\nsame process. This strongly suggests that the arithmetic mean is an unsuit-\\nable measure to use. In contrast, if we assume that 1 document in 10,000 is\\nrelevant to the query, the harmonic mean score of this strategy is 0.02%. The\\nharmonic mean is always less than or equal to the arithmetic mean and the\\ngeometric mean. When the values of two numbers differ greatly, the har-\\nmonic mean is closer to their minimum than to their arithmetic mean; see\\nFigure 8.1.\\n?\\nExercise 8.1\\n[⋆]\\nAn IR system returns 8 relevant documents, and 10 nonrelevant documents. There\\nare a total of 20 relevant documents in the collection. What is the precision of the\\nsystem on this search, and what is its recall?\\nExercise 8.2\\n[⋆]\\nThe balanced F measure (a.k.a. F1) is deﬁned as the harmonic mean of precision and\\nrecall. What is the advantage of using the harmonic mean rather than “averaging”\\n(using the arithmetic mean)?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n158\\n8\\nEvaluation in information retrieval\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRecall\\nPrecision\\n◮Figure 8.2\\nPrecision/recall graph.\\nExercise 8.3\\n[⋆⋆]\\nDerive the equivalence between the two formulas for F measure shown in Equa-\\ntion (8.5), given that α = 1/(β2 + 1).\\n8.4\\nEvaluation of ranked retrieval results\\nPrecision, recall, and the F measure are set-based measures. They are com-\\nputed using unordered sets of documents. We need to extend these measures\\n(or to deﬁne new measures) if we are to evaluate the ranked retrieval results\\nthat are now standard with search engines. In a ranked retrieval context,\\nappropriate sets of retrieved documents are naturally given by the top k re-\\ntrieved documents. For each such set, precision and recall values can be\\nplotted to give a precision-recall curve, such as the one shown in Figure 8.2.\\nPRECISION-RECALL\\nCURVE\\nPrecision-recall curves have a distinctive saw-tooth shape: if the (k + 1)th\\ndocument retrieved is nonrelevant then recall is the same as for the top k\\ndocuments, but precision has dropped. If it is relevant, then both precision\\nand recall increase, and the curve jags up and to the right. It is often useful to\\nremove these jiggles and the standard way to do this is with an interpolated\\nprecision: the interpolated precision pinterp at a certain recall level r is deﬁned\\nINTERPOLATED\\nPRECISION\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n159\\nRecall\\nInterp.\\nPrecision\\n0.0\\n1.00\\n0.1\\n0.67\\n0.2\\n0.63\\n0.3\\n0.55\\n0.4\\n0.45\\n0.5\\n0.41\\n0.6\\n0.36\\n0.7\\n0.29\\n0.8\\n0.13\\n0.9\\n0.10\\n1.0\\n0.08\\n◮Table 8.1\\nCalculation of 11-point Interpolated Average Precision. This is for the\\nprecision-recall curve shown in Figure 8.2.\\nas the highest precision found for any recall level r′ ≥r:\\npinterp(r) = max\\nr′≥r p(r′)\\n(8.7)\\nThe justiﬁcation is that almost anyone would be prepared to look at a few\\nmore documents if it would increase the percentage of the viewed set that\\nwere relevant (that is, if the precision of the larger set is higher). Interpolated\\nprecision is shown by a thinner line in Figure 8.2. With this deﬁnition, the\\ninterpolated precision at a recall of 0 is well-deﬁned (Exercise 8.4).\\nExamining the entire precision-recall curve is very informative, but there\\nis often a desire to boil this information down to a few numbers, or perhaps\\neven a single number. The traditional way of doing this (used for instance\\nin the ﬁrst 8 TREC Ad Hoc evaluations) is the 11-point interpolated average\\n11-POINT\\nINTERPOLATED\\nAVERAGE PRECISION\\nprecision. For each information need, the interpolated precision is measured\\nat the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. For the precision-recall curve in\\nFigure 8.2, these 11 values are shown in Table 8.1. For each recall level, we\\nthen calculate the arithmetic mean of the interpolated precision at that recall\\nlevel for each information need in the test collection. A composite precision-\\nrecall curve showing 11 points can then be graphed. Figure 8.3 shows an\\nexample graph of such results from a representative good system at TREC 8.\\nIn recent years, other measures have become more common. Most stan-\\ndard among the TREC community is Mean Average Precision (MAP), which\\nMEAN AVERAGE\\nPRECISION\\nprovides a single-ﬁgure measure of quality across recall levels. Among eval-\\nuation measures, MAP has been shown to have especially good discrimina-\\ntion and stability. For a single information need, Average Precision is the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n160\\n8\\nEvaluation in information retrieval\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nRecall\\nPrecision\\n◮Figure 8.3\\nAveraged 11-point precision/recall graph across 50 queries for a rep-\\nresentative TREC system. The Mean Average Precision for this system is 0.2553.\\naverage of the precision value obtained for the set of top k documents exist-\\ning after each relevant document is retrieved, and this value is then averaged\\nover information needs. That is, if the set of relevant documents for an in-\\nformation need qj ∈Q is {d1, . . . dmj} and Rjk is the set of ranked retrieval\\nresults from the top result until you get to document dk, then\\nMAP(Q) =\\n1\\n|Q|\\n|Q|\\n∑\\nj=1\\n1\\nmj\\nmj\\n∑\\nk=1\\nPrecision(Rjk)\\n(8.8)\\nWhen a relevant document is not retrieved at all,1 the precision value in the\\nabove equation is taken to be 0. For a single information need, the average\\nprecision approximates the area under the uninterpolated precision-recall\\ncurve, and so the MAP is roughly the average area under the precision-recall\\ncurve for a set of queries.\\nUsing MAP, ﬁxed recall levels are not chosen, and there is no interpola-\\ntion. The MAP value for a test collection is the arithmetic mean of average\\n1. A system may not fully order all documents in the collection in response to a query or at\\nany rate an evaluation exercise may be based on submitting only the top k results for each\\ninformation need.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n161\\nprecision values for individual information needs. (This has the effect of\\nweighting each information need equally in the ﬁnal reported number, even\\nif many documents are relevant to some queries whereas very few are rele-\\nvant to other queries.) Calculated MAP scores normally vary widely across\\ninformation needs when measured within a single system, for instance, be-\\ntween 0.1 and 0.7. Indeed, there is normally more agreement in MAP for\\nan individual information need across systems than for MAP scores for dif-\\nferent information needs for the same system. This means that a set of test\\ninformation needs must be large and diverse enough to be representative of\\nsystem effectiveness across different queries.\\nThe above measures factor in precision at all recall levels. For many promi-\\nPRECISION AT k\\nnent applications, particularly web search, this may not be germane to users.\\nWhat matters is rather how many good results there are on the ﬁrst page or\\nthe ﬁrst three pages. This leads to measuring precision at ﬁxed low levels of\\nretrieved results, such as 10 or 30 documents. This is referred to as “Precision\\nat k”, for example “Precision at 10”. It has the advantage of not requiring any\\nestimate of the size of the set of relevant documents but the disadvantages\\nthat it is the least stable of the commonly used evaluation measures and that\\nit does not average well, since the total number of relevant documents for a\\nquery has a strong inﬂuence on precision at k.\\nAn alternative, which alleviates this problem, is R-precision. It requires\\nR-PRECISION\\nhaving a set of known relevant documents Rel, from which we calculate the\\nprecision of the top Rel documents returned. (The set Rel may be incomplete,\\nsuch as when Rel is formed by creating relevance judgments for the pooled\\ntop k results of particular systems in a set of experiments.) R-precision ad-\\njusts for the size of the set of relevant documents: A perfect system could\\nscore 1 on this metric for each query, whereas, even a perfect system could\\nonly achieve a precision at 20 of 0.4 if there were only 8 documents in the\\ncollection relevant to an information need. Averaging this measure across\\nqueries thus makes more sense. This measure is harder to explain to naive\\nusers than Precision at k but easier to explain than MAP. If there are |Rel|\\nrelevant documents for a query, we examine the top |Rel| results of a sys-\\ntem, and ﬁnd that r are relevant, then by deﬁnition, not only is the precision\\n(and hence R-precision) r/|Rel|, but the recall of this result set is also r/|Rel|.\\nThus, R-precision turns out to be identical to the break-even point, another\\nBREAK-EVEN POINT\\nmeasure which is sometimes used, deﬁned in terms of this equality relation-\\nship holding. Like Precision at k, R-precision describes only one point on\\nthe precision-recall curve, rather than attempting to summarize effectiveness\\nacross the curve, and it is somewhat unclear why you should be interested\\nin the break-even point rather than either the best point on the curve (the\\npoint with maximal F-measure) or a retrieval level of interest to a particular\\napplication (Precision at k). Nevertheless, R-precision turns out to be highly\\ncorrelated with MAP empirically, despite measuring only a single point on\\n', 'Online edition (c)\\n2009 Cambridge UP\\n162\\n8\\nEvaluation in information retrieval\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n1 − specificity\\nsensitivity ( = recall)\\n◮Figure 8.4\\nThe ROC curve corresponding to the precision-recall curve in Fig-\\nure 8.2.\\n.\\nthe curve.\\nAnother concept sometimes used in evaluation is an ROC curve. (“ROC”\\nROC CURVE\\nstands for “Receiver Operating Characteristics”, but knowing that doesn’t\\nhelp most people.) An ROC curve plots the true positive rate or sensitiv-\\nity against the false positive rate or (1 −speciﬁcity). Here, sensitivity is just\\nSENSITIVITY\\nanother term for recall. The false positive rate is given by f p/( f p + tn). Fig-\\nure 8.4 shows the ROC curve corresponding to the precision-recall curve in\\nFigure 8.2. An ROC curve always goes from the bottom left to the top right of\\nthe graph. For a good system, the graph climbs steeply on the left side. For\\nunranked result sets, speciﬁcity, given by tn/( f p + tn), was not seen as a very\\nSPECIFICITY\\nuseful notion. Because the set of true negatives is always so large, its value\\nwould be almost 1 for all information needs (and, correspondingly, the value\\nof the false positive rate would be almost 0). That is, the “interesting” part of\\nFigure 8.2 is 0 < recall < 0.4, a part which is compressed to a small corner\\nof Figure 8.4. But an ROC curve could make sense when looking over the\\nfull retrieval spectrum, and it provides another way of looking at the data.\\nIn many ﬁelds, a common aggregate measure is to report the area under the\\nROC curve, which is the ROC analog of MAP. Precision-recall curves are\\nsometimes loosely referred to as ROC curves. This is understandable, but\\nnot accurate.\\nA ﬁnal approach that has seen increasing adoption, especially when em-\\nployed with machine learning approaches to ranking (see Section 15.4, page 341)\\nis measures of cumulative gain, and in particular normalized discounted cumu-\\nCUMULATIVE GAIN\\nNORMALIZED\\nDISCOUNTED\\nCUMULATIVE GAIN\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.4\\nEvaluation of ranked retrieval results\\n163\\nlative gain (NDCG). NDCG is designed for situations of non-binary notions\\nNDCG\\nof relevance (cf. Section 8.5.1). Like precision at k, it is evaluated over some\\nnumber k of top search results. For a set of queries Q, let R(j, d) be the rele-\\nvance score assessors gave to document d for query j. Then,\\nNDCG(Q, k) =\\n1\\n|Q|\\n|Q|\\n∑\\nj=1\\nZkj\\nk\\n∑\\nm=1\\n2R(j,m) −1\\nlog2(1 + m),\\n(8.9)\\nwhere Zkj is a normalization factor calculated to make it so that a perfect\\nranking’s NDCG at k for query j is 1. For queries for which k′ < k documents\\nare retrieved, the last summation is done up to k′.\\n?\\nExercise 8.4\\n[⋆]\\nWhat are the possible values for interpolated precision at a recall level of 0?\\nExercise 8.5\\n[⋆⋆]\\nMust there always be a break-even point between precision and recall? Either show\\nthere must be or give a counter-example.\\nExercise 8.6\\n[⋆⋆]\\nWhat is the relationship between the value of F1 and the break-even point?\\nExercise 8.7\\n[⋆⋆]\\nThe Dice coefﬁcient of two sets is a measure of their intersection scaled by their size\\nDICE COEFFICIENT\\n(giving a value in the range 0 to 1):\\nDice(X, Y) = 2|X ∩Y|\\n|X| + |Y|\\nShow that the balanced F-measure (F1) is equal to the Dice coefﬁcient of the retrieved\\nand relevant document sets.\\nExercise 8.8\\n[⋆]\\nConsider an information need for which there are 4 relevant documents in the collec-\\ntion. Contrast two systems run on this collection. Their top 10 results are judged for\\nrelevance as follows (the leftmost item is the top ranked search result):\\nSystem 1\\nR N R N N\\nN N N R\\nR\\nSystem 2\\nN R N N R\\nR\\nR N N N\\na. What is the MAP of each system? Which has a higher MAP?\\nb. Does this result intuitively make sense? What does it say about what is important\\nin getting a good MAP score?\\nc. What is the R-precision of each system? (Does it rank the systems the same as\\nMAP?)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n164\\n8\\nEvaluation in information retrieval\\nExercise 8.9\\n[⋆⋆]\\nThe following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned\\ndocuments in a ranked list of 20 documents retrieved in response to a query from a\\ncollection of 10,000 documents. The top of the ranked list (the document the system\\nthinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant\\ndocuments. Assume that there are 8 relevant documents in total in the collection.\\nR R N N N\\nN N N R N\\nR N N N R\\nN N N N R\\na. What is the precision of the system on the top 20?\\nb. What is the F1 on the top 20?\\nc. What is the uninterpolated precision of the system at 25% recall?\\nd. What is the interpolated precision at 33% recall?\\ne. Assume that these 20 documents are the complete result set of the system. What\\nis the MAP for the query?\\nAssume, now, instead, that the system returned the entire 10,000 documents in a\\nranked list, and these are the ﬁrst 20 results returned.\\nf. What is the largest possible MAP that this system could have?\\ng. What is the smallest possible MAP that this system could have?\\nh. In a set of experiments, only the top 20 results are evaluated by hand. The result\\nin (e) is used to approximate the range (f)–(g). For this example, how large (in\\nabsolute terms) can the error for the MAP be by calculating (e) instead of (f) and\\n(g) for this query?\\n8.5\\nAssessing relevance\\nTo properly evaluate a system, your test information needs must be germane\\nto the documents in the test document collection, and appropriate for pre-\\ndicted usage of the system. These information needs are best designed by\\ndomain experts. Using random combinations of query terms as an informa-\\ntion need is generally not a good idea because typically they will not resem-\\nble the actual distribution of information needs.\\nGiven information needs and documents, you need to collect relevance\\nassessments. This is a time-consuming and expensive process involving hu-\\nman beings. For tiny collections like Cranﬁeld, exhaustive judgments of rel-\\nevance for each query and document pair were obtained. For large modern\\ncollections, it is usual for relevance to be assessed only for a subset of the\\ndocuments for each query. The most standard approach is pooling, where rel-\\nPOOLING\\nevance is assessed over a subset of the collection that is formed from the top\\nk documents returned by a number of different IR systems (usually the ones\\nto be evaluated), and perhaps other sources such as the results of Boolean\\nkeyword searches or documents found by expert searchers in an interactive\\nprocess.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.5\\nAssessing relevance\\n165\\nJudge 2 Relevance\\nYes\\nNo\\nTotal\\nJudge 1\\nYes\\n300\\n20\\n320\\nRelevance\\nNo\\n10\\n70\\n80\\nTotal\\n310\\n90\\n400\\nObserved proportion of the times the judges agreed\\nP(A) = (300 + 70)/400 = 370/400 = 0.925\\nPooled marginals\\nP(nonrelevant) = (80 + 90)/(400 + 400) = 170/800 = 0.2125\\nP(relevant) = (320 + 310)/(400 + 400) = 630/800 = 0.7878\\nProbability that the two judges agreed by chance\\nP(E) = P(nonrelevant)2 + P(relevant)2 = 0.21252 + 0.78782 = 0.665\\nKappa statistic\\nκ = (P(A) −P(E))/(1 −P(E)) = (0.925 −0.665)/(1 −0.665) = 0.776\\n◮Table 8.2\\nCalculating the kappa statistic.\\nA human is not a device that reliably reports a gold standard judgment\\nof relevance of a document to a query. Rather, humans and their relevance\\njudgments are quite idiosyncratic and variable. But this is not a problem\\nto be solved: in the ﬁnal analysis, the success of an IR system depends on\\nhow good it is at satisfying the needs of these idiosyncratic humans, one\\ninformation need at a time.\\nNevertheless, it is interesting to consider and measure how much agree-\\nment between judges there is on relevance judgments. In the social sciences,\\na common measure for agreement between judges is the kappa statistic. It is\\nKAPPA STATISTIC\\ndesigned for categorical judgments and corrects a simple agreement rate for\\nthe rate of chance agreement.\\nkappa = P(A) −P(E)\\n1 −P(E)\\n(8.10)\\nwhere P(A) is the proportion of the times the judges agreed, and P(E) is the\\nproportion of the times they would be expected to agree by chance. There\\nare choices in how the latter is estimated: if we simply say we are making\\na two-class decision and assume nothing more, then the expected chance\\nagreement rate is 0.5. However, normally the class distribution assigned is\\nskewed, and it is usual to use marginal statistics to calculate expected agree-\\nMARGINAL\\nment.2 There are still two ways to do it depending on whether one pools\\n2. For a contingency table, as in Table 8.2, a marginal statistic is formed by summing a row or\\ncolumn. The marginal ai.k = ∑j aijk.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n166\\n8\\nEvaluation in information retrieval\\nthe marginal distribution across judges or uses the marginals for each judge\\nseparately; both forms have been used, but we present the pooled version\\nbecause it is more conservative in the presence of systematic differences in as-\\nsessments across judges. The calculations are shown in Table 8.2. The kappa\\nvalue will be 1 if two judges always agree, 0 if they agree only at the rate\\ngiven by chance, and negative if they are worse than random. If there are\\nmore than two judges, it is normal to calculate an average pairwise kappa\\nvalue. As a rule of thumb, a kappa value above 0.8 is taken as good agree-\\nment, a kappa value between 0.67 and 0.8 is taken as fair agreement, and\\nagreement below 0.67 is seen as data providing a dubious basis for an evalu-\\nation, though the precise cutoffs depend on the purposes for which the data\\nwill be used.\\nInterjudge agreement of relevance has been measured within the TREC\\nevaluations and for medical IR collections. Using the above rules of thumb,\\nthe level of agreement normally falls in the range of “fair” (0.67–0.8). The fact\\nthat human agreement on a binary relevance judgment is quite modest is one\\nreason for not requiring more ﬁne-grained relevance labeling from the test\\nset creator. To answer the question of whether IR evaluation results are valid\\ndespite the variation of individual assessors’ judgments, people have exper-\\nimented with evaluations taking one or the other of two judges’ opinions as\\nthe gold standard. The choice can make a considerable absolute difference to\\nreported scores, but has in general been found to have little impact on the rel-\\native effectiveness ranking of either different systems or variants of a single\\nsystem which are being compared for effectiveness.\\n8.5.1\\nCritiques and justiﬁcations of the concept of relevance\\nThe advantage of system evaluation, as enabled by the standard model of\\nrelevant and nonrelevant documents, is that we have a ﬁxed setting in which\\nwe can vary IR systems and system parameters to carry out comparative ex-\\nperiments. Such formal testing is much less expensive and allows clearer\\ndiagnosis of the effect of changing system parameters than doing user stud-\\nies of retrieval effectiveness. Indeed, once we have a formal measure that\\nwe have conﬁdence in, we can proceed to optimize effectiveness by machine\\nlearning methods, rather than tuning parameters by hand. Of course, if the\\nformal measure poorly describes what users actually want, doing this will\\nnot be effective in improving user satisfaction. Our perspective is that, in\\npractice, the standard formal measures for IR evaluation, although a simpli-\\nﬁcation, are good enough, and recent work in optimizing formal evaluation\\nmeasures in IR has succeeded brilliantly. There are numerous examples of\\ntechniques developed in formal evaluation settings, which improve effec-\\ntiveness in operational settings, such as the development of document length\\nnormalization methods within the context of TREC (Sections 6.4.4 and 11.4.3)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.5\\nAssessing relevance\\n167\\nand machine learning methods for adjusting parameter weights in scoring\\n(Section 6.1.2).\\nThat is not to say that there are not problems latent within the abstrac-\\ntions used. The relevance of one document is treated as independent of the\\nrelevance of other documents in the collection. (This assumption is actually\\nbuilt into most retrieval systems – documents are scored against queries, not\\nagainst each other – as well as being assumed in the evaluation methods.)\\nAssessments are binary: there aren’t any nuanced assessments of relevance.\\nRelevance of a document to an information need is treated as an absolute,\\nobjective decision. But judgments of relevance are subjective, varying across\\npeople, as we discussed above. In practice, human assessors are also imper-\\nfect measuring instruments, susceptible to failures of understanding and at-\\ntention. We also have to assume that users’ information needs do not change\\nas they start looking at retrieval results. Any results based on one collection\\nare heavily skewed by the choice of collection, queries, and relevance judg-\\nment set: the results may not translate from one domain to another or to a\\ndifferent user population.\\nSome of these problems may be ﬁxable. A number of recent evaluations,\\nincluding INEX, some TREC tracks, and NTCIR have adopted an ordinal\\nnotion of relevance with documents divided into 3 or 4 classes, distinguish-\\ning slightly relevant documents from highly relevant documents. See Sec-\\ntion 10.4 (page 210) for a detailed discussion of how this is implemented in\\nthe INEX evaluations.\\nOne clear problem with the relevance-based assessment that we have pre-\\nsented is the distinction between relevance and marginal relevance: whether\\nMARGINAL RELEVANCE\\na document still has distinctive usefulness after the user has looked at cer-\\ntain other documents (Carbonell and Goldstein 1998). Even if a document\\nis highly relevant, its information can be completely redundant with other\\ndocuments which have already been examined. The most extreme case of\\nthis is documents that are duplicates – a phenomenon that is actually very\\ncommon on the World Wide Web – but it can also easily occur when sev-\\neral documents provide a similar precis of an event. In such circumstances,\\nmarginal relevance is clearly a better measure of utility to the user. Maximiz-\\ning marginal relevance requires returning documents that exhibit diversity\\nand novelty. One way to approach measuring this is by using distinct facts\\nor entities as evaluation units. This perhaps more directly measures true\\nutility to the user but doing this makes it harder to create a test collection.\\n?\\nExercise 8.10\\n[⋆⋆]\\nBelow is a table showing how two human judges rated the relevance of a set of 12\\ndocuments to a particular information need (0 = nonrelevant, 1 = relevant). Let us as-\\nsume that you’ve written an IR system that for this query returns the set of documents\\n{4, 5, 6, 7, 8}.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n168\\n8\\nEvaluation in information retrieval\\ndocID\\nJudge 1\\nJudge 2\\n1\\n0\\n0\\n2\\n0\\n0\\n3\\n1\\n1\\n4\\n1\\n1\\n5\\n1\\n0\\n6\\n1\\n0\\n7\\n1\\n0\\n8\\n1\\n0\\n9\\n0\\n1\\n10\\n0\\n1\\n11\\n0\\n1\\n12\\n0\\n1\\na. Calculate the kappa measure between the two judges.\\nb. Calculate precision, recall, and F1 of your system if a document is considered rel-\\nevant only if the two judges agree.\\nc. Calculate precision, recall, and F1 of your system if a document is considered rel-\\nevant if either judge thinks it is relevant.\\n8.6\\nA broader perspective: System quality and user utility\\nFormal evaluation measures are at some distance from our ultimate interest\\nin measures of human utility: how satisﬁed is each user with the results the\\nsystem gives for each information need that they pose? The standard way to\\nmeasure human satisfaction is by various kinds of user studies. These might\\ninclude quantitative measures, both objective, such as time to complete a\\ntask, as well as subjective, such as a score for satisfaction with the search\\nengine, and qualitative measures, such as user comments on the search in-\\nterface. In this section we will touch on other system aspects that allow quan-\\ntitative evaluation and the issue of user utility.\\n8.6.1\\nSystem issues\\nThere are many practical benchmarks on which to rate an information re-\\ntrieval system beyond its retrieval quality. These include:\\n• How fast does it index, that is, how many documents per hour does it\\nindex for a certain distribution over document lengths? (cf. Chapter 4)\\n• How fast does it search, that is, what is its latency as a function of index\\nsize?\\n• How expressive is its query language? How fast is it on complex queries?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.6\\nA broader perspective: System quality and user utility\\n169\\n• How large is its document collection, in terms of the number of doc-\\numents or the collection having information distributed across a broad\\nrange of topics?\\nAll these criteria apart from query language expressiveness are straightfor-\\nwardly measurable: we can quantify the speed or size. Various kinds of fea-\\nture checklists can make query language expressiveness semi-precise.\\n8.6.2\\nUser utility\\nWhat we would really like is a way of quantifying aggregate user happiness,\\nbased on the relevance, speed, and user interface of a system. One part of\\nthis is understanding the distribution of people we wish to make happy, and\\nthis depends entirely on the setting. For a web search engine, happy search\\nusers are those who ﬁnd what they want. One indirect measure of such users\\nis that they tend to return to the same engine. Measuring the rate of return\\nof users is thus an effective metric, which would of course be more effective\\nif you could also measure how much these users used other search engines.\\nBut advertisers are also users of modern web search engines. They are happy\\nif customers click through to their sites and then make purchases. On an\\neCommerce web site, a user is likely to be wanting to purchase something.\\nThus, we can measure the time to purchase, or the fraction of searchers who\\nbecome buyers. On a shopfront web site, perhaps both the user’s and the\\nstore owner’s needs are satisﬁed if a purchase is made. Nevertheless, in\\ngeneral, we need to decide whether it is the end user’s or the eCommerce\\nsite owner’s happiness that we are trying to optimize. Usually, it is the store\\nowner who is paying us.\\nFor an “enterprise” (company, government, or academic) intranet search\\nengine, the relevant metric is more likely to be user productivity: how much\\ntime do users spend looking for information that they need. There are also\\nmany other practical criteria concerning such matters as information secu-\\nrity, which we mentioned in Section 4.6 (page 80).\\nUser happiness is elusive to measure, and this is part of why the standard\\nmethodology uses the proxy of relevance of search results. The standard\\ndirect way to get at user satisfaction is to run user studies, where people en-\\ngage in tasks, and usually various metrics are measured, the participants are\\nobserved, and ethnographic interview techniques are used to get qualitative\\ninformation on satisfaction. User studies are very useful in system design,\\nbut they are time consuming and expensive to do. They are also difﬁcult to\\ndo well, and expertise is required to design the studies and to interpret the\\nresults. We will not discuss the details of human usability testing here.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n170\\n8\\nEvaluation in information retrieval\\n8.6.3\\nReﬁning a deployed system\\nIf an IR system has been built and is being used by a large number of users,\\nthe system’s builders can evaluate possible changes by deploying variant\\nversions of the system and recording measures that are indicative of user\\nsatisfaction with one variant vs. others as they are being used. This method\\nis frequently used by web search engines.\\nThe most common version of this is A/B testing, a term borrowed from the\\nA/B TEST\\nadvertising industry. For such a test, precisely one thing is changed between\\nthe current system and a proposed system, and a small proportion of traf-\\nﬁc (say, 1–10% of users) is randomly directed to the variant system, while\\nmost users use the current system. For example, if we wish to investigate a\\nchange to the ranking algorithm, we redirect a random sample of users to\\na variant system and evaluate measures such as the frequency with which\\npeople click on the top result, or any result on the ﬁrst page. (This particular\\nanalysis method is referred to as clickthrough log analysis or clickstream min-\\nCLICKTHROUGH LOG\\nANALYSIS\\nCLICKSTREAM MINING\\ning. It is further discussed as a method of implicit feedback in Section 9.1.7\\n(page 187).)\\nThe basis of A/B testing is running a bunch of single variable tests (either\\nin sequence or in parallel): for each test only one parameter is varied from the\\ncontrol (the current live system). It is therefore easy to see whether varying\\neach parameter has a positive or negative effect. Such testing of a live system\\ncan easily and cheaply gauge the effect of a change on users, and, with a\\nlarge enough user base, it is practical to measure even very small positive\\nand negative effects. In principle, more analytic power can be achieved by\\nvarying multiple things at once in an uncorrelated (random) way, and doing\\nstandard multivariate statistical analysis, such as multiple linear regression.\\nIn practice, though, A/B testing is widely used, because A/B tests are easy\\nto deploy, easy to understand, and easy to explain to management.\\n8.7\\nResults snippets\\nHaving chosen or ranked the documents matching a query, we wish to pre-\\nsent a results list that will be informative to the user. In many cases the\\nuser will not want to examine all the returned documents and so we want\\nto make the results list informative enough that the user can do a ﬁnal rank-\\ning of the documents for themselves based on relevance to their information\\nneed.3 The standard way of doing this is to provide a snippet, a short sum-\\nSNIPPET\\nmary of the document, which is designed so as to allow the user to decide\\nits relevance. Typically, the snippet consists of the document title and a short\\n3. There are exceptions, in domains where recall is emphasized. For instance, in many legal\\ndisclosure cases, a legal associate will review every document that matches a keyword search.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.7\\nResults snippets\\n171\\nsummary, which is automatically extracted. The question is how to design\\nthe summary so as to maximize its usefulness to the user.\\nThe two basic kinds of summaries are static, which are always the same\\nSTATIC SUMMARY\\nregardless of the query, and dynamic (or query-dependent), which are cus-\\nDYNAMIC SUMMARY\\ntomized according to the user’s information need as deduced from a query.\\nDynamic summaries attempt to explain why a particular document was re-\\ntrieved for the query at hand.\\nA static summary is generally comprised of either or both a subset of the\\ndocument and metadata associated with the document. The simplest form\\nof summary takes the ﬁrst two sentences or 50 words of a document, or ex-\\ntracts particular zones of a document, such as the title and author. Instead of\\nzones of a document, the summary can instead use metadata associated with\\nthe document. This may be an alternative way to provide an author or date,\\nor may include elements which are designed to give a summary, such as the\\ndescription metadata which can appear in the meta element of a web\\nHTML page. This summary is typically extracted and cached at indexing\\ntime, in such a way that it can be retrieved and presented quickly when dis-\\nplaying search results, whereas having to access the actual document content\\nmight be a relatively expensive operation.\\nThere has been extensive work within natural language processing (NLP)\\non better ways to do text summarization. Most such work still aims only to\\nTEXT SUMMARIZATION\\nchoose sentences from the original document to present and concentrates on\\nhow to select good sentences. The models typically combine positional fac-\\ntors, favoring the ﬁrst and last paragraphs of documents and the ﬁrst and last\\nsentences of paragraphs, with content factors, emphasizing sentences with\\nkey terms, which have low document frequency in the collection as a whole,\\nbut high frequency and good distribution across the particular document\\nbeing returned. In sophisticated NLP approaches, the system synthesizes\\nsentences for a summary, either by doing full text generation or by editing\\nand perhaps combining sentences used in the document. For example, it\\nmight delete a relative clause or replace a pronoun with the noun phrase\\nthat it refers to. This last class of methods remains in the realm of research\\nand is seldom used for search results: it is easier, safer, and often even better\\nto just use sentences from the original document.\\nDynamic summaries display one or more “windows” on the document,\\naiming to present the pieces that have the most utility to the user in evalu-\\nating the document with respect to their information need. Usually these\\nwindows contain one or several of the query terms, and so are often re-\\nferred to as keyword-in-context (KWIC) snippets, though sometimes they may\\nKEYWORD-IN-CONTEXT\\nstill be pieces of the text such as the title that are selected for their query-\\nindependent information value just as in the case of static summarization.\\nDynamic summaries are generated in conjunction with scoring. If the query\\nis found as a phrase, occurrences of the phrase in the document will be\\n', 'Online edition (c)\\n2009 Cambridge UP\\n172\\n8\\nEvaluation in information retrieval\\n... In recent years, Papua New Guinea has faced severe economic\\ndifﬁculties and economic growth has slowed, partly as a result of weak\\ngovernance and civil war, and partly as a result of external factors such as\\nthe Bougainville civil war which led to the closure in 1989 of the Panguna\\nmine (at that time the most important foreign exchange earner and\\ncontributor to Government ﬁnances), the Asian ﬁnancial crisis, a decline in\\nthe prices of gold and copper, and a fall in the production of oil. PNG’s\\neconomic development record over the past few years is evidence that\\ngovernance issues underly many of the country’s problems. Good\\ngovernance, which may be deﬁned as the transparent and accountable\\nmanagement of human, natural, economic and ﬁnancial resources for the\\npurposes of equitable and sustainable development, ﬂows from proper\\npublic sector management, efﬁcient ﬁscal and accounting mechanisms, and\\na willingness to make service delivery a priority in practice. ...\\n◮Figure 8.5\\nAn example of selecting text for a dynamic snippet. This snippet was\\ngenerated for a document in response to the query new guinea economic development.\\nThe ﬁgure shows in bold italic where the selected snippet text occurred in the original\\ndocument.\\nshown as the summary. If not, windows within the document that contain\\nmultiple query terms will be selected. Commonly these windows may just\\nstretch some number of words to the left and right of the query terms. This is\\na place where NLP techniques can usefully be employed: users prefer snip-\\npets that read well because they contain complete phrases.\\nDynamic summaries are generally regarded as greatly improving the us-\\nability of IR systems, but they present a complication for IR system design. A\\ndynamic summary cannot be precomputed, but, on the other hand, if a sys-\\ntem has only a positional index, then it cannot easily reconstruct the context\\nsurrounding search engine hits in order to generate such a dynamic sum-\\nmary. This is one reason for using static summaries. The standard solution\\nto this in a world of large and cheap disk drives is to locally cache all the\\ndocuments at index time (notwithstanding that this approach raises various\\nlegal, information security and control issues that are far from resolved) as\\nshown in Figure 7.5 (page 147). Then, a system can simply scan a document\\nwhich is about to appear in a displayed results list to ﬁnd snippets containing\\nthe query words. Beyond simply access to the text, producing a good KWIC\\nsnippet requires some care. Given a variety of keyword occurrences in a\\ndocument, the goal is to choose fragments which are: (i) maximally informa-\\ntive about the discussion of those terms in the document, (ii) self-contained\\nenough to be easy to read, and (iii) short enough to ﬁt within the normally\\nstrict constraints on the space available for summaries.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.8\\nReferences and further reading\\n173\\nGenerating snippets must be fast since the system is typically generating\\nmany snippets for each query that it handles. Rather than caching an entire\\ndocument, it is common to cache only a generous but ﬁxed size preﬁx of\\nthe document, such as perhaps 10,000 characters. For most common, short\\ndocuments, the entire document is thus cached, but huge amounts of local\\nstorage will not be wasted on potentially vast documents. Summaries of\\ndocuments whose length exceeds the preﬁx size will be based on material\\nin the preﬁx only, which is in general a useful zone in which to look for a\\ndocument summary anyway.\\nIf a document has been updated since it was last processed by a crawler\\nand indexer, these changes will be neither in the cache nor in the index. In\\nthese circumstances, neither the index nor the summary will accurately re-\\nﬂect the current contents of the document, but it is the differences between\\nthe summary and the actual document content that will be more glaringly\\nobvious to the end user.\\n8.8\\nReferences and further reading\\nDeﬁnition and implementation of the notion of relevance to a query got off\\nto a rocky start in 1953. Swanson (1988) reports that in an evaluation in that\\nyear between two teams, they agreed that 1390 documents were variously\\nrelevant to a set of 98 questions, but disagreed on a further 1577 documents,\\nand the disagreements were never resolved.\\nRigorous formal testing of IR systems was ﬁrst completed in the Cranﬁeld\\nexperiments, beginning in the late 1950s. A retrospective discussion of the\\nCranﬁeld test collection and experimentation with it can be found in (Clever-\\ndon 1991). The other seminal series of early IR experiments were those on the\\nSMART system by Gerard Salton and colleagues (Salton 1971b; 1991). The\\nTREC evaluations are described in detail by Voorhees and Harman (2005).\\nOnline information is available at http://trec.nist.gov/. Initially, few researchers\\ncomputed the statistical signiﬁcance of their experimental results, but the IR\\ncommunity increasingly demands this (Hull 1993). User studies of IR system\\neffectiveness began more recently (Saracevic and Kantor 1988; 1996).\\nThe notions of recall and precision were ﬁrst used by Kent et al. (1955),\\nalthough the term precision did not appear until later. The F measure (or,\\nF MEASURE\\nrather its complement E = 1 −F) was introduced by van Rijsbergen (1979).\\nHe provides an extensive theoretical discussion, which shows how adopting\\na principle of decreasing marginal relevance (at some point a user will be\\nunwilling to sacriﬁce a unit of precision for an added unit of recall) leads to\\nthe harmonic mean being the appropriate method for combining precision\\nand recall (and hence to its adoption rather than the minimum or geometric\\nmean).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n174\\n8\\nEvaluation in information retrieval\\nBuckley and Voorhees (2000) compare several evaluation measures, in-\\ncluding precision at k, MAP, and R-precision, and evaluate the error rate of\\neach measure. R-precision was adopted as the ofﬁcial evaluation metric in\\nR-PRECISION\\nthe TREC HARD track (Allan 2005). Aslam and Yilmaz (2005) examine its\\nsurprisingly close correlation to MAP, which had been noted in earlier stud-\\nies (Tague-Sutcliffe and Blustein 1995, Buckley and Voorhees 2000). A stan-\\ndard program for evaluating IR systems which computes many measures of\\nranked retrieval effectiveness is Chris Buckley’s trec_eval program used\\nin the TREC evaluations. It can be downloaded from: http://trec.nist.gov/trec_eval/.\\nKekäläinen and Järvelin (2002) argue for the superiority of graded rele-\\nvance judgments when dealing with very large document collections, and\\nJärvelin and Kekäläinen (2002) introduce cumulated gain-based methods for\\nIR system evaluation in this context. Sakai (2007) does a study of the stabil-\\nity and sensitivity of evaluation measures based on graded relevance judg-\\nments from NTCIR tasks, and concludes that NDCG is best for evaluating\\ndocument ranking.\\nSchamber et al. (1990) examine the concept of relevance, stressing its multi-\\ndimensional and context-speciﬁc nature, but also arguing that it can be mea-\\nsured effectively. (Voorhees 2000) is the standard article for examining vari-\\nation in relevance judgments and their effects on retrieval system scores and\\nranking for the TREC Ad Hoc task. Voorhees concludes that although the\\nnumbers change, the rankings are quite stable. Hersh et al. (1994) present\\nsimilar analysis for a medical IR collection. In contrast, Kekäläinen (2005)\\nanalyze some of the later TRECs, exploring a 4-way relevance judgment and\\nthe notion of cumulative gain, arguing that the relevance measure used does\\nsubstantially affect system rankings. See also Harter (1998). Zobel (1998)\\nstudies whether the pooling method used by TREC to collect a subset of doc-\\numents that will be evaluated for relevance is reliable and fair, and concludes\\nthat it is.\\nThe kappa statistic and its use for language-related purposes is discussed\\nKAPPA STATISTIC\\nby Carletta (1996). Many standard sources (e.g., Siegel and Castellan 1988)\\npresent pooled calculation of the expected agreement, but Di Eugenio and\\nGlass (2004) argue for preferring the unpooled agreement (though perhaps\\npresenting multiple measures). For further discussion of alternative mea-\\nsures of agreement, which may in fact be better, see Lombard et al. (2002)\\nand Krippendorff (2003).\\nText summarization has been actively explored for many years. Modern\\nwork on sentence selection was initiated by Kupiec et al. (1995). More recent\\nwork includes (Barzilay and Elhadad 1997) and (Jing 2000), together with\\na broad selection of work appearing at the yearly DUC conferences and at\\nother NLP venues. Tombros and Sanderson (1998) demonstrate the advan-\\ntages of dynamic summaries in the IR context. Turpin et al. (2007) address\\nhow to generate snippets efﬁciently.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n8.8\\nReferences and further reading\\n175\\nClickthrough log analysis is studied in (Joachims 2002b, Joachims et al.\\n2005).\\nIn a series of papers, Hersh, Turpin and colleagues show how improve-\\nments in formal retrieval effectiveness, as evaluated in batch experiments, do\\nnot always translate into an improved system for users (Hersh et al. 2000a;b;\\n2001, Turpin and Hersh 2001; 2002).\\nUser interfaces for IR and human factors such as models of human infor-\\nmation seeking and usability testing are outside the scope of what we cover\\nin this book. More information on these topics can be found in other text-\\nbooks, including (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) and (Korfhage\\n1997), and collections focused on cognitive aspects (Spink and Cole 2005).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n177\\n9\\nRelevance feedback and query\\nexpansion\\nIn most collections, the same concept may be referred to using different\\nwords. This issue, known as synonymy, has an impact on the recall of most\\nSYNONYMY\\ninformation retrieval systems. For example, you would want a search for\\naircraft to match plane (but only for references to an airplane, not a woodwork-\\ning plane), and for a search on thermodynamics to match references to heat in\\nappropriate discussions. Users often attempt to address this problem them-\\nselves by manually reﬁning a query, as was discussed in Section 1.4; in this\\nchapter we discuss ways in which a system can help with query reﬁnement,\\neither fully automatically or with the user in the loop.\\nThe methods for tackling this problem split into two major classes: global\\nmethods and local methods. Global methods are techniques for expanding\\nor reformulating query terms independent of the query and results returned\\nfrom it, so that changes in the query wording will cause the new query to\\nmatch other semantically similar terms. Global methods include:\\n• Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2)\\n• Query expansion via automatic thesaurus generation (Section 9.2.3)\\n• Techniques like spelling correction (discussed in Chapter 3)\\nLocal methods adjust a query relative to the documents that initially appear\\nto match the query. The basic methods here are:\\n• Relevance feedback (Section 9.1)\\n• Pseudo relevance feedback, also known as Blind relevance feedback (Sec-\\ntion 9.1.6)\\n• (Global) indirect relevance feedback (Section 9.1.7)\\nIn this chapter, we will mention all of these approaches, but we will concen-\\ntrate on relevance feedback, which is one of the most used and most success-\\nful approaches.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n178\\n9\\nRelevance feedback and query expansion\\n9.1\\nRelevance feedback and pseudo relevance feedback\\nThe idea of relevance feedback (RF) is to involve the user in the retrieval process\\nRELEVANCE FEEDBACK\\nso as to improve the ﬁnal result set. In particular, the user gives feedback on\\nthe relevance of documents in an initial set of results. The basic procedure is:\\n• The user issues a (short, simple) query.\\n• The system returns an initial set of retrieval results.\\n• The user marks some returned documents as relevant or nonrelevant.\\n• The system computes a better representation of the information need based\\non the user feedback.\\n• The system displays a revised set of retrieval results.\\nRelevance feedback can go through one or more iterations of this sort. The\\nprocess exploits the idea that it may be difﬁcult to formulate a good query\\nwhen you don’t know the collection well, but it is easy to judge particular\\ndocuments, and so it makes sense to engage in iterative query reﬁnement\\nof this sort. In such a scenario, relevance feedback can also be effective in\\ntracking a user’s evolving information need: seeing some documents may\\nlead users to reﬁne their understanding of the information they are seeking.\\nImage search provides a good example of relevance feedback. Not only is\\nit easy to see the results at work, but this is a domain where a user can easily\\nhave difﬁculty formulating what they want in words, but can easily indicate\\nrelevant or nonrelevant images. After the user enters an initial query for bike\\non the demonstration system at:\\nhttp://nayana.ece.ucsb.edu/imsearch/imsearch.html\\nthe initial results (in this case, images) are returned. In Figure 9.1 (a), the\\nuser has selected some of them as relevant. These will be used to reﬁne the\\nquery, while other displayed results have no effect on the reformulation. Fig-\\nure 9.1 (b) then shows the new top-ranked results calculated after this round\\nof relevance feedback.\\nFigure 9.2 shows a textual IR example where the user wishes to ﬁnd out\\nabout new applications of space satellites.\\n9.1.1\\nThe Rocchio algorithm for relevance feedback\\nThe Rocchio Algorithm is the classic algorithm for implementing relevance\\nfeedback. It models a way of incorporating relevance feedback information\\ninto the vector space model of Section 6.3.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n179\\n(a)\\n(b)\\n◮Figure 9.1\\nRelevance feedback searching over images. (a) The user views the\\ninitial query results for a query of bike, selects the ﬁrst, third and fourth result in\\nthe top row and the fourth result in the bottom row as relevant, and submits this\\nfeedback. (b) The users sees the revised result set. Precision is greatly improved.\\nFrom http://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n180\\n9\\nRelevance feedback and query expansion\\n(a)\\nQuery: New space satellite applications\\n(b)\\n+\\n1. 0.539, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer\\n+\\n2. 0.533, 07/09/91, NASA Scratches Environment Gear From Satel-\\nlite Plan\\n3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan, But\\nUrges Launches of Smaller Probes\\n4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes Incredi-\\nble Feat: Staying Within Budget\\n5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Pro-\\nposes Satellites for Climate Research\\n6. 0.524, 08/22/90, Report Provides Support for the Critics Of Using\\nBig Satellites to Study Climate\\n7.\\n0.516, 04/13/87, Arianespace Receives Satellite Launch Pact\\nFrom Telesat Canada\\n+\\n8. 0.509, 12/02/87, Telecommunications Tale of Two Companies\\n(c)\\n2.074 new\\n15.106 space\\n30.816 satellite\\n5.660 application\\n5.991 nasa\\n5.196 eos\\n4.196 launch\\n3.972 aster\\n3.516 instrument\\n3.446 arianespace\\n3.004 bundespost\\n2.806 ss\\n2.790 rocket\\n2.053 scientist\\n2.003 broadcast\\n1.172 earth\\n0.836 oil\\n0.646 measure\\n(d)\\n*\\n1. 0.513, 07/09/91, NASA Scratches Environment Gear From Satel-\\nlite Plan\\n*\\n2. 0.500, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer\\n3. 0.493, 08/07/89, When the Pentagon Launches a Secret Satellite,\\nSpace Sleuths Do Some Spy Work of Their Own\\n4. 0.493, 07/31/89, NASA Uses ‘Warm’ Superconductors For Fast\\nCircuit\\n*\\n5. 0.492, 12/02/87, Telecommunications Tale of Two Companies\\n6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile For\\nCommercial Use\\n7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Match\\nthe Soviets In Rocket Launchers\\n8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Cost $90\\nMillion\\n◮Figure 9.2\\nExample of relevance feedback on a text collection. (a) The initial query\\n(a). (b) The user marks some relevant documents (shown with a plus sign). (c) The\\nquery is then expanded by 18 terms with weights as shown. (d) The revised top\\nresults are then shown. A * marks the documents which were judged relevant in the\\nrelevance feedback phase.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n181\\n◮Figure 9.3\\nThe Rocchio optimal query for separating relevant and nonrelevant\\ndocuments.\\nThe underlying theory.\\nWe want to ﬁnd a query vector, denoted as ⃗q, that\\nmaximizes similarity with relevant documents while minimizing similarity\\nwith nonrelevant documents. If Cr is the set of relevant documents and Cnr\\nis the set of nonrelevant documents, then we wish to ﬁnd:1\\n⃗qopt = arg max\\n⃗q\\n[sim(⃗q, Cr) −sim(⃗q, Cnr)],\\n(9.1)\\nwhere sim is deﬁned as in Equation 6.10. Under cosine similarity, the optimal\\nquery vector⃗qopt for separating the relevant and nonrelevant documents is:\\n⃗qopt =\\n1\\n|Cr| ∑\\n⃗dj∈Cr\\n⃗dj −\\n1\\n|Cnr| ∑\\n⃗dj∈Cnr\\n⃗dj\\n(9.2)\\nThat is, the optimal query is the vector difference between the centroids of the\\nrelevant and nonrelevant documents; see Figure 9.3. However, this observa-\\ntion is not terribly useful, precisely because the full set of relevant documents\\nis not known: it is what we want to ﬁnd.\\nThe Rocchio (1971) algorithm.\\nThis was the relevance feedback mecha-\\nROCCHIO ALGORITHM\\n1. In the equation, arg maxx f (x) returns a value of x which maximizes the value of the function\\nf (x). Similarly, arg minx f (x) returns a value of x which minimizes the value of the function\\nf (x).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n182\\n9\\nRelevance feedback and query expansion\\n◮Figure 9.4\\nAn application of Rocchio’s algorithm. Some documents have been\\nlabeled as relevant and nonrelevant and the initial query vector is moved in response\\nto this feedback.\\nnism introduced in and popularized by Salton’s SMART system around 1970.\\nIn a real IR query context, we have a user query and partial knowledge of\\nknown relevant and nonrelevant documents. The algorithm proposes using\\nthe modiﬁed query ⃗qm:\\n⃗qm = α⃗q0 + β 1\\n|Dr| ∑\\n⃗dj∈Dr\\n⃗dj −γ\\n1\\n|Dnr| ∑\\n⃗dj∈Dnr\\n⃗dj\\n(9.3)\\nwhere q0 is the original query vector, Dr and Dnr are the set of known rel-\\nevant and nonrelevant documents respectively, and α, β, and γ are weights\\nattached to each term. These control the balance between trusting the judged\\ndocument set versus the query: if we have a lot of judged documents, we\\nwould like a higher β and γ. Starting from q0, the new query moves you\\nsome distance toward the centroid of the relevant documents and some dis-\\ntance away from the centroid of the nonrelevant documents. This new query\\ncan be used for retrieval in the standard vector space model (see Section 6.3).\\nWe can easily leave the positive quadrant of the vector space by subtracting\\noff a nonrelevant document’s vector. In the Rocchio algorithm, negative term\\nweights are ignored. That is, the term weight is set to 0. Figure 9.4 shows the\\neffect of applying relevance feedback.\\nRelevance feedback can improve both recall and precision. But, in prac-\\ntice, it has been shown to be most useful for increasing recall in situations\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n183\\nwhere recall is important. This is partly because the technique expands the\\nquery, but it is also partly an effect of the use case: when they want high\\nrecall, users can be expected to take time to review results and to iterate on\\nthe search. Positive feedback also turns out to be much more valuable than\\nnegative feedback, and so most IR systems set γ < β. Reasonable values\\nmight be α = 1, β = 0.75, and γ = 0.15. In fact, many systems, such as\\nthe image search system in Figure 9.1, allow only positive feedback, which\\nis equivalent to setting γ = 0. Another alternative is to use only the marked\\nnonrelevant document which received the highest ranking from the IR sys-\\ntem as negative feedback (here, |Dnr| = 1 in Equation (9.3)). While many of\\nthe experimental results comparing various relevance feedback variants are\\nrather inconclusive, some studies have suggested that this variant, called Ide\\nIDE DEC-HI\\ndec-hi is the most effective or at least the most consistent performer.\\n$\\n9.1.2\\nProbabilistic relevance feedback\\nRather than reweighting the query in a vector space, if a user has told us\\nsome relevant and nonrelevant documents, then we can proceed to build a\\nclassiﬁer. One way of doing this is with a Naive Bayes probabilistic model.\\nIf R is a Boolean indicator variable expressing the relevance of a document,\\nthen we can estimate P(xt = 1|R), the probability of a term t appearing in a\\ndocument, depending on whether it is relevant or not, as:\\nˆP(xt = 1|R = 1)\\n=\\n|VRt|/|VR|\\n(9.4)\\nˆP(xt = 1|R = 0)\\n=\\n(d ft −|VRt|)/(N −|VR|)\\nwhere N is the total number of documents, d ft is the number that contain\\nt, VR is the set of known relevant documents, and VRt is the subset of this\\nset containing t. Even though the set of known relevant documents is a per-\\nhaps small subset of the true set of relevant documents, if we assume that\\nthe set of relevant documents is a small subset of the set of all documents\\nthen the estimates given above will be reasonable. This gives a basis for\\nanother way of changing the query term weights. We will discuss such prob-\\nabilistic approaches more in Chapters 11 and 13, and in particular outline\\nthe application to relevance feedback in Section 11.3.4 (page 228). For the\\nmoment, observe that using just Equation (9.4) as a basis for term-weighting\\nis likely insufﬁcient. The equations use only collection statistics and infor-\\nmation about the term distribution within the documents judged relevant.\\nThey preserve no memory of the original query.\\n9.1.3\\nWhen does relevance feedback work?\\nThe success of relevance feedback depends on certain assumptions. Firstly,\\nthe user has to have sufﬁcient knowledge to be able to make an initial query\\n', 'Online edition (c)\\n2009 Cambridge UP\\n184\\n9\\nRelevance feedback and query expansion\\nwhich is at least somewhere close to the documents they desire. This is\\nneeded anyhow for successful information retrieval in the basic case, but\\nit is important to see the kinds of problems that relevance feedback cannot\\nsolve alone. Cases where relevance feedback alone is not sufﬁcient include:\\n• Misspellings. If the user spells a term in a different way to the way it\\nis spelled in any document in the collection, then relevance feedback is\\nunlikely to be effective. This can be addressed by the spelling correction\\ntechniques of Chapter 3.\\n• Cross-language information retrieval. Documents in another language\\nare not nearby in a vector space based on term distribution. Rather, docu-\\nments in the same language cluster more closely together.\\n• Mismatch of searcher’s vocabulary versus collection vocabulary. If the\\nuser searches for laptop but all the documents use the term notebook com-\\nputer, then the query will fail, and relevance feedback is again most likely\\nineffective.\\nSecondly, the relevance feedback approach requires relevant documents to\\nbe similar to each other. That is, they should cluster. Ideally, the term dis-\\ntribution in all relevant documents will be similar to that in the documents\\nmarked by the users, while the term distribution in all nonrelevant docu-\\nments will be different from those in relevant documents. Things will work\\nwell if all relevant documents are tightly clustered around a single proto-\\ntype, or, at least, if there are different prototypes, if the relevant documents\\nhave signiﬁcant vocabulary overlap, while similarities between relevant and\\nnonrelevant documents are small. Implicitly, the Rocchio relevance feedback\\nmodel treats relevant documents as a single cluster, which it models via the\\ncentroid of the cluster. This approach does not work as well if the relevant\\ndocuments are a multimodal class, that is, they consist of several clusters of\\ndocuments within the vector space. This can happen with:\\n• Subsets of the documents using different vocabulary, such as Burma vs.\\nMyanmar\\n• A query for which the answer set is inherently disjunctive, such as Pop\\nstars who once worked at Burger King.\\n• Instances of a general concept, which often appear as a disjunction of\\nmore speciﬁc concepts, for example, felines.\\nGood editorial content in the collection can often provide a solution to this\\nproblem. For example, an article on the attitudes of different groups to the\\nsituation in Burma could introduce the terminology used by different parties,\\nthus linking the document clusters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n185\\nRelevance feedback is not necessarily popular with users. Users are often\\nreluctant to provide explicit feedback, or in general do not wish to prolong\\nthe search interaction. Furthermore, it is often harder to understand why a\\nparticular document was retrieved after relevance feedback is applied.\\nRelevance feedback can also have practical problems. The long queries\\nthat are generated by straightforward application of relevance feedback tech-\\nniques are inefﬁcient for a typical IR system. This results in a high computing\\ncost for the retrieval and potentially long response times for the user. A par-\\ntial solution to this is to only reweight certain prominent terms in the relevant\\ndocuments, such as perhaps the top 20 terms by term frequency. Some ex-\\nperimental results have also suggested that using a limited number of terms\\nlike this may give better results (Harman 1992) though other work has sug-\\ngested that using more terms is better in terms of retrieved document quality\\n(Buckley et al. 1994b).\\n9.1.4\\nRelevance feedback on the web\\nSome web search engines offer a similar/related pages feature: the user in-\\ndicates a document in the results set as exemplary from the standpoint of\\nmeeting his information need and requests more documents like it. This can\\nbe viewed as a particular simple form of relevance feedback. However, in\\ngeneral relevance feedback has been little used in web search. One exception\\nwas the Excite web search engine, which initially provided full relevance\\nfeedback. However, the feature was in time dropped, due to lack of use. On\\nthe web, few people use advanced search interfaces and most would like to\\ncomplete their search in a single interaction. But the lack of uptake also prob-\\nably reﬂects two other factors: relevance feedback is hard to explain to the\\naverage user, and relevance feedback is mainly a recall enhancing strategy,\\nand web search users are only rarely concerned with getting sufﬁcient recall.\\nSpink et al. (2000) present results from the use of relevance feedback in\\nthe Excite search engine. Only about 4% of user query sessions used the\\nrelevance feedback option, and these were usually exploiting the “More like\\nthis” link next to each result. About 70% of users only looked at the ﬁrst\\npage of results and did not pursue things any further. For people who used\\nrelevance feedback, results were improved about two thirds of the time.\\nAn important more recent thread of work is the use of clickstream data\\n(what links a user clicks on) to provide indirect relevance feedback. Use\\nof this data is studied in detail in (Joachims 2002b, Joachims et al. 2005).\\nThe very successful use of web link structure (see Chapter 21) can also be\\nviewed as implicit feedback, but provided by page authors rather than read-\\ners (though in practice most authors are also readers).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n186\\n9\\nRelevance feedback and query expansion\\n?\\nExercise 9.1\\nIn Rocchio’s algorithm, what weight setting for α/β/γ does a “Find pages like this\\none” search correspond to?\\nExercise 9.2\\n[⋆]\\nGive three reasons why relevance feedback has been little used in web search.\\n9.1.5\\nEvaluation of relevance feedback strategies\\nInteractive relevance feedback can give very substantial gains in retrieval\\nperformance. Empirically, one round of relevance feedback is often very\\nuseful. Two rounds is sometimes marginally more useful. Successful use of\\nrelevance feedback requires enough judged documents, otherwise the pro-\\ncess is unstable in that it may drift away from the user’s information need.\\nAccordingly, having at least ﬁve judged documents is recommended.\\nThere is some subtlety to evaluating the effectiveness of relevance feed-\\nback in a sound and enlightening way. The obvious ﬁrst strategy is to start\\nwith an initial query q0 and to compute a precision-recall graph. Following\\none round of feedback from the user, we compute the modiﬁed query qm\\nand again compute a precision-recall graph. Here, in both rounds we assess\\nperformance over all documents in the collection, which makes comparisons\\nstraightforward. If we do this, we ﬁnd spectacular gains from relevance feed-\\nback: gains on the order of 50% in mean average precision. But unfortunately\\nit is cheating. The gains are partly due to the fact that known relevant doc-\\numents (judged by the user) are now ranked higher. Fairness demands that\\nwe should only evaluate with respect to documents not seen by the user.\\nA second idea is to use documents in the residual collection (the set of doc-\\numents minus those assessed relevant) for the second round of evaluation.\\nThis seems like a more realistic evaluation. Unfortunately, the measured per-\\nformance can then often be lower than for the original query. This is partic-\\nularly the case if there are few relevant documents, and so a fair proportion\\nof them have been judged by the user in the ﬁrst round. The relative per-\\nformance of variant relevance feedback methods can be validly compared,\\nbut it is difﬁcult to validly compare performance with and without relevance\\nfeedback because the collection size and the number of relevant documents\\nchanges from before the feedback to after it.\\nThus neither of these methods is fully satisfactory. A third method is to\\nhave two collections, one which is used for the initial query and relevance\\njudgments, and the second that is then used for comparative evaluation. The\\nperformance of both q0 and qm can be validly compared on the second col-\\nlection.\\nPerhaps the best evaluation of the utility of relevance feedback is to do user\\nstudies of its effectiveness, in particular by doing a time-based comparison:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.1\\nRelevance feedback and pseudo relevance feedback\\n187\\nPrecision at k = 50\\nTerm weighting\\nno RF\\npseudo RF\\nlnc.ltc\\n64.2%\\n72.7%\\nLnu.ltu\\n74.2%\\n87.0%\\n◮Figure 9.5\\nResults showing pseudo relevance feedback greatly improving perfor-\\nmance. These results are taken from the Cornell SMART system at TREC 4 (Buckley\\net al. 1995), and also contrast the use of two different length normalization schemes\\n(L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20\\nterms to each query.\\nhow fast does a user ﬁnd relevant documents with relevance feedback vs.\\nanother strategy (such as query reformulation), or alternatively, how many\\nrelevant documents does a user ﬁnd in a certain amount of time. Such no-\\ntions of user utility are fairest and closest to real system usage.\\n9.1.6\\nPseudo relevance feedback\\nPseudo relevance feedback, also known as blind relevance feedback, provides a\\nPSEUDO RELEVANCE\\nFEEDBACK\\nBLIND RELEVANCE\\nFEEDBACK\\nmethod for automatic local analysis. It automates the manual part of rele-\\nvance feedback, so that the user gets improved retrieval performance with-\\nout an extended interaction. The method is to do normal retrieval to ﬁnd an\\ninitial set of most relevant documents, to then assume that the top k ranked\\ndocuments are relevant, and ﬁnally to do relevance feedback as before under\\nthis assumption.\\nThis automatic technique mostly works. Evidence suggests that it tends\\nto work better than global analysis (Section 9.2). It has been found to im-\\nprove performance in the TREC ad hoc task. See for example the results in\\nFigure 9.5. But it is not without the dangers of an automatic process. For\\nexample, if the query is about copper mines and the top several documents\\nare all about mines in Chile, then there may be query drift in the direction of\\ndocuments on Chile.\\n9.1.7\\nIndirect relevance feedback\\nWe can also use indirect sources of evidence rather than explicit feedback on\\nrelevance as the basis for relevance feedback. This is often called implicit (rel-\\nIMPLICIT RELEVANCE\\nFEEDBACK\\nevance) feedback. Implicit feedback is less reliable than explicit feedback, but is\\nmore useful than pseudo relevance feedback, which contains no evidence of\\nuser judgments. Moreover, while users are often reluctant to provide explicit\\nfeedback, it is easy to collect implicit feedback in large quantities for a high\\nvolume system, such as a web search engine.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n188\\n9\\nRelevance feedback and query expansion\\nOn the web, DirectHit introduced the idea of ranking more highly docu-\\nments that users chose to look at more often. In other words, clicks on links\\nwere assumed to indicate that the page was likely relevant to the query. This\\napproach makes various assumptions, such as that the document summaries\\ndisplayed in results lists (on whose basis users choose which documents to\\nclick on) are indicative of the relevance of these documents. In the original\\nDirectHit search engine, the data about the click rates on pages was gathered\\nglobally, rather than being user or query speciﬁc. This is one form of the gen-\\neral area of clickstream mining. Today, a closely related approach is used in\\nCLICKSTREAM MINING\\nranking the advertisements that match a web search query (Chapter 19).\\n9.1.8\\nSummary\\nRelevance feedback has been shown to be very effective at improving rele-\\nvance of results. Its successful use requires queries for which the set of rele-\\nvant documents is medium to large. Full relevance feedback is often onerous\\nfor the user, and its implementation is not very efﬁcient in most IR systems.\\nIn many cases, other types of interactive retrieval may improve relevance by\\nabout as much with less work.\\nBeyond the core ad hoc retrieval scenario, other uses of relevance feedback\\ninclude:\\n• Following a changing information need (e.g., names of car models of in-\\nterest change over time)\\n• Maintaining an information ﬁlter (e.g., for a news feed). Such ﬁlters are\\ndiscussed further in Chapter 13.\\n• Active learning (deciding which examples it is most useful to know the\\nclass of to reduce annotation costs).\\n?\\nExercise 9.3\\nUnder what conditions would the modiﬁed query qm in Equation 9.3 be the same as\\nthe original query q0? In all other cases, is qm closer than q0 to the centroid of the\\nrelevant documents?\\nExercise 9.4\\nWhy is positive feedback likely to be more useful than negative feedback to an IR\\nsystem? Why might only using one nonrelevant document be more effective than\\nusing several?\\nExercise 9.5\\nSuppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs. The\\nuser examines two documents, d1 and d2. She judges d1, with the content CDs cheap\\nsoftware cheap CDs relevant and d2 with content cheap thrills DVDs nonrelevant. As-\\nsume that we are using direct term frequency (with no scaling and no document\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.2\\nGlobal methods for query reformulation\\n189\\nfrequency). There is no need to length-normalize vectors. Using Rocchio relevance\\nfeedback as in Equation (9.3) what would the revised query vector be after relevance\\nfeedback? Assume α = 1, β = 0.75, γ = 0.25.\\nExercise 9.6\\n[⋆]\\nOmar has implemented a relevance feedback web search system, where he is going\\nto do relevance feedback based only on words in the title text returned for a page (for\\nefﬁciency). The user is going to rank 3 results. The ﬁrst user, Jinxing, queries for:\\nbanana slug\\nand the top three titles returned are:\\nbanana slug Ariolimax columbianus\\nSanta Cruz mountains banana slug\\nSanta Cruz Campus Mascot\\nJinxing judges the ﬁrst two documents relevant, and the third nonrelevant. Assume\\nthat Omar’s search engine uses term frequency but no length normalization nor IDF.\\nAssume that he is using the Rocchio relevance feedback mechanism, with α = β =\\nγ = 1. Show the ﬁnal revised query that would be run. (Please list the vector elements\\nin alphabetical order.)\\n9.2\\nGlobal methods for query reformulation\\nIn this section we more brieﬂy discuss three global methods for expanding a\\nquery: by simply aiding the user in doing so, by using a manual thesaurus,\\nand through building a thesaurus automatically.\\n9.2.1\\nVocabulary tools for query reformulation\\nVarious user supports in the search process can help the user see how their\\nsearches are or are not working. This includes information about words that\\nwere omitted from the query because they were on stop lists, what words\\nwere stemmed to, the number of hits on each term or phrase, and whether\\nwords were dynamically turned into phrases. The IR system might also sug-\\ngest search terms by means of a thesaurus or a controlled vocabulary. A user\\ncan also be allowed to browse lists of the terms that are in the inverted index,\\nand thus ﬁnd good terms that appear in the collection.\\n9.2.2\\nQuery expansion\\nIn relevance feedback, users give additional input on documents (by mark-\\ning documents in the results set as relevant or not), and this input is used\\nto reweight the terms in the query for documents. In query expansion on the\\nQUERY EXPANSION\\nother hand, users give additional input on query words or phrases, possibly\\nsuggesting additional query terms. Some search engines (especially on the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n190\\n9\\nRelevance feedback and query expansion\\nY\\na\\nh\\no\\no\\n!\\nM\\ny\\nY\\na\\nh\\no\\no\\n!\\nM\\na\\ni\\nl\\nW\\ne\\nl\\nc\\no\\nm\\ne\\n,\\nG\\nu\\ne\\ns\\nt\\n[\\nS\\ni\\ng\\nn\\nI\\nn\\n]\\nH\\ne\\nl\\np\\nS\\ne\\na\\nr\\nc\\nh\\np\\na\\nl\\nm\\nW\\ne\\nb\\nI\\nm\\na\\ng\\ne\\ns\\nV\\ni\\nd\\ne\\no\\nL\\no\\nc\\na\\nl\\nS\\nh\\no\\np\\np\\ni\\nn\\ng\\nm\\no\\nr\\ne\\nO\\np\\nt\\ni\\no\\nn\\ns\\nA\\nl\\ns\\no\\nt\\nr\\ny\\n:\\nS\\nP\\nO\\nN\\nS\\nO\\nR\\nR\\nE\\nS\\nU\\nL\\nT\\nS\\np\\na\\nl\\nm\\nt\\nr\\ne\\ne\\ns\\n,\\np\\na\\nl\\nm\\ns\\np\\nr\\ni\\nn\\ng\\ns\\n,\\np\\na\\nl\\nm\\nc\\ne\\nn\\nt\\nr\\no\\n,\\np\\na\\nl\\nm\\nt\\nr\\ne\\no\\n,\\nM\\no\\nr\\ne\\n.\\n.\\n.\\nP\\na\\nl\\nm\\nb\\nA\\nT\\n&\\nT\\na\\nt\\nt\\n.\\nc\\no\\nm\\n/\\nw\\ni\\nr\\ne\\nl\\ne\\ns\\ns\\nl\\nG\\no\\nm\\no\\nb\\ni\\nl\\ne\\ne\\nf\\nf\\no\\nr\\nt\\nl\\ne\\ns\\ns\\nl\\ny\\nw\\ni\\nt\\nh\\nt\\nh\\ne\\nP\\nA\\nL\\nM\\nT\\nr\\ne\\no\\nf\\nr\\no\\nm\\nA\\nT\\n&\\nT\\n(\\nC\\ni\\nn\\ng\\nu\\nl\\na\\nr\\n)\\n.\\nP\\na\\nl\\nm\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\nP\\na\\nl\\nm\\n.\\nc\\no\\nm\\nl\\nO\\nr\\ng\\na\\nn\\ni\\nz\\ne\\nr\\n,\\nP\\nl\\na\\nn\\nn\\ne\\nr\\n,\\nW\\ni\\nF\\ni\\n,\\nM\\nu\\ns\\ni\\nc\\nB\\nl\\nu\\ne\\nt\\no\\no\\nt\\nh\\n,\\nG\\na\\nm\\ne\\ns\\n,\\nP\\nh\\no\\nt\\no\\ns\\n&\\nV\\ni\\nd\\ne\\no\\n.\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\nM\\na\\nk\\ne\\nr\\no\\nf\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\nP\\nD\\nA\\nd\\ne\\nv\\ni\\nc\\ne\\ns\\nt\\nh\\na\\nt\\na\\nl\\nl\\no\\nw\\nm\\no\\nb\\ni\\nl\\ne\\nu\\ns\\ne\\nr\\ns\\nt\\no\\nm\\na\\nn\\na\\ng\\ne\\ns\\nc\\nh\\ne\\nd\\nu\\nl\\ne\\ns\\n,\\nc\\no\\nn\\nt\\na\\nc\\nt\\ns\\n,\\na\\nn\\nd\\no\\nt\\nh\\ne\\nr\\np\\ne\\nr\\ns\\no\\nn\\na\\nl\\na\\nn\\nd\\nb\\nu\\ns\\ni\\nn\\ne\\ns\\ns\\ni\\nn\\nf\\no\\nr\\nm\\na\\nt\\ni\\no\\nn\\n.\\nw\\nw\\nw\\n.\\np\\na\\nl\\nm\\n.\\nc\\no\\nm\\nl\\nC\\na\\nc\\nh\\ne\\nd\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\nb\\nT\\nr\\ne\\no\\na\\nn\\nd\\nC\\ne\\nn\\nt\\nr\\no\\ns\\nm\\na\\nr\\nt\\np\\nh\\no\\nn\\ne\\ns\\n,\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\n,\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\nP\\na\\nl\\nm\\n,\\nI\\nn\\nc\\n.\\n,\\ni\\nn\\nn\\no\\nv\\na\\nt\\no\\nr\\no\\nf\\ne\\na\\ns\\ny\\nl\\nt\\no\\nl\\nu\\ns\\ne\\nm\\no\\nb\\ni\\nl\\ne\\np\\nr\\no\\nd\\nu\\nc\\nt\\ns\\ni\\nn\\nc\\nl\\nu\\nd\\ni\\nn\\ng\\nP\\na\\nl\\nm\\n®\\nT\\nr\\ne\\no\\n_\\na\\nn\\nd\\nC\\ne\\nn\\nt\\nr\\no\\n_\\ns\\nm\\na\\nr\\nt\\np\\nh\\no\\nn\\ne\\ns\\n,\\nP\\na\\nl\\nm\\nh\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\n,\\ns\\ne\\nr\\nv\\ni\\nc\\ne\\ns\\n,\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\n.\\nw\\nw\\nw\\n.\\np\\na\\nl\\nm\\n.\\nc\\no\\nm\\n/\\nu\\ns\\nl\\nC\\na\\nc\\nh\\ne\\nd\\nS\\nP\\nO\\nN\\nS\\nO\\nR\\nR\\nE\\nS\\nU\\nL\\nT\\nS\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\ns\\na\\nt\\nD\\ne\\nl\\nl\\nS\\nt\\na\\ny\\nC\\no\\nn\\nn\\ne\\nc\\nt\\ne\\nd\\nw\\ni\\nt\\nh\\nH\\na\\nn\\nd\\nh\\ne\\nl\\nd\\nP\\nC\\ns\\n&\\nP\\nD\\nA\\ns\\n.\\nS\\nh\\no\\np\\na\\nt\\nD\\ne\\nl\\nl\\n™\\nO\\nf\\nf\\ni\\nc\\ni\\na\\nl\\nS\\ni\\nt\\ne\\n.\\nw\\nw\\nw\\n.\\nD\\ne\\nl\\nl\\n.\\nc\\no\\nm\\nB\\nu\\ny\\nP\\na\\nl\\nm\\nC\\ne\\nn\\nt\\nr\\no\\nC\\na\\ns\\ne\\ns\\nU\\nl\\nt\\ni\\nm\\na\\nt\\ne\\ns\\ne\\nl\\ne\\nc\\nt\\ni\\no\\nn\\no\\nf\\nc\\na\\ns\\ne\\ns\\na\\nn\\nd\\na\\nc\\nc\\ne\\ns\\ns\\no\\nr\\ni\\ne\\ns\\nf\\no\\nr\\nb\\nu\\ns\\ni\\nn\\ne\\ns\\ns\\nd\\ne\\nv\\ni\\nc\\ne\\ns\\n.\\nw\\nw\\nw\\n.\\nC\\na\\ns\\ne\\ns\\n.\\nc\\no\\nm\\nF\\nr\\ne\\ne\\nP\\nl\\na\\nm\\nT\\nr\\ne\\no\\nG\\ne\\nt\\nA\\nF\\nr\\ne\\ne\\nP\\na\\nl\\nm\\nT\\nr\\ne\\no\\n7\\n0\\n0\\nW\\nP\\nh\\no\\nn\\ne\\n.\\nP\\na\\nr\\nt\\ni\\nc\\ni\\np\\na\\nt\\ne\\nT\\no\\nd\\na\\ny\\n.\\nE\\nv\\na\\nl\\nu\\na\\nt\\ni\\no\\nn\\nN\\na\\nt\\ni\\no\\nn\\n.\\nc\\no\\nm\\n/\\nt\\nr\\ne\\no\\n1\\nª\\n1\\n0\\no\\nf\\na\\nb\\no\\nu\\nt\\n5\\n3\\n4\\n,\\n0\\n0\\n0\\n,\\n0\\n0\\n0\\nf\\no\\nr\\np\\na\\nl\\nm\\n(\\nA\\nb\\no\\nu\\nt\\nt\\nh\\ni\\ns\\np\\na\\ng\\ne\\n)\\nª\\n0\\n.\\n1\\n1\\ns\\ne\\nc\\n.\\n◮Figure 9.6\\nAn example of query expansion in the interface of the Yahoo! web\\nsearch engine in 2006. The expanded query suggestions appear just below the “Search\\nResults” bar.\\nweb) suggest related queries in response to a query; the users then opt to use\\none of these alternative query suggestions. Figure 9.6 shows an example of\\nquery suggestion options being presented in the Yahoo! web search engine.\\nThe central question in this form of query expansion is how to generate al-\\nternative or expanded queries for the user. The most common form of query\\nexpansion is global analysis, using some form of thesaurus. For each term\\nt in a query, the query can be automatically expanded with synonyms and\\nrelated words of t from the thesaurus. Use of a thesaurus can be combined\\nwith ideas of term weighting: for instance, one might weight added terms\\nless than original query terms.\\nMethods for building a thesaurus for query expansion include:\\n• Use of a controlled vocabulary that is maintained by human editors. Here,\\nthere is a canonical term for each concept. The subject headings of tra-\\nditional library subject indexes, such as the Library of Congress Subject\\nHeadings, or the Dewey Decimal system are examples of a controlled\\nvocabulary. Use of a controlled vocabulary is quite common for well-\\nresourced domains. A well-known example is the Uniﬁed Medical Lan-\\nguage System (UMLS) used with MedLine for querying the biomedical\\nresearch literature. For example, in Figure 9.7, neoplasms was added to a\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.2\\nGlobal methods for query reformulation\\n191\\n• User query: cancer\\n• PubMed query: (“neoplasms”[TIAB] NOT Medline[SB]) OR “neoplasms”[MeSH\\nTerms] OR cancer[Text Word]\\n• User query: skin itch\\n• PubMed query: (“skin”[MeSH Terms] OR (“integumentary system”[TIAB] NOT\\nMedline[SB]) OR “integumentary system”[MeSH Terms] OR skin[Text Word]) AND\\n((“pruritus”[TIAB] NOT Medline[SB]) OR “pruritus”[MeSH Terms] OR itch[Text\\nWord])\\n◮Figure 9.7\\nExamples of query expansion via the PubMed thesaurus. When a user\\nissues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/,\\ntheir query is mapped on to the Medline vocabulary as shown.\\nsearch for cancer. This Medline query expansion also contrasts with the\\nYahoo! example. The Yahoo! interface is a case of interactive query expan-\\nsion, whereas PubMed does automatic query expansion. Unless the user\\nchooses to examine the submitted query, they may not even realize that\\nquery expansion has occurred.\\n• A manual thesaurus. Here, human editors have built up sets of synony-\\nmous names for concepts, without designating a canonical term.\\nThe\\nUMLS metathesaurus is one example of a thesaurus. Statistics Canada\\nmaintains a thesaurus of preferred terms, synonyms, broader terms, and\\nnarrower terms for matters on which the government collects statistics,\\nsuch as goods and services. This thesaurus is also bilingual English and\\nFrench.\\n• An automatically derived thesaurus. Here, word co-occurrence statistics\\nover a collection of documents in a domain are used to automatically in-\\nduce a thesaurus; see Section 9.2.3.\\n• Query reformulations based on query log mining. Here, we exploit the\\nmanual query reformulations of other users to make suggestions to a new\\nuser. This requires a huge query volume, and is thus particularly appro-\\npriate to web search.\\nThesaurus-based query expansion has the advantage of not requiring any\\nuser input. Use of query expansion generally increases recall and is widely\\nused in many science and engineering ﬁelds. As well as such global analysis\\ntechniques, it is also possible to do query expansion by local analysis, for\\ninstance, by analyzing the documents in the result set. User input is now\\n', 'Online edition (c)\\n2009 Cambridge UP\\n192\\n9\\nRelevance feedback and query expansion\\nWord\\nNearest neighbors\\nabsolutely\\nabsurd, whatsoever, totally, exactly, nothing\\nbottomed\\ndip, copper, drops, topped, slide, trimmed\\ncaptivating\\nshimmer, stunningly, superbly, plucky, witty\\ndoghouse\\ndog, porch, crawling, beside, downstairs\\nmakeup\\nrepellent, lotion, glossy, sunscreen, skin, gel\\nmediating\\nreconciliation, negotiate, case, conciliation\\nkeeping\\nhoping, bring, wiping, could, some, would\\nlithographs\\ndrawings, Picasso, Dali, sculptures, Gauguin\\npathogens\\ntoxins, bacteria, organisms, bacterial, parasite\\nsenses\\ngrasp, psyche, truly, clumsy, naive, innate\\n◮Figure 9.8\\nAn example of an automatically generated thesaurus. This example\\nis based on the work in Schütze (1998), which employs latent semantic indexing (see\\nChapter 18).\\nusually required, but a distinction remains as to whether the user is giving\\nfeedback on documents or on query terms.\\n9.2.3\\nAutomatic thesaurus generation\\nAs an alternative to the cost of a manual thesaurus, we could attempt to\\ngenerate a thesaurus automatically by analyzing a collection of documents.\\nThere are two main approaches. One is simply to exploit word cooccurrence.\\nWe say that words co-occurring in a document or paragraph are likely to be\\nin some sense similar or related in meaning, and simply count text statistics\\nto ﬁnd the most similar words. The other approach is to use a shallow gram-\\nmatical analysis of the text and to exploit grammatical relations or grammat-\\nical dependencies. For example, we say that entities that are grown, cooked,\\neaten, and digested, are more likely to be food items. Simply using word\\ncooccurrence is more robust (it cannot be misled by parser errors), but using\\ngrammatical relations is more accurate.\\nThe simplest way to compute a co-occurrence thesaurus is based on term-\\nterm similarities. We begin with a term-document matrix A, where each cell\\nAt,d is a weighted count wt,d for term t and document d, with weighting so\\nA has length-normalized rows. If we then calculate C = AAT, then Cu,v is\\na similarity score between terms u and v, with a larger number being better.\\nFigure 9.8 shows an example of a thesaurus derived in basically this manner,\\nbut with an extra step of dimensionality reduction via Latent Semantic In-\\ndexing, which we discuss in Chapter 18. While some of the thesaurus terms\\nare good or at least suggestive, others are marginal or bad. The quality of the\\nassociations is typically a problem. Term ambiguity easily introduces irrel-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n9.3\\nReferences and further reading\\n193\\nevant statistically correlated terms. For example, a query for Apple computer\\nmay expand to Apple red fruit computer. In general these thesauri suffer from\\nboth false positives and false negatives. Moreover, since the terms in the au-\\ntomatic thesaurus are highly correlated in documents anyway (and often the\\ncollection used to derive the thesaurus is the same as the one being indexed),\\nthis form of query expansion may not retrieve many additional documents.\\nQuery expansion is often effective in increasing recall. However, there is\\na high cost to manually producing a thesaurus and then updating it for sci-\\nentiﬁc and terminological developments within a ﬁeld. In general a domain-\\nspeciﬁc thesaurus is required: general thesauri and dictionaries give far too\\nlittle coverage of the rich domain-particular vocabularies of most scientiﬁc\\nﬁelds. However, query expansion may also signiﬁcantly decrease precision,\\nparticularly when the query contains ambiguous terms. For example, if the\\nuser searches for interest rate, expanding the query to interest rate fascinate eval-\\nuate is unlikely to be useful. Overall, query expansion is less successful than\\nrelevance feedback, though it may be as good as pseudo relevance feedback.\\nIt does, however, have the advantage of being much more understandable to\\nthe system user.\\n?\\nExercise 9.7\\nIf A is simply a Boolean cooccurrence matrix, then what do you get as the entries in\\nC?\\n9.3\\nReferences and further reading\\nWork in information retrieval quickly confronted the problem of variant ex-\\npression which meant that the words in a query might not appear in a doc-\\nument, despite it being relevant to the query. An early experiment about\\n1960 cited by Swanson (1988) found that only 11 out of 23 documents prop-\\nerly indexed under the subject toxicity had any use of a word containing the\\nstem toxi. There is also the issue of translation, of users knowing what terms\\na document will use. Blair and Maron (1985) conclude that “it is impossibly\\ndifﬁcult for users to predict the exact words, word combinations, and phrases\\nthat are used by all (or most) relevant documents and only (or primarily) by\\nthose documents”.\\nThe main initial papers on relevance feedback using vector space models\\nall appear in Salton (1971b), including the presentation of the Rocchio al-\\ngorithm (Rocchio 1971) and the Ide dec-hi variant along with evaluation of\\nseveral variants (Ide 1971). Another variant is to regard all documents in\\nthe collection apart from those judged relevant as nonrelevant, rather than\\nonly ones that are explicitly judged nonrelevant. However, Schütze et al.\\n(1995) and Singhal et al. (1997) show that better results are obtained for rout-\\ning by using only documents close to the query of interest rather than all\\n', 'Online edition (c)\\n2009 Cambridge UP\\n194\\n9\\nRelevance feedback and query expansion\\ndocuments. Other later work includes Salton and Buckley (1990), Riezler\\net al. (2007) (a statistical NLP approach to RF) and the recent survey paper\\nRuthven and Lalmas (2003).\\nThe effectiveness of interactive relevance feedback systems is discussed in\\n(Salton 1989, Harman 1992, Buckley et al. 1994b). Koenemann and Belkin\\n(1996) do user studies of the effectiveness of relevance feedback.\\nTraditionally Roget’s thesaurus has been the best known English language\\nthesaurus (Roget 1946). In recent computational work, people almost always\\nuse WordNet (Fellbaum 1998), not only because it is free, but also because of\\nits rich link structure. It is available at: http://wordnet.princeton.edu.\\nQiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus gener-\\nation. Xu and Croft (1996) explore using both local and global query expan-\\nsion.\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n195\\n10\\nXML retrieval\\nInformation retrieval systems are often contrasted with relational databases.\\nTraditionally, IR systems have retrieved information from unstructured text\\n– by which we mean “raw” text without markup. Databases are designed\\nfor querying relational data: sets of records that have values for predeﬁned\\nattributes such as employee number, title and salary. There are fundamental\\ndifferences between information retrieval and database systems in terms of\\nretrieval model, data structures and query language as shown in Table 10.1.1\\nSome highly structured text search problems are most efﬁciently handled\\nby a relational database, for example, if the employee table contains an at-\\ntribute for short textual job descriptions and you want to ﬁnd all employees\\nwho are involved with invoicing. In this case, the SQL query:\\nselect lastname from employees where job_desc like ’invoic%’;\\nmay be sufﬁcient to satisfy your information need with high precision and\\nrecall.\\nHowever, many structured data sources containing text are best modeled\\nas structured documents rather than relational data. We call the search over\\nsuch structured documents structured retrieval. Queries in structured retrieval\\nSTRUCTURED\\nRETRIEVAL\\ncan be either structured or unstructured, but we will assume in this chap-\\nter that the collection consists only of structured documents. Applications\\nof structured retrieval include digital libraries, patent databases, blogs, text\\nin which entities like persons and locations have been tagged (in a process\\ncalled named entity tagging) and output from ofﬁce suites like OpenOfﬁce\\nthat save documents as marked up text. In all of these applications, we want\\nto be able to run queries that combine textual criteria with structural criteria.\\nExamples of such queries are give me a full-length article on fast fourier transforms\\n(digital libraries), give me patents whose claims mention RSA public key encryption\\n1. In most modern database systems, one can enable full-text search for text columns. This\\nusually means that an inverted index is created and Boolean or vector space search enabled,\\neffectively combining core database with information retrieval technologies.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n196\\n10\\nXML retrieval\\nRDB search\\nunstructured retrieval\\nstructured retrieval\\nobjects\\nrecords\\nunstructured documents\\ntrees with text at leaves\\nmodel\\nrelational model\\nvector space & others\\n?\\nmain data structure\\ntable\\ninverted index\\n?\\nqueries\\nSQL\\nfree text queries\\n?\\n◮Table 10.1\\nRDB (relational database) search, unstructured information retrieval\\nand structured information retrieval. There is no consensus yet as to which methods\\nwork best for structured retrieval although many researchers believe that XQuery\\n(page 215) will become the standard for structured queries.\\nand that cite US patent 4,405,829 (patents), or give me articles about sightseeing\\ntours of the Vatican and the Coliseum (entity-tagged text). These three queries\\nare structured queries that cannot be answered well by an unranked retrieval\\nsystem. As we argued in Example 1.1 (page 15) unranked retrieval models\\nlike the Boolean model suffer from low recall. For instance, an unranked\\nsystem would return a potentially large number of articles that mention the\\nVatican, the Coliseum and sightseeing tours without ranking the ones that\\nare most relevant for the query ﬁrst. Most users are also notoriously bad at\\nprecisely stating structural constraints. For instance, users may not know\\nfor which structured elements the search system supports search. In our ex-\\nample, the user may be unsure whether to issue the query as sightseeing AND\\n(COUNTRY:Vatican OR LANDMARK:Coliseum) , as sightseeing AND (STATE:Vatican OR\\nBUILDING:Coliseum) or in some other form. Users may also be completely un-\\nfamiliar with structured search and advanced search interfaces or unwilling\\nto use them. In this chapter, we look at how ranked retrieval methods can be\\nadapted to structured documents to address these problems.\\nWe will only look at one standard for encoding structured documents: Ex-\\ntensible Markup Language or XML, which is currently the most widely used\\nXML\\nsuch standard. We will not cover the speciﬁcs that distinguish XML from\\nother types of markup such as HTML and SGML. But most of what we say\\nin this chapter is applicable to markup languages in general.\\nIn the context of information retrieval, we are only interested in XML as\\na language for encoding text and documents. A perhaps more widespread\\nuse of XML is to encode non-text data. For example, we may want to export\\ndata in XML format from an enterprise resource planning system and then\\nread them into an analytics program to produce graphs for a presentation.\\nThis type of application of XML is called data-centric because numerical and\\nDATA-CENTRIC XML\\nnon-text attribute-value data dominate and text is usually a small fraction of\\nthe overall data. Most data-centric XML is stored in databases – in contrast\\nto the inverted index-based methods for text-centric XML that we present in\\nthis chapter.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.1\\nBasic XML concepts\\n197\\nWe call XML retrieval structured retrieval in this chapter. Some researchers\\nprefer the term semistructured retrieval to distinguish XML retrieval from database\\nSEMISTRUCTURED\\nRETRIEVAL\\nquerying. We have adopted the terminology that is widespread in the XML\\nretrieval community. For instance, the standard way of referring to XML\\nqueries is structured queries, not semistructured queries. The term structured\\nretrieval is rarely used for database querying and it always refers to XML\\nretrieval in this book.\\nThere is a second type of information retrieval problem that is intermediate\\nbetween unstructured retrieval and querying a relational database: paramet-\\nric and zone search, which we discussed in Section 6.1 (page 110). In the\\ndata model of parametric and zone search, there are parametric ﬁelds (re-\\nlational attributes like date or ﬁle-size) and zones – text attributes that each\\ntake a chunk of unstructured text as value, e.g., author and title in Figure 6.1\\n(page 111). The data model is ﬂat, that is, there is no nesting of attributes.\\nThe number of attributes is small. In contrast, XML documents have the\\nmore complex tree structure that we see in Figure 10.2 in which attributes\\nare nested. The number of attributes and nodes is greater than in parametric\\nand zone search.\\nAfter presenting the basic concepts of XML in Section 10.1, this chapter\\nﬁrst discusses the challenges we face in XML retrieval (Section 10.2). Next we\\ndescribe a vector space model for XML retrieval (Section 10.3). Section 10.4\\npresents INEX, a shared task evaluation that has been held for a number of\\nyears and currently is the most important venue for XML retrieval research.\\nWe discuss the differences between data-centric and text-centric approaches\\nto XML in Section 10.5.\\n10.1\\nBasic XML concepts\\nAn XML document is an ordered, labeled tree. Each node of the tree is an\\nXML element and is written with an opening and closing tag. An element can\\nXML ELEMENT\\nhave one or more XML attributes. In the XML document in Figure 10.1, the\\nXML ATTRIBUTE\\nscene element is enclosed by the two tags <scene ...> and </scene>. It\\nhas an attribute number with value vii and two child elements, title and verse.\\nFigure 10.2 shows Figure 10.1 as a tree. The leaf nodes of the tree consist of\\ntext, e.g., Shakespeare, Macbeth, and Macbeth’s castle. The tree’s internal nodes\\nencode either the structure of the document (title, act, and scene) or metadata\\nfunctions (author).\\nThe standard for accessing and processing XML documents is the XML\\nDocument Object Model or DOM. The DOM represents elements, attributes\\nXML DOM\\nand text within elements as nodes in a tree. Figure 10.2 is a simpliﬁed DOM\\nrepresentation of the XML document in Figure 10.1.2 With a DOM API, we\\n2. The representation is simpliﬁed in a number of respects. For example, we do not show the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n198\\n10\\nXML retrieval\\n<play>\\n<author>Shakespeare</author>\\n<title>Macbeth</title>\\n<act number=\"I\">\\n<scene number=\"vii\">\\n<title>Macbeth’s castle</title>\\n<verse>Will I with wine and wassail ...</verse>\\n</scene>\\n</act>\\n</play>\\n◮Figure 10.1\\nAn XML document.\\nroot element\\nplay\\nelement\\nauthor\\nelement\\nact\\nelement\\ntitle\\ntext\\nShakespeare\\ntext\\nMacbeth\\nattribute\\nnumber=\"I\"\\nelement\\nscene\\nattribute\\nnumber=\"vii\"\\nelement\\nverse\\nelement\\ntitle\\ntext\\nWill I with ...\\ntext\\nMacbeth’s castle\\n◮Figure 10.2\\nThe XML document in Figure 10.1 as a simpliﬁed DOM object.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.1\\nBasic XML concepts\\n199\\n//article\\n[.//yr = 2001 or .//yr = 2002]\\n//section\\n[about(.,summer holidays)]\\nholidays\\nsummer\\nsection\\narticle\\n◮Figure 10.3\\nAn XML query in NEXI format and its partial representation as a tree.\\ncan process an XML document by starting at the root element and then de-\\nscending down the tree from parents to children.\\nXPath is a standard for enumerating paths in an XML document collection.\\nXPATH\\nWe will also refer to paths as XML contexts or simply contexts in this chapter.\\nXML CONTEXT\\nOnly a small subset of XPath is needed for our purposes. The XPath expres-\\nsion node selects all nodes of that name. Successive elements of a path are\\nseparated by slashes, so act/scene selects all scene elements whose par-\\nent is an act element. Double slashes indicate that an arbitrary number of\\nelements can intervene on a path: play//scene selects all scene elements\\noccurring in a play element. In Figure 10.2 this set consists of a single scene el-\\nement, which is accessible via the path play, act, scene from the top. An initial\\nslash starts the path at the root element. /play/title selects the play’s ti-\\ntle in Figure 10.1, /play//title selects a set with two members (the play’s\\ntitle and the scene’s title), and /scene/title selects no elements. For no-\\ntational convenience, we allow the ﬁnal element of a path to be a vocabulary\\nterm and separate it from the element path by the symbol #, even though this\\ndoes not conform to the XPath standard. For example, title#\"Macbeth\"\\nselects all titles containing the term Macbeth.\\nWe also need the concept of schema in this chapter. A schema puts con-\\nSCHEMA\\nstraints on the structure of allowable XML documents for a particular ap-\\nplication. A schema for Shakespeare’s plays may stipulate that scenes can\\nonly occur as children of acts and that only acts and scenes have the num-\\nber attribute. Two standards for schemas for XML documents are XML DTD\\nXML DTD\\n(document type deﬁnition) and XML Schema. Users can only write structured\\nXML SCHEMA\\nqueries for an XML retrieval system if they have some minimal knowledge\\nabout the schema of the collection.\\nroot node and text is not embedded in text nodes. See http://www.w3.org/DOM/.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n200\\n10\\nXML retrieval\\nM’s castle\\ntitle\\nWill I . . .\\nverse\\nscene\\nJulius Caesar\\ntitle\\nbook\\nGallic war\\ntitle\\nJulius Caesar\\nauthor\\nbook\\nd1\\nq1\\nq2\\n◮Figure 10.4\\nTree representation of XML documents and queries.\\nA common format for XML queries is NEXI (Narrowed Extended XPath\\nNEXI\\nI). We give an example in Figure 10.3. We display the query on four lines for\\ntypographical convenience, but it is intended to be read as one unit without\\nline breaks. In particular, //section is embedded under //article.\\nThe query in Figure 10.3 speciﬁes a search for sections about the sum-\\nmer holidays that are part of articles from 2001 or 2002. As in XPath dou-\\nble slashes indicate that an arbitrary number of elements can intervene on\\na path.\\nThe dot in a clause in square brackets refers to the element the\\nclause modiﬁes. The clause [.//yr = 2001 or .//yr = 2002] mod-\\niﬁes //article. Thus, the dot refers to //article in this case. Similarly,\\nthe dot in [about(., summer holidays)] refers to the section that the\\nclause modiﬁes.\\nThe two yr conditions are relational attribute constraints. Only articles\\nwhose yr attribute is 2001 or 2002 (or that contain an element whose yr\\nattribute is 2001 or 2002) are to be considered. The about clause is a ranking\\nconstraint: Sections that occur in the right type of article are to be ranked\\naccording to how relevant they are to the topic summer holidays.\\nWe usually handle relational attribute constraints by preﬁltering or post-\\nﬁltering: We simply exclude all elements from the result set that do not meet\\nthe relational attribute constraints. In this chapter, we will not address how\\nto do this efﬁciently and instead focus on the core information retrieval prob-\\nlem in XML retrieval, namely how to rank documents according to the rele-\\nvance criteria expressed in the about conditions of the NEXI query.\\nIf we discard relational attributes, we can represent documents as trees\\nwith only one type of node: element nodes. In other words, we remove\\nall attribute nodes from the XML document, such as the number attribute in\\nFigure 10.1. Figure 10.4 shows a subtree of the document in Figure 10.1 as an\\nelement-node tree (labeled d1).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n201\\nWe can represent queries as trees in the same way. This is a query-by-\\nexample approach to query language design because users pose queries by\\ncreating objects that satisfy the same formal description as documents. In\\nFigure 10.4, q1 is a search for books whose titles score highly for the keywords\\nJulius Caesar. q2 is a search for books whose author elements score highly for\\nJulius Caesar and whose title elements score highly for Gallic war.3\\n10.2\\nChallenges in XML retrieval\\nIn this section, we discuss a number of challenges that make structured re-\\ntrieval more difﬁcult than unstructured retrieval. Recall from page 195 the\\nbasic setting we assume in structured retrieval: the collection consists of\\nstructured documents and queries are either structured (as in Figure 10.3)\\nor unstructured (e.g., summer holidays).\\nThe ﬁrst challenge in structured retrieval is that users want us to return\\nparts of documents (i.e., XML elements), not entire documents as IR systems\\nusually do in unstructured retrieval. If we query Shakespeare’s plays for\\nMacbeth’s castle, should we return the scene, the act or the entire play in Fig-\\nure 10.2? In this case, the user is probably looking for the scene. On the other\\nhand, an otherwise unspeciﬁed search for Macbeth should return the play of\\nthis name, not a subunit.\\nOne criterion for selecting the most appropriate part of a document is the\\nstructured document retrieval principle:\\nSTRUCTURED\\nDOCUMENT RETRIEVAL\\nPRINCIPLE\\nStructured document retrieval principle. A system should always re-\\ntrieve the most speciﬁc part of a document answering the query.\\nThis principle motivates a retrieval strategy that returns the smallest unit\\nthat contains the information sought, but does not go below this level. How-\\never, it can be hard to implement this principle algorithmically. Consider the\\nquery title#\"Macbeth\" applied to Figure 10.2. The title of the tragedy,\\nMacbeth, and the title of Act I, Scene vii, Macbeth’s castle, are both good hits\\nbecause they contain the matching term Macbeth. But in this case, the title of\\nthe tragedy, the higher node, is preferred. Deciding which level of the tree is\\nright for answering a query is difﬁcult.\\nParallel to the issue of which parts of a document to return to the user is\\nthe issue of which parts of a document to index. In Section 2.1.2 (page 20), we\\ndiscussed the need for a document unit or indexing unit in indexing and re-\\nINDEXING UNIT\\ntrieval. In unstructured retrieval, it is usually clear what the right document\\n3. To represent the semantics of NEXI queries fully we would also need to designate one node\\nin the tree as a “target node”, for example, the section in the tree in Figure 10.3. Without the\\ndesignation of a target node, the tree in Figure 10.3 is not a search for sections embedded in\\narticles (as speciﬁed by NEXI), but a search for articles that contain sections.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n202\\n10\\nXML retrieval\\n◮Figure 10.5\\nPartitioning an XML document into non-overlapping indexing units.\\nunit is: ﬁles on your desktop, email messages, web pages on the web etc. In\\nstructured retrieval, there are a number of different approaches to deﬁning\\nthe indexing unit.\\nOne approach is to group nodes into non-overlapping pseudodocuments\\nas shown in Figure 10.5. In the example, books, chapters and sections have\\nbeen designated to be indexing units, but without overlap. For example, the\\nleftmost dashed indexing unit contains only those parts of the tree domi-\\nnated by book that are not already part of other indexing units. The disad-\\nvantage of this approach is that pseudodocuments may not make sense to\\nthe user because they are not coherent units. For instance, the leftmost in-\\ndexing unit in Figure 10.5 merges three disparate elements, the class, author\\nand title elements.\\nWe can also use one of the largest elements as the indexing unit, for exam-\\nple, the book element in a collection of books or the play element for Shake-\\nspeare’s works. We can then postprocess search results to ﬁnd for each book\\nor play the subelement that is the best hit. For example, the query Macbeth’s\\ncastle may return the play Macbeth, which we can then postprocess to identify\\nact I, scene vii as the best-matching subelement. Unfortunately, this two-\\nstage retrieval process fails to return the best subelement for many queries\\nbecause the relevance of a whole book is often not a good predictor of the\\nrelevance of small subelements within it.\\nInstead of retrieving large units and identifying subelements (top down),\\nwe can also search all leaves, select the most relevant ones and then extend\\nthem to larger units in postprocessing (bottom up). For the query Macbeth’s\\ncastle in Figure 10.1, we would retrieve the title Macbeth’s castle in the ﬁrst\\npass and then decide in a postprocessing step whether to return the title, the\\nscene, the act or the play. This approach has a similar problem as the last one:\\nThe relevance of a leaf element is often not a good predictor of the relevance\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n203\\nof elements it is contained in.\\nThe least restrictive approach is to index all elements. This is also prob-\\nlematic. Many XML elements are not meaningful search results, e.g., typo-\\ngraphical elements like <b>definitely</b> or an ISBN number which\\ncannot be interpreted without context. Also, indexing all elements means\\nthat search results will be highly redundant. For the query Macbeth’s castle\\nand the document in Figure 10.1, we would return all of the play, act, scene\\nand title elements on the path between the root node and Macbeth’s castle.\\nThe leaf node would then occur four times in the result set, once directly and\\nthree times as part of other elements. We call elements that are contained\\nwithin each other nested. Returning redundant nested elements in a list of\\nNESTED ELEMENTS\\nreturned hits is not very user-friendly.\\nBecause of the redundancy caused by nested elements it is common to re-\\nstrict the set of elements that are eligible to be returned. Restriction strategies\\ninclude:\\n• discard all small elements\\n• discard all element types that users do not look at (this requires a working\\nXML retrieval system that logs this information)\\n• discard all element types that assessors generally do not judge to be rele-\\nvant (if relevance assessments are available)\\n• only keep element types that a system designer or librarian has deemed\\nto be useful search results\\nIn most of these approaches, result sets will still contain nested elements.\\nThus, we may want to remove some elements in a postprocessing step to re-\\nduce redundancy. Alternatively, we can collapse several nested elements in\\nthe results list and use highlighting of query terms to draw the user’s atten-\\ntion to the relevant passages. If query terms are highlighted, then scanning a\\nmedium-sized element (e.g., a section) takes little more time than scanning a\\nsmall subelement (e.g., a paragraph). Thus, if the section and the paragraph\\nboth occur in the results list, it is sufﬁcient to show the section. An additional\\nadvantage of this approach is that the paragraph is presented together with\\nits context (i.e., the embedding section). This context may be helpful in in-\\nterpreting the paragraph (e.g., the source of the information reported) even\\nif the paragraph on its own satisﬁes the query.\\nIf the user knows the schema of the collection and is able to specify the\\ndesired type of element, then the problem of redundancy is alleviated as few\\nnested elements have the same type. But as we discussed in the introduction,\\nusers often don’t know what the name of an element in the collection is (Is the\\nVatican a country or a city?) or they may not know how to compose structured\\nqueries at all.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n204\\n10\\nXML retrieval\\nGates\\nbook\\nGates\\nauthor\\nbook\\nGates\\ncreator\\nbook\\nGates\\nlastname\\nBill\\nﬁrstname\\nauthor\\nbook\\nq3\\nq4\\nd2\\nd3\\n◮Figure 10.6\\nSchema heterogeneity: intervening nodes and mismatched names.\\nA challenge in XML retrieval related to nesting is that we may need to\\ndistinguish different contexts of a term when we compute term statistics for\\nranking, in particular inverse document frequency (idf) statistics as deﬁned\\nin Section 6.2.1 (page 117). For example, the term Gates under the node author\\nis unrelated to an occurrence under a content node like section if used to refer\\nto the plural of gate. It makes little sense to compute a single document\\nfrequency for Gates in this example.\\nOne solution is to compute idf for XML-context/term pairs, e.g., to com-\\npute different idf weights for author#\"Gates\" and section#\"Gates\".\\nUnfortunately, this scheme will run into sparse data problems – that is, many\\nXML-context pairs occur too rarely to reliably estimate df (see Section 13.2,\\npage 260, for a discussion of sparseness). A compromise is only to con-\\nsider the parent node x of the term and not the rest of the path from the\\nroot to x to distinguish contexts. There are still conﬂations of contexts that\\nare harmful in this scheme. For instance, we do not distinguish names of\\nauthors and names of corporations if both have the parent node name. But\\nmost important distinctions, like the example contrast author#\"Gates\" vs.\\nsection#\"Gates\", will be respected.\\nIn many cases, several different XML schemas occur in a collection since\\nthe XML documents in an IR application often come from more than one\\nsource. This phenomenon is called schema heterogeneity or schema diversity\\nSCHEMA\\nHETEROGENEITY\\nand presents yet another challenge. As illustrated in Figure 10.6 comparable\\nelements may have different names: creator in d2 vs. author in d3. In other\\ncases, the structural organization of the schemas may be different: Author\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.2\\nChallenges in XML retrieval\\n205\\nnames are direct descendants of the node author in q3, but there are the in-\\ntervening nodes ﬁrstname and lastname in d3. If we employ strict matching\\nof trees, then q3 will retrieve neither d2 nor d3 although both documents are\\nrelevant. Some form of approximate matching of element names in combina-\\ntion with semi-automatic matching of different document structures can help\\nhere. Human editing of correspondences of elements in different schemas\\nwill usually do better than automatic methods.\\nSchema heterogeneity is one reason for query-document mismatches like\\nq3/d2 and q3/d3. Another reason is that users often are not familiar with the\\nelement names and the structure of the schemas of collections they search\\nas mentioned. This poses a challenge for interface design in XML retrieval.\\nIdeally, the user interface should expose the tree structure of the collection\\nand allow users to specify the elements they are querying. If we take this\\napproach, then designing the query interface in structured retrieval is more\\ncomplex than a search box for keyword queries in unstructured retrieval.\\nWe can also support the user by interpreting all parent-child relationships\\nin queries as descendant relationships with any number of intervening nodes\\nallowed. We call such queries extended queries. The tree in Figure 10.3 and q4\\nEXTENDED QUERY\\nin Figure 10.6 are examples of extended queries. We show edges that are\\ninterpreted as descendant relationships as dashed arrows. In q4, a dashed\\narrow connects book and Gates. As a pseudo-XPath notation for q4, we adopt\\nbook//#\"Gates\": a book that somewhere in its structure contains the word\\nGates where the path from the book node to Gates can be arbitrarily long.\\nThe pseudo-XPath notation for the extended query that in addition speciﬁes\\nthat Gates occurs in a section of the book is book//section//#\"Gates\".\\nIt is convenient for users to be able to issue such extended queries without\\nhaving to specify the exact structural conﬁguration in which a query term\\nshould occur – either because they do not care about the exact conﬁguration\\nor because they do not know enough about the schema of the collection to be\\nable to specify it.\\nIn Figure 10.7, the user is looking for a chapter entitled FFT (q5). Sup-\\npose there is no such chapter in the collection, but that there are references to\\nbooks on FFT (d4). A reference to a book on FFT is not exactly what the user\\nis looking for, but it is better than returning nothing. Extended queries do not\\nhelp here. The extended query q6 also returns nothing. This is a case where\\nwe may want to interpret the structural constraints speciﬁed in the query as\\nhints as opposed to as strict conditions. As we will discuss in Section 10.4,\\nusers prefer a relaxed interpretation of structural constraints: Elements that\\ndo not meet structural constraints perfectly should be ranked lower, but they\\nshould not be omitted from search results.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n206\\n10\\nXML retrieval\\nFFT\\ntitle\\nchapter\\nFFT\\ntitle\\nchapter\\nFFT\\ntitle\\nencryption\\ntitle\\nreferences\\nchapter\\nbook\\nq5\\nq6\\nd4\\n◮Figure 10.7\\nA structural mismatch between two queries and a document.\\n10.3\\nA vector space model for XML retrieval\\nIn this section, we present a simple vector space model for XML retrieval.\\nIt is not intended to be a complete description of a state-of-the-art system.\\nInstead, we want to give the reader a ﬂavor of how documents can be repre-\\nsented and retrieved in XML retrieval.\\nTo take account of structure in retrieval in Figure 10.4, we want a book\\nentitled Julius Caesar to be a match for q1 and no match (or a lower weighted\\nmatch) for q2. In unstructured retrieval, there would be a single dimension\\nof the vector space for Caesar. In XML retrieval, we must separate the title\\nword Caesar from the author name Caesar. One way of doing this is to have\\neach dimension of the vector space encode a word together with its position\\nwithin the XML tree.\\nFigure 10.8 illustrates this representation. We ﬁrst take each text node\\n(which in our setup is always a leaf) and break it into multiple nodes, one for\\neach word. So the leaf node Bill Gates is split into two leaves Bill and Gates.\\nNext we deﬁne the dimensions of the vector space to be lexicalized subtrees\\nof documents – subtrees that contain at least one vocabulary term. A sub-\\nset of these possible lexicalized subtrees is shown in the ﬁgure, but there are\\nothers – e.g., the subtree corresponding to the whole document with the leaf\\nnode Gates removed. We can now represent queries and documents as vec-\\ntors in this space of lexicalized subtrees and compute matches between them.\\nThis means that we can use the vector space formalism from Chapter 6 for\\nXML retrieval. The main difference is that the dimensions of vector space\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.3\\nA vector space model for XML retrieval\\n207\\n◮Figure 10.8\\nA mapping of an XML document (left) to a set of lexicalized subtrees\\n(right).\\nin unstructured retrieval are vocabulary terms whereas they are lexicalized\\nsubtrees in XML retrieval.\\nThere is a tradeoff between the dimensionality of the space and accuracy\\nof query results. If we trivially restrict dimensions to vocabulary terms, then\\nwe have a standard vector space retrieval system that will retrieve many\\ndocuments that do not match the structure of the query (e.g., Gates in the\\ntitle as opposed to the author element). If we create a separate dimension\\nfor each lexicalized subtree occurring in the collection, the dimensionality of\\nthe space becomes too large. A compromise is to index all paths that end\\nin a single vocabulary term, in other words, all XML-context/term pairs.\\nWe call such an XML-context/term pair a structural term and denote it by\\nSTRUCTURAL TERM\\n⟨c, t⟩: a pair of XML-context c and vocabulary term t. The document in\\nFigure 10.8 has nine structural terms. Seven are shown (e.g., \"Bill\" and\\nAuthor#\"Bill\") and two are not shown: /Book/Author#\"Bill\" and\\n/Book/Author#\"Gates\". The tree with the leaves Bill and Gates is a lexical-\\nized subtree that is not a structural term. We use the previously introduced\\npseudo-XPath notation for structural terms.\\nAs we discussed in the last section users are bad at remembering details\\nabout the schema and at constructing queries that comply with the schema.\\nWe will therefore interpret all queries as extended queries – that is, there can\\nbe an arbitrary number of intervening nodes in the document for any parent-\\nchild node pair in the query. For example, we interpret q5 in Figure 10.7 as\\nq6.\\nBut we still prefer documents that match the query structure closely by\\n', 'Online edition (c)\\n2009 Cambridge UP\\n208\\n10\\nXML retrieval\\ninserting fewer additional nodes. We ensure that retrieval results respect this\\npreference by computing a weight for each match. A simple measure of the\\nsimilarity of a path cq in a query and a path cd in a document is the following\\ncontext resemblance function CR:\\nCONTEXT\\nRESEMBLANCE\\nCR(cq, cd) =\\n(\\n1+|cq|\\n1+|cd|\\nif cq matches cd\\n0\\nif cq does not match cd\\n(10.1)\\nwhere |cq| and |cd| are the number of nodes in the query path and document\\npath, respectively, and cq matches cd iff we can transform cq into cd by in-\\nserting additional nodes. Two examples from Figure 10.6 are CR(cq4, cd2) =\\n3/4 = 0.75 and CR(cq4, cd3) = 3/5 = 0.6 where cq4, cd2 and cd3 are the rele-\\nvant paths from top to leaf node in q4, d2 and d3, respectively. The value of\\nCR(cq, cd) is 1.0 if q and d are identical.\\nThe ﬁnal score for a document is computed as a variant of the cosine mea-\\nsure (Equation (6.10), page 121), which we call SIMNOMERGE for reasons\\nthat will become clear shortly. SIMNOMERGE is deﬁned as follows:\\nSIMNOMERGE(q, d) = ∑\\nck∈B ∑\\ncl∈B\\nCR(ck, cl) ∑\\nt∈V\\nweight(q, t, ck)\\nweight(d, t, cl)\\nq\\n∑c∈B,t∈V weight2(d, t, c)\\n(10.2)\\nwhere V is the vocabulary of non-structural terms; B is the set of all XML con-\\ntexts; and weight(q, t, c) and weight(d, t, c) are the weights of term t in XML\\ncontext c in query q and document d, respectively. We compute the weights\\nusing one of the weightings from Chapter 6, such as idft · wft,d. The inverse\\ndocument frequency idft depends on which elements we use to compute\\ndft as discussed in Section 10.2. The similarity measure SIMNOMERGE(q, d)\\nis not a true cosine measure since its value can be larger than 1.0 (Exer-\\ncise 10.11). We divide by\\nq\\n∑c∈B,t∈V weight2(d, t, c) to normalize for doc-\\nument length (Section 6.3.1, page 121). We have omitted query length nor-\\nmalization to simplify the formula. It has no effect on ranking since, for\\na given query, the normalizer\\nq\\n∑c∈B,t∈V weight2(q, t, c) is the same for all\\ndocuments.\\nThe algorithm for computing SIMNOMERGE for all documents in the col-\\nlection is shown in Figure 10.9. The array normalizer in Figure 10.9 contains\\nq\\n∑c∈B,t∈V weight2(d, t, c) from Equation (10.2) for each document.\\nWe give an example of how SIMNOMERGE computes query-document\\nsimilarities in Figure 10.10. ⟨c1, t⟩is one of the structural terms in the query.\\nWe successively retrieve all postings lists for structural terms ⟨c′, t⟩with the\\nsame vocabulary term t. Three example postings lists are shown. For the\\nﬁrst one, we have CR(c1, c1) = 1.0 since the two contexts are identical. The\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.3\\nA vector space model for XML retrieval\\n209\\nSCOREDOCUMENTSWITHSIMNOMERGE(q, B, V, N, normalizer)\\n1\\nfor n ←1 to N\\n2\\ndo score[n] ←0\\n3\\nfor each ⟨cq, t⟩∈q\\n4\\ndo wq ←WEIGHT(q, t, cq)\\n5\\nfor each c ∈B\\n6\\ndo if CR(cq, c) > 0\\n7\\nthen postings ←GETPOSTINGS(⟨c, t⟩)\\n8\\nfor each posting ∈postings\\n9\\ndo x ←CR(cq, c) ∗wq ∗weight(posting)\\n10\\nscore[docID(posting)] += x\\n11\\nfor n ←1 to N\\n12\\ndo score[n] ←score[n]/normalizer[n]\\n13\\nreturn score\\n◮Figure 10.9\\nThe algorithm for scoring documents with SIMNOMERGE.\\nquery\\n⟨c1, t⟩\\nCR(c1, c1)=1.0\\nCR(c1, c2)=0\\nCR(c1, c3)=0.63\\ninverted index\\n⟨c1, t⟩\\n−→\\n⟨d1, 0.5⟩\\n⟨d4, 0.1⟩\\n⟨d9, 0.2⟩\\n...\\n⟨c2, t⟩\\n−→\\n⟨d2, 0.25⟩\\n⟨d3, 0.1⟩\\n⟨d12, 0.9⟩\\n...\\n⟨c3, t⟩\\n−→\\n⟨d3, 0.7⟩\\n⟨d6, 0.8⟩\\n⟨d9, 0.6⟩\\n...\\n◮Figure 10.10\\nScoring of a query with one structural term in SIMNOMERGE.\\nnext context has no context resemblance with c1: CR(c1, c2) = 0 and the cor-\\nresponding postings list is ignored. The context match of c1 with c3 is 0.63>0\\nand it will be processed. In this example, the highest ranking document is d9\\nwith a similarity of 1.0 × 0.2 + 0.63 × 0.6 = 0.578. To simplify the ﬁgure, the\\nquery weight of ⟨c1, t⟩is assumed to be 1.0.\\nThe query-document similarity function in Figure 10.9 is called SIMNOMERGE\\nbecause different XML contexts are kept separate for the purpose of weight-\\ning. An alternative similarity function is SIMMERGE which relaxes the match-\\ning conditions of query and document further in the following three ways.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n210\\n10\\nXML retrieval\\n• We collect the statistics used for computing weight(q, t, c) and weight(d, t, c)\\nfrom all contexts that have a non-zero resemblance to c (as opposed to just\\nfrom c as in SIMNOMERGE). For instance, for computing the document\\nfrequency of the structural term atl#\"recognition\", we also count\\noccurrences of recognition in XML contexts fm/atl, article//atl etc.\\n• We modify Equation (10.2) by merging all structural terms in the docu-\\nment that have a non-zero context resemblance to a given query struc-\\ntural term. For example, the contexts /play/act/scene/title and\\n/play/title in the document will be merged when matching against\\nthe query term /play/title#\"Macbeth\".\\n• The context resemblance function is further relaxed: Contexts have a non-\\nzero resemblance in many cases where the deﬁnition of CR in Equation (10.1)\\nreturns 0.\\nSee the references in Section 10.6 for details.\\nThese three changes alleviate the problem of sparse term statistics dis-\\ncussed in Section 10.2 and increase the robustness of the matching function\\nagainst poorly posed structural queries. The evaluation of SIMNOMERGE\\nand SIMMERGE in the next section shows that the relaxed matching condi-\\ntions of SIMMERGE increase the effectiveness of XML retrieval.\\n?\\nExercise 10.1\\nConsider computing df for a structural term as the number of times that the structural\\nterm occurs under a particular parent node. Assume the following: the structural\\nterm ⟨c, t⟩= author#\"Herbert\" occurs once as the child of the node squib; there are\\n10 squib nodes in the collection; ⟨c, t⟩occurs 1000 times as the child of article; there are\\n1,000,000 article nodes in the collection. The idf weight of ⟨c, t⟩then is log2 10/1 ≈3.3\\nwhen occurring as the child of squib and log2 1,000,000/1000 ≈10.0 when occurring\\nas the child of article. (i) Explain why this is not an appropriate weighting for ⟨c, t⟩.\\nWhy should ⟨c, t⟩not receive a weight that is three times higher in articles than in\\nsquibs? (ii) Suggest a better way of computing idf.\\nExercise 10.2\\nWrite down all the structural terms occurring in the XML document in Figure 10.8.\\nExercise 10.3\\nHow many structural terms does the document in Figure 10.1 yield?\\n10.4\\nEvaluation of XML retrieval\\nThe premier venue for research on XML retrieval is the INEX (INitiative for\\nINEX\\nthe Evaluation of XML retrieval) program, a collaborative effort that has pro-\\nduced reference collections, sets of queries, and relevance judgments.\\nA\\nyearly INEX meeting is held to present and discuss research results. The\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.4\\nEvaluation of XML retrieval\\n211\\n12,107\\nnumber of documents\\n494 MB\\nsize\\n1995–2002\\ntime of publication of articles\\n1,532\\naverage number of XML nodes per document\\n6.9\\naverage depth of a node\\n30\\nnumber of CAS topics\\n30\\nnumber of CO topics\\n◮Table 10.2\\nINEX 2002 collection statistics.\\nIEEE Transac-\\ntion on Pat-\\ntern Analysis\\njournal title\\nActivity\\nrecognition\\narticle title\\nThis work fo-\\ncuses on . . .\\nparagraph\\nIntroduction\\ntitle\\nfront matter\\nsection\\nbody\\narticle\\n◮Figure 10.11\\nSimpliﬁed schema of the documents in the INEX collection.\\nINEX 2002 collection consisted of about 12,000 articles from IEEE journals.\\nWe give collection statistics in Table 10.2 and show part of the schema of\\nthe collection in Figure 10.11. The IEEE journal collection was expanded in\\n2005. Since 2006 INEX uses the much larger English Wikipedia as a test col-\\nlection. The relevance of documents is judged by human assessors using the\\nmethodology introduced in Section 8.1 (page 152), appropriately modiﬁed\\nfor structured documents as we will discuss shortly.\\nTwo types of information needs or topics in INEX are content-only or CO\\ntopics and content-and-structure (CAS) topics. CO topics are regular key-\\nCO TOPICS\\nword queries as in unstructured information retrieval. CAS topics have struc-\\nCAS TOPICS\\ntural constraints in addition to keywords. We already encountered an exam-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n212\\n10\\nXML retrieval\\nple of a CAS topic in Figure 10.3. The keywords in this case are summer and\\nholidays and the structural constraints specify that the keywords occur in a\\nsection that in turn is part of an article and that this article has an embedded\\nyear attribute with value 2001 or 2002.\\nSince CAS queries have both structural and content criteria, relevance as-\\nsessments are more complicated than in unstructured retrieval. INEX 2002\\ndeﬁned component coverage and topical relevance as orthogonal dimen-\\nsions of relevance. The component coverage dimension evaluates whether the\\nCOMPONENT\\nCOVERAGE\\nelement retrieved is “structurally” correct, i.e., neither too low nor too high\\nin the tree. We distinguish four cases:\\n• Exact coverage (E). The information sought is the main topic of the com-\\nponent and the component is a meaningful unit of information.\\n• Too small (S). The information sought is the main topic of the component,\\nbut the component is not a meaningful (self-contained) unit of informa-\\ntion.\\n• Too large (L). The information sought is present in the component, but is\\nnot the main topic.\\n• No coverage (N). The information sought is not a topic of the component.\\nThe topical relevance dimension also has four levels: highly relevant (3),\\nTOPICAL RELEVANCE\\nfairly relevant (2), marginally relevant (1) and nonrelevant (0). Components\\nare judged on both dimensions and the judgments are then combined into\\na digit-letter code. 2S is a fairly relevant component that is too small and\\n3E is a highly relevant component that has exact coverage. In theory, there\\nare 16 combinations of coverage and relevance, but many cannot occur. For\\nexample, a nonrelevant component cannot have exact coverage, so the com-\\nbination 3N is not possible.\\nThe relevance-coverage combinations are quantized as follows:\\nQ(rel, cov) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n1.00\\nif\\n(rel, cov) = 3E\\n0.75\\nif\\n(rel, cov) ∈{2E, 3L}\\n0.50\\nif\\n(rel, cov) ∈{1E, 2L, 2S}\\n0.25\\nif\\n(rel, cov) ∈{1S, 1L}\\n0.00\\nif\\n(rel, cov) = 0N\\nThis evaluation scheme takes account of the fact that binary relevance judg-\\nments, which are standard in unstructured information retrieval (Section 8.5.1,\\npage 166), are not appropriate for XML retrieval. A 2S component provides\\nincomplete information and may be difﬁcult to interpret without more con-\\ntext, but it does answer the query partially. The quantization function Q\\ndoes not impose a binary choice relevant/nonrelevant and instead allows us\\nto grade the component as partially relevant.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.4\\nEvaluation of XML retrieval\\n213\\nalgorithm\\naverage precision\\nSIMNOMERGE\\n0.242\\nSIMMERGE\\n0.271\\n◮Table 10.3\\nINEX 2002 results of the vector space model in Section 10.3 for content-\\nand-structure (CAS) queries and the quantization function Q.\\nThe number of relevant components in a retrieved set A of components\\ncan then be computed as:\\n#(relevant items retrieved) = ∑\\nc∈A\\nQ(rel(c), cov(c))\\nAs an approximation, the standard deﬁnitions of precision, recall and F from\\nChapter 8 can be applied to this modiﬁed deﬁnition of relevant items re-\\ntrieved, with some subtleties because we sum graded as opposed to binary\\nrelevance assessments. See the references on focused retrieval in Section 10.6\\nfor further discussion.\\nOne ﬂaw of measuring relevance this way is that overlap is not accounted\\nfor. We discussed the concept of marginal relevance in the context of un-\\nstructured retrieval in Section 8.5.1 (page 166). This problem is worse in\\nXML retrieval because of the problem of multiple nested elements occur-\\nring in a search result as we discussed on page 203. Much of the recent focus\\nat INEX has been on developing algorithms and evaluation measures that\\nreturn non-redundant results lists and evaluate them properly. See the refer-\\nences in Section 10.6.\\nTable 10.3 shows two INEX 2002 runs of the vector space system we de-\\nscribed in Section 10.3. The better run is the SIMMERGE run, which incor-\\nporates few structural constraints and mostly relies on keyword matching.\\nSIMMERGE’s median average precision (where the median is with respect to\\naverage precision numbers over topics) is only 0.147. Effectiveness in XML\\nretrieval is often lower than in unstructured retrieval since XML retrieval is\\nharder. Instead of just ﬁnding a document, we have to ﬁnd the subpart of a\\ndocument that is most relevant to the query. Also, XML retrieval effective-\\nness – when evaluated as described here – can be lower than unstructured\\nretrieval effectiveness on a standard evaluation because graded judgments\\nlower measured performance. Consider a system that returns a document\\nwith graded relevance 0.6 and binary relevance 1 at the top of the retrieved\\nlist. Then, interpolated precision at 0.00 recall (cf. page 158) is 1.0 on a binary\\nevaluation, but can be as low as 0.6 on a graded evaluation.\\nTable 10.3 gives us a sense of the typical performance of XML retrieval,\\nbut it does not compare structured with unstructured retrieval. Table 10.4\\ndirectly shows the effect of using structure in retrieval. The results are for a\\n', 'Online edition (c)\\n2009 Cambridge UP\\n214\\n10\\nXML retrieval\\ncontent only\\nfull structure\\nimprovement\\nprecision at 5\\n0.2000\\n0.3265\\n63.3%\\nprecision at 10\\n0.1820\\n0.2531\\n39.1%\\nprecision at 20\\n0.1700\\n0.1796\\n5.6%\\nprecision at 30\\n0.1527\\n0.1531\\n0.3%\\n◮Table 10.4\\nA comparison of content-only and full-structure search in INEX\\n2003/2004.\\nlanguage-model-based system (cf. Chapter 12) that is evaluated on a subset\\nof CAS topics from INEX 2003 and 2004. The evaluation metric is precision\\nat k as deﬁned in Chapter 8 (page 161). The discretization function used for\\nthe evaluation maps highly relevant elements (roughly corresponding to the\\n3E elements deﬁned for Q) to 1 and all other elements to 0. The content-\\nonly system treats queries and documents as unstructured bags of words.\\nThe full-structure model ranks elements that satisfy structural constraints\\nhigher than elements that do not. For instance, for the query in Figure 10.3\\nan element that contains the phrase summer holidays in a section will be rated\\nhigher than one that contains it in an abstract.\\nThe table shows that structure helps increase precision at the top of the\\nresults list. There is a large increase of precision at k = 5 and at k = 10. There\\nis almost no improvement at k = 30. These results demonstrate the beneﬁts\\nof structured retrieval. Structured retrieval imposes additional constraints on\\nwhat to return and documents that pass the structural ﬁlter are more likely\\nto be relevant. Recall may suffer because some relevant documents will be\\nﬁltered out, but for precision-oriented tasks structured retrieval is superior.\\n10.5\\nText-centric vs. data-centric XML retrieval\\nIn the type of structured retrieval we cover in this chapter, XML structure\\nserves as a framework within which we match the text of the query with the\\ntext of the XML documents. This exempliﬁes a system that is optimized for\\ntext-centric XML. While both text and structure are important, we give higher\\nTEXT-CENTRIC XML\\npriority to text. We do this by adapting unstructured retrieval methods to\\nhandling additional structural constraints. The premise of our approach is\\nthat XML document retrieval is characterized by (i) long text ﬁelds (e.g., sec-\\ntions of a document), (ii) inexact matching, and (iii) relevance-ranked results.\\nRelational databases do not deal well with this use case.\\nIn contrast, data-centric XML mainly encodes numerical and non-text attribute-\\nDATA-CENTRIC XML\\nvalue data.\\nWhen querying data-centric XML, we want to impose exact\\nmatch conditions in most cases. This puts the emphasis on the structural\\naspects of XML documents and queries. An example is:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.5\\nText-centric vs. data-centric XML retrieval\\n215\\nFind employees whose salary is the same this month as it was 12 months\\nago.\\nThis query requires no ranking. It is purely structural and an exact matching\\nof the salaries in the two time periods is probably sufﬁcient to meet the user’s\\ninformation need.\\nText-centric approaches are appropriate for data that are essentially text\\ndocuments, marked up as XML to capture document structure. This is be-\\ncoming a de facto standard for publishing text databases since most text\\ndocuments have some form of interesting structure – paragraphs, sections,\\nfootnotes etc. Examples include assembly manuals, issues of journals, Shake-\\nspeare’s collected works and newswire articles.\\nData-centric approaches are commonly used for data collections with com-\\nplex structures that mainly contain non-text data. A text-centric retrieval\\nengine will have a hard time with proteomic data in bioinformatics or with\\nthe representation of a city map that (together with street names and other\\ntextual descriptions) forms a navigational database.\\nTwo other types of queries that are difﬁcult to handle in a text-centric struc-\\ntured retrieval model are joins and ordering constraints. The query for em-\\nployees with unchanged salary requires a join. The following query imposes\\nan ordering constraint:\\nRetrieve the chapter of the book Introduction to algorithms that follows\\nthe chapter Binomial heaps.\\nThis query relies on the ordering of elements in XML – in this case the order-\\ning of chapter elements underneath the book node. There are powerful query\\nlanguages for XML that can handle numerical attributes, joins and ordering\\nconstraints. The best known of these is XQuery, a language proposed for\\nstandardization by the W3C. It is designed to be broadly applicable in all ar-\\neas where XML is used. Due to its complexity, it is challenging to implement\\nan XQuery-based ranked retrieval system with the performance characteris-\\ntics that users have come to expect in information retrieval. This is currently\\none of the most active areas of research in XML retrieval.\\nRelational databases are better equipped to handle many structural con-\\nstraints, particularly joins (but ordering is also difﬁcult in a database frame-\\nwork – the tuples of a relation in the relational calculus are not ordered). For\\nthis reason, most data-centric XML retrieval systems are extensions of rela-\\ntional databases (see the references in Section 10.6). If text ﬁelds are short,\\nexact matching meets user needs and retrieval results in form of unordered\\nsets are acceptable, then using a relational database for XML retrieval is ap-\\npropriate.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n216\\n10\\nXML retrieval\\n10.6\\nReferences and further reading\\nThere are many good introductions to XML, including (Harold and Means\\n2004). Table 10.1 is inspired by a similar table in (van Rijsbergen 1979). Sec-\\ntion 10.4 follows the overview of INEX 2002 by Gövert and Kazai (2003),\\npublished in the proceedings of the meeting (Fuhr et al. 2003a). The pro-\\nceedings of the four following INEX meetings were published as Fuhr et al.\\n(2003b), Fuhr et al. (2005), Fuhr et al. (2006), and Fuhr et al. (2007). An up-\\ntodate overview article is Fuhr and Lalmas (2007). The results in Table 10.4\\nare from (Kamps et al. 2006). Chu-Carroll et al. (2006) also present evidence\\nthat XML queries increase precision compared with unstructured queries.\\nInstead of coverage and relevance, INEX now evaluates on the related but\\ndifferent dimensions of exhaustivity and speciﬁcity (Lalmas and Tombros\\n2007). Trotman et al. (2006) relate the tasks investigated at INEX to real world\\nuses of structured retrieval such as structured book search on internet book-\\nstore sites.\\nThe structured document retrieval principle is due to Chiaramella et al.\\n(1996). Figure 10.5 is from (Fuhr and Großjohann 2004). Rahm and Bernstein\\n(2001) give a survey of automatic schema matching that is applicable to XML.\\nThe vector-space based XML retrieval method in Section 10.3 is essentially\\nIBM Haifa’s JuruXML system as presented by Mass et al. (2003) and Carmel\\net al. (2003). Schlieder and Meuss (2002) and Grabs and Schek (2002) describe\\nsimilar approaches. Carmel et al. (2003) represent queries as XML fragments.\\nXML FRAGMENT\\nThe trees that represent XML queries in this chapter are all XML fragments,\\nbut XML fragments also permit the operators +, −and phrase on content\\nnodes.\\nWe chose to present the vector space model for XML retrieval because it\\nis simple and a natural extension of the unstructured vector space model\\nin Chapter 6. But many other unstructured retrieval methods have been\\napplied to XML retrieval with at least as much success as the vector space\\nmodel. These methods include language models (cf. Chapter 12, e.g., Kamps\\net al. (2004), List et al. (2005), Ogilvie and Callan (2005)), systems that use\\na relational database as a backend (Mihajlovi´c et al. 2005, Theobald et al.\\n2005; 2008), probabilistic weighting (Lu et al. 2007), and fusion (Larson 2005).\\nThere is currently no consensus as to what the best approach to XML retrieval\\nis.\\nMost early work on XML retrieval accomplished relevance ranking by fo-\\ncusing on individual terms, including their structural contexts, in query and\\ndocument. As in unstructured information retrieval, there is a trend in more\\nrecent work to model relevance ranking as combining evidence from dis-\\nparate measurements about the query, the document and their match. The\\ncombination function can be tuned manually (Arvola et al. 2005, Sigurbjörns-\\nson et al. 2004) or trained using machine learning methods (Vittaut and Gal-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n10.7\\nExercises\\n217\\nlinari (2006), cf. Section 15.4.1, page 341).\\nAn active area of XML retrieval research is focused retrieval (Trotman et al.\\nFOCUSED RETRIEVAL\\n2007), which aims to avoid returning nested elements that share one or more\\ncommon subelements (cf. discussion in Section 10.2, page 203). There is ev-\\nidence that users dislike redundancy caused by nested elements (Betsi et al.\\n2006). Focused retrieval requires evaluation measures that penalize redun-\\ndant results lists (Kazai and Lalmas 2006, Lalmas et al. 2007). Trotman and\\nGeva (2006) argue that XML retrieval is a form of passage retrieval. In passage\\nPASSAGE RETRIEVAL\\nretrieval (Salton et al. 1993, Hearst and Plaunt 1993, Zobel et al. 1995, Hearst\\n1997, Kaszkiel and Zobel 1997), the retrieval system returns short passages\\ninstead of documents in response to a user query. While element bound-\\naries in XML documents are cues for identifying good segment boundaries\\nbetween passages, the most relevant passage often does not coincide with an\\nXML element.\\nIn the last several years, the query format at INEX has been the NEXI stan-\\ndard proposed by Trotman and Sigurbjörnsson (2004). Figure 10.3 is from\\ntheir paper. O’Keefe and Trotman (2004) give evidence that users cannot reli-\\nably distinguish the child and descendant axes. This justiﬁes only permitting\\ndescendant axes in NEXI (and XML fragments). These structural constraints\\nwere only treated as “hints” in recent INEXes. Assessors can judge an ele-\\nment highly relevant even though it violates one of the structural constraints\\nspeciﬁed in a NEXI query.\\nAn alternative to structured query languages like NEXI is a more sophisti-\\ncated user interface for query formulation (Tannier and Geva 2005, van Zwol\\net al. 2006, Woodley and Geva 2006).\\nA broad overview of XML retrieval that covers database as well as IR ap-\\nproaches is given by Amer-Yahia and Lalmas (2006) and an extensive refer-\\nence list on the topic can be found in (Amer-Yahia et al. 2005). Chapter 6\\nof Grossman and Frieder (2004) is a good introduction to structured text re-\\ntrieval from a database perspective. The proposed standard for XQuery is\\navailable at http://www.w3.org/TR/xquery/ including an extension for full-text\\nqueries (Amer-Yahia et al. 2006): http://www.w3.org/TR/xquery-full-text/. Work\\nthat has looked at combining the relational database and the unstructured\\ninformation retrieval approaches includes (Fuhr and Rölleke 1997), (Navarro\\nand Baeza-Yates 1997), (Cohen 1998), and (Chaudhuri et al. 2006).\\n10.7\\nExercises\\n?\\nExercise 10.4\\nFind a reasonably sized XML document collection (or a collection using a markup lan-\\nguage different from XML like HTML) on the web and download it. Jon Bosak’s XML\\nedition of Shakespeare and of various religious works at http://www.ibiblio.org/bosak/ or\\nthe ﬁrst 10,000 documents of the Wikipedia are good choices. Create three CAS topics\\n', 'Online edition (c)\\n2009 Cambridge UP\\n218\\n10\\nXML retrieval\\nof the type shown in Figure 10.3 that you would expect to do better than analogous\\nCO topics. Explain why an XML retrieval system would be able to exploit the XML\\nstructure of the documents to achieve better retrieval results on the topics than an\\nunstructured retrieval system.\\nExercise 10.5\\nFor the collection and the topics in Exercise 10.4, (i) are there pairs of elements e1 and\\ne2, with e2 a subelement of e1 such that both answer one of the topics? Find one case\\neach where (ii) e1 (iii) e2 is the better answer to the query.\\nExercise 10.6\\nImplement the (i) SIMMERGE (ii) SIMNOMERGE algorithm in Section 10.3 and run it\\nfor the collection and the topics in Exercise 10.4. (iii) Evaluate the results by assigning\\nbinary relevance judgments to the ﬁrst ﬁve documents of the three retrieved lists for\\neach algorithm. Which algorithm performs better?\\nExercise 10.7\\nAre all of the elements in Exercise 10.4 appropriate to be returned as hits to a user or\\nare there elements (as in the example <b>definitely</b> on page 203) that you\\nwould exclude?\\nExercise 10.8\\nWe discussed the tradeoff between accuracy of results and dimensionality of the vec-\\ntor space on page 207. Give an example of an information need that we can answer\\ncorrectly if we index all lexicalized subtrees, but cannot answer if we only index struc-\\ntural terms.\\nExercise 10.9\\nIf we index all structural terms, what is the size of the index as a function of text size?\\nExercise 10.10\\nIf we index all lexicalized subtrees, what is the size of the index as a function of text\\nsize?\\nExercise 10.11\\nGive an example of a query-document pair for which SIMNOMERGE(q, d) is larger\\nthan 1.0.\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n219\\n11\\nProbabilistic information\\nretrieval\\nDuring the discussion of relevance feedback in Section 9.1.2, we observed\\nthat if we have some known relevant and nonrelevant documents, then we\\ncan straightforwardly start to estimate the probability of a term t appearing\\nin a relevant document P(t|R = 1), and that this could be the basis of a\\nclassiﬁer that decides whether documents are relevant or not. In this chapter,\\nwe more systematically introduce this probabilistic approach to IR, which\\nprovides a different formal basis for a retrieval model and results in different\\ntechniques for setting term weights.\\nUsers start with information needs, which they translate into query repre-\\nsentations. Similarly, there are documents, which are converted into document\\nrepresentations (the latter differing at least by how text is tokenized, but per-\\nhaps containing fundamentally less information, as when a non-positional\\nindex is used). Based on these two representations, a system tries to de-\\ntermine how well documents satisfy information needs. In the Boolean or\\nvector space models of IR, matching is done in a formally deﬁned but seman-\\ntically imprecise calculus of index terms. Given only a query, an IR system\\nhas an uncertain understanding of the information need. Given the query\\nand document representations, a system has an uncertain guess of whether\\na document has content relevant to the information need. Probability theory\\nprovides a principled foundation for such reasoning under uncertainty. This\\nchapter provides one answer as to how to exploit this foundation to estimate\\nhow likely it is that a document is relevant to an information need.\\nThere is more than one possible retrieval model which has a probabilistic\\nbasis. Here, we will introduce probability theory and the Probability Rank-\\ning Principle (Sections 11.1–11.2), and then concentrate on the Binary Inde-\\npendence Model (Section 11.3), which is the original and still most inﬂuential\\nprobabilistic retrieval model. Finally, we will introduce related but extended\\nmethods which use term counts, including the empirically successful Okapi\\nBM25 weighting scheme, and Bayesian Network models for IR (Section 11.4).\\nIn Chapter 12, we then present the alternative probabilistic language model-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n220\\n11\\nProbabilistic information retrieval\\ning approach to IR, which has been developed with considerable success in\\nrecent years.\\n11.1\\nReview of basic probability theory\\nWe hope that the reader has seen a little basic probability theory previously.\\nWe will give a very quick review; some references for further reading appear\\nat the end of the chapter. A variable A represents an event (a subset of the\\nspace of possible outcomes). Equivalently, we can represent the subset via a\\nrandom variable, which is a function from outcomes to real numbers; the sub-\\nRANDOM VARIABLE\\nset is the domain over which the random variable A has a particular value.\\nOften we will not know with certainty whether an event is true in the world.\\nWe can ask the probability of the event 0 ≤P(A) ≤1. For two events A and\\nB, the joint event of both events occurring is described by the joint probabil-\\nity P(A, B). The conditional probability P(A|B) expresses the probability of\\nevent A given that event B occurred. The fundamental relationship between\\njoint and conditional probabilities is given by the chain rule:\\nCHAIN RULE\\nP(A, B) = P(A ∩B) = P(A|B)P(B) = P(B|A)P(A)\\n(11.1)\\nWithout making any assumptions, the probability of a joint event equals the\\nprobability of one of the events multiplied by the probability of the other\\nevent conditioned on knowing the ﬁrst event happened.\\nWriting P(A) for the complement of an event, we similarly have:\\nP(A, B) = P(B|A)P(A)\\n(11.2)\\nProbability theory also has a partition rule, which says that if an event B can\\nPARTITION RULE\\nbe divided into an exhaustive set of disjoint subcases, then the probability of\\nB is the sum of the probabilities of the subcases. A special case of this rule\\ngives that:\\nP(B) = P(A, B) + P(A, B)\\n(11.3)\\nFrom these we can derive Bayes’ Rule for inverting conditional probabili-\\nBAYES’ RULE\\nties:\\nP(A|B) = P(B|A)P(A)\\nP(B)\\n=\\n\"\\nP(B|A)\\n∑X∈{A,A} P(B|X)P(X)\\n#\\nP(A)\\n(11.4)\\nThis equation can also be thought of as a way of updating probabilities. We\\nstart off with an initial estimate of how likely the event A is when we do\\nnot have any other information; this is the prior probability P(A). Bayes’ rule\\nPRIOR PROBABILITY\\nlets us derive a posterior probability P(A|B) after having seen the evidence B,\\nPOSTERIOR\\nPROBABILITY\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.2\\nThe Probability Ranking Principle\\n221\\nbased on the likelihood of B occurring in the two cases that A does or does not\\nhold.1\\nFinally, it is often useful to talk about the odds of an event, which provide\\nODDS\\na kind of multiplier for how probabilities change:\\nOdds:\\nO(A) = P(A)\\nP(A) =\\nP(A)\\n1 −P(A)\\n(11.5)\\n11.2\\nThe Probability Ranking Principle\\n11.2.1\\nThe 1/0 loss case\\nWe assume a ranked retrieval setup as in Section 6.3, where there is a collec-\\ntion of documents, the user issues a query, and an ordered list of documents\\nis returned. We also assume a binary notion of relevance as in Chapter 8. For\\na query q and a document d in the collection, let Rd,q be an indicator random\\nvariable that says whether d is relevant with respect to a given query q. That\\nis, it takes on a value of 1 when the document is relevant and 0 otherwise. In\\ncontext we will often write just R for Rd,q.\\nUsing a probabilistic model, the obvious order in which to present doc-\\numents to the user is to rank documents by their estimated probability of\\nrelevance with respect to the information need: P(R = 1|d, q). This is the ba-\\nsis of the Probability Ranking Principle (PRP) (van Rijsbergen 1979, 113–114):\\nPROBABILITY\\nRANKING PRINCIPLE\\n“If a reference retrieval system’s response to each request is a ranking\\nof the documents in the collection in order of decreasing probability\\nof relevance to the user who submitted the request, where the prob-\\nabilities are estimated as accurately as possible on the basis of what-\\never data have been made available to the system for this purpose, the\\noverall effectiveness of the system to its user will be the best that is\\nobtainable on the basis of those data.”\\nIn the simplest case of the PRP, there are no retrieval costs or other utility\\nconcerns that would differentially weight actions or errors. You lose a point\\nfor either returning a nonrelevant document or failing to return a relevant\\ndocument (such a binary situation where you are evaluated on your accuracy\\nis called 1/0 loss). The goal is to return the best possible results as the top k\\n1/0 LOSS\\ndocuments, for any value of k the user chooses to examine. The PRP then\\nsays to simply rank all documents in decreasing order of P(R = 1|d, q). If\\na set of retrieval results is to be returned, rather than an ordering, the Bayes\\nBAYES OPTIMAL\\nDECISION RULE\\n1. The term likelihood is just a synonym of probability. It is the probability of an event or data\\naccording to a model. The term is usually used when people are thinking of holding the data\\nﬁxed, while varying the model.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n222\\n11\\nProbabilistic information retrieval\\nOptimal Decision Rule, the decision which minimizes the risk of loss, is to\\nsimply return documents that are more likely relevant than nonrelevant:\\nd is relevant iff P(R = 1|d, q) > P(R = 0|d, q)\\n(11.6)\\nTheorem 11.1. The PRP is optimal, in the sense that it minimizes the expected loss\\n(also known as the Bayes risk) under 1/0 loss.\\nBAYES RISK\\nThe proof can be found in Ripley (1996). However, it requires that all proba-\\nbilities are known correctly. This is never the case in practice. Nevertheless,\\nthe PRP still provides a very useful foundation for developing models of IR.\\n11.2.2\\nThe PRP with retrieval costs\\nSuppose, instead, that we assume a model of retrieval costs. Let C1 be the\\ncost of not retrieving a relevant document and C0 the cost of retrieval of a\\nnonrelevant document. Then the Probability Ranking Principle says that if\\nfor a speciﬁc document d and for all documents d′ not yet retrieved\\nC0 · P(R = 0|d) −C1 · P(R = 1|d) ≤C0 · P(R = 0|d′) −C1 · P(R = 1|d′)\\n(11.7)\\nthen d is the next document to be retrieved. Such a model gives a formal\\nframework where we can model differential costs of false positives and false\\nnegatives and even system performance issues at the modeling stage, rather\\nthan simply at the evaluation stage, as we did in Section 8.6 (page 168). How-\\never, we will not further consider loss/utility models in this chapter.\\n11.3\\nThe Binary Independence Model\\nThe Binary Independence Model (BIM) we present in this section is the model\\nBINARY\\nINDEPENDENCE\\nMODEL\\nthat has traditionally been used with the PRP. It introduces some simple as-\\nsumptions, which make estimating the probability function P(R|d, q) prac-\\ntical. Here, “binary” is equivalent to Boolean: documents and queries are\\nboth represented as binary term incidence vectors. That is, a document d\\nis represented by the vector ⃗x = (x1, . . . , xM) where xt = 1 if term t is\\npresent in document d and xt = 0 if t is not present in d. With this rep-\\nresentation, many possible documents have the same vector representation.\\nSimilarly, we represent q by the incidence vector ⃗q (the distinction between\\nq and ⃗q is less central since commonly q is in the form of a set of words).\\n“Independence” means that terms are modeled as occurring in documents\\nindependently. The model recognizes no association between terms. This\\nassumption is far from correct, but it nevertheless often gives satisfactory\\nresults in practice; it is the “naive” assumption of Naive Bayes models, dis-\\ncussed further in Section 13.4 (page 265). Indeed, the Binary Independence\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n223\\nModel is exactly the same as the multivariate Bernoulli Naive Bayes model\\npresented in Section 13.3 (page 263). In a sense this assumption is equivalent\\nto an assumption of the vector space model, where each term is a dimension\\nthat is orthogonal to all other terms.\\nWe will ﬁrst present a model which assumes that the user has a single\\nstep information need. As discussed in Chapter 9, seeing a range of results\\nmight let the user reﬁne their information need. Fortunately, as mentioned\\nthere, it is straightforward to extend the Binary Independence Model so as to\\nprovide a framework for relevance feedback, and we present this model in\\nSection 11.3.4.\\nTo make a probabilistic retrieval strategy precise, we need to estimate how\\nterms in documents contribute to relevance, speciﬁcally, we wish to know\\nhow term frequency, document frequency, document length, and other statis-\\ntics that we can compute inﬂuence judgments about document relevance,\\nand how they can be reasonably combined to estimate the probability of doc-\\nument relevance. We then order documents by decreasing estimated proba-\\nbility of relevance.\\nWe assume here that the relevance of each document is independent of the\\nrelevance of other documents. As we noted in Section 8.5.1 (page 166), this\\nis incorrect: the assumption is especially harmful in practice if it allows a\\nsystem to return duplicate or near duplicate documents. Under the BIM, we\\nmodel the probability P(R|d, q) that a document is relevant via the probabil-\\nity in terms of term incidence vectors P(R|⃗x,⃗q). Then, using Bayes rule, we\\nhave:\\nP(R = 1|⃗x,⃗q)\\n=\\nP(⃗x|R = 1,⃗q)P(R = 1|⃗q)\\nP(⃗x|⃗q)\\n(11.8)\\nP(R = 0|⃗x,⃗q)\\n=\\nP(⃗x|R = 0,⃗q)P(R = 0|⃗q)\\nP(⃗x|⃗q)\\nHere, P(⃗x|R = 1,⃗q) and P(⃗x|R = 0,⃗q) are the probability that if a relevant or\\nnonrelevant, respectively, document is retrieved, then that document’s rep-\\nresentation is ⃗x. You should think of this quantity as deﬁned with respect to\\na space of possible documents in a domain. How do we compute all these\\nprobabilities? We never know the exact probabilities, and so we have to use\\nestimates: Statistics about the actual document collection are used to estimate\\nthese probabilities. P(R = 1|⃗q) and P(R = 0|⃗q) indicate the prior probability\\nof retrieving a relevant or nonrelevant document respectively for a query ⃗q.\\nAgain, if we knew the percentage of relevant documents in the collection,\\nthen we could use this number to estimate P(R = 1|⃗q) and P(R = 0|⃗q). Since\\na document is either relevant or nonrelevant to a query, we must have that:\\nP(R = 1|⃗x,⃗q) + P(R = 0|⃗x,⃗q) = 1\\n(11.9)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n224\\n11\\nProbabilistic information retrieval\\n11.3.1\\nDeriving a ranking function for query terms\\nGiven a query q, we wish to order returned documents by descending P(R =\\n1|d, q). Under the BIM, this is modeled as ordering by P(R = 1|⃗x,⃗q). Rather\\nthan estimating this probability directly, because we are interested only in the\\nranking of documents, we work with some other quantities which are easier\\nto compute and which give the same ordering of documents. In particular,\\nwe can rank documents by their odds of relevance (as the odds of relevance\\nis monotonic with the probability of relevance). This makes things easier,\\nbecause we can ignore the common denominator in (11.8), giving:\\nO(R|⃗x,⃗q) = P(R = 1|⃗x,⃗q)\\nP(R = 0|⃗x,⃗q) =\\nP(R=1|⃗q)P(⃗x|R=1,⃗q)\\nP(⃗x|⃗q)\\nP(R=0|⃗q)P(⃗x|R=0,⃗q)\\nP(⃗x|⃗q)\\n= P(R = 1|⃗q)\\nP(R = 0|⃗q) · P(⃗x|R = 1,⃗q)\\nP(⃗x|R = 0,⃗q)\\n(11.10)\\nThe left term in the rightmost expression of Equation (11.10) is a constant for\\na given query. Since we are only ranking documents, there is thus no need\\nfor us to estimate it. The right-hand term does, however, require estimation,\\nand this initially appears to be difﬁcult: How can we accurately estimate the\\nprobability of an entire term incidence vector occurring? It is at this point that\\nwe make the Naive Bayes conditional independence assumption that the presence\\nNAIVE BAYES\\nASSUMPTION\\nor absence of a word in a document is independent of the presence or absence\\nof any other word (given the query):\\nP(⃗x|R = 1,⃗q)\\nP(⃗x|R = 0,⃗q) =\\nM\\n∏\\nt=1\\nP(xt|R = 1,⃗q)\\nP(xt|R = 0,⃗q)\\n(11.11)\\nSo:\\nO(R|⃗x,⃗q) = O(R|⃗q) ·\\nM\\n∏\\nt=1\\nP(xt|R = 1,⃗q)\\nP(xt|R = 0,⃗q)\\n(11.12)\\nSince each xt is either 0 or 1, we can separate the terms to give:\\nO(R|⃗x,⃗q) = O(R|⃗q) · ∏\\nt:xt=1\\nP(xt = 1|R = 1,⃗q)\\nP(xt = 1|R = 0,⃗q) · ∏\\nt:xt=0\\nP(xt = 0|R = 1,⃗q)\\nP(xt = 0|R = 0,⃗q)\\n(11.13)\\nHenceforth, let pt = P(xt = 1|R = 1,⃗q) be the probability of a term appear-\\ning in a document relevant to the query, and ut = P(xt = 1|R = 0,⃗q) be the\\nprobability of a term appearing in a nonrelevant document. These quantities\\ncan be visualized in the following contingency table where the columns add\\nto 1:\\n(11.14)\\ndocument\\nrelevant (R = 1)\\nnonrelevant (R = 0)\\nTerm present\\nxt = 1\\npt\\nut\\nTerm absent\\nxt = 0\\n1 −pt\\n1 −ut\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n225\\nLet us make an additional simplifying assumption that terms not occur-\\nring in the query are equally likely to occur in relevant and nonrelevant doc-\\numents: that is, if qt = 0 then pt = ut. (This assumption can be changed,\\nas when doing relevance feedback in Section 11.3.4.) Then we need only\\nconsider terms in the products that appear in the query, and so,\\nO(R|⃗q,⃗x) = O(R|⃗q) ·\\n∏\\nt:xt=qt=1\\npt\\nut ·\\n∏\\nt:xt=0,qt=1\\n1 −pt\\n1 −ut\\n(11.15)\\nThe left product is over query terms found in the document and the right\\nproduct is over query terms not found in the document.\\nWe can manipulate this expression by including the query terms found in\\nthe document into the right product, but simultaneously dividing through\\nby them in the left product, so the value is unchanged. Then we have:\\nO(R|⃗q,⃗x) = O(R|⃗q) ·\\n∏\\nt:xt=qt=1\\npt(1 −ut)\\nut(1 −pt) · ∏\\nt:qt=1\\n1 −pt\\n1 −ut\\n(11.16)\\nThe left product is still over query terms found in the document, but the right\\nproduct is now over all query terms. That means that this right product is a\\nconstant for a particular query, just like the odds O(R|⃗q). So the only quantity\\nthat needs to be estimated to rank documents for relevance to a query is the\\nleft product. We can equally rank documents by the logarithm of this term,\\nsince log is a monotonic function. The resulting quantity used for ranking is\\ncalled the Retrieval Status Value (RSV) in this model:\\nRETRIEVAL STATUS\\nVALUE\\nRSVd = log\\n∏\\nt:xt=qt=1\\npt(1 −ut)\\nut(1 −pt) =\\n∑\\nt:xt=qt=1\\nlog pt(1 −ut)\\nut(1 −pt)\\n(11.17)\\nSo everything comes down to computing the RSV. Deﬁne ct:\\nct = log pt(1 −ut)\\nut(1 −pt) = log\\npt\\n(1 −pt) + log 1 −ut\\nut\\n(11.18)\\nThe ct terms are log odds ratios for the terms in the query. We have the\\nodds of the term appearing if the document is relevant (pt/(1 −pt)) and the\\nodds of the term appearing if the document is nonrelevant (ut/(1 −ut)). The\\nodds ratio is the ratio of two such odds, and then we ﬁnally take the log of that\\nODDS RATIO\\nquantity. The value will be 0 if a term has equal odds of appearing in relevant\\nand nonrelevant documents, and positive if it is more likely to appear in\\nrelevant documents. The ct quantities function as term weights in the model,\\nand the document score for a query is RSVd = ∑xt=qt=1 ct. Operationally, we\\nsum them in accumulators for query terms appearing in documents, just as\\nfor the vector space model calculations discussed in Section 7.1 (page 135).\\nWe now turn to how we estimate these ct quantities for a particular collection\\nand query.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n226\\n11\\nProbabilistic information retrieval\\n11.3.2\\nProbability estimates in theory\\nFor each term t, what would these ct numbers look like for the whole collec-\\ntion? (11.19) gives a contingency table of counts of documents in the collec-\\ntion, where dft is the number of documents that contain term t:\\n(11.19)\\ndocuments\\nrelevant\\nnonrelevant\\nTotal\\nTerm present\\nxt = 1\\ns\\ndft −s\\ndft\\nTerm absent\\nxt = 0\\nS −s\\n(N −dft) −(S −s)\\nN −dft\\nTotal\\nS\\nN −S\\nN\\nUsing this, pt = s/S and ut = (dft −s)/(N −S) and\\nct = K(N, dft, S, s) = log\\ns/(S −s)\\n(dft −s)/((N −dft) −(S −s))\\n(11.20)\\nTo avoid the possibility of zeroes (such as if every or no relevant document\\nhas a particular term) it is fairly standard to add 1\\n2 to each of the quantities\\nin the center 4 terms of (11.19), and then to adjust the marginal counts (the\\ntotals) accordingly (so, the bottom right cell totals N + 2). Then we have:\\nˆct = K(N, dft, S, s) = log\\n(s + 1\\n2)/(S −s + 1\\n2)\\n(dft −s + 1\\n2)/(N −dft −S + s + 1\\n2)\\n(11.21)\\nAdding 1\\n2 in this way is a simple form of smoothing. For trials with cat-\\negorical outcomes (such as noting the presence or absence of a term), one\\nway to estimate the probability of an event from data is simply to count the\\nnumber of times an event occurred divided by the total number of trials.\\nThis is referred to as the relative frequency of the event. Estimating the prob-\\nRELATIVE FREQUENCY\\nability as the relative frequency is the maximum likelihood estimate (or MLE),\\nMAXIMUM LIKELIHOOD\\nESTIMATE\\nMLE\\nbecause this value makes the observed data maximally likely. However, if\\nwe simply use the MLE, then the probability given to events we happened to\\nsee is usually too high, whereas other events may be completely unseen and\\ngiving them as a probability estimate their relative frequency of 0 is both an\\nunderestimate, and normally breaks our models, since anything multiplied\\nby 0 is 0. Simultaneously decreasing the estimated probability of seen events\\nand increasing the probability of unseen events is referred to as smoothing.\\nSMOOTHING\\nOne simple way of smoothing is to add a number α to each of the observed\\ncounts. These pseudocounts correspond to the use of a uniform distribution\\nPSEUDOCOUNTS\\nover the vocabulary as a Bayesian prior, following Equation (11.4). We ini-\\nBAYESIAN PRIOR\\ntially assume a uniform distribution over events, where the size of α denotes\\nthe strength of our belief in uniformity, and we then update the probability\\nbased on observed events. Since our belief in uniformity is weak, we use\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n227\\nα = 1\\n2. This is a form of maximum a posteriori (MAP) estimation, where we\\nMAXIMUM A\\nPOSTERIORI\\nMAP\\nchoose the most likely point value for probabilities based on the prior and\\nthe observed evidence, following Equation (11.4). We will further discuss\\nmethods of smoothing estimated counts to give probability models in Sec-\\ntion 12.2.2 (page 243); the simple method of adding 1\\n2 to each observed count\\nwill do for now.\\n11.3.3\\nProbability estimates in practice\\nUnder the assumption that relevant documents are a very small percentage\\nof the collection, it is plausible to approximate statistics for nonrelevant doc-\\numents by statistics from the whole collection. Under this assumption, ut\\n(the probability of term occurrence in nonrelevant documents for a query) is\\ndft/N and\\nlog[(1 −ut)/ut] = log[(N −dft)/dft] ≈log N/dft\\n(11.22)\\nIn other words, we can provide a theoretical justiﬁcation for the most fre-\\nquently used form of idf weighting, which we saw in Section 6.2.1.\\nThe approximation technique in Equation (11.22) cannot easily be extended\\nto relevant documents. The quantity pt can be estimated in various ways:\\n1. We can use the frequency of term occurrence in known relevant docu-\\nments (if we know some). This is the basis of probabilistic approaches to\\nrelevance feedback weighting in a feedback loop, discussed in the next\\nsubsection.\\n2. Croft and Harper (1979) proposed using a constant in their combination\\nmatch model. For instance, we might assume that pt is constant over all\\nterms xt in the query and that pt = 0.5. This means that each term has\\neven odds of appearing in a relevant document, and so the pt and (1 −pt)\\nfactors cancel out in the expression for RSV. Such an estimate is weak, but\\ndoesn’t disagree violently with our hopes for the search terms appearing\\nin many but not all relevant documents. Combining this method with our\\nearlier approximation for ut, the document ranking is determined simply\\nby which query terms occur in documents scaled by their idf weighting.\\nFor short documents (titles or abstracts) in situations in which iterative\\nsearching is undesirable, using this weighting term alone can be quite\\nsatisfactory, although in many other circumstances we would like to do\\nbetter.\\n3. Greiff (1998) argues that the constant estimate of pt in the Croft and Harper\\n(1979) model is theoretically problematic and not observed empirically: as\\nmight be expected, pt is shown to rise with dft. Based on his data analysis,\\na plausible proposal would be to use the estimate pt = 1\\n3 + 2\\n3dft/N.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n228\\n11\\nProbabilistic information retrieval\\nIterative methods of estimation, which combine some of the above ideas,\\nare discussed in the next subsection.\\n11.3.4\\nProbabilistic approaches to relevance feedback\\nWe can use (pseudo-)relevance feedback, perhaps in an iterative process of\\nestimation, to get a more accurate estimate of pt. The probabilistic approach\\nto relevance feedback works as follows:\\n1. Guess initial estimates of pt and ut. This can be done using the probability\\nestimates of the previous section. For instance, we can assume that pt is\\nconstant over all xt in the query, in particular, perhaps taking pt = 1\\n2.\\n2. Use the current estimates of pt and ut to determine a best guess at the set\\nof relevant documents R = {d : Rd,q = 1}. Use this model to retrieve a set\\nof candidate relevant documents, which we present to the user.\\n3. We interact with the user to reﬁne the model of R. We do this by learn-\\ning from the user relevance judgments for some subset of documents V.\\nBased on relevance judgments, V is partitioned into two subsets: VR =\\n{d ∈V, Rd,q = 1} ⊂R and VNR = {d ∈V, Rd,q = 0}, which is disjoint\\nfrom R.\\n4. We reestimate pt and ut on the basis of known relevant and nonrelevant\\ndocuments. If the sets VR and VNR are large enough, we may be able\\nto estimate these quantities directly from these documents as maximum\\nlikelihood estimates:\\npt = |VRt|/|VR|\\n(11.23)\\n(where VRt is the set of documents in VR containing xt). In practice,\\nwe usually need to smooth these estimates. We can do this by adding\\n1\\n2 to both the count |VRt| and to the number of relevant documents not\\ncontaining the term, giving:\\npt = |VRt| + 1\\n2\\n|VR| + 1\\n(11.24)\\nHowever, the set of documents judged by the user (V) is usually very\\nsmall, and so the resulting statistical estimate is quite unreliable (noisy),\\neven if the estimate is smoothed. So it is often better to combine the new\\ninformation with the original guess in a process of Bayesian updating. In\\nthis case we have:\\np(k+1)\\nt\\n= |VRt| + κp(k)\\nt\\n|VR| + κ\\n(11.25)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.3\\nThe Binary Independence Model\\n229\\nHere p(k)\\nt\\nis the kth estimate for pt in an iterative updating process and\\nis used as a Bayesian prior in the next iteration with a weighting of κ.\\nRelating this equation back to Equation (11.4) requires a bit more proba-\\nbility theory than we have presented here (we need to use a beta distribu-\\ntion prior, conjugate to the Bernoulli random variable Xt). But the form\\nof the resulting equation is quite straightforward: rather than uniformly\\ndistributing pseudocounts, we now distribute a total of κ pseudocounts\\naccording to the previous estimate, which acts as the prior distribution.\\nIn the absence of other evidence (and assuming that the user is perhaps\\nindicating roughly 5 relevant or nonrelevant documents) then a value\\nof around κ = 5 is perhaps appropriate. That is, the prior is strongly\\nweighted so that the estimate does not change too much from the evi-\\ndence provided by a very small number of documents.\\n5. Repeat the above process from step 2, generating a succession of approxi-\\nmations to R and hence pt, until the user is satisﬁed.\\nIt is also straightforward to derive a pseudo-relevance feedback version of\\nthis algorithm, where we simply pretend that VR = V. More brieﬂy:\\n1. Assume initial estimates for pt and ut as above.\\n2. Determine a guess for the size of the relevant document set. If unsure, a\\nconservative (too small) guess is likely to be best. This motivates use of a\\nﬁxed size set V of highest ranked documents.\\n3. Improve our guesses for pt and ut. We choose from the methods of Equa-\\ntions (11.23) and (11.25) for re-estimating pt, except now based on the set\\nV instead of VR. If we let Vt be the subset of documents in V containing\\nxt and use add 1\\n2 smoothing, we get:\\npt = |Vt| + 1\\n2\\n|V| + 1\\n(11.26)\\nand if we assume that documents that are not retrieved are nonrelevant\\nthen we can update our ut estimates as:\\nut = dft −|Vt| + 1\\n2\\nN −|V| + 1\\n(11.27)\\n4. Go to step 2 until the ranking of the returned results converges.\\nOnce we have a real estimate for pt then the ct weights used in the RSV\\nvalue look almost like a tf-idf value. For instance, using Equation (11.18),\\n', 'Online edition (c)\\n2009 Cambridge UP\\n230\\n11\\nProbabilistic information retrieval\\nEquation (11.22), and Equation (11.26), we have:\\nct = log\\n\\x14\\npt\\n1 −pt · 1 −ut\\nut\\n\\x15\\n≈log\\n\"\\n|Vt| + 1\\n2\\n|V| −|Vt| + 1 · N\\ndft\\n#\\n(11.28)\\nBut things aren’t quite the same: pt/(1 −pt) measures the (estimated) pro-\\nportion of relevant documents that the term t occurs in, not term frequency.\\nMoreover, if we apply log identities:\\nct = log\\n|Vt| + 1\\n2\\n|V| −|Vt| + 1 + log N\\ndft\\n(11.29)\\nwe see that we are now adding the two log scaled components rather than\\nmultiplying them.\\n?\\nExercise 11.1\\nWork through the derivation of Equation (11.20) from Equations (11.18) and (11.19).\\nExercise 11.2\\nWhat are the differences between standard vector space tf-idf weighting and the BIM\\nprobabilistic retrieval model (in the case where no document relevance information\\nis available)?\\nExercise 11.3\\n[⋆⋆]\\nLet Xt be a random variable indicating whether the term t appears in a document.\\nSuppose we have |R| relevant documents in the document collection and that Xt = 1\\nin s of the documents. Take the observed data to be just these observations of Xt for\\neach document in R. Show that the MLE for the parameter pt = P(Xt = 1|R = 1,⃗q),\\nthat is, the value for pt which maximizes the probability of the observed data, is\\npt = s/|R|.\\nExercise 11.4\\nDescribe the differences between vector space relevance feedback and probabilistic\\nrelevance feedback.\\n11.4\\nAn appraisal and some extensions\\n11.4.1\\nAn appraisal of probabilistic models\\nProbabilistic methods are one of the oldest formal models in IR. Already\\nin the 1970s they were held out as an opportunity to place IR on a ﬁrmer\\ntheoretical footing, and with the resurgence of probabilistic methods in com-\\nputational linguistics in the 1990s, that hope has returned, and probabilis-\\ntic methods are again one of the currently hottest topics in IR. Traditionally,\\nprobabilistic IR has had neat ideas but the methods have never won on per-\\nformance. Getting reasonable approximations of the needed probabilities for\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.4\\nAn appraisal and some extensions\\n231\\na probabilistic IR model is possible, but it requires some major assumptions.\\nIn the BIM these are:\\n• a Boolean representation of documents/queries/relevance\\n• term independence\\n• terms not in the query don’t affect the outcome\\n• document relevance values are independent\\nIt is perhaps the severity of the modeling assumptions that makes achieving\\ngood performance difﬁcult. A general problem seems to be that probabilistic\\nmodels either require partial relevance information or else only allow for\\nderiving apparently inferior term weighting models.\\nThings started to change in the 1990s when the BM25 weighting scheme,\\nwhich we discuss in the next section, showed very good performance, and\\nstarted to be adopted as a term weighting scheme by many groups. The\\ndifference between “vector space” and “probabilistic” IR systems is not that\\ngreat: in either case, you build an information retrieval scheme in the exact\\nsame way that we discussed in Chapter 7. For a probabilistic IR system, it’s\\njust that, at the end, you score queries not by cosine similarity and tf-idf in\\na vector space, but by a slightly different formula motivated by probability\\ntheory. Indeed, sometimes people have changed an existing vector-space\\nIR system into an effectively probabilistic system simply by adopted term\\nweighting formulas from probabilistic models. In this section, we brieﬂy\\npresent three extensions of the traditional probabilistic model, and in the next\\nchapter, we look at the somewhat different probabilistic language modeling\\napproach to IR.\\n11.4.2\\nTree-structured dependencies between terms\\nSome of the assumptions of the BIM can be removed. For example, we can\\nremove the assumption that terms are independent. This assumption is very\\nfar from true in practice. A case that particularly violates this assumption is\\nterm pairs like Hong and Kong, which are strongly dependent. But dependen-\\ncies can occur in various complex conﬁgurations, such as between the set of\\nterms New, York, England, City, Stock, Exchange, and University. van Rijsbergen\\n(1979) proposed a simple, plausible model which allowed a tree structure of\\nterm dependencies, as in Figure 11.1. In this model each term can be directly\\ndependent on only one other term, giving a tree structure of dependencies.\\nWhen it was invented in the 1970s, estimation problems held back the practi-\\ncal success of this model, but the idea was reinvented as the Tree Augmented\\nNaive Bayes model by Friedman and Goldszmidt (1996), who used it with\\nsome success on various machine learning data sets.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n232\\n11\\nProbabilistic information retrieval\\nx1\\nx2\\nx3\\nx4\\nx5\\nx6\\nx7\\n◮Figure 11.1\\nA tree of dependencies between terms. In this graphical model rep-\\nresentation, a term xi is directly dependent on a term xk if there is an arrow xk →xi.\\n11.4.3\\nOkapi BM25: a non-binary model\\nThe BIM was originally designed for short catalog records and abstracts of\\nfairly consistent length, and it works reasonably in these contexts, but for\\nmodern full-text search collections, it seems clear that a model should pay\\nattention to term frequency and document length, as in Chapter 6. The BM25\\nBM25 WEIGHTS\\nweighting scheme, often called Okapi weighting, after the system in which it was\\nOKAPI WEIGHTING\\nﬁrst implemented, was developed as a way of building a probabilistic model\\nsensitive to these quantities while not introducing too many additional pa-\\nrameters into the model (Spärck Jones et al. 2000). We will not develop the\\nfull theory behind the model here, but just present a series of forms that\\nbuild up to the standard form now used for document scoring. The simplest\\nscore for document d is just idf weighting of the query terms present, as in\\nEquation (11.22):\\nRSVd = ∑\\nt∈q\\nlog N\\ndft\\n(11.30)\\nSometimes, an alternative version of idf is used. If we start with the formula\\nin Equation (11.21) but in the absence of relevance feedback information we\\nestimate that S = s = 0, then we get an alternative idf formulation as follows:\\nRSVd = ∑\\nt∈q\\nlog N −dft + 1\\n2\\ndft + 1\\n2\\n(11.31)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.4\\nAn appraisal and some extensions\\n233\\nThis variant behaves slightly strangely: if a term occurs in over half the doc-\\numents in the collection then this model gives a negative term weight, which\\nis presumably undesirable. But, assuming the use of a stop list, this normally\\ndoesn’t happen, and the value for each summand can be given a ﬂoor of 0.\\nWe can improve on Equation (11.30) by factoring in the frequency of each\\nterm and document length:\\nRSVd = ∑\\nt∈q\\nlog\\n\\x14 N\\ndft\\n\\x15\\n·\\n(k1 + 1)tftd\\nk1((1 −b) + b × (Ld/Lave)) + tftd\\n(11.32)\\nHere, tftd is the frequency of term t in document d, and Ld and Lave are the\\nlength of document d and the average document length for the whole col-\\nlection. The variable k1 is a positive tuning parameter that calibrates the\\ndocument term frequency scaling. A k1 value of 0 corresponds to a binary\\nmodel (no term frequency), and a large value corresponds to using raw term\\nfrequency. b is another tuning parameter (0 ≤b ≤1) which determines\\nthe scaling by document length: b = 1 corresponds to fully scaling the term\\nweight by the document length, while b = 0 corresponds to no length nor-\\nmalization.\\nIf the query is long, then we might also use similar weighting for query\\nterms. This is appropriate if the queries are paragraph long information\\nneeds, but unnecessary for short queries.\\nRSVd = ∑\\nt∈q\\n\\x14\\nlog N\\ndft\\n\\x15\\n·\\n(k1 + 1)tftd\\nk1((1 −b) + b × (Ld/Lave)) + tftd\\n· (k3 + 1)tftq\\nk3 + tftq\\n(11.33)\\nwith tftq being the frequency of term t in the query q, and k3 being another\\npositive tuning parameter that this time calibrates term frequency scaling\\nof the query. In the equation presented, there is no length normalization of\\nqueries (it is as if b = 0 here). Length normalization of the query is unnec-\\nessary because retrieval is being done with respect to a single ﬁxed query.\\nThe tuning parameters of these formulas should ideally be set to optimize\\nperformance on a development test collection (see page 153). That is, we\\ncan search for values of these parameters that maximize performance on a\\nseparate development test collection (either manually or with optimization\\nmethods such as grid search or something more advanced), and then use\\nthese parameters on the actual test collection. In the absence of such opti-\\nmization, experiments have shown reasonable values are to set k1 and k3 to\\na value between 1.2 and 2 and b = 0.75.\\nIf we have relevance judgments available, then we can use the full form of\\n(11.21) in place of the approximation log(N/dft) introduced in (11.22):\\nRSVd\\n= ∑\\nt∈q\\nlog\\n\"\"\\n(|VRt| + 1\\n2)/(|VNRt| + 1\\n2)\\n(dft −|VRt| + 1\\n2)/(N −dft −|VR| + |VRt| + 1\\n2)\\n#\\n(11.34)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n234\\n11\\nProbabilistic information retrieval\\n×\\n(k1 + 1)tftd\\nk1((1 −b) + b(Ld/Lave)) + tftd\\n× (k3 + 1)tftq\\nk3 + tftq\\n\\x15\\nHere, VRt, NVRt, and VR are used as in Section 11.3.4. The ﬁrst part of the\\nexpression reﬂects relevance feedback (or just idf weighting if no relevance\\ninformation is available), the second implements document term frequency\\nand document length scaling, and the third considers term frequency in the\\nquery.\\nRather than just providing a term weighting method for terms in a user’s\\nquery, relevance feedback can also involve augmenting the query (automat-\\nically or with manual review) with some (say, 10–20) of the top terms in the\\nknown-relevant documents as ordered by the relevance factor ˆct from Equa-\\ntion (11.21), and the above formula can then be used with such an augmented\\nquery vector⃗q.\\nThe BM25 term weighting formulas have been used quite widely and quite\\nsuccessfully across a range of collections and search tasks. Especially in the\\nTREC evaluations, they performed well and were widely adopted by many\\ngroups. See Spärck Jones et al. (2000) for extensive motivation and discussion\\nof experimental results.\\n11.4.4\\nBayesian network approaches to IR\\nTurtle and Croft (1989; 1991) introduced into information retrieval the use\\nof Bayesian networks (Jensen and Jensen 2001), a form of probabilistic graph-\\nBAYESIAN NETWORKS\\nical model. We skip the details because fully introducing the formalism of\\nBayesian networks would require much too much space, but conceptually,\\nBayesian networks use directed graphs to show probabilistic dependencies\\nbetween variables, as in Figure 11.1, and have led to the development of so-\\nphisticated algorithms for propagating inﬂuence so as to allow learning and\\ninference with arbitrary knowledge within arbitrary directed acyclic graphs.\\nTurtle and Croft used a sophisticated network to better model the complex\\ndependencies between a document and a user’s information need.\\nThe model decomposes into two parts: a document collection network and\\na query network. The document collection network is large, but can be pre-\\ncomputed: it maps from documents to terms to concepts. The concepts are\\na thesaurus-based expansion of the terms appearing in the document. The\\nquery network is relatively small but a new network needs to be built each\\ntime a query comes in, and then attached to the document network. The\\nquery network maps from query terms, to query subexpressions (built us-\\ning probabilistic or “noisy” versions of AND and OR operators), to the user’s\\ninformation need.\\nThe result is a ﬂexible probabilistic network which can generalize vari-\\nous simpler Boolean and probabilistic models. Indeed, this is the primary\\n', 'Online edition (c)\\n2009 Cambridge UP\\n11.5\\nReferences and further reading\\n235\\ncase of a statistical ranked retrieval model that naturally supports structured\\nquery operators. The system allowed efﬁcient large-scale retrieval, and was\\nthe basis of the InQuery text retrieval system, built at the University of Mas-\\nsachusetts. This system performed very well in TREC evaluations and for a\\ntime was sold commercially. On the other hand, the model still used various\\napproximations and independence assumptions to make parameter estima-\\ntion and computation possible. There has not been much follow-on work\\nalong these lines, but we would note that this model was actually built very\\nearly on in the modern era of using Bayesian networks, and there have been\\nmany subsequent developments in the theory, and the time is perhaps right\\nfor a new generation of Bayesian network-based information retrieval sys-\\ntems.\\n11.5\\nReferences and further reading\\nLonger introductions to probability theory can be found in most introduc-\\ntory probability and statistics books, such as (Grinstead and Snell 1997, Rice\\n2006, Ross 2006). An introduction to Bayesian utility theory can be found in\\n(Ripley 1996).\\nThe probabilistic approach to IR originated in the UK in the 1950s. The\\nﬁrst major presentation of a probabilistic model is Maron and Kuhns (1960).\\nRobertson and Jones (1976) introduce the main foundations of the BIM and\\nvan Rijsbergen (1979) presents in detail the classic BIM probabilistic model.\\nThe idea of the PRP is variously attributed to S. E. Robertson, M. E. Maron\\nand W. S. Cooper (the term “Probabilistic Ordering Principle” is used in\\nRobertson and Jones (1976), but PRP dominates in later work). Fuhr (1992)\\nis a more recent presentation of probabilistic IR, which includes coverage of\\nother approaches such as probabilistic logics and Bayesian networks. Crestani\\net al. (1998) is another survey.Spärck Jones et al. (2000) is the deﬁnitive pre-\\nsentation of probabilistic IR experiments by the “London school”, and Robert-\\nson (2005) presents a retrospective on the group’s participation in TREC eval-\\nuations, including detailed discussion of the Okapi BM25 scoring function\\nand its development. Robertson et al. (2004) extend BM25 to the case of mul-\\ntiple weighted ﬁelds.\\nThe open-source Indri search engine, which is distributed with the Lemur\\ntoolkit (http://www.lemurproject.org/) merges ideas from Bayesian inference net-\\nworks and statistical language modeling approaches (see Chapter 12), in par-\\nticular preserving the former’s support for structured query operators.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n237\\n12\\nLanguage models for information\\nretrieval\\nA common suggestion to users for coming up with good queries is to think\\nof words that would likely appear in a relevant document, and to use those\\nwords as the query. The language modeling approach to IR directly models\\nthat idea: a document is a good match to a query if the document model\\nis likely to generate the query, which will in turn happen if the document\\ncontains the query words often. This approach thus provides a different real-\\nization of some of the basic ideas for document ranking which we saw in Sec-\\ntion 6.2 (page 117). Instead of overtly modeling the probability P(R = 1|q, d)\\nof relevance of a document d to a query q, as in the traditional probabilis-\\ntic approach to IR (Chapter 11), the basic language modeling approach in-\\nstead builds a probabilistic language model Md from each document d, and\\nranks documents based on the probability of the model generating the query:\\nP(q|Md).\\nIn this chapter, we ﬁrst introduce the concept of language models (Sec-\\ntion 12.1) and then describe the basic and most commonly used language\\nmodeling approach to IR, the Query Likelihood Model (Section 12.2). Af-\\nter some comparisons between the language modeling approach and other\\napproaches to IR (Section 12.3), we ﬁnish by brieﬂy describing various ex-\\ntensions to the language modeling approach (Section 12.4).\\n12.1\\nLanguage models\\n12.1.1\\nFinite automata and language models\\nWhat do we mean by a document model generating a query? A traditional\\ngenerative model of a language, of the kind familiar from formal language\\nGENERATIVE MODEL\\ntheory, can be used either to recognize or to generate strings. For example,\\nthe ﬁnite automaton shown in Figure 12.1 can generate strings that include\\nthe examples shown. The full set of strings that can be generated is called\\nthe language of the automaton.1\\nLANGUAGE\\n', 'Online edition (c)\\n2009 Cambridge UP\\n238\\n12\\nLanguage models for information retrieval\\nI\\nwish\\nI wish\\nI wish I wish\\nI wish I wish I wish\\nI wish I wish I wish I wish I wish I wish\\n...\\nCANNOT GENERATE: wish I wish\\n◮Figure 12.1\\nA simple ﬁnite automaton and some of the strings in the language it\\ngenerates. →shows the start state of the automaton and a double circle indicates a\\n(possible) ﬁnishing state.\\nq1\\nP(STOP|q1) = 0.2\\nthe\\n0.2\\na\\n0.1\\nfrog\\n0.01\\ntoad\\n0.01\\nsaid\\n0.03\\nlikes\\n0.02\\nthat\\n0.04\\n...\\n...\\n◮Figure 12.2\\nA one-state ﬁnite automaton that acts as a unigram language model.\\nWe show a partial speciﬁcation of the state emission probabilities.\\nIf instead each node has a probability distribution over generating differ-\\nent terms, we have a language model. The notion of a language model is\\ninherently probabilistic. A language model is a function that puts a probability\\nLANGUAGE MODEL\\nmeasure over strings drawn from some vocabulary. That is, for a language\\nmodel M over an alphabet Σ:\\n∑\\ns∈Σ∗\\nP(s) = 1\\n(12.1)\\nOne simple kind of language model is equivalent to a probabilistic ﬁnite\\nautomaton consisting of just a single node with a single probability distri-\\nbution over producing different terms, so that ∑t∈V P(t) = 1, as shown\\nin Figure 12.2. After generating each word, we decide whether to stop or\\nto loop around and then produce another word, and so the model also re-\\nquires a probability of stopping in the ﬁnishing state. Such a model places a\\nprobability distribution over any sequence of words. By construction, it also\\nprovides a model for generating text according to its distribution.\\n1. Finite automata can have outputs attached to either their states or their arcs; we use states\\nhere, because that maps directly on to the way probabilistic automata are usually formalized.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.1\\nLanguage models\\n239\\nModel M1\\nModel M2\\nthe\\n0.2\\nthe\\n0.15\\na\\n0.1\\na\\n0.12\\nfrog\\n0.01\\nfrog\\n0.0002\\ntoad\\n0.01\\ntoad\\n0.0001\\nsaid\\n0.03\\nsaid\\n0.03\\nlikes\\n0.02\\nlikes\\n0.04\\nthat\\n0.04\\nthat\\n0.04\\ndog\\n0.005\\ndog\\n0.01\\ncat\\n0.003\\ncat\\n0.015\\nmonkey\\n0.001\\nmonkey\\n0.002\\n...\\n...\\n...\\n...\\n◮Figure 12.3\\nPartial speciﬁcation of two unigram language models.\\n\\x0f\\nExample 12.1:\\nTo ﬁnd the probability of a word sequence, we just multiply the\\nprobabilities which the model gives to each word in the sequence, together with the\\nprobability of continuing or stopping after producing each word. For example,\\nP(frog said that toad likes frog)\\n=\\n(0.01 × 0.03 × 0.04 × 0.01 × 0.02 × 0.01)\\n(12.2)\\n×(0.8 × 0.8 × 0.8 × 0.8 × 0.8 × 0.8 × 0.2)\\n≈\\n0.000000000001573\\nAs you can see, the probability of a particular string/document, is usually a very\\nsmall number! Here we stopped after generating frog the second time. The ﬁrst line of\\nnumbers are the term emission probabilities, and the second line gives the probabil-\\nity of continuing or stopping after generating each word. An explicit stop probability\\nis needed for a ﬁnite automaton to be a well-formed language model according to\\nEquation (12.1). Nevertheless, most of the time, we will omit to include STOP and\\n(1 −STOP) probabilities (as do most other authors). To compare two models for a\\ndata set, we can calculate their likelihood ratio, which results from simply dividing the\\nLIKELIHOOD RATIO\\nprobability of the data according to one model by the probability of the data accord-\\ning to the other model. Providing that the stop probability is ﬁxed, its inclusion will\\nnot alter the likelihood ratio that results from comparing the likelihood of two lan-\\nguage models generating a string. Hence, it will not alter the ranking of documents.2\\nNevertheless, formally, the numbers will no longer truly be probabilities, but only\\nproportional to probabilities. See Exercise 12.4.\\n\\x0f\\nExample 12.2:\\nSuppose, now, that we have two language models M1 and M2,\\nshown partially in Figure 12.3. Each gives a probability estimate to a sequence of\\n2. In the IR context that we are leading up to, taking the stop probability to be ﬁxed across\\nmodels seems reasonable. This is because we are generating queries, and the length distribution\\nof queries is ﬁxed and independent of the document from which we are generating the language\\nmodel.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n240\\n12\\nLanguage models for information retrieval\\nterms, as already illustrated in Example 12.1. The language model that gives the\\nhigher probability to the sequence of terms is more likely to have generated the term\\nsequence. This time, we will omit STOP probabilities from our calculations. For the\\nsequence shown, we get:\\n(12.3)\\ns\\nfrog\\nsaid\\nthat\\ntoad\\nlikes\\nthat\\ndog\\nM1\\n0.01\\n0.03\\n0.04\\n0.01\\n0.02\\n0.04\\n0.005\\nM2\\n0.0002\\n0.03\\n0.04\\n0.0001\\n0.04\\n0.04\\n0.01\\nP(s|M1) = 0.00000000000048\\nP(s|M2) = 0.000000000000000384\\nand we see that P(s|M1) > P(s|M2). We present the formulas here in terms of prod-\\nucts of probabilities, but, as is common in probabilistic applications, in practice it is\\nusually best to work with sums of log probabilities (cf. page 258).\\n12.1.2\\nTypes of language models\\nHow do we build probabilities over sequences of terms? We can always\\nuse the chain rule from Equation (11.1) to decompose the probability of a\\nsequence of events into the probability of each successive event conditioned\\non earlier events:\\nP(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t1t2)P(t4|t1t2t3)\\n(12.4)\\nThe simplest form of language model simply throws away all conditioning\\ncontext, and estimates each term independently. Such a model is called a\\nunigram language model:\\nUNIGRAM LANGUAGE\\nMODEL\\nPuni(t1t2t3t4) = P(t1)P(t2)P(t3)P(t4)\\n(12.5)\\nThere are many more complex kinds of language models, such as bigram\\nBIGRAM LANGUAGE\\nMODEL\\nlanguage models, which condition on the previous term,\\nPbi(t1t2t3t4) = P(t1)P(t2|t1)P(t3|t2)P(t4|t3)\\n(12.6)\\nand even more complex grammar-based language models such as proba-\\nbilistic context-free grammars. Such models are vital for tasks like speech\\nrecognition, spelling correction, and machine translation, where you need\\nthe probability of a term conditioned on surrounding context. However,\\nmost language-modeling work in IR has used unigram language models.\\nIR is not the place where you most immediately need complex language\\nmodels, since IR does not directly depend on the structure of sentences to\\nthe extent that other tasks like speech recognition do. Unigram models are\\noften sufﬁcient to judge the topic of a text. Moreover, as we shall see, IR lan-\\nguage models are frequently estimated from a single document and so it is\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.1\\nLanguage models\\n241\\nquestionable whether there is enough training data to do more. Losses from\\ndata sparseness (see the discussion on page 260) tend to outweigh any gains\\nfrom richer models. This is an example of the bias-variance tradeoff (cf. Sec-\\ntion 14.6, page 308): With limited training data, a more constrained model\\ntends to perform better. In addition, unigram models are more efﬁcient to\\nestimate and apply than higher-order models. Nevertheless, the importance\\nof phrase and proximity queries in IR in general suggests that future work\\nshould make use of more sophisticated language models, and some has be-\\ngun to (see Section 12.5, page 252). Indeed, making this move parallels the\\nmodel of van Rijsbergen in Chapter 11 (page 231).\\n12.1.3\\nMultinomial distributions over words\\nUnder the unigram language model the order of words is irrelevant, and so\\nsuch models are often called “bag of words” models, as discussed in Chap-\\nter 6 (page 117). Even though there is no conditioning on preceding context,\\nthis model nevertheless still gives the probability of a particular ordering of\\nterms. However, any other ordering of this bag of terms will have the same\\nprobability. So, really, we have a multinomial distribution over words. So long\\nMULTINOMIAL\\nDISTRIBUTION\\nas we stick to unigram models, the language model name and motivation\\ncould be viewed as historical rather than necessary. We could instead just\\nrefer to the model as a multinomial model. From this perspective, the equa-\\ntions presented above do not present the multinomial probability of a bag of\\nwords, since they do not sum over all possible orderings of those words, as\\nis done by the multinomial coefﬁcient (the ﬁrst term on the right-hand side)\\nin the standard presentation of a multinomial model:\\nP(d) =\\nLd!\\ntft1,d!tft2,d! · · · tftM,d! P(t1)tft1,dP(t2)tft2,d · · · P(tM)tftM,d\\n(12.7)\\nHere, Ld = ∑1≤i≤M tfti,d is the length of document d, M is the size of the term\\nvocabulary, and the products are now over the terms in the vocabulary, not\\nthe positions in the document. However, just as with STOP probabilities, in\\npractice we can also leave out the multinomial coefﬁcient in our calculations,\\nsince, for a particular bag of words, it will be a constant, and so it has no effect\\non the likelihood ratio of two different models generating a particular bag of\\nwords. Multinomial distributions also appear in Section 13.2 (page 258).\\nThe fundamental problem in designing language models is that we do not\\nknow what exactly we should use as the model Md. However, we do gener-\\nally have a sample of text that is representative of that model. This problem\\nmakes a lot of sense in the original, primary uses of language models. For ex-\\nample, in speech recognition, we have a training sample of (spoken) text. But\\nwe have to expect that, in the future, users will use different words and in\\n', 'Online edition (c)\\n2009 Cambridge UP\\n242\\n12\\nLanguage models for information retrieval\\ndifferent sequences, which we have never observed before, and so the model\\nhas to generalize beyond the observed data to allow unknown words and se-\\nquences. This interpretation is not so clear in the IR case, where a document\\nis ﬁnite and usually ﬁxed. The strategy we adopt in IR is as follows. We\\npretend that the document d is only a representative sample of text drawn\\nfrom a model distribution, treating it like a ﬁne-grained topic. We then esti-\\nmate a language model from this sample, and use that model to calculate the\\nprobability of observing any word sequence, and, ﬁnally, we rank documents\\naccording to their probability of generating the query.\\n?\\nExercise 12.1\\n[⋆]\\nIncluding stop probabilities in the calculation, what will the sum of the probability\\nestimates of all strings in the language of length 1 be? Assume that you generate a\\nword and then decide whether to stop or not (i.e., the null string is not part of the\\nlanguage).\\nExercise 12.2\\n[⋆]\\nIf the stop probability is omitted from calculations, what will the sum of the scores\\nassigned to strings in the language of length 1 be?\\nExercise 12.3\\n[⋆]\\nWhat is the likelihood ratio of the document according to M1 and M2 in Exam-\\nple 12.2?\\nExercise 12.4\\n[⋆]\\nNo explicit STOP probability appeared in Example 12.2. Assuming that the STOP\\nprobability of each model is 0.1, does this change the likelihood ratio of a document\\naccording to the two models?\\nExercise 12.5\\n[⋆⋆]\\nHow might a language model be used in a spelling correction system? In particular,\\nconsider the case of context-sensitive spelling correction, and correcting incorrect us-\\nages of words, such as their in Are you their? (See Section 3.5 (page 65) for pointers to\\nsome literature on this topic.)\\n12.2\\nThe query likelihood model\\n12.2.1\\nUsing query likelihood language models in IR\\nLanguage modeling is a quite general formal approach to IR, with many vari-\\nant realizations. The original and basic method for using language models\\nin IR is the query likelihood model. In it, we construct from each document d\\nQUERY LIKELIHOOD\\nMODEL\\nin the collection a language model Md. Our goal is to rank documents by\\nP(d|q), where the probability of a document is interpreted as the likelihood\\nthat it is relevant to the query. Using Bayes rule (as introduced in Section 11.1,\\npage 220), we have:\\nP(d|q) = P(q|d)P(d)/P(q)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n243\\nP(q) is the same for all documents, and so can be ignored. The prior prob-\\nability of a document P(d) is often treated as uniform across all d and so it\\ncan also be ignored, but we could implement a genuine prior which could in-\\nclude criteria like authority, length, genre, newness, and number of previous\\npeople who have read the document. But, given these simpliﬁcations, we\\nreturn results ranked by simply P(q|d), the probability of the query q under\\nthe language model derived from d. The Language Modeling approach thus\\nattempts to model the query generation process: Documents are ranked by\\nthe probability that a query would be observed as a random sample from the\\nrespective document model.\\nThe most common way to do this is using the multinomial unigram lan-\\nguage model, which is equivalent to a multinomial Naive Bayes model (page 263),\\nwhere the documents are the classes, each treated in the estimation as a sep-\\narate “language”. Under this model, we have that:\\nP(q|Md) = Kq ∏\\nt∈V\\nP(t|Md)tft,d\\n(12.8)\\nwhere, again Kq = Ld!/(tft1,d!tft2,d! · · · tftM,d!) is the multinomial coefﬁcient\\nfor the query q, which we will henceforth ignore, since it is a constant for a\\nparticular query.\\nFor retrieval based on a language model (henceforth LM), we treat the\\ngeneration of queries as a random process. The approach is to\\n1. Infer a LM for each document.\\n2. Estimate P(q|Mdi), the probability of generating the query according to\\neach of these document models.\\n3. Rank the documents according to these probabilities.\\nThe intuition of the basic model is that the user has a prototype document in\\nmind, and generates a query based on words that appear in this document.\\nOften, users have a reasonable idea of terms that are likely to occur in doc-\\numents of interest and they will choose query terms that distinguish these\\ndocuments from others in the collection.3 Collection statistics are an integral\\npart of the language model, rather than being used heuristically as in many\\nother approaches.\\n12.2.2\\nEstimating the query generation probability\\nIn this section we describe how to estimate P(q|Md). The probability of pro-\\nducing the query given the LM Md of document d using maximum likelihood\\n3.\\nOf course, in other cases, they do not. The answer to this within the language modeling\\napproach is translation language models, as brieﬂy discussed in Section 12.4.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n244\\n12\\nLanguage models for information retrieval\\nestimation (MLE) and the unigram assumption is:\\nˆP(q|Md) = ∏\\nt∈q\\nˆPmle(t|Md) = ∏\\nt∈q\\ntft,d\\nLd\\n(12.9)\\nwhere Md is the language model of document d, tft,d is the (raw) term fre-\\nquency of term t in document d, and Ld is the number of tokens in docu-\\nment d. That is, we just count up how often each word occurred, and divide\\nthrough by the total number of words in the document d. This is the same\\nmethod of calculating an MLE as we saw in Section 11.3.2 (page 226), but\\nnow using a multinomial over word counts.\\nThe classic problem with using language models is one of estimation (the\\nˆ symbol on the P’s is used above to stress that the model is estimated):\\nterms appear very sparsely in documents. In particular, some words will\\nnot have appeared in the document at all, but are possible words for the in-\\nformation need, which the user may have used in the query. If we estimate\\nˆP(t|Md) = 0 for a term missing from a document d, then we get a strict\\nconjunctive semantics: documents will only give a query non-zero probabil-\\nity if all of the query terms appear in the document. Zero probabilities are\\nclearly a problem in other uses of language models, such as when predicting\\nthe next word in a speech recognition application, because many words will\\nbe sparsely represented in the training data. It may seem rather less clear\\nwhether this is problematic in an IR application. This could be thought of\\nas a human-computer interface issue: vector space systems have generally\\npreferred more lenient matching, though recent web search developments\\nhave tended more in the direction of doing searches with such conjunctive\\nsemantics. Regardless of the approach here, there is a more general prob-\\nlem of estimation: occurring words are also badly estimated; in particular,\\nthe probability of words occurring once in the document is normally over-\\nestimated, since their one occurrence was partly by chance. The answer to\\nthis (as we saw in Section 11.3.2, page 226) is smoothing. But as people have\\ncome to understand the LM approach better, it has become apparent that the\\nrole of smoothing in this model is not only to avoid zero probabilities. The\\nsmoothing of terms actually implements major parts of the term weighting\\ncomponent (Exercise 12.8). It is not just that an unsmoothed model has con-\\njunctive semantics; an unsmoothed model works badly because it lacks parts\\nof the term weighting component.\\nThus, we need to smooth probabilities in our document language mod-\\nels: to discount non-zero probabilities and to give some probability mass to\\nunseen words. There’s a wide space of approaches to smoothing probabil-\\nity distributions to deal with this problem. In Section 11.3.2 (page 226), we\\nalready discussed adding a number (1, 1/2, or a small α) to the observed\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n245\\ncounts and renormalizing to give a probability distribution.4\\nIn this sec-\\ntion we will mention a couple of other smoothing methods, which involve\\ncombining observed counts with a more general reference probability distri-\\nbution. The general approach is that a non-occurring term should be possi-\\nble in a query, but its probability should be somewhat close to but no more\\nlikely than would be expected by chance from the whole collection. That is,\\nif tft,d = 0 then\\nˆP(t|Md) ≤cft/T\\nwhere cft is the raw count of the term in the collection, and T is the raw size\\n(number of tokens) of the entire collection. A simple idea that works well in\\npractice is to use a mixture between a document-speciﬁc multinomial distri-\\nbution and a multinomial distribution estimated from the entire collection:\\nˆP(t|d) = λ ˆPmle(t|Md) + (1 −λ) ˆPmle(t|Mc)\\n(12.10)\\nwhere 0 < λ < 1 and Mc is a language model built from the entire doc-\\nument collection. This mixes the probability from the document with the\\ngeneral collection frequency of the word. Such a model is referred to as a\\nlinear interpolation language model.5 Correctly setting λ is important to the\\nLINEAR\\nINTERPOLATION\\ngood performance of this model.\\nAn alternative is to use a language model built from the whole collection\\nas a prior distribution in a Bayesian updating process (rather than a uniform\\nBAYESIAN SMOOTHING\\ndistribution, as we saw in Section 11.3.2). We then get the following equation:\\nˆP(t|d) = tft,d + α ˆP(t|Mc)\\nLd + α\\n(12.11)\\nBoth of these smoothing methods have been shown to perform well in IR\\nexperiments; we will stick with the linear interpolation smoothing method\\nfor the rest of this section. While different in detail, they are both conceptu-\\nally similar: in both cases the probability estimate for a word present in the\\ndocument combines a discounted MLE and a fraction of the estimate of its\\nprevalence in the whole collection, while for words not present in a docu-\\nment, the estimate is just a fraction of the estimate of the prevalence of the\\nword in the whole collection.\\nThe role of smoothing in LMs for IR is not simply or principally to avoid es-\\ntimation problems. This was not clear when the models were ﬁrst proposed,\\nbut it is now understood that smoothing is essential to the good properties\\n4. In the context of probability theory, (re)normalization refers to summing numbers that cover\\nan event space and dividing them through by their sum, so that the result is a probability distri-\\nbution which sums to 1. This is distinct from both the concept of term normalization in Chapter 2\\nand the concept of length normalization in Chapter 6, which is done with a L2 norm.\\n5. It is also referred to as Jelinek-Mercer smoothing.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n246\\n12\\nLanguage models for information retrieval\\nof the models. The reason for this is explored in Exercise 12.8. The extent\\nof smoothing in these two models is controlled by the λ and α parameters: a\\nsmall value of λ or a large value of α means more smoothing. This parameter\\ncan be tuned to optimize performance using a line search (or, for the linear\\ninterpolation model, by other methods, such as the expectation maximimiza-\\ntion algorithm; see Section 16.5, page 368). The value need not be a constant.\\nOne approach is to make the value a function of the query size. This is useful\\nbecause a small amount of smoothing (a “conjunctive-like” search) is more\\nsuitable for short queries, while a lot of smoothing is more suitable for long\\nqueries.\\nTo summarize, the retrieval ranking for a query q under the basic LM for\\nIR we have been considering is given by:\\nP(d|q) ∝P(d)∏\\nt∈q\\n((1 −λ)P(t|Mc) + λP(t|Md))\\n(12.12)\\nThis equation captures the probability that the document that the user had\\nin mind was in fact d.\\n\\x0f\\nExample 12.3:\\nSuppose the document collection contains two documents:\\n•\\nd1: Xyzzy reports a proﬁt but revenue is down\\n•\\nd2: Quorus narrows quarter loss but revenue decreases further\\nThe model will be MLE unigram models from the documents and collection, mixed\\nwith λ = 1/2.\\nSuppose the query is revenue down. Then:\\nP(q|d1)\\n=\\n[(1/8 + 2/16)/2] × [(1/8 + 1/16)/2]\\n(12.13)\\n=\\n1/8 × 3/32 = 3/256\\nP(q|d2)\\n=\\n[(1/8 + 2/16)/2] × [(0/8 + 1/16)/2]\\n=\\n1/8 × 1/32 = 1/256\\nSo, the ranking is d1 > d2.\\n12.2.3\\nPonte and Croft’s Experiments\\nPonte and Croft (1998) present the ﬁrst experiments on the language model-\\ning approach to information retrieval. Their basic approach is the model that\\nwe have presented until now. However, we have presented an approach\\nwhere the language model is a mixture of two multinomials, much as in\\n(Miller et al. 1999, Hiemstra 2000) rather than Ponte and Croft’s multivari-\\nate Bernoulli model. The use of multinomials has been standard in most\\nsubsequent work in the LM approach and experimental results in IR, as\\nwell as evidence from text classiﬁcation which we consider in Section 13.3\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.2\\nThe query likelihood model\\n247\\nPrecision\\nRec.\\ntf-idf\\nLM\\n%chg\\n0.0\\n0.7439\\n0.7590\\n+2.0\\n0.1\\n0.4521\\n0.4910\\n+8.6\\n0.2\\n0.3514\\n0.4045\\n+15.1\\n*\\n0.3\\n0.2761\\n0.3342\\n+21.0\\n*\\n0.4\\n0.2093\\n0.2572\\n+22.9\\n*\\n0.5\\n0.1558\\n0.2061\\n+32.3\\n*\\n0.6\\n0.1024\\n0.1405\\n+37.1\\n*\\n0.7\\n0.0451\\n0.0760\\n+68.7\\n*\\n0.8\\n0.0160\\n0.0432\\n+169.6\\n*\\n0.9\\n0.0033\\n0.0063\\n+89.3\\n1.0\\n0.0028\\n0.0050\\n+76.9\\nAve\\n0.1868\\n0.2233\\n+19.55\\n*\\n◮Figure 12.4\\nResults of a comparison of tf-idf with language modeling (LM) term\\nweighting by Ponte and Croft (1998). The version of tf-idf from the INQUERY IR sys-\\ntem includes length normalization of tf. The table gives an evaluation according to\\n11-point average precision with signiﬁcance marked with a * according to a Wilcoxon\\nsigned rank test. The language modeling approach always does better in these exper-\\niments, but note that where the approach shows signiﬁcant gains is at higher levels\\nof recall.\\n(page 263), suggests that it is superior. Ponte and Croft argued strongly for\\nthe effectiveness of the term weights that come from the language modeling\\napproach over traditional tf-idf weights. We present a subset of their results\\nin Figure 12.4 where they compare tf-idf to language modeling by evaluating\\nTREC topics 202–250 over TREC disks 2 and 3. The queries are sentence-\\nlength natural language queries. The language modeling approach yields\\nsigniﬁcantly better results than their baseline tf-idf based term weighting ap-\\nproach. And indeed the gains shown here have been extended in subsequent\\nwork.\\n?\\nExercise 12.6\\n[⋆]\\nConsider making a language model from the following training text:\\nthe martian has landed on the latin pop sensation ricky martin\\na. Under a MLE-estimated unigram probability model, what are P(the) and P(martian)?\\nb. Under a MLE-estimated bigram model, what are P(sensation|pop) and P(pop|the)?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n248\\n12\\nLanguage models for information retrieval\\nExercise 12.7\\n[⋆⋆]\\nSuppose we have a collection that consists of the 4 documents given in the below\\ntable.\\ndocID\\nDocument text\\n1\\nclick go the shears boys click click click\\n2\\nclick click\\n3\\nmetal here\\n4\\nmetal shears click here\\nBuild a query likelihood language model for this document collection. Assume a\\nmixture model between the documents and the collection, with both weighted at 0.5.\\nMaximum likelihood estimation (mle) is used to estimate both as unigram models.\\nWork out the model probabilities of the queries click, shears, and hence click shears for\\neach document, and use those probabilities to rank the documents returned by each\\nquery. Fill in these probabilities in the below table:\\nQuery\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nclick\\nshears\\nclick shears\\nWhat is the ﬁnal ranking of the documents for the query click shears?\\nExercise 12.8\\n[⋆⋆]\\nUsing the calculations in Exercise 12.7 as inspiration or as examples where appro-\\npriate, write one sentence each describing the treatment that the model in Equa-\\ntion (12.10) gives to each of the following quantities. Include whether it is present\\nin the model or not and whether the effect is raw or scaled.\\na. Term frequency in a document\\nb. Collection frequency of a term\\nc. Document frequency of a term\\nd. Length normalization of a term\\nExercise 12.9\\n[⋆⋆]\\nIn the mixture model approach to the query likelihood model (Equation (12.12)), the\\nprobability estimate of a term is based on the term frequency of a word in a document,\\nand the collection frequency of the word. Doing this certainly guarantees that each\\nterm of a query (in the vocabulary) has a non-zero chance of being generated by each\\ndocument. But it has a more subtle but important effect of implementing a form of\\nterm weighting, related to what we saw in Chapter 6. Explain how this works. In\\nparticular, include in your answer a concrete numeric example showing this term\\nweighting at work.\\n12.3\\nLanguage modeling versus other approaches in IR\\nThe language modeling approach provides a novel way of looking at the\\nproblem of text retrieval, which links it with a lot of recent work in speech\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.3\\nLanguage modeling versus other approaches in IR\\n249\\nand language processing. As Ponte and Croft (1998) emphasize, the language\\nmodeling approach to IR provides a different approach to scoring matches\\nbetween queries and documents, and the hope is that the probabilistic lan-\\nguage modeling foundation improves the weights that are used, and hence\\nthe performance of the model. The major issue is estimation of the docu-\\nment model, such as choices of how to smooth it effectively. The model\\nhas achieved very good retrieval results. Compared to other probabilistic\\napproaches, such as the BIM from Chapter 11, the main difference initially\\nappears to be that the LM approach does away with explicitly modeling rel-\\nevance (whereas this is the central variable evaluated in the BIM approach).\\nBut this may not be the correct way to think about things, as some of the\\npapers in Section 12.5 further discuss. The LM approach assumes that docu-\\nments and expressions of information needs are objects of the same type, and\\nassesses their match by importing the tools and methods of language mod-\\neling from speech and natural language processing. The resulting model is\\nmathematically precise, conceptually simple, computationally tractable, and\\nintuitively appealing. This seems similar to the situation with XML retrieval\\n(Chapter 10): there the approaches that assume queries and documents are\\nobjects of the same type are also among the most successful.\\nOn the other hand, like all IR models, you can also raise objections to the\\nmodel. The assumption of equivalence between document and information\\nneed representation is unrealistic. Current LM approaches use very simple\\nmodels of language, usually unigram models. Without an explicit notion of\\nrelevance, relevance feedback is difﬁcult to integrate into the model, as are\\nuser preferences. It also seems necessary to move beyond a unigram model\\nto accommodate notions of phrase or passage matching or Boolean retrieval\\noperators. Subsequent work in the LM approach has looked at addressing\\nsome of these concerns, including putting relevance back into the model and\\nallowing a language mismatch between the query language and the docu-\\nment language.\\nThe model has signiﬁcant relations to traditional tf-idf models. Term fre-\\nquency is directly represented in tf-idf models, and much recent work has\\nrecognized the importance of document length normalization. The effect of\\ndoing a mixture of document generation probability with collection gener-\\nation probability is a little like idf: terms rare in the general collection but\\ncommon in some documents will have a greater inﬂuence on the ranking of\\ndocuments. In most concrete realizations, the models share treating terms as\\nif they were independent. On the other hand, the intuitions are probabilistic\\nrather than geometric, the mathematical models are more principled rather\\nthan heuristic, and the details of how statistics like term frequency and doc-\\nument length are used differ. If you are concerned mainly with performance\\nnumbers, recent work has shown the LM approach to be very effective in re-\\ntrieval experiments, beating tf-idf and BM25 weights. Nevertheless, there is\\n', 'Online edition (c)\\n2009 Cambridge UP\\n250\\n12\\nLanguage models for information retrieval\\nQuery\\nQuery model\\nP(t|Query)\\nDocument\\nDoc. model\\nP(t|Document)\\n(a)\\n(b)\\n(c)\\n◮Figure 12.5\\nThree ways of developing the language modeling approach: (a) query\\nlikelihood, (b) document likelihood, and (c) model comparison.\\nperhaps still insufﬁcient evidence that its performance so greatly exceeds that\\nof a well-tuned traditional vector space retrieval system as to justify chang-\\ning an existing implementation.\\n12.4\\nExtended language modeling approaches\\nIn this section we brieﬂy mention some of the work that extends the basic\\nlanguage modeling approach.\\nThere are other ways to think of using the language modeling idea in IR\\nsettings, and many of them have been tried in subsequent work. Rather than\\nlooking at the probability of a document language model Md generating the\\nquery, you can look at the probability of a query language model Mq gener-\\nating the document. The main reason that doing things in this direction and\\ncreating a document likelihood model is less appealing is that there is much less\\nDOCUMENT\\nLIKELIHOOD MODEL\\ntext available to estimate a language model based on the query text, and so\\nthe model will be worse estimated, and will have to depend more on being\\nsmoothed with some other language model. On the other hand, it is easy to\\nsee how to incorporate relevance feedback into such a model: you can ex-\\npand the query with terms taken from relevant documents in the usual way\\nand hence update the language model Mq (Zhai and Lafferty 2001a). Indeed,\\nwith appropriate modeling choices, this approach leads to the BIM model of\\nChapter 11. The relevance model of Lavrenko and Croft (2001) is an instance\\nof a document likelihood model, which incorporates pseudo-relevance feed-\\nback into a language modeling approach. It achieves very strong empirical\\nresults.\\nRather than directly generating in either direction, we can make a lan-\\nguage model from both the document and query, and then ask how different\\nthese two language models are from each other. Lafferty and Zhai (2001) lay\\n', 'Online edition (c)\\n2009 Cambridge UP\\n12.4\\nExtended language modeling approaches\\n251\\nout these three ways of thinking about the problem, which we show in Fig-\\nure 12.5, and develop a general risk minimization approach for document\\nretrieval. For instance, one way to model the risk of returning a document d\\nas relevant to a query q is to use the Kullback-Leibler (KL) divergence between\\nKULLBACK-LEIBLER\\nDIVERGENCE\\ntheir respective language models:\\nR(d; q) = KL(Md∥Mq) = ∑\\nt∈V\\nP(t|Mq) log P(t|Mq)\\nP(t|Md)\\n(12.14)\\nKL divergence is an asymmetric divergence measure originating in informa-\\ntion theory, which measures how bad the probability distribution Mq is at\\nmodeling Md (Cover and Thomas 1991, Manning and Schütze 1999). Laf-\\nferty and Zhai (2001) present results suggesting that a model comparison\\napproach outperforms both query-likelihood and document-likelihood ap-\\nproaches. One disadvantage of using KL divergence as a ranking function\\nis that scores are not comparable across queries. This does not matter for ad\\nhoc retrieval, but is important in other applications such as topic tracking.\\nKraaij and Spitters (2003) suggest an alternative proposal which models sim-\\nilarity as a normalized log-likelihood ratio (or, equivalently, as a difference\\nbetween cross-entropies).\\nBasic LMs do not address issues of alternate expression, that is, synonymy,\\nor any deviation in use of language between queries and documents. Berger\\nand Lafferty (1999) introduce translation models to bridge this query-document\\ngap. A translation model lets you generate query words not in a document by\\nTRANSLATION MODEL\\ntranslation to alternate terms with similar meaning. This also provides a ba-\\nsis for performing cross-language IR. We assume that the translation model\\ncan be represented by a conditional probability distribution T(·|·) between\\nvocabulary terms. The form of the translation query generation model is\\nthen:\\nP(q|Md) = ∏\\nt∈q ∑\\nv∈V\\nP(v|Md)T(t|v)\\n(12.15)\\nThe term P(v|Md) is the basic document language model, and the term T(t|v)\\nperforms translation. This model is clearly more computationally intensive\\nand we need to build a translation model. The translation model is usually\\nbuilt using separate resources (such as a traditional thesaurus or bilingual\\ndictionary or a statistical machine translation system’s translation diction-\\nary), but can be built using the document collection if there are pieces of\\ntext that naturally paraphrase or summarize other pieces of text. Candi-\\ndate examples are documents and their titles or abstracts, or documents and\\nanchor-text pointing to them in a hypertext environment.\\nBuilding extended LM approaches remains an active area of research. In\\ngeneral, translation models, relevance feedback models, and model compar-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n252\\n12\\nLanguage models for information retrieval\\nison approaches have all been demonstrated to improve performance over\\nthe basic query likelihood LM.\\n12.5\\nReferences and further reading\\nFor more details on the basic concepts of probabilistic language models and\\ntechniques for smoothing, see either Manning and Schütze (1999, Chapter 6)\\nor Jurafsky and Martin (2008, Chapter 4).\\nThe important initial papers that originated the language modeling ap-\\nproach to IR are: (Ponte and Croft 1998, Hiemstra 1998, Berger and Lafferty\\n1999, Miller et al. 1999). Other relevant papers can be found in the next sev-\\neral years of SIGIR proceedings. (Croft and Lafferty 2003) contains a col-\\nlection of papers from a workshop on language modeling approaches and\\nHiemstra and Kraaij (2005) review one prominent thread of work on using\\nlanguage modeling approaches for TREC tasks. Zhai and Lafferty (2001b)\\nclarify the role of smoothing in LMs for IR and present detailed empirical\\ncomparisons of different smoothing methods. Zaragoza et al. (2003) advo-\\ncate using full Bayesian predictive distributions rather than MAP point es-\\ntimates, but while they outperform Bayesian smoothing, they fail to outper-\\nform a linear interpolation. Zhai and Lafferty (2002) argue that a two-stage\\nsmoothing model with ﬁrst Bayesian smoothing followed by linear interpo-\\nlation gives a good model of the task, and performs better and more stably\\nthan a single form of smoothing. A nice feature of the LM approach is that it\\nprovides a convenient and principled way to put various kinds of prior infor-\\nmation into the model; Kraaij et al. (2002) demonstrate this by showing the\\nvalue of link information as a prior in improving web entry page retrieval\\nperformance. As brieﬂy discussed in Chapter 16 (page 353), Liu and Croft\\n(2004) show some gains by smoothing a document LM with estimates from\\na cluster of similar documents; Tao et al. (2006) report larger gains by doing\\ndocument-similarity based smoothing.\\nHiemstra and Kraaij (2005) present TREC results showing a LM approach\\nbeating use of BM25 weights.\\nRecent work has achieved some gains by\\ngoing beyond the unigram model, providing the higher order models are\\nsmoothed with lower order models (Gao et al. 2004, Cao et al. 2005), though\\nthe gains to date remain modest. Spärck Jones (2004) presents a critical view-\\npoint on the rationale for the language modeling approach, but Lafferty and\\nZhai (2003) argue that a uniﬁed account can be given of the probabilistic\\nsemantics underlying both the language modeling approach presented in\\nthis chapter and the classical probabilistic information retrieval approach of\\nChapter 11. The Lemur Toolkit (http://www.lemurproject.org/) provides a ﬂexi-\\nble open source framework for investigating language modeling approaches\\nto IR.\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n253\\n13\\nText classiﬁcation and Naive\\nBayes\\nThus far, this book has mainly discussed the process of ad hoc retrieval, where\\nusers have transient information needs that they try to address by posing\\none or more queries to a search engine. However, many users have ongoing\\ninformation needs. For example, you might need to track developments in\\nmulticore computer chips. One way of doing this is to issue the query multi-\\ncore AND computer AND chip against an index of recent newswire articles each\\nmorning. In this and the following two chapters we examine the question:\\nHow can this repetitive task be automated? To this end, many systems sup-\\nport standing queries. A standing query is like any other query except that it\\nSTANDING QUERY\\nis periodically executed on a collection to which new documents are incre-\\nmentally added over time.\\nIf your standing query is just multicore AND computer AND chip, you will tend\\nto miss many relevant new articles which use other terms such as multicore\\nprocessors. To achieve good recall, standing queries thus have to be reﬁned\\nover time and can gradually become quite complex. In this example, using a\\nBoolean search engine with stemming, you might end up with a query like\\n(multicore OR multi-core) AND (chip OR processor OR microprocessor).\\nTo capture the generality and scope of the problem space to which stand-\\ning queries belong, we now introduce the general notion of a classiﬁcation\\nCLASSIFICATION\\nproblem. Given a set of classes, we seek to determine which class(es) a given\\nobject belongs to. In the example, the standing query serves to divide new\\nnewswire articles into the two classes: documents about multicore computer chips\\nand documents not about multicore computer chips. We refer to this as two-class\\nclassiﬁcation. Classiﬁcation using standing queries is also called routing or\\nROUTING\\nﬁlteringand will be discussed further in Section 15.3.1 (page 335).\\nFILTERING\\nA class need not be as narrowly focused as the standing query multicore\\ncomputer chips. Often, a class is a more general subject area like China or coffee.\\nSuch more general classes are usually referred to as topics, and the classiﬁca-\\ntion task is then called text classiﬁcation, text categorization, topic classiﬁcation,\\nTEXT CLASSIFICATION\\nor topic spotting. An example for China appears in Figure 13.1. Standing\\nqueries and topics differ in their degree of speciﬁcity, but the methods for\\n', 'Online edition (c)\\n2009 Cambridge UP\\n254\\n13\\nText classiﬁcation and Naive Bayes\\nsolving routing, ﬁltering, and text classiﬁcation are essentially the same. We\\ntherefore include routing and ﬁltering under the rubric of text classiﬁcation\\nin this and the following chapters.\\nThe notion of classiﬁcation is very general and has many applications within\\nand beyond information retrieval (IR). For instance, in computer vision, a\\nclassiﬁer may be used to divide images into classes such as landscape, por-\\ntrait, and neither. We focus here on examples from information retrieval such\\nas:\\n• Several of the preprocessing steps necessary for indexing as discussed in\\nChapter 2: detecting a document’s encoding (ASCII, Unicode UTF-8 etc;\\npage 20); word segmentation (Is the white space between two letters a\\nword boundary or not? page 24 ) ; truecasing (page 30); and identifying\\nthe language of a document (page 46).\\n• The automatic detection of spam pages (which then are not included in\\nthe search engine index).\\n• The automatic detection of sexually explicit content (which is included in\\nsearch results only if the user turns an option such as SafeSearch off).\\n• Sentiment detection or the automatic classiﬁcation of a movie or product\\nSENTIMENT DETECTION\\nreview as positive or negative. An example application is a user search-\\ning for negative reviews before buying a camera to make sure it has no\\nundesirable features or quality problems.\\n• Personal email sorting. A user may have folders like talk announcements,\\nEMAIL SORTING\\nelectronic bills, email from family and friends, and so on, and may want a\\nclassiﬁer to classify each incoming email and automatically move it to the\\nappropriate folder. It is easier to ﬁnd messages in sorted folders than in\\na very large inbox. The most common case of this application is a spam\\nfolder that holds all suspected spam messages.\\n• Topic-speciﬁc or vertical search. Vertical search engines restrict searches to\\nVERTICAL SEARCH\\nENGINE\\na particular topic. For example, the query computer science on a vertical\\nsearch engine for the topic China will return a list of Chinese computer\\nscience departments with higher precision and recall than the query com-\\nputer science China on a general purpose search engine. This is because the\\nvertical search engine does not include web pages in its index that contain\\nthe term china in a different sense (e.g., referring to a hard white ceramic),\\nbut does include relevant pages even if they do not explicitly mention the\\nterm China.\\n• Finally, the ranking function in ad hoc information retrieval can also be\\nbased on a document classiﬁer as we will explain in Section 15.4 (page 341).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n255\\nThis list shows the general importance of classiﬁcation in IR. Most retrieval\\nsystems today contain multiple components that use some form of classiﬁer.\\nThe classiﬁcation task we will use as an example in this book is text classiﬁ-\\ncation.\\nA computer is not essential for classiﬁcation. Many classiﬁcation tasks\\nhave traditionally been solved manually. Books in a library are assigned\\nLibrary of Congress categories by a librarian. But manual classiﬁcation is\\nexpensive to scale. The multicore computer chips example illustrates one al-\\nternative approach: classiﬁcation by the use of standing queries – which can\\nbe thought of as rules – most commonly written by hand. As in our exam-\\nRULES IN TEXT\\nCLASSIFICATION\\nple (multicore OR multi-core) AND (chip OR processor OR microprocessor), rules are\\nsometimes equivalent to Boolean expressions.\\nA rule captures a certain combination of keywords that indicates a class.\\nHand-coded rules have good scaling properties, but creating and maintain-\\ning them over time is labor intensive. A technically skilled person (e.g., a\\ndomain expert who is good at writing regular expressions) can create rule\\nsets that will rival or exceed the accuracy of the automatically generated clas-\\nsiﬁers we will discuss shortly; however, it can be hard to ﬁnd someone with\\nthis specialized skill.\\nApart from manual classiﬁcation and hand-crafted rules, there is a third\\napproach to text classiﬁcation, namely, machine learning-based text classiﬁ-\\ncation. It is the approach that we focus on in the next several chapters. In\\nmachine learning, the set of rules or, more generally, the decision criterion of\\nthe text classiﬁer, is learned automatically from training data. This approach\\nis also called statistical text classiﬁcation if the learning method is statistical.\\nSTATISTICAL TEXT\\nCLASSIFICATION\\nIn statistical text classiﬁcation, we require a number of good example docu-\\nments (or training documents) for each class. The need for manual classiﬁ-\\ncation is not eliminated because the training documents come from a person\\nwho has labeled them – where labeling refers to the process of annotating\\nLABELING\\neach document with its class. But labeling is arguably an easier task than\\nwriting rules. Almost anybody can look at a document and decide whether\\nor not it is related to China. Sometimes such labeling is already implicitly\\npart of an existing workﬂow. For instance, you may go through the news\\narticles returned by a standing query each morning and give relevance feed-\\nback (cf. Chapter 9) by moving the relevant articles to a special folder like\\nmulticore-processors.\\nWe begin this chapter with a general introduction to the text classiﬁcation\\nproblem including a formal deﬁnition (Section 13.1); we then cover Naive\\nBayes, a particularly simple and effective classiﬁcation method (Sections 13.2–\\n13.4). All of the classiﬁcation algorithms we study represent documents in\\nhigh-dimensional spaces. To improve the efﬁciency of these algorithms, it\\nis generally desirable to reduce the dimensionality of these spaces; to this\\nend, a technique known as feature selection is commonly applied in text clas-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n256\\n13\\nText classiﬁcation and Naive Bayes\\nsiﬁcation as discussed in Section 13.5. Section 13.6 covers evaluation of text\\nclassiﬁcation. In the following chapters, Chapters 14 and 15, we look at two\\nother families of classiﬁcation methods, vector space classiﬁers and support\\nvector machines.\\n13.1\\nThe text classiﬁcation problem\\nIn text classiﬁcation, we are given a description d ∈X of a document, where\\nX is the document space; and a ﬁxed set of classes C = {c1, c2, . . . , cJ}. Classes\\nDOCUMENT SPACE\\nCLASS\\nare also called categories or labels. Typically, the document space X is some\\ntype of high-dimensional space, and the classes are human deﬁned for the\\nneeds of an application, as in the examples China and documents that talk\\nabout multicore computer chips above. We are given a training set D of labeled\\nTRAINING SET\\ndocuments ⟨d, c⟩,where ⟨d, c⟩∈X × C. For example:\\n⟨d, c⟩= ⟨Beijing joins the World Trade Organization, China⟩\\nfor the one-sentence document Beijing joins the World Trade Organization and\\nthe class (or label) China.\\nUsing a learning method or learning algorithm, we then wish to learn a clas-\\nLEARNING METHOD\\nsiﬁer or classiﬁcation function γ that maps documents to classes:\\nCLASSIFIER\\nγ : X →C\\n(13.1)\\nThis type of learning is called supervised learning because a supervisor (the\\nSUPERVISED LEARNING\\nhuman who deﬁnes the classes and labels training documents) serves as a\\nteacher directing the learning process. We denote the supervised learning\\nmethod by Γ and write Γ(D) = γ. The learning method Γ takes the training\\nset D as input and returns the learned classiﬁcation function γ.\\nMost names for learning methods Γ are also used for classiﬁers γ. We\\ntalk about the Naive Bayes (NB) learning method Γ when we say that “Naive\\nBayes is robust,” meaning that it can be applied to many different learning\\nproblems and is unlikely to produce classiﬁers that fail catastrophically. But\\nwhen we say that “Naive Bayes had an error rate of 20%,” we are describing\\nan experiment in which a particular NB classiﬁer γ (which was produced by\\nthe NB learning method) had a 20% error rate in an application.\\nFigure 13.1 shows an example of text classiﬁcation from the Reuters-RCV1\\ncollection, introduced in Section 4.2, page 69. There are six classes (UK, China,\\n..., sports), each with three training documents. We show a few mnemonic\\nwords for each document’s content. The training set provides some typical\\nexamples for each class, so that we can learn the classiﬁcation function γ.\\nOnce we have learned γ, we can apply it to the test set (or test data), for ex-\\nTEST SET\\nample, the new document ﬁrst private Chinese airline whose class is unknown.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.1\\nThe text classiﬁcation problem\\n257\\nclasses:\\ntraining\\nset:\\ntest\\nset:\\nregions\\nindustries\\nsubject areas\\nγ(d′) =China\\nﬁrst\\nprivate\\nChinese\\nairline\\nUK\\nChina\\npoultry\\ncoffee\\nelections\\nsports\\nLondon\\ncongestion\\nBig Ben\\nParliament\\nthe Queen\\nWindsor\\nBeijing\\nOlympics\\nGreat Wall\\ntourism\\ncommunist\\nMao\\nchicken\\nfeed\\nducks\\npate\\nturkey\\nbird ﬂu\\nbeans\\nroasting\\nrobusta\\narabica\\nharvest\\nKenya\\nvotes\\nrecount\\nrun-off\\nseat\\ncampaign\\nTV ads\\nbaseball\\ndiamond\\nsoccer\\nforward\\ncaptain\\nteam\\nd′\\n◮Figure 13.1\\nClasses, training set, and test set in text classiﬁcation .\\nIn Figure 13.1, the classiﬁcation function assigns the new document to class\\nγ(d) = China, which is the correct assignment.\\nThe classes in text classiﬁcation often have some interesting structure such\\nas the hierarchy in Figure 13.1. There are two instances each of region cate-\\ngories, industry categories, and subject area categories. A hierarchy can be\\nan important aid in solving a classiﬁcation problem; see Section 15.3.2 for\\nfurther discussion. Until then, we will make the assumption in the text clas-\\nsiﬁcation chapters that the classes form a set with no subset relationships\\nbetween them.\\nDeﬁnition (13.1) stipulates that a document is a member of exactly one\\nclass. This is not the most appropriate model for the hierarchy in Figure 13.1.\\nFor instance, a document about the 2008 Olympics should be a member of\\ntwo classes: the China class and the sports class. This type of classiﬁcation\\nproblem is referred to as an any-of problem and we will return to it in Sec-\\ntion 14.5 (page 306). For the time being, we only consider one-of problems\\nwhere a document is a member of exactly one class.\\nOur goal in text classiﬁcation is high accuracy on test data or new data – for\\nexample, the newswire articles that we will encounter tomorrow morning\\nin the multicore chip example. It is easy to achieve high accuracy on the\\ntraining set (e.g., we can simply memorize the labels). But high accuracy on\\nthe training set in general does not mean that the classiﬁer will work well on\\n', 'Online edition (c)\\n2009 Cambridge UP\\n258\\n13\\nText classiﬁcation and Naive Bayes\\nnew data in an application. When we use the training set to learn a classiﬁer\\nfor test data, we make the assumption that training data and test data are\\nsimilar or from the same distribution. We defer a precise deﬁnition of this\\nnotion to Section 14.6 (page 308).\\n13.2\\nNaive Bayes text classiﬁcation\\nThe ﬁrst supervised learning method we introduce is the multinomial Naive\\nMULTINOMIAL NAIVE\\nBAYES\\nBayes or multinomial NB model, a probabilistic learning method. The proba-\\nbility of a document d being in class c is computed as\\nP(c|d) ∝P(c) ∏\\n1≤k≤nd\\nP(tk|c)\\n(13.2)\\nwhere P(tk|c) is the conditional probability of term tk occurring in a docu-\\nment of class c.1 We interpret P(tk|c) as a measure of how much evidence\\ntk contributes that c is the correct class. P(c) is the prior probability of a\\ndocument occurring in class c. If a document’s terms do not provide clear\\nevidence for one class versus another, we choose the one that has a higher\\nprior probability. ⟨t1, t2, . . . , tnd⟩are the tokens in d that are part of the vocab-\\nulary we use for classiﬁcation and nd is the number of such tokens in d. For\\nexample, ⟨t1, t2, . . . , tnd⟩for the one-sentence document Beijing and Taipei join\\nthe WTO might be ⟨Beijing, Taipei, join, WTO⟩, with nd = 4, if we treat the terms\\nand and the as stop words.\\nIn text classiﬁcation, our goal is to ﬁnd the best class for the document. The\\nbest class in NB classiﬁcation is the most likely or maximum a posteriori (MAP)\\nMAXIMUM A\\nPOSTERIORI CLASS\\nclass cmap:\\ncmap = arg max\\nc∈C\\nˆP(c|d) = arg max\\nc∈C\\nˆP(c) ∏\\n1≤k≤nd\\nˆP(tk|c).\\n(13.3)\\nWe write ˆP for P because we do not know the true values of the parameters\\nP(c) and P(tk|c), but estimate them from the training set as we will see in a\\nmoment.\\nIn Equation (13.3), many conditional probabilities are multiplied, one for\\neach position 1 ≤k ≤nd. This can result in a ﬂoating point underﬂow.\\nIt is therefore better to perform the computation by adding logarithms of\\nprobabilities instead of multiplying probabilities. The class with the highest\\nlog probability score is still the most probable; log(xy) = log(x) + log(y)\\nand the logarithm function is monotonic. Hence, the maximization that is\\n1. We will explain in the next section why P(c|d) is proportional to (∝), not equal to the quantity\\non the right.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.2\\nNaive Bayes text classiﬁcation\\n259\\nactually done in most implementations of NB is:\\ncmap = arg max\\nc∈C\\n[log ˆP(c) + ∑\\n1≤k≤nd\\nlog ˆP(tk|c)].\\n(13.4)\\nEquation (13.4) has a simple interpretation. Each conditional parameter\\nlog ˆP(tk|c) is a weight that indicates how good an indicator tk is for c. Sim-\\nilarly, the prior log ˆP(c) is a weight that indicates the relative frequency of\\nc. More frequent classes are more likely to be the correct class than infre-\\nquent classes. The sum of log prior and term weights is then a measure of\\nhow much evidence there is for the document being in the class, and Equa-\\ntion (13.4) selects the class for which we have the most evidence.\\nWe will initially work with this intuitive interpretation of the multinomial\\nNB model and defer a formal derivation to Section 13.4.\\nHow do we estimate the parameters ˆP(c) and ˆP(tk|c)? We ﬁrst try the\\nmaximum likelihood estimate (MLE; Section 11.3.2, page 226), which is sim-\\nply the relative frequency and corresponds to the most likely value of each\\nparameter given the training data. For the priors this estimate is:\\nˆP(c) = Nc\\nN ,\\n(13.5)\\nwhere Nc is the number of documents in class c and N is the total number of\\ndocuments.\\nWe estimate the conditional probability ˆP(t|c) as the relative frequency of\\nterm t in documents belonging to class c:\\nˆP(t|c) =\\nTct\\n∑t′∈V Tct′ ,\\n(13.6)\\nwhere Tct is the number of occurrences of t in training documents from class\\nc, including multiple occurrences of a term in a document. We have made the\\npositional independence assumption here, which we will discuss in more detail\\nin the next section: Tct is a count of occurrences in all positions k in the doc-\\numents in the training set. Thus, we do not compute different estimates for\\ndifferent positions and, for example, if a word occurs twice in a document,\\nin positions k1 and k2, then ˆP(tk1|c) = ˆP(tk2|c).\\nThe problem with the MLE estimate is that it is zero for a term–class combi-\\nnation that did not occur in the training data. If the term WTO in the training\\ndata only occurred in China documents, then the MLE estimates for the other\\nclasses, for example UK, will be zero:\\nˆP(WTO|UK) = 0.\\nNow, the one-sentence document Britain is a member of the WTO will get a\\nconditional probability of zero for UK because we are multiplying the condi-\\ntional probabilities for all terms in Equation (13.2). Clearly, the model should\\n', 'Online edition (c)\\n2009 Cambridge UP\\n260\\n13\\nText classiﬁcation and Naive Bayes\\nTRAINMULTINOMIALNB(C, D)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nN ←COUNTDOCS(D)\\n3\\nfor each c ∈C\\n4\\ndo Nc ←COUNTDOCSINCLASS(D, c)\\n5\\nprior[c] ←Nc/N\\n6\\ntextc ←CONCATENATETEXTOFALLDOCSINCLASS(D, c)\\n7\\nfor each t ∈V\\n8\\ndo Tct ←COUNTTOKENSOFTERM(textc, t)\\n9\\nfor each t ∈V\\n10\\ndo condprob[t][c] ←\\nTct+1\\n∑t′(Tct′+1)\\n11\\nreturn V, prior, condprob\\nAPPLYMULTINOMIALNB(C, V, prior, condprob, d)\\n1\\nW ←EXTRACTTOKENSFROMDOC(V, d)\\n2\\nfor each c ∈C\\n3\\ndo score[c] ←log prior[c]\\n4\\nfor each t ∈W\\n5\\ndo score[c] += log condprob[t][c]\\n6\\nreturn arg maxc∈C score[c]\\n◮Figure 13.2\\nNaive Bayes algorithm (multinomial model): Training and testing.\\nassign a high probability to the UK class because the term Britain occurs. The\\nproblem is that the zero probability for WTO cannot be “conditioned away,”\\nno matter how strong the evidence for the class UK from other features. The\\nestimate is 0 because of sparseness: The training data are never large enough\\nSPARSENESS\\nto represent the frequency of rare events adequately, for example, the fre-\\nquency of WTO occurring in UK documents.\\nTo eliminate zeros, we use add-one or Laplace smoothing, which simply adds\\nADD-ONE SMOOTHING\\none to each count (cf. Section 11.3.2):\\nˆP(t|c) =\\nTct + 1\\n∑t′∈V(Tct′ + 1) =\\nTct + 1\\n(∑t′∈V Tct′) + B,\\n(13.7)\\nwhere B = |V| is the number of terms in the vocabulary. Add-one smoothing\\ncan be interpreted as a uniform prior (each term occurs once for each class)\\nthat is then updated as evidence from the training data comes in. Note that\\nthis is a prior probability for the occurrence of a term as opposed to the prior\\nprobability of a class which we estimate in Equation (13.5) on the document\\nlevel.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.2\\nNaive Bayes text classiﬁcation\\n261\\n◮Table 13.1\\nData for parameter estimation examples.\\ndocID\\nwords in document\\nin c = China?\\ntraining set\\n1\\nChinese Beijing Chinese\\nyes\\n2\\nChinese Chinese Shanghai\\nyes\\n3\\nChinese Macao\\nyes\\n4\\nTokyo Japan Chinese\\nno\\ntest set\\n5\\nChinese Chinese Chinese Tokyo Japan\\n?\\n◮Table 13.2\\nTraining and test times for NB.\\nmode\\ntime complexity\\ntraining\\nΘ(|D|Lave + |C||V|)\\ntesting\\nΘ(La + |C|Ma) = Θ(|C|Ma)\\nWe have now introduced all the elements we need for training and apply-\\ning an NB classiﬁer. The complete algorithm is described in Figure 13.2.\\n\\x0f\\nExample 13.1:\\nFor the example in Table 13.1, the multinomial parameters we\\nneed to classify the test document are the priors ˆP(c) = 3/4 and ˆP(c) = 1/4 and the\\nfollowing conditional probabilities:\\nˆP(Chinese|c)\\n=\\n(5 + 1)/(8 + 6) = 6/14 = 3/7\\nˆP(Tokyo|c) = ˆP(Japan|c)\\n=\\n(0 + 1)/(8 + 6) = 1/14\\nˆP(Chinese|c)\\n=\\n(1 + 1)/(3 + 6) = 2/9\\nˆP(Tokyo|c) = ˆP(Japan|c)\\n=\\n(1 + 1)/(3 + 6) = 2/9\\nThe denominators are (8 + 6) and (3 + 6) because the lengths of textc and textc are 8\\nand 3, respectively, and because the constant B in Equation (13.7) is 6 as the vocabu-\\nlary consists of six terms.\\nWe then get:\\nˆP(c|d5)\\n∝\\n3/4 · (3/7)3 · 1/14 · 1/14 ≈0.0003.\\nˆP(c|d5)\\n∝\\n1/4 · (2/9)3 · 2/9 · 2/9 ≈0.0001.\\nThus, the classiﬁer assigns the test document to c = China. The reason for this clas-\\nsiﬁcation decision is that the three occurrences of the positive indicator Chinese in d5\\noutweigh the occurrences of the two negative indicators Japan and Tokyo.\\nWhat is the time complexity of NB? The complexity of computing the pa-\\nrameters is Θ(|C||V|) because the set of parameters consists of |C||V| con-\\nditional probabilities and |C| priors. The preprocessing necessary for com-\\nputing the parameters (extracting the vocabulary, counting terms, etc.) can\\nbe done in one pass through the training data. The time complexity of this\\n', 'Online edition (c)\\n2009 Cambridge UP\\n262\\n13\\nText classiﬁcation and Naive Bayes\\ncomponent is therefore Θ(|D|Lave), where |D| is the number of documents\\nand Lave is the average length of a document.\\nWe use Θ(|D|Lave) as a notation for Θ(T) here, where T is the length of the\\ntraining collection. This is nonstandard; Θ(.) is not deﬁned for an average.\\nWe prefer expressing the time complexity in terms of D and Lave because\\nthese are the primary statistics used to characterize training collections.\\nThe time complexity of APPLYMULTINOMIALNB in Figure 13.2 is Θ(|C|La).\\nLa and Ma are the numbers of tokens and types, respectively, in the test doc-\\nument. APPLYMULTINOMIALNB can be modiﬁed to be Θ(La + |C|Ma) (Ex-\\nercise 13.8). Finally, assuming that the length of test documents is bounded,\\nΘ(La + |C|Ma) = Θ(|C|Ma) because La < b|C|Ma for a ﬁxed constant b.2\\nTable 13.2 summarizes the time complexities. In general, we have |C||V| <\\n|D|Lave, so both training and testing complexity are linear in the time it takes\\nto scan the data. Because we have to look at the data at least once, NB can be\\nsaid to have optimal time complexity. Its efﬁciency is one reason why NB is\\na popular text classiﬁcation method.\\n13.2.1\\nRelation to multinomial unigram language model\\nThe multinomial NB model is formally identical to the multinomial unigram\\nlanguage model (Section 12.2.1, page 242). In particular, Equation (13.2) is\\na special case of Equation (12.12) from page 243, which we repeat here for\\nλ = 1:\\nP(d|q) ∝P(d)∏\\nt∈q\\nP(t|Md).\\n(13.8)\\nThe document d in text classiﬁcation (Equation (13.2)) takes the role of the\\nquery in language modeling (Equation (13.8)) and the classes c in text clas-\\nsiﬁcation take the role of the documents d in language modeling. We used\\nEquation (13.8) to rank documents according to the probability that they are\\nrelevant to the query q. In NB classiﬁcation, we are usually only interested\\nin the top-ranked class.\\nWe also used MLE estimates in Section 12.2.2 (page 243) and encountered\\nthe problem of zero estimates owing to sparse data (page 244); but instead\\nof add-one smoothing, we used a mixture of two distributions to address the\\nproblem there. Add-one smoothing is closely related to add- 1\\n2 smoothing in\\nSection 11.3.4 (page 228).\\n?\\nExercise 13.1\\nWhy is |C||V| < |D|Lave in Table 13.2 expected to hold for most text collections?\\n2. Our assumption here is that the length of test documents is bounded. La would exceed\\nb|C|Ma for extremely long test documents.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.3\\nThe Bernoulli model\\n263\\nTRAINBERNOULLINB(C, D)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nN ←COUNTDOCS(D)\\n3\\nfor each c ∈C\\n4\\ndo Nc ←COUNTDOCSINCLASS(D, c)\\n5\\nprior[c] ←Nc/N\\n6\\nfor each t ∈V\\n7\\ndo Nct ←COUNTDOCSINCLASSCONTAININGTERM(D, c, t)\\n8\\ncondprob[t][c] ←(Nct + 1)/(Nc + 2)\\n9\\nreturn V, prior, condprob\\nAPPLYBERNOULLINB(C, V, prior, condprob, d)\\n1\\nVd ←EXTRACTTERMSFROMDOC(V, d)\\n2\\nfor each c ∈C\\n3\\ndo score[c] ←log prior[c]\\n4\\nfor each t ∈V\\n5\\ndo if t ∈Vd\\n6\\nthen score[c] += log condprob[t][c]\\n7\\nelse score[c] += log(1 −condprob[t][c])\\n8\\nreturn arg maxc∈C score[c]\\n◮Figure 13.3\\nNB algorithm (Bernoulli model): Training and testing. The add-one\\nsmoothing in Line 8 (top) is in analogy to Equation (13.7) with B = 2.\\n13.3\\nThe Bernoulli model\\nThere are two different ways we can set up an NB classiﬁer. The model we in-\\ntroduced in the previous section is the multinomial model. It generates one\\nterm from the vocabulary in each position of the document, where we as-\\nsume a generative model that will be discussed in more detail in Section 13.4\\n(see also page 237).\\nAn alternative to the multinomial model is the multivariate Bernoulli model\\nor Bernoulli model. It is equivalent to the binary independence model of Sec-\\nBERNOULLI MODEL\\ntion 11.3 (page 222), which generates an indicator for each term of the vo-\\ncabulary, either 1 indicating presence of the term in the document or 0 indi-\\ncating absence. Figure 13.3 presents training and testing algorithms for the\\nBernoulli model. The Bernoulli model has the same time complexity as the\\nmultinomial model.\\nThe different generation models imply different estimation strategies and\\ndifferent classiﬁcation rules. The Bernoulli model estimates ˆP(t|c) as the frac-\\ntion of documents of class c that contain term t (Figure 13.3, TRAINBERNOULLI-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n264\\n13\\nText classiﬁcation and Naive Bayes\\nNB, line 8). In contrast, the multinomial model estimates ˆP(t|c) as the frac-\\ntion of tokens or fraction of positions in documents of class c that contain term\\nt (Equation (13.7)). When classifying a test document, the Bernoulli model\\nuses binary occurrence information, ignoring the number of occurrences,\\nwhereas the multinomial model keeps track of multiple occurrences. As a\\nresult, the Bernoulli model typically makes many mistakes when classifying\\nlong documents. For example, it may assign an entire book to the class China\\nbecause of a single occurrence of the term China.\\nThe models also differ in how nonoccurring terms are used in classiﬁca-\\ntion. They do not affect the classiﬁcation decision in the multinomial model;\\nbut in the Bernoulli model the probability of nonoccurrence is factored in\\nwhen computing P(c|d) (Figure 13.3, APPLYBERNOULLINB, Line 7). This is\\nbecause only the Bernoulli NB model models absence of terms explicitly.\\n\\x0f\\nExample 13.2:\\nApplying the Bernoulli model to the example in Table 13.1, we\\nhave the same estimates for the priors as before:\\nˆP(c) = 3/4, ˆP(c) = 1/4. The\\nconditional probabilities are:\\nˆP(Chinese|c)\\n=\\n(3 + 1)/(3 + 2) = 4/5\\nˆP(Japan|c) = ˆP(Tokyo|c)\\n=\\n(0 + 1)/(3 + 2) = 1/5\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c)\\n=\\n(1 + 1)/(3 + 2) = 2/5\\nˆP(Chinese|c)\\n=\\n(1 + 1)/(1 + 2) = 2/3\\nˆP(Japan|c) = ˆP(Tokyo|c)\\n=\\n(1 + 1)/(1 + 2) = 2/3\\nˆP(Beijing|c) = ˆP(Macao|c) = ˆP(Shanghai|c)\\n=\\n(0 + 1)/(1 + 2) = 1/3\\nThe denominators are (3 + 2) and (1 + 2) because there are three documents in c\\nand one document in c and because the constant B in Equation (13.7) is 2 – there are\\ntwo cases to consider for each term, occurrence and nonoccurrence.\\nThe scores of the test document for the two classes are\\nˆP(c|d5)\\n∝\\nˆP(c) · ˆP(Chinese|c) · ˆP(Japan|c) · ˆP(Tokyo|c)\\n· (1 −ˆP(Beijing|c)) · (1 −ˆP(Shanghai|c)) · (1 −ˆP(Macao|c))\\n=\\n3/4 · 4/5 · 1/5 · 1/5 · (1−2/5) · (1−2/5) · (1−2/5)\\n≈\\n0.005\\nand, analogously,\\nˆP(c|d5)\\n∝\\n1/4 · 2/3 · 2/3 · 2/3 · (1−1/3) · (1−1/3) · (1−1/3)\\n≈\\n0.022\\nThus, the classiﬁer assigns the test document to c = not-China. When looking only\\nat binary occurrence and not at term frequency, Japan and Tokyo are indicators for c\\n(2/3 > 1/5) and the conditional probabilities of Chinese for c and c are not different\\nenough (4/5 vs. 2/3) to affect the classiﬁcation decision.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n265\\n13.4\\nProperties of Naive Bayes\\nTo gain a better understanding of the two models and the assumptions they\\nmake, let us go back and examine how we derived their classiﬁcation rules in\\nChapters 11 and 12. We decide class membership of a document by assigning\\nit to the class with the maximum a posteriori probability (cf. Section 11.3.2,\\npage 226), which we compute as follows:\\ncmap\\n=\\narg max\\nc∈C\\nP(c|d)\\n=\\narg max\\nc∈C\\nP(d|c)P(c)\\nP(d)\\n(13.9)\\n=\\narg max\\nc∈C\\nP(d|c)P(c),\\n(13.10)\\nwhere Bayes’ rule (Equation (11.4), page 220) is applied in (13.9) and we drop\\nthe denominator in the last step because P(d) is the same for all classes and\\ndoes not affect the argmax.\\nWe can interpret Equation (13.10) as a description of the generative process\\nwe assume in Bayesian text classiﬁcation. To generate a document, we ﬁrst\\nchoose class c with probability P(c) (top nodes in Figures 13.4 and 13.5). The\\ntwo models differ in the formalization of the second step, the generation of\\nthe document given the class, corresponding to the conditional distribution\\nP(d|c):\\nMultinomial\\nP(d|c)\\n=\\nP(⟨t1, . . . , tk, . . . , tnd⟩|c)\\n(13.11)\\nBernoulli\\nP(d|c)\\n=\\nP(⟨e1, . . . , ei, . . . , eM⟩|c),\\n(13.12)\\nwhere ⟨t1, . . . , tnd⟩is the sequence of terms as it occurs in d (minus terms\\nthat were excluded from the vocabulary) and ⟨e1, . . . , ei, . . . , eM⟩is a binary\\nvector of dimensionality M that indicates for each term whether it occurs in\\nd or not.\\nIt should now be clearer why we introduced the document space X in\\nEquation (13.1) when we deﬁned the classiﬁcation problem. A critical step\\nin solving a text classiﬁcation problem is to choose the document represen-\\ntation. ⟨t1, . . . , tnd⟩and ⟨e1, . . . , eM⟩are two different document representa-\\ntions. In the ﬁrst case, X is the set of all term sequences (or, more precisely,\\nsequences of term tokens). In the second case, X is {0, 1}M.\\nWe cannot use Equations (13.11) and (13.12) for text classiﬁcation directly.\\nFor the Bernoulli model, we would have to estimate 2M|C| different param-\\neters, one for each possible combination of M values ei and a class. The\\nnumber of parameters in the multinomial case has the same order of magni-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n266\\n13\\nText classiﬁcation and Naive Bayes\\nC=China\\nX1=Beijing\\nX2=and\\nX3=Taipei\\nX4=join\\nX5=WTO\\n◮Figure 13.4\\nThe multinomial NB model.\\ntude.3 This being a very large quantity, estimating these parameters reliably\\nis infeasible.\\nTo reduce the number of parameters, we make the Naive Bayes conditional\\nCONDITIONAL\\nINDEPENDENCE\\nASSUMPTION\\nindependence assumption. We assume that attribute values are independent of\\neach other given the class:\\nMultinomial\\nP(d|c)\\n=\\nP(⟨t1, . . . , tnd⟩|c) = ∏\\n1≤k≤nd\\nP(Xk = tk|c)\\n(13.13)\\nBernoulli\\nP(d|c)\\n=\\nP(⟨e1, . . . , eM⟩|c) = ∏\\n1≤i≤M\\nP(Ui = ei|c).\\n(13.14)\\nWe have introduced two random variables here to make the two different\\ngenerative models explicit. Xk is the random variable for position k in the\\nRANDOM VARIABLE X\\ndocument and takes as values terms from the vocabulary. P(Xk = t|c) is the\\nprobability that in a document of class c the term t will occur in position k. Ui\\nRANDOM VARIABLE U\\nis the random variable for vocabulary term i and takes as values 0 (absence)\\nand 1 (presence). ˆP(Ui = 1|c) is the probability that in a document of class c\\nthe term ti will occur – in any position and possibly multiple times.\\nWe illustrate the conditional independence assumption in Figures 13.4 and 13.5.\\nThe class China generates values for each of the ﬁve term attributes (multi-\\nnomial) or six binary attributes (Bernoulli) with a certain probability, inde-\\npendent of the values of the other attributes. The fact that a document in the\\nclass China contains the term Taipei does not make it more likely or less likely\\nthat it also contains Beijing.\\nIn reality, the conditional independence assumption does not hold for text\\ndata. Terms are conditionally dependent on each other. But as we will dis-\\ncuss shortly, NB models perform well despite the conditional independence\\nassumption.\\n3. In fact, if the length of documents is not bounded, the number of parameters in the multino-\\nmial case is inﬁnite.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n267\\nUAlaska=0\\nUBeijing=1\\nUIndia=0\\nUjoin=1\\nUTaipei=1\\nUWTO=1\\nC=China\\n◮Figure 13.5\\nThe Bernoulli NB model.\\nEven when assuming conditional independence, we still have too many\\nparameters for the multinomial model if we assume a different probability\\ndistribution for each position k in the document. The position of a term in a\\ndocument by itself does not carry information about the class. Although\\nthere is a difference between China sues France and France sues China, the\\noccurrence of China in position 1 versus position 3 of the document is not\\nuseful in NB classiﬁcation because we look at each term separately. The con-\\nditional independence assumption commits us to this way of processing the\\nevidence.\\nAlso, if we assumed different term distributions for each position k, we\\nwould have to estimate a different set of parameters for each k. The probabil-\\nity of bean appearing as the ﬁrst term of a coffee document could be different\\nfrom it appearing as the second term, and so on. This again causes problems\\nin estimation owing to data sparseness.\\nFor these reasons, we make a second independence assumption for the\\nmultinomial model, positional independence: The conditional probabilities for\\nPOSITIONAL\\nINDEPENDENCE\\na term are the same independent of position in the document.\\nP(Xk1 = t|c) = P(Xk2 = t|c)\\nfor all positions k1, k2, terms t and classes c. Thus, we have a single dis-\\ntribution of terms that is valid for all positions ki and we can use X as its\\nsymbol.4 Positional independence is equivalent to adopting the bag of words\\nmodel, which we introduced in the context of ad hoc retrieval in Chapter 6\\n(page 117).\\nWith conditional and positional independence assumptions, we only need\\nto estimate Θ(M|C|) parameters P(tk|c) (multinomial model) or P(ei|c) (Bernoulli\\n4. Our terminology is nonstandard. The random variable X is a categorical variable, not a multi-\\nnomial variable, and the corresponding NB model should perhaps be called a sequence model. We\\nhave chosen to present this sequence model and the multinomial model in Section 13.4.1 as the\\nsame model because they are computationally identical.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n268\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.3\\nMultinomial versus Bernoulli model.\\nmultinomial model\\nBernoulli model\\nevent model\\ngeneration of token\\ngeneration of document\\nrandom variable(s)\\nX = t iff t occurs at given pos\\nUt = 1 iff t occurs in doc\\ndocument representation\\nd = ⟨t1, . . . , tk, . . . , tnd⟩, tk ∈V\\nd = ⟨e1, . . . , ei, . . . , eM⟩,\\nei ∈{0, 1}\\nparameter estimation\\nˆP(X = t|c)\\nˆP(Ui = e|c)\\ndecision rule: maximize\\nˆP(c) ∏1≤k≤nd ˆP(X = tk|c)\\nˆP(c) ∏ti∈V ˆP(Ui = ei|c)\\nmultiple occurrences\\ntaken into account\\nignored\\nlength of docs\\ncan handle longer docs\\nworks best for short docs\\n# features\\ncan handle more\\nworks best with fewer\\nestimate for term the\\nˆP(X = the|c) ≈0.05\\nˆP(Uthe = 1|c) ≈1.0\\nmodel), one for each term–class combination, rather than a number that is\\nat least exponential in M, the size of the vocabulary.\\nThe independence\\nassumptions reduce the number of parameters to be estimated by several\\norders of magnitude.\\nTo summarize, we generate a document in the multinomial model (Fig-\\nure 13.4) by ﬁrst picking a class C = c with P(c) where C is a random variable\\nRANDOM VARIABLE C\\ntaking values from C as values. Next we generate term tk in position k with\\nP(Xk = tk|c) for each of the nd positions of the document. The Xk all have\\nthe same distribution over terms for a given c. In the example in Figure 13.4,\\nwe show the generation of ⟨t1, t2, t3, t4, t5⟩= ⟨Beijing, and, Taipei, join, WTO⟩,\\ncorresponding to the one-sentence document Beijing and Taipei join WTO.\\nFor a completely speciﬁed document generation model, we would also\\nhave to deﬁne a distribution P(nd|c) over lengths. Without it, the multino-\\nmial model is a token generation model rather than a document generation\\nmodel.\\nWe generate a document in the Bernoulli model (Figure 13.5) by ﬁrst pick-\\ning a class C = c with P(c) and then generating a binary indicator ei for each\\nterm ti of the vocabulary (1 ≤i ≤M). In the example in Figure 13.5, we\\nshow the generation of ⟨e1, e2, e3, e4, e5, e6⟩= ⟨0, 1, 0, 1, 1, 1⟩, corresponding,\\nagain, to the one-sentence document Beijing and Taipei join WTO where we\\nhave assumed that and is a stop word.\\nWe compare the two models in Table 13.3, including estimation equations\\nand decision rules.\\nNaive Bayes is so called because the independence assumptions we have\\njust made are indeed very naive for a model of natural language. The condi-\\ntional independence assumption states that features are independent of each\\nother given the class. This is hardly ever true for terms in documents. In\\nmany cases, the opposite is true. The pairs hong and kong or london and en-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.4\\nProperties of Naive Bayes\\n269\\n◮Table 13.4\\nCorrect estimation implies accurate prediction, but accurate predic-\\ntion does not imply correct estimation.\\nc1\\nc2\\nclass selected\\ntrue probability P(c|d)\\n0.6\\n0.4\\nc1\\nˆP(c) ∏1≤k≤nd ˆP(tk|c) (Equation (13.13))\\n0.00099\\n0.00001\\nNB estimate ˆP(c|d)\\n0.99\\n0.01\\nc1\\nglish in Figure 13.7 are examples of highly dependent terms. In addition, the\\nmultinomial model makes an assumption of positional independence. The\\nBernoulli model ignores positions in documents altogether because it only\\ncares about absence or presence. This bag-of-words model discards all in-\\nformation that is communicated by the order of words in natural language\\nsentences. How can NB be a good text classiﬁer when its model of natural\\nlanguage is so oversimpliﬁed?\\nThe answer is that even though the probability estimates of NB are of low\\nquality, its classiﬁcation decisions are surprisingly good. Consider a document\\nd with true probabilities P(c1|d) = 0.6 and P(c2|d) = 0.4 as shown in Ta-\\nble 13.4. Assume that d contains many terms that are positive indicators for\\nc1 and many terms that are negative indicators for c2. Thus, when using the\\nmultinomial model in Equation (13.13), ˆP(c1) ∏1≤k≤nd ˆP(tk|c1) will be much\\nlarger than ˆP(c2) ∏1≤k≤nd ˆP(tk|c2) (0.00099 vs. 0.00001 in the table). After di-\\nvision by 0.001 to get well-formed probabilities for P(c|d), we end up with\\none estimate that is close to 1.0 and one that is close to 0.0. This is common:\\nThe winning class in NB classiﬁcation usually has a much larger probabil-\\nity than the other classes and the estimates diverge very signiﬁcantly from\\nthe true probabilities. But the classiﬁcation decision is based on which class\\ngets the highest score. It does not matter how accurate the estimates are. De-\\nspite the bad estimates, NB estimates a higher probability for c1 and therefore\\nassigns d to the correct class in Table 13.4. Correct estimation implies accurate\\nprediction, but accurate prediction does not imply correct estimation. NB classiﬁers\\nestimate badly, but often classify well.\\nEven if it is not the method with the highest accuracy for text, NB has many\\nvirtues that make it a strong contender for text classiﬁcation. It excels if there\\nare many equally important features that jointly contribute to the classiﬁ-\\ncation decision. It is also somewhat robust to noise features (as deﬁned in\\nthe next section) and concept drift – the gradual change over time of the con-\\nCONCEPT DRIFT\\ncept underlying a class like US president from Bill Clinton to George W. Bush\\n(see Section 13.7). Classiﬁers like kNN (Section 14.3, page 297) can be care-\\nfully tuned to idiosyncratic properties of a particular time period. This will\\nthen hurt them when documents in the following time period have slightly\\n', 'Online edition (c)\\n2009 Cambridge UP\\n270\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.5\\nA set of documents for which the NB independence assumptions are\\nproblematic.\\n(1)\\nHe moved from London, Ontario, to London, England.\\n(2)\\nHe moved from London, England, to London, Ontario.\\n(3)\\nHe moved from England to London, Ontario.\\ndifferent properties.\\nThe Bernoulli model is particularly robust with respect to concept drift.\\nWe will see in Figure 13.8 that it can have decent performance when using\\nfewer than a dozen terms. The most important indicators for a class are less\\nlikely to change. Thus, a model that only relies on these features is more\\nlikely to maintain a certain level of accuracy in concept drift.\\nNB’s main strength is its efﬁciency: Training and classiﬁcation can be ac-\\ncomplished with one pass over the data. Because it combines efﬁciency with\\ngood accuracy it is often used as a baseline in text classiﬁcation research.\\nIt is often the method of choice if (i) squeezing out a few extra percentage\\npoints of accuracy is not worth the trouble in a text classiﬁcation application,\\n(ii) a very large amount of training data is available and there is more to be\\ngained from training on a lot of data than using a better classiﬁer on a smaller\\ntraining set, or (iii) if its robustness to concept drift can be exploited.\\nIn this book, we discuss NB as a classiﬁer for text. The independence as-\\nsumptions do not hold for text. However, it can be shown that NB is an\\noptimal classiﬁer (in the sense of minimal error rate on new data) for data\\nOPTIMAL CLASSIFIER\\nwhere the independence assumptions do hold.\\n13.4.1\\nA variant of the multinomial model\\nAn alternative formalization of the multinomial model represents each doc-\\nument d as an M-dimensional vector of counts ⟨tft1,d, . . . , tftM,d⟩where tfti,d\\nis the term frequency of ti in d. P(d|c) is then computed as follows (cf. Equa-\\ntion (12.8), page 243);\\nP(d|c) = P(⟨tft1,d, . . . , tftM,d⟩|c) ∝∏\\n1≤i≤M\\nP(X = ti|c)tfti,d\\n(13.15)\\nNote that we have omitted the multinomial factor. See Equation (12.8) (page 243).\\nEquation (13.15) is equivalent to the sequence model in Equation (13.2) as\\nP(X = ti|c)tfti,d = 1 for terms that do not occur in d (tfti,d = 0) and a term\\nthat occurs tfti,d ≥1 times will contribute tfti,d factors both in Equation (13.2)\\nand in Equation (13.15).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n271\\nSELECTFEATURES(D, c, k)\\n1\\nV ←EXTRACTVOCABULARY(D)\\n2\\nL ←[]\\n3\\nfor each t ∈V\\n4\\ndo A(t, c) ←COMPUTEFEATUREUTILITY(D, t, c)\\n5\\nAPPEND(L, ⟨A(t, c), t⟩)\\n6\\nreturn FEATURESWITHLARGESTVALUES(L, k)\\n◮Figure 13.6\\nBasic feature selection algorithm for selecting the k best features.\\n?\\nExercise 13.2\\n[⋆]\\nWhich of the documents in Table 13.5 have identical and different bag of words rep-\\nresentations for (i) the Bernoulli model (ii) the multinomial model? If there are differ-\\nences, describe them.\\nExercise 13.3\\nThe rationale for the positional independence assumption is that there is no useful\\ninformation in the fact that a term occurs in position k of a document. Find exceptions.\\nConsider formulaic documents with a ﬁxed document structure.\\nExercise 13.4\\nTable 13.3 gives Bernoulli and multinomial estimates for the word the. Explain the\\ndifference.\\n13.5\\nFeature selection\\nFeature selection is the process of selecting a subset of the terms occurring\\nFEATURE SELECTION\\nin the training set and using only this subset as features in text classiﬁca-\\ntion. Feature selection serves two main purposes. First, it makes training\\nand applying a classiﬁer more efﬁcient by decreasing the size of the effective\\nvocabulary. This is of particular importance for classiﬁers that, unlike NB,\\nare expensive to train. Second, feature selection often increases classiﬁca-\\ntion accuracy by eliminating noise features. A noise feature is one that, when\\nNOISE FEATURE\\nadded to the document representation, increases the classiﬁcation error on\\nnew data. Suppose a rare term, say arachnocentric, has no information about\\na class, say China, but all instances of arachnocentric happen to occur in China\\ndocuments in our training set. Then the learning method might produce a\\nclassiﬁer that misassigns test documents containing arachnocentric to China.\\nSuch an incorrect generalization from an accidental property of the training\\nset is called overﬁtting.\\nOVERFITTING\\nWe can view feature selection as a method for replacing a complex clas-\\nsiﬁer (using all features) with a simpler one (using a subset of the features).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n272\\n13\\nText classiﬁcation and Naive Bayes\\nIt may appear counterintuitive at ﬁrst that a seemingly weaker classiﬁer is\\nadvantageous in statistical text classiﬁcation, but when discussing the bias-\\nvariance tradeoff in Section 14.6 (page 308), we will see that weaker models\\nare often preferable when limited training data are available.\\nThe basic feature selection algorithm is shown in Figure 13.6. For a given\\nclass c, we compute a utility measure A(t, c) for each term of the vocabulary\\nand select the k terms that have the highest values of A(t, c). All other terms\\nare discarded and not used in classiﬁcation. We will introduce three different\\nutility measures in this section: mutual information, A(t, c) = I(Ut; Cc); the\\nχ2 test, A(t, c) = X2(t, c); and frequency, A(t, c) = N(t, c).\\nOf the two NB models, the Bernoulli model is particularly sensitive to\\nnoise features. A Bernoulli NB classiﬁer requires some form of feature se-\\nlection or else its accuracy will be low.\\nThis section mainly addresses feature selection for two-class classiﬁcation\\ntasks like China versus not-China. Section 13.5.5 brieﬂy discusses optimiza-\\ntions for systems with more than two classes.\\n13.5.1\\nMutual information\\nA common feature selection method is to compute A(t, c) as the expected\\nmutual information (MI) of term t and class c.5 MI measures how much in-\\nMUTUAL INFORMATION\\nformation the presence/absence of a term contributes to making the correct\\nclassiﬁcation decision on c. Formally:\\nI(U; C)\\n=\\n∑\\net∈{1,0} ∑\\nec∈{1,0}\\nP(U = et, C = ec) log2\\nP(U = et, C = ec)\\nP(U = et)P(C = ec),\\n(13.16)\\nwhere U is a random variable that takes values et = 1 (the document contains\\nterm t) and et = 0 (the document does not contain t), as deﬁned on page 266,\\nand C is a random variable that takes values ec = 1 (the document is in class\\nc) and ec = 0 (the document is not in class c). We write Ut and Cc if it is not\\nclear from context which term t and class c we are referring to.\\nForMLEs of the probabilities, Equation (13.16) is equivalent to Equation (13.17):\\nI(U; C)\\n=\\nN11\\nN log2\\nNN11\\nN1.N.1\\n+ N01\\nN log2\\nNN01\\nN0.N.1\\n(13.17)\\n+ N10\\nN log2\\nNN10\\nN1.N.0\\n+ N00\\nN log2\\nNN00\\nN0.N.0\\nwhere the Ns are counts of documents that have the values of et and ec that\\nare indicated by the two subscripts. For example, N10 is the number of doc-\\n5. Take care not to confuse expected mutual information with pointwise mutual information,\\nwhich is deﬁned as log N11/E11 where N11 and E11 are deﬁned as in Equation (13.18). The\\ntwo measures have different properties. See Section 13.7.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n273\\numents that contain t (et = 1) and are not in c (ec = 0). N1. = N10 + N11 is\\nthe number of documents that contain t (et = 1) and we count documents\\nindependent of class membership (ec ∈{0, 1}). N = N00 + N01 + N10 + N11\\nis the total number of documents. An example of one of the MLE estimates\\nthat transform Equation (13.16) into Equation (13.17) is P(U = 1, C = 1) =\\nN11/N.\\n\\x0f\\nExample 13.3:\\nConsider the class poultry and the term export in Reuters-RCV1.\\nThe counts of the number of documents with the four possible combinations of indi-\\ncator values are as follows:\\nec = epoultry = 1\\nec = epoultry = 0\\net = eexport = 1\\nN11 = 49\\nN10 = 27,652\\net = eexport = 0\\nN01 = 141\\nN00 = 774,106\\nAfter plugging these values into Equation (13.17) we get:\\nI(U; C)\\n=\\n49\\n801,948 log2\\n801,948 · 49\\n(49+27,652)(49+141)\\n+\\n141\\n801,948 log2\\n801,948 · 141\\n(141+774,106)(49+141)\\n+ 27,652\\n801,948 log2\\n801,948 · 27,652\\n(49+27,652)(27,652+774,106)\\n+ 774,106\\n801,948 log2\\n801,948 · 774,106\\n(141+774,106)(27,652+774,106)\\n≈\\n0.0001105\\nTo select k terms t1, . . . , tk for a given class, we use the feature selection al-\\ngorithm in Figure 13.6: We compute the utility measure as A(t, c) = I(Ut, Cc)\\nand select the k terms with the largest values.\\nMutual information measures how much information – in the information-\\ntheoretic sense – a term contains about the class. If a term’s distribution is\\nthe same in the class as it is in the collection as a whole, then I(U; C) =\\n0. MI reaches its maximum value if the term is a perfect indicator for class\\nmembership, that is, if the term is present in a document if and only if the\\ndocument is in the class.\\nFigure 13.7 shows terms with high mutual information scores for the six\\nclasses in Figure 13.1.6 The selected terms (e.g., london, uk, british for the class\\nUK) are of obvious utility for making classiﬁcation decisions for their respec-\\ntive classes. At the bottom of the list for UK we ﬁnd terms like peripherals\\nand tonight (not shown in the ﬁgure) that are clearly not helpful in deciding\\n6. Feature scores were computed on the ﬁrst 100,000 documents, except for poultry, a rare class,\\nfor which 800,000 documents were used. We have omitted numbers and other special words\\nfrom the top ten lists.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n274\\n13\\nText classiﬁcation and Naive Bayes\\nUK\\nlondon\\n0.1925\\nuk\\n0.0755\\nbritish\\n0.0596\\nstg\\n0.0555\\nbritain\\n0.0469\\nplc\\n0.0357\\nengland\\n0.0238\\npence\\n0.0212\\npounds\\n0.0149\\nenglish\\n0.0126\\nChina\\nchina\\n0.0997\\nchinese\\n0.0523\\nbeijing\\n0.0444\\nyuan\\n0.0344\\nshanghai\\n0.0292\\nhong\\n0.0198\\nkong\\n0.0195\\nxinhua\\n0.0155\\nprovince\\n0.0117\\ntaiwan\\n0.0108\\npoultry\\npoultry\\n0.0013\\nmeat\\n0.0008\\nchicken\\n0.0006\\nagriculture\\n0.0005\\navian\\n0.0004\\nbroiler\\n0.0003\\nveterinary\\n0.0003\\nbirds\\n0.0003\\ninspection\\n0.0003\\npathogenic\\n0.0003\\ncoffee\\ncoffee\\n0.0111\\nbags\\n0.0042\\ngrowers\\n0.0025\\nkg\\n0.0019\\ncolombia\\n0.0018\\nbrazil\\n0.0016\\nexport\\n0.0014\\nexporters\\n0.0013\\nexports\\n0.0013\\ncrop\\n0.0012\\nelections\\nelection\\n0.0519\\nelections\\n0.0342\\npolls\\n0.0339\\nvoters\\n0.0315\\nparty\\n0.0303\\nvote\\n0.0299\\npoll\\n0.0225\\ncandidate\\n0.0202\\ncampaign\\n0.0202\\ndemocratic\\n0.0198\\nsports\\nsoccer\\n0.0681\\ncup\\n0.0515\\nmatch\\n0.0441\\nmatches\\n0.0408\\nplayed\\n0.0388\\nleague\\n0.0386\\nbeat\\n0.0301\\ngame\\n0.0299\\ngames\\n0.0284\\nteam\\n0.0264\\n◮Figure 13.7\\nFeatures with high mutual information scores for six Reuters-RCV1\\nclasses.\\nwhether the document is in the class. As you might expect, keeping the in-\\nformative terms and eliminating the non-informative ones tends to reduce\\nnoise and improve the classiﬁer’s accuracy.\\nSuch an accuracy increase can be observed in Figure 13.8, which shows\\nF1 as a function of vocabulary size after feature selection for Reuters-RCV1.7\\nComparing F1 at 132,776 features (corresponding to selection of all features)\\nand at 10–100 features, we see that MI feature selection increases F1 by about\\n0.1 for the multinomial model and by more than 0.2 for the Bernoulli model.\\nFor the Bernoulli model, F1 peaks early, at ten features selected. At that point,\\nthe Bernoulli model is better than the multinomial model. When basing a\\nclassiﬁcation decision on only a few features, it is more robust to consider bi-\\nnary occurrence only. For the multinomial model (MI feature selection), the\\npeak occurs later, at 100 features, and its effectiveness recovers somewhat at\\n7. We trained the classiﬁers on the ﬁrst 100,000 documents and computed F1 on the next 100,000.\\nThe graphs are averages over ﬁve classes.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n275\\n#\\n# #\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n1\\n10\\n100\\n1000\\n10000\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nnumber of features selected\\nF1 measure\\no\\no o oo\\no\\no\\no\\no\\no\\no\\no\\no\\no\\nx\\nx\\nx x\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n#\\no\\nx\\nb\\nmultinomial, MI\\nmultinomial, chisquare\\nmultinomial, frequency\\nbinomial, MI\\n◮Figure 13.8\\nEffect of feature set size on accuracy for multinomial and Bernoulli\\nmodels.\\nthe end when we use all features. The reason is that the multinomial takes\\nthe number of occurrences into account in parameter estimation and clas-\\nsiﬁcation and therefore better exploits a larger number of features than the\\nBernoulli model. Regardless of the differences between the two methods,\\nusing a carefully selected subset of the features results in better effectiveness\\nthan using all features.\\n13.5.2\\nχ2 Feature selection\\nAnother popular feature selection method is χ2. In statistics, the χ2 test is\\nχ2 FEATURE SELECTION\\napplied to test the independence of two events, where two events A and B are\\ndeﬁned to be independent if P(AB) = P(A)P(B) or, equivalently, P(A|B) =\\nINDEPENDENCE\\nP(A) and P(B|A) = P(B). In feature selection, the two events are occurrence\\nof the term and occurrence of the class. We then rank terms with respect to\\nthe following quantity:\\nX2(D, t, c) =\\n∑\\net∈{0,1} ∑\\nec∈{0,1}\\n(Netec −Eetec)2\\nEetec\\n(13.18)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n276\\n13\\nText classiﬁcation and Naive Bayes\\nwhere et and ec are deﬁned as in Equation (13.16). N is the observed frequency\\nin D and E the expected frequency. For example, E11 is the expected frequency\\nof t and c occurring together in a document assuming that term and class are\\nindependent.\\n\\x0f\\nExample 13.4:\\nWe ﬁrst compute E11 for the data in Example 13.3:\\nE11\\n=\\nN × P(t) × P(c) = N × N11 + N10\\nN\\n× N11 + N01\\nN\\n=\\nN × 49 + 141\\nN\\n× 49 + 27652\\nN\\n≈6.6\\nwhere N is the total number of documents as before.\\nWe compute the other Eetec in the same way:\\nepoultry = 1\\nepoultry = 0\\neexport = 1\\nN11 = 49\\nE11 ≈6.6\\nN10 = 27,652\\nE10 ≈27,694.4\\neexport = 0\\nN01 = 141\\nE01 ≈183.4\\nN00 = 774,106\\nE00 ≈774,063.6\\nPlugging these values into Equation (13.18), we get a X2 value of 284:\\nX2(D, t, c) =\\n∑\\net∈{0,1}\\n∑\\nec∈{0,1}\\n(Netec −Eetec)2\\nEetec\\n≈284\\nX2 is a measure of how much expected counts E and observed counts N\\ndeviate from each other. A high value of X2 indicates that the hypothesis of\\nindependence, which implies that expected and observed counts are similar,\\nis incorrect. In our example, X2 ≈284 > 10.83. Based on Table 13.6, we\\ncan reject the hypothesis that poultry and export are independent with only a\\n0.001 chance of being wrong.8 Equivalently, we say that the outcome X2 ≈\\n284 > 10.83 is statistically signiﬁcant at the 0.001 level. If the two events are\\nSTATISTICAL\\nSIGNIFICANCE\\ndependent, then the occurrence of the term makes the occurrence of the class\\nmore likely (or less likely), so it should be helpful as a feature. This is the\\nrationale of χ2 feature selection.\\nAn arithmetically simpler way of computing X2 is the following:\\nX2(D, t, c) =\\n(N11 + N10 + N01 + N00) × (N11N00 −N10N01)2\\n(N11 + N01) × (N11 + N10) × (N10 + N00) × (N01 + N00)\\n(13.19)\\nThis is equivalent to Equation (13.18) (Exercise 13.14).\\n8. We can make this inference because, if the two events are independent, then X2 ∼χ2, where\\nχ2 is the χ2 distribution. See, for example, Rice (2006).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.5\\nFeature selection\\n277\\n◮Table 13.6\\nCritical values of the χ2 distribution with one degree of freedom. For\\nexample, if the two events are independent, then P(X2 > 6.63) < 0.01. So for X2 >\\n6.63 the assumption of independence can be rejected with 99% conﬁdence.\\np\\nχ2 critical value\\n0.1\\n2.71\\n0.05\\n3.84\\n0.01\\n6.63\\n0.005\\n7.88\\n0.001\\n10.83\\n$\\nAssessing χ2 as a feature selection method\\nFrom a statistical point of view, χ2 feature selection is problematic. For a\\ntest with one degree of freedom, the so-called Yates correction should be\\nused (see Section 13.7), which makes it harder to reach statistical signiﬁcance.\\nAlso, whenever a statistical test is used multiple times, then the probability\\nof getting at least one error increases. If 1,000 hypotheses are rejected, each\\nwith 0.05 error probability, then 0.05 × 1000 = 50 calls of the test will be\\nwrong on average. However, in text classiﬁcation it rarely matters whether a\\nfew additional terms are added to the feature set or removed from it. Rather,\\nthe relative importance of features is important. As long as χ2 feature selec-\\ntion only ranks features with respect to their usefulness and is not used to\\nmake statements about statistical dependence or independence of variables,\\nwe need not be overly concerned that it does not adhere strictly to statistical\\ntheory.\\n13.5.3\\nFrequency-based feature selection\\nA third feature selection method is frequency-based feature selection, that is,\\nselecting the terms that are most common in the class. Frequency can be\\neither deﬁned as document frequency (the number of documents in the class\\nc that contain the term t) or as collection frequency (the number of tokens of\\nt that occur in documents in c). Document frequency is more appropriate for\\nthe Bernoulli model, collection frequency for the multinomial model.\\nFrequency-based feature selection selects some frequent terms that have\\nno speciﬁc information about the class, for example, the days of the week\\n(Monday, Tuesday, ...), which are frequent across classes in newswire text.\\nWhen many thousands of features are selected, then frequency-based fea-\\nture selection often does well. Thus, if somewhat suboptimal accuracy is\\nacceptable, then frequency-based feature selection can be a good alternative\\nto more complex methods. However, Figure 13.8 is a case where frequency-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n278\\n13\\nText classiﬁcation and Naive Bayes\\nbased feature selection performs a lot worse than MI and χ2 and should not\\nbe used.\\n13.5.4\\nFeature selection for multiple classiﬁers\\nIn an operational system with a large number of classiﬁers, it is desirable\\nto select a single set of features instead of a different one for each classiﬁer.\\nOne way of doing this is to compute the X2 statistic for an n × 2 table where\\nthe columns are occurrence and nonoccurrence of the term and each row\\ncorresponds to one of the classes. We can then select the k terms with the\\nhighest X2 statistic as before.\\nMore commonly, feature selection statistics are ﬁrst computed separately\\nfor each class on the two-class classiﬁcation task c versus c and then com-\\nbined. One combination method computes a single ﬁgure of merit for each\\nfeature, for example, by averaging the values A(t, c) for feature t, and then\\nselects the k features with highest ﬁgures of merit. Another frequently used\\ncombination method selects the top k/n features for each of n classiﬁers and\\nthen combines these n sets into one global feature set.\\nClassiﬁcation accuracy often decreases when selecting k common features\\nfor a system with n classiﬁers as opposed to n different sets of size k. But even\\nif it does, the gain in efﬁciency owing to a common document representation\\nmay be worth the loss in accuracy.\\n13.5.5\\nComparison of feature selection methods\\nMutual information and χ2 represent rather different feature selection meth-\\nods. The independence of term t and class c can sometimes be rejected with\\nhigh conﬁdence even if t carries little information about membership of a\\ndocument in c. This is particularly true for rare terms. If a term occurs once\\nin a large collection and that one occurrence is in the poultry class, then this\\nis statistically signiﬁcant. But a single occurrence is not very informative\\naccording to the information-theoretic deﬁnition of information.\\nBecause\\nits criterion is signiﬁcance, χ2 selects more rare terms (which are often less\\nreliable indicators) than mutual information. But the selection criterion of\\nmutual information also does not necessarily select the terms that maximize\\nclassiﬁcation accuracy.\\nDespite the differences between the two methods, the classiﬁcation accu-\\nracy of feature sets selected with χ2 and MI does not seem to differ systemat-\\nically. In most text classiﬁcation problems, there are a few strong indicators\\nand many weak indicators. As long as all strong indicators and a large num-\\nber of weak indicators are selected, accuracy is expected to be good. Both\\nmethods do this.\\nFigure 13.8 compares MI and χ2 feature selection for the multinomial model.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n279\\nPeak effectiveness is virtually the same for both methods. χ2 reaches this\\npeak later, at 300 features, probably because the rare, but highly signiﬁcant\\nfeatures it selects initially do not cover all documents in the class. However,\\nfeatures selected later (in the range of 100–300)are of better quality than those\\nselected by MI.\\nAll three methods – MI, χ2 and frequency based – are greedy methods.\\nGREEDY FEATURE\\nSELECTION\\nThey may select features that contribute no incremental information over\\npreviously selected features. In Figure 13.7, kong is selected as the seventh\\nterm even though it is highly correlated with previously selected hong and\\ntherefore redundant. Although such redundancy can negatively impact ac-\\ncuracy, non-greedy methods (see Section 13.7 for references) are rarely used\\nin text classiﬁcation due to their computational cost.\\n?\\nExercise 13.5\\nConsider the following frequencies for the class coffee for four terms in the ﬁrst 100,000\\ndocuments of Reuters-RCV1:\\nterm\\nN00\\nN01\\nN10\\nN11\\nbrazil\\n98,012\\n102\\n1835\\n51\\ncouncil\\n96,322\\n133\\n3525\\n20\\nproducers\\n98,524\\n119\\n1118\\n34\\nroasted\\n99,824\\n143\\n23\\n10\\nSelect two of these four terms based on (i) χ2, (ii) mutual information, (iii) frequency.\\n13.6\\nEvaluation of text classiﬁcation\\n] Historically, the classic Reuters-21578 collection was the main benchmark\\nfor text classiﬁcation evaluation. This is a collection of 21,578 newswire ar-\\nticles, originally collected and labeled by Carnegie Group, Inc. and Reuters,\\nLtd. in the course of developing the CONSTRUE text classiﬁcation system.\\nIt is much smaller than and predates the Reuters-RCV1 collection discussed\\nin Chapter 4 (page 69). The articles are assigned classes from a set of 118\\ntopic categories. A document may be assigned several classes or none, but\\nthe commonest case is single assignment (documents with at least one class\\nreceived an average of 1.24 classes). The standard approach to this any-of\\nproblem (Chapter 14, page 306) is to learn 118 two-class classiﬁers, one for\\neach class, where the two-class classiﬁer for class c is the classiﬁer for the two\\nTWO-CLASS CLASSIFIER\\nclasses c and its complement c.\\nFor each of these classiﬁers, we can measure recall, precision, and accu-\\nracy. In recent work, people almost invariably use the ModApte split, which\\nMODAPTE SPLIT\\nincludes only documents that were viewed and assessed by a human indexer,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n280\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.7\\nThe ten largest classes in the Reuters-21578 collection with number of\\ndocuments in training and test sets.\\nclass\\n# train\\n# testclass\\n# train\\n# test\\nearn\\n2877\\n1087 trade\\n369\\n119\\nacquisitions\\n1650\\n179 interest\\n347\\n131\\nmoney-fx\\n538\\n179 ship\\n197\\n89\\ngrain\\n433\\n149 wheat\\n212\\n71\\ncrude\\n389\\n189 corn\\n182\\n56\\nand comprises 9,603 training documents and 3,299 test documents. The dis-\\ntribution of documents in classes is very uneven, and some work evaluates\\nsystems on only documents in the ten largest classes. They are listed in Ta-\\nble 13.7. A typical document with topics is shown in Figure 13.9.\\nIn Section 13.1, we stated as our goal in text classiﬁcation the minimization\\nof classiﬁcation error on test data. Classiﬁcation error is 1.0 minus classiﬁca-\\ntion accuracy, the proportion of correct decisions, a measure we introduced\\nin Section 8.3 (page 155). This measure is appropriate if the percentage of\\ndocuments in the class is high, perhaps 10% to 20% and higher. But as we\\ndiscussed in Section 8.3, accuracy is not a good measure for “small” classes\\nbecause always saying no, a strategy that defeats the purpose of building a\\nclassiﬁer, will achieve high accuracy. The always-no classiﬁer is 99% accurate\\nfor a class with relative frequency 1%. For small classes, precision, recall and\\nF1 are better measures.\\nWe will use effectiveness as a generic term for measures that evaluate the\\nEFFECTIVENESS\\nquality of classiﬁcation decisions, including precision, recall, F1, and accu-\\nracy. Performance refers to the computational efﬁciency of classiﬁcation and\\nPERFORMANCE\\nEFFICIENCY\\nIR systems in this book. However, many researchers mean effectiveness, not\\nefﬁciency of text classiﬁcation when they use the term performance.\\nWhen we process a collection with several two-class classiﬁers (such as\\nReuters-21578 with its 118 classes), we often want to compute a single ag-\\ngregate measure that combines the measures for individual classiﬁers. There\\nare two methods for doing this. Macroaveraging computes a simple aver-\\nMACROAVERAGING\\nage over classes. Microaveraging pools per-document decisions across classes,\\nMICROAVERAGING\\nand then computes an effectiveness measure on the pooled contingency ta-\\nble. Table 13.8 gives an example.\\nThe differences between the two methods can be large. Macroaveraging\\ngives equal weight to each class, whereas microaveraging gives equal weight\\nto each per-document classiﬁcation decision. Because the F1 measure ignores\\ntrue negatives and its magnitude is mostly determined by the number of\\ntrue positives, large classes dominate small classes in microaveraging. In the\\nexample, microaveraged precision (0.83) is much closer to the precision of\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n281\\n<REUTERS TOPICS=’’YES’’ LEWISSPLIT=’’TRAIN’’\\nCGISPLIT=’’TRAINING-SET’’ OLDID=’’12981’’ NEWID=’’798’’>\\n<DATE> 2-MAR-1987 16:51:43.42</DATE>\\n<TOPICS><D>livestock</D><D>hog</D></TOPICS>\\n<TITLE>AMERICAN PORK CONGRESS KICKS OFF TOMORROW</TITLE>\\n<DATELINE> CHICAGO, March 2 - </DATELINE><BODY>The American Pork\\nCongress kicks off tomorrow, March 3, in Indianapolis with 160\\nof the nations pork producers from 44 member states determining\\nindustry positions on a number of issues, according to the\\nNational Pork Producers Council, NPPC.\\nDelegates to the three day Congress will be considering 26\\nresolutions concerning various issues, including the future\\ndirection of farm policy and the tax law as it applies to the\\nagriculture sector. The delegates will also debate whether to\\nendorse concepts of a national PRV (pseudorabies virus) control\\nand eradication program, the NPPC said. A large\\ntrade show, in conjunction with the congress, will feature\\nthe latest in technology in all areas of the industry, the NPPC\\nadded. Reuter\\n\\\\&\\\\#3;</BODY></TEXT></REUTERS>\\n◮Figure 13.9\\nA sample document from the Reuters-21578 collection.\\nc2 (0.9) than to the precision of c1 (0.5) because c2 is ﬁve times larger than\\nc1. Microaveraged results are therefore really a measure of effectiveness on\\nthe large classes in a test collection. To get a sense of effectiveness on small\\nclasses, you should compute macroaveraged results.\\nIn one-of classiﬁcation (Section 14.5, page 306), microaveraged F1 is the\\nsame as accuracy (Exercise 13.6).\\nTable 13.9 gives microaveraged and macroaveraged effectiveness of Naive\\nBayes for the ModApte split of Reuters-21578. To give a sense of the relative\\neffectiveness of NB, we compare it with linear SVMs (rightmost column; see\\nChapter 15), one of the most effective classiﬁers, but also one that is more\\nexpensive to train than NB. NB has a microaveraged F1 of 80%, which is\\n9% less than the SVM (89%), a 10% relative decrease (row “micro-avg-L (90\\nclasses)”). So there is a surprisingly small effectiveness penalty for its sim-\\nplicity and efﬁciency. However, on small classes, some of which only have on\\nthe order of ten positive examples in the training set, NB does much worse.\\nIts macroaveraged F1 is 13% below the SVM, a 22% relative decrease (row\\n“macro-avg (90 classes)”).\\nThe table also compares NB with the other classiﬁers we cover in this book:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n282\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.8\\nMacro- and microaveraging. “Truth” is the true class and “call” the\\ndecision of the classiﬁer. In this example, macroaveraged precision is [10/(10 + 10) +\\n90/(10 + 90)]/2 = (0.5 + 0.9)/2 = 0.7. Microaveraged precision is 100/(100 + 20) ≈\\n0.83.\\nclass 1\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n10\\n10\\ncall:\\nno\\n10\\n970\\nclass 2\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n90\\n10\\ncall:\\nno\\n10\\n890\\npooled table\\ntruth:\\ntruth:\\nyes\\nno\\ncall:\\nyes\\n100\\n20\\ncall:\\nno\\n20\\n1860\\n◮Table 13.9\\nText classiﬁcation effectiveness numbers on Reuters-21578 for F1 (in\\npercent). Results from Li and Yang (2003) (a), Joachims (1998) (b: kNN) and Dumais\\net al. (1998) (b: NB, Rocchio, trees, SVM).\\n(a)\\nNB\\nRocchio\\nkNN\\nSVM\\nmicro-avg-L (90 classes)\\n80\\n85\\n86\\n89\\nmacro-avg (90 classes)\\n47\\n59\\n60\\n60\\n(b)\\nNB\\nRocchio\\nkNN\\ntrees\\nSVM\\nearn\\n96\\n93\\n97\\n98\\n98\\nacq\\n88\\n65\\n92\\n90\\n94\\nmoney-fx\\n57\\n47\\n78\\n66\\n75\\ngrain\\n79\\n68\\n82\\n85\\n95\\ncrude\\n80\\n70\\n86\\n85\\n89\\ntrade\\n64\\n65\\n77\\n73\\n76\\ninterest\\n65\\n63\\n74\\n67\\n78\\nship\\n85\\n49\\n79\\n74\\n86\\nwheat\\n70\\n69\\n77\\n93\\n92\\ncorn\\n65\\n48\\n78\\n92\\n90\\nmicro-avg (top 10)\\n82\\n65\\n82\\n88\\n92\\nmicro-avg-D (118 classes)\\n75\\n62\\nn/a\\nn/a\\n87\\nRocchio and kNN. In addition, we give numbers for decision trees, an impor-\\nDECISION TREES\\ntant classiﬁcation method we do not cover. The bottom part of the table\\nshows that there is considerable variation from class to class. For instance,\\nNB beats kNN on ship, but is much worse on money-fx.\\nComparing parts (a) and (b) of the table, one is struck by the degree to\\nwhich the cited papers’ results differ. This is partly due to the fact that the\\nnumbers in (b) are break-even scores (cf. page 161) averaged over 118 classes,\\nwhereas the numbers in (a) are true F1 scores (computed without any know-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n283\\nledge of the test set) averaged over ninety classes. This is unfortunately typ-\\nical of what happens when comparing different results in text classiﬁcation:\\nThere are often differences in the experimental setup or the evaluation that\\ncomplicate the interpretation of the results.\\nThese and other results have shown that the average effectiveness of NB\\nis uncompetitive with classiﬁers like SVMs when trained and tested on inde-\\npendent and identically distributed (i.i.d.) data, that is, uniform data with all the\\ngood properties of statistical sampling. However, these differences may of-\\nten be invisible or even reverse themselves when working in the real world\\nwhere, usually, the training sample is drawn from a subset of the data to\\nwhich the classiﬁer will be applied, the nature of the data drifts over time\\nrather than being stationary (the problem of concept drift we mentioned on\\npage 269), and there may well be errors in the data (among other problems).\\nMany practitioners have had the experience of being unable to build a fancy\\nclassiﬁer for a certain problem that consistently performs better than NB.\\nOur conclusion from the results in Table 13.9 is that, although most re-\\nsearchers believe that an SVM is better than kNN and kNN better than NB,\\nthe ranking of classiﬁers ultimately depends on the class, the document col-\\nlection, and the experimental setup. In text classiﬁcation, there is always\\nmore to know than simply which machine learning algorithm was used, as\\nwe further discuss in Section 15.3 (page 334).\\nWhen performing evaluations like the one in Table 13.9, it is important to\\nmaintain a strict separation between the training set and the test set. We can\\neasily make correct classiﬁcation decisions on the test set by using informa-\\ntion we have gleaned from the test set, such as the fact that a particular term\\nis a good predictor in the test set (even though this is not the case in the train-\\ning set). A more subtle example of using knowledge about the test set is to\\ntry a large number of values of a parameter (e.g., the number of selected fea-\\ntures) and select the value that is best for the test set. As a rule, accuracy on\\nnew data – the type of data we will encounter when we use the classiﬁer in\\nan application – will be much lower than accuracy on a test set that the clas-\\nsiﬁer has been tuned for. We discussed the same problem in ad hoc retrieval\\nin Section 8.1 (page 153).\\nIn a clean statistical text classiﬁcation experiment, you should never run\\nany program on or even look at the test set while developing a text classiﬁca-\\ntion system. Instead, set aside a development set for testing while you develop\\nDEVELOPMENT SET\\nyour method. When such a set serves the primary purpose of ﬁnding a good\\nvalue for a parameter, for example, the number of selected features, then it\\nis also called held-out data. Train the classiﬁer on the rest of the training set\\nHELD-OUT DATA\\nwith different parameter values, and then select the value that gives best re-\\nsults on the held-out part of the training set. Ideally, at the very end, when\\nall parameters have been set and the method is fully speciﬁed, you run one\\nﬁnal experiment on the test set and publish the results. Because no informa-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n284\\n13\\nText classiﬁcation and Naive Bayes\\n◮Table 13.10\\nData for parameter estimation exercise.\\ndocID\\nwords in document\\nin c = China?\\ntraining set\\n1\\nTaipei Taiwan\\nyes\\n2\\nMacao Taiwan Shanghai\\nyes\\n3\\nJapan Sapporo\\nno\\n4\\nSapporo Osaka Taiwan\\nno\\ntest set\\n5\\nTaiwan Taiwan Sapporo\\n?\\ntion about the test set was used in developing the classiﬁer, the results of this\\nexperiment should be indicative of actual performance in practice.\\nThis ideal often cannot be met; researchers tend to evaluate several sys-\\ntems on the same test set over a period of several years. But it is neverthe-\\nless highly important to not look at the test data and to run systems on it as\\nsparingly as possible. Beginners often violate this rule, and their results lose\\nvalidity because they have implicitly tuned their system to the test data sim-\\nply by running many variant systems and keeping the tweaks to the system\\nthat worked best on the test set.\\n?\\nExercise 13.6\\n[⋆⋆]\\nAssume a situation where every document in the test collection has been assigned\\nexactly one class, and that a classiﬁer also assigns exactly one class to each document.\\nThis setup is called one-of classiﬁcation (Section 14.5, page 306). Show that in one-of\\nclassiﬁcation (i) the total number of false positive decisions equals the total number\\nof false negative decisions and (ii) microaveraged F1 and accuracy are identical.\\nExercise 13.7\\nThe class priors in Figure 13.2 are computed as the fraction of documents in the class\\nas opposed to the fraction of tokens in the class. Why?\\nExercise 13.8\\nThe function APPLYMULTINOMIALNB in Figure 13.2 has time complexity Θ(La +\\n|C|La). How would you modify the function so that its time complexity is Θ(La +\\n|C|Ma)?\\nExercise 13.9\\nBased on the data in Table 13.10, (i) estimate a multinomial Naive Bayes classiﬁer, (ii)\\napply the classiﬁer to the test document, (iii) estimate a Bernoulli NB classiﬁer, (iv)\\napply the classiﬁer to the test document. You need not estimate parameters that you\\ndon’t need for classifying the test document.\\nExercise 13.10\\nYour task is to classify words as English or not English. Words are generated by a\\nsource with the following distribution:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.6\\nEvaluation of text classiﬁcation\\n285\\nevent\\nword\\nEnglish?\\nprobability\\n1\\nozb\\nno\\n4/9\\n2\\nuzu\\nno\\n4/9\\n3\\nzoo\\nyes\\n1/18\\n4\\nbun\\nyes\\n1/18\\n(i) Compute the parameters (priors and conditionals) of a multinomial NB classi-\\nﬁer that uses the letters b, n, o, u, and z as features. Assume a training set that\\nreﬂects the probability distribution of the source perfectly. Make the same indepen-\\ndence assumptions that are usually made for a multinomial classiﬁer that uses terms\\nas features for text classiﬁcation. Compute parameters using smoothing, in which\\ncomputed-zero probabilities are smoothed into probability 0.01, and computed-nonzero\\nprobabilities are untouched. (This simplistic smoothing may cause P(A) + P(A) > 1.\\nSolutions are not required to correct this.) (ii) How does the classiﬁer classify the\\nword zoo? (iii) Classify the word zoo using a multinomial classiﬁer as in part (i), but\\ndo not make the assumption of positional independence. That is, estimate separate\\nparameters for each position in a word. You only need to compute the parameters\\nyou need for classifying zoo.\\nExercise 13.11\\nWhat are the values of I(Ut; Cc) and X2(D, t, c) if term and class are completely inde-\\npendent? What are the values if they are completely dependent?\\nExercise 13.12\\nThe feature selection method in Equation (13.16) is most appropriate for the Bernoulli\\nmodel. Why? How could one modify it for the multinomial model?\\nExercise 13.13\\nFeatures can also be selected according toinformation gain (IG), which is deﬁned as:\\nINFORMATION GAIN\\nIG(D, t, c) = H(pD) −\\n∑\\nx∈{Dt+,Dt−}\\n|x|\\n|D| H(px)\\nwhere H is entropy, D is the training set, and Dt+, and Dt−are the subset of D with\\nterm t, and the subset of D without term t, respectively. pA is the class distribution\\nin (sub)collection A, e.g., pA(c) = 0.25, pA(c) = 0.75 if a quarter of the documents in\\nA are in class c.\\nShow that mutual information and information gain are equivalent.\\nExercise 13.14\\nShow that the two X2 formulas (Equations (13.18) and (13.19)) are equivalent.\\nExercise 13.15\\nIn the χ2 example on page 276 we have |N11 −E11| = |N10 −E10| = |N01 −E01| =\\n|N00 −E00|. Show that this holds in general.\\nExercise 13.16\\nχ2 and mutual information do not distinguish between positively and negatively cor-\\nrelated features. Because most good text classiﬁcation features are positively corre-\\nlated (i.e., they occur more often in c than in c), one may want to explicitly rule out\\nthe selection of negative indicators. How would you do this?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n286\\n13\\nText classiﬁcation and Naive Bayes\\n13.7\\nReferences and further reading\\nGeneral introductions to statistical classiﬁcation and machine learning can be\\nfound in (Hastie et al. 2001), (Mitchell 1997), and (Duda et al. 2000), including\\nmany important methods (e.g., decision trees and boosting) that we do not\\ncover. A comprehensive review of text classiﬁcation methods and results is\\n(Sebastiani 2002). Manning and Schütze (1999, Chapter 16) give an accessible\\nintroduction to text classiﬁcation with coverage of decision trees, perceptrons\\nand maximum entropy models. More information on the superlinear time\\ncomplexity of learning methods that are more accurate than Naive Bayes can\\nbe found in (Perkins et al. 2003) and (Joachims 2006a).\\nMaron and Kuhns (1960) described one of the ﬁrst NB text classiﬁers. Lewis\\n(1998) focuses on the history of NB classiﬁcation. Bernoulli and multinomial\\nmodels and their accuracy for different collections are discussed by McCal-\\nlum and Nigam (1998). Eyheramendy et al. (2003) present additional NB\\nmodels. Domingos and Pazzani (1997), Friedman (1997), and Hand and Yu\\n(2001) analyze why NB performs well although its probability estimates are\\npoor. The ﬁrst paper also discusses NB’s optimality when the independence\\nassumptions are true of the data. Pavlov et al. (2004) propose a modiﬁed\\ndocument representation that partially addresses the inappropriateness of\\nthe independence assumptions. Bennett (2000) attributes the tendency of NB\\nprobability estimates to be close to either 0 or 1 to the effect of document\\nlength. Ng and Jordan (2001) show that NB is sometimes (although rarely)\\nsuperior to discriminative methods because it more quickly reaches its opti-\\nmal error rate. The basic NB model presented in this chapter can be tuned for\\nbetter effectiveness (Rennie et al. 2003;Kołcz and Yih 2007). The problem of\\nconcept drift and other reasons why state-of-the-art classiﬁers do not always\\nexcel in practice are discussed by Forman (2006) and Hand (2006).\\nEarly uses of mutual information and χ2 for feature selection in text clas-\\nsiﬁcation are Lewis and Ringuette (1994) and Schütze et al. (1995), respec-\\ntively. Yang and Pedersen (1997) review feature selection methods and their\\nimpact on classiﬁcation effectiveness. They ﬁnd that pointwise mutual infor-\\nPOINTWISE MUTUAL\\nINFORMATION\\nmation is not competitive with other methods. Yang and Pedersen refer to\\nexpected mutual information (Equation (13.16)) as information gain (see Ex-\\nercise 13.13, page 285). (Snedecor and Cochran 1989) is a good reference for\\nthe χ2 test in statistics, including the Yates’ correction for continuity for 2 × 2\\ntables. Dunning (1993) discusses problems of the χ2 test when counts are\\nsmall. Nongreedy feature selection techniques are described by Hastie et al.\\n(2001). Cohen (1995) discusses the pitfalls of using multiple signiﬁcance tests\\nand methods to avoid them. Forman (2004) evaluates different methods for\\nfeature selection for multiple classiﬁers.\\nDavid D. Lewis deﬁnes the ModApte split at www.daviddlewis.com/resources/testcollections/reuters215\\nbased on Apté et al. (1994). Lewis (1995) describes utility measures for the\\nUTILITY MEASURE\\n', 'Online edition (c)\\n2009 Cambridge UP\\n13.7\\nReferences and further reading\\n287\\nevaluation of text classiﬁcation systems. Yang and Liu (1999) employ signif-\\nicance tests in the evaluation of text classiﬁcation methods.\\nLewis et al. (2004) ﬁnd that SVMs (Chapter 15) perform better on Reuters-\\nRCV1 than kNN and Rocchio (Chapter 14).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n289\\n14\\nVector space classiﬁcation\\nThe document representation in Naive Bayes is a sequence of terms or a bi-\\nnary vector ⟨e1, . . . , e|V|⟩∈{0, 1}|V|. In this chapter we adopt a different\\nrepresentation for text classiﬁcation, the vector space model, developed in\\nChapter 6. It represents each document as a vector with one real-valued com-\\nponent, usually a tf-idf weight, for each term. Thus, the document space X,\\nthe domain of the classiﬁcation function γ, is R|V|. This chapter introduces a\\nnumber of classiﬁcation methods that operate on real-valued vectors.\\nThe basic hypothesis in using the vector space model for classiﬁcation is\\nthe contiguity hypothesis.\\nCONTIGUITY\\nHYPOTHESIS\\nContiguity hypothesis. Documents in the same class form a contigu-\\nous region and regions of different classes do not overlap.\\nThere are many classiﬁcation tasks, in particular the type of text classiﬁcation\\nthat we encountered in Chapter 13, where classes can be distinguished by\\nword patterns. For example, documents in the class China tend to have high\\nvalues on dimensions like Chinese, Beijing, and Mao whereas documents in the\\nclass UK tend to have high values for London, British and Queen. Documents\\nof the two classes therefore form distinct contiguous regions as shown in\\nFigure 14.1 and we can draw boundaries that separate them and classify new\\ndocuments. How exactly this is done is the topic of this chapter.\\nWhether or not a set of documents is mapped into a contiguous region de-\\npends on the particular choices we make for the document representation:\\ntype of weighting, stop list etc. To see that the document representation is\\ncrucial, consider the two classes written by a group vs. written by a single per-\\nson. Frequent occurrence of the ﬁrst person pronoun I is evidence for the\\nsingle-person class. But that information is likely deleted from the document\\nrepresentation if we use a stop list. If the document representation chosen\\nis unfavorable, the contiguity hypothesis will not hold and successful vector\\nspace classiﬁcation is not possible.\\nThe same considerations that led us to prefer weighted representations, in\\nparticular length-normalized tf-idf representations, in Chapters 6 and 7 also\\n', 'Online edition (c)\\n2009 Cambridge UP\\n290\\n14\\nVector space classiﬁcation\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\nChina\\nKenya\\nUK\\n⋆\\n◮Figure 14.1\\nVector space classiﬁcation into three classes.\\napply here. For example, a term with 5 occurrences in a document should get\\na higher weight than a term with one occurrence, but a weight 5 times larger\\nwould give too much emphasis to the term. Unweighted and unnormalized\\ncounts should not be used in vector space classiﬁcation.\\nWe introduce two vector space classiﬁcation methods in this chapter, Roc-\\nchio and kNN. Rocchio classiﬁcation (Section 14.2) divides the vector space\\ninto regions centered on centroids or prototypes, one for each class, computed\\nPROTOTYPE\\nas the center of mass of all documents in the class. Rocchio classiﬁcation is\\nsimple and efﬁcient, but inaccurate if classes are not approximately spheres\\nwith similar radii.\\nkNN or k nearest neighbor classiﬁcation (Section 14.3) assigns the majority\\nclass of the k nearest neighbors to a test document. kNN requires no explicit\\ntraining and can use the unprocessed training set directly in classiﬁcation.\\nIt is less efﬁcient than other classiﬁcation methods in classifying documents.\\nIf the training set is large, then kNN can handle non-spherical and other\\ncomplex classes better than Rocchio.\\nA large number of text classiﬁers can be viewed as linear classiﬁers – clas-\\nsiﬁers that classify based on a simple linear combination of the features (Sec-\\ntion 14.4). Such classiﬁers partition the space of features into regions sepa-\\nrated by linear decision hyperplanes, in a manner to be detailed below. Because\\nof the bias-variance tradeoff (Section 14.6) more complex nonlinear models\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.1\\nDocument representations and measures of relatedness in vector spaces\\n291\\ndtrue\\ndprojected\\nx1\\nx2 x3 x4\\nx5\\nx′\\n1\\nx′\\n2 x′\\n3 x′\\n4\\nx′\\n5\\nx′\\n1\\nx′\\n2\\nx′\\n3\\nx′\\n4\\nx′\\n5\\n◮Figure 14.2\\nProjections of small areas of the unit sphere preserve distances. Left:\\nA projection of the 2D semicircle to 1D. For the points x1, x2, x3, x4, x5 at x coordinates\\n−0.9, −0.2, 0, 0.2, 0.9 the distance |x2x3| ≈0.201 only differs by 0.5% from |x′\\n2x′\\n3| =\\n0.2; but |x1x3|/|x′\\n1x′\\n3| = dtrue/dprojected ≈1.06/0.9 ≈1.18 is an example of a large\\ndistortion (18%) when projecting a large area. Right: The corresponding projection of\\nthe 3D hemisphere to 2D.\\nare not systematically better than linear models. Nonlinear models have\\nmore parameters to ﬁt on a limited amount of training data and are more\\nlikely to make mistakes for small and noisy data sets.\\nWhen applying two-class classiﬁers to problems with more than two classes,\\nthere are one-of tasks – a document must be assigned to exactly one of several\\nmutually exclusive classes – and any-of tasks – a document can be assigned to\\nany number of classes as we will explain in Section 14.5. Two-class classiﬁers\\nsolve any-of problems and can be combined to solve one-of problems.\\n14.1\\nDocument representations and measures of relatedness in vec-\\ntor spaces\\nAs in Chapter 6, we represent documents as vectors in R|V| in this chapter.\\nTo illustrate properties of document vectors in vector classiﬁcation, we will\\nrender these vectors as points in a plane as in the example in Figure 14.1.\\nIn reality, document vectors are length-normalized unit vectors that point\\nto the surface of a hypersphere. We can view the 2D planes in our ﬁgures\\nas projections onto a plane of the surface of a (hyper-)sphere as shown in\\nFigure 14.2. Distances on the surface of the sphere and on the projection\\nplane are approximately the same as long as we restrict ourselves to small\\nareas of the surface and choose an appropriate projection (Exercise 14.1).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n292\\n14\\nVector space classiﬁcation\\nDecisions of many vector space classiﬁers are based on a notion of dis-\\ntance, e.g., when computing the nearest neighbors in kNN classiﬁcation.\\nWe will use Euclidean distance in this chapter as the underlying distance\\nmeasure. We observed earlier (Exercise 6.18, page 131) that there is a direct\\ncorrespondence between cosine similarity and Euclidean distance for length-\\nnormalized vectors. In vector space classiﬁcation, it rarely matters whether\\nthe relatedness of two documents is expressed in terms of similarity or dis-\\ntance.\\nHowever, in addition to documents, centroids or averages of vectors also\\nplay an important role in vector space classiﬁcation. Centroids are not length-\\nnormalized. For unnormalized vectors, dot product, cosine similarity and\\nEuclidean distance all have different behavior in general (Exercise 14.6). We\\nwill be mostly concerned with small local regions when computing the sim-\\nilarity between a document and a centroid, and the smaller the region the\\nmore similar the behavior of the three measures is.\\n?\\nExercise 14.1\\nFor small areas, distances on the surface of the hypersphere are approximated well\\nby distances on its projection (Figure 14.2) because α ≈sin α for small angles. For\\nwhat size angle is the distortion α/ sin(α) (i) 1.01, (ii) 1.05 and (iii) 1.1?\\n14.2\\nRocchio classiﬁcation\\nFigure 14.1 shows three classes, China, UK and Kenya, in a two-dimensional\\n(2D) space. Documents are shown as circles, diamonds and X’s. The bound-\\naries in the ﬁgure, which we call decision boundaries, are chosen to separate\\nDECISION BOUNDARY\\nthe three classes, but are otherwise arbitrary. To classify a new document,\\ndepicted as a star in the ﬁgure, we determine the region it occurs in and as-\\nsign it the class of that region – China in this case. Our task in vector space\\nclassiﬁcation is to devise algorithms that compute good boundaries where\\n“good” means high classiﬁcation accuracy on data unseen during training.\\nPerhaps the best-known way of computing good class boundaries is Roc-\\nROCCHIO\\nCLASSIFICATION\\nchio classiﬁcation, which uses centroids to deﬁne the boundaries. The centroid\\nCENTROID\\nof a class c is computed as the vector average or center of mass of its mem-\\nbers:\\n⃗µ(c) =\\n1\\n|Dc| ∑\\nd∈Dc\\n⃗v(d)\\n(14.1)\\nwhere Dc is the set of documents in D whose class is c: Dc = {d : ⟨d, c⟩∈D}.\\nWe denote the normalized vector of d by ⃗v(d) (Equation (6.11), page 122).\\nThree example centroids are shown as solid circles in Figure 14.3.\\nThe boundary between two classes in Rocchio classiﬁcation is the set of\\npoints with equal distance from the two centroids. For example, |a1| = |a2|,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.2\\nRocchio classiﬁcation\\n293\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\nChina\\nKenya\\nUK\\n⋆\\na1\\na2\\nb1\\nb2\\nc1\\nc2\\n◮Figure 14.3\\nRocchio classiﬁcation.\\n|b1| = |b2|, and |c1| = |c2| in the ﬁgure. This set of points is always a line.\\nThe generalization of a line in M-dimensional space is a hyperplane, which\\nwe deﬁne as the set of points ⃗x that satisfy:\\n⃗wT⃗x = b\\n(14.2)\\nwhere ⃗w is the M-dimensional normal vector1 of the hyperplane and b is a\\nNORMAL VECTOR\\nconstant. This deﬁnition of hyperplanes includes lines (any line in 2D can\\nbe deﬁned by w1x1 + w2x2 = b) and 2-dimensional planes (any plane in 3D\\ncan be deﬁned by w1x1 + w2x2 + w3x3 = b). A line divides a plane in two,\\na plane divides 3-dimensional space in two, and hyperplanes divide higher-\\ndimensional spaces in two.\\nThus, the boundaries of class regions in Rocchio classiﬁcation are hyper-\\nplanes. The classiﬁcation rule in Rocchio is to classify a point in accordance\\nwith the region it falls into. Equivalently, we determine the centroid⃗µ(c) that\\nthe point is closest to and then assign it to c. As an example, consider the star\\nin Figure 14.3. It is located in the China region of the space and Rocchio\\ntherefore assigns it to China. We show the Rocchio algorithm in pseudocode\\nin Figure 14.4.\\n1. Recall from basic linear algebra that ⃗v · ⃗w = ⃗vT⃗w, i.e., the dot product of ⃗v and ⃗w equals the\\nproduct by matrix multiplication of the transpose of ⃗v and ⃗w.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n294\\n14\\nVector space classiﬁcation\\nterm weights\\nvector\\nChinese\\nJapan\\nTokyo\\nMacao\\nBeijing\\nShanghai\\n⃗d1\\n0\\n0\\n0\\n0\\n1.0\\n0\\n⃗d2\\n0\\n0\\n0\\n0\\n0\\n1.0\\n⃗d3\\n0\\n0\\n0\\n1.0\\n0\\n0\\n⃗d4\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n⃗d5\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n⃗µc\\n0\\n0\\n0\\n0.33\\n0.33\\n0.33\\n⃗µc\\n0\\n0.71\\n0.71\\n0\\n0\\n0\\n◮Table 14.1\\nVectors and class centroids for the data in Table 13.1.\\n\\x0f\\nExample 14.1:\\nTable 14.1 shows the tf-idf vector representations of the ﬁve docu-\\nments in Table 13.1 (page 261), using the formula (1+ log10 tft,d) log10(4/dft) if tft,d >\\n0 (Equation (6.14), page 127). The two class centroids are µc = 1/3 · (⃗d1 + ⃗d2 + ⃗d3)\\nand µc = 1/1 · (⃗d4).\\nThe distances of the test document from the centroids are\\n|µc −⃗d5| ≈1.15 and |µc −⃗d5| = 0.0. Thus, Rocchio assigns d5 to c.\\nThe separating hyperplane in this case has the following parameters:\\n⃗w\\n≈\\n(0 −0.71 −0.71 1/3 1/3 1/3)T\\nb\\n=\\n−1/3\\nSee Exercise 14.15 for how to compute ⃗w and b. We can easily verify that this hy-\\nperplane separates the documents as desired: ⃗wT⃗d1 ≈0 · 0 + −0.71 · 0 + −0.71 · 0 +\\n1/3 · 0 + 1/3 · 1.0 + 1/3 · 0 = 1/3 > b (and, similarly, ⃗wT⃗di > b for i = 2 and i = 3)\\nand ⃗wT⃗d4 = −1 < b. Thus, documents in c are above the hyperplane (⃗wT⃗d > b) and\\ndocuments in c are below the hyperplane (⃗wT⃗d < b).\\nThe assignment criterion in Figure 14.4 is Euclidean distance (APPLYROC-\\nCHIO, line 1). An alternative is cosine similarity:\\nAssign d to class c = arg max\\nc′\\ncos(⃗µ(c′),⃗v(d))\\nAs discussed in Section 14.1, the two assignment criteria will sometimes\\nmake different classiﬁcation decisions. We present the Euclidean distance\\nvariant of Rocchio classiﬁcation here because it emphasizes Rocchio’s close\\ncorrespondence to K-means clustering (Section 16.4, page 360).\\nRocchio classiﬁcation is a form of Rocchio relevance feedback (Section 9.1.1,\\npage 178). The average of the relevant documents, corresponding to the most\\nimportant component of the Rocchio vector in relevance feedback (Equa-\\ntion (9.3), page 182), is the centroid of the “class” of relevant documents.\\nWe omit the query component of the Rocchio formula in Rocchio classiﬁca-\\ntion since there is no query in text classiﬁcation. Rocchio classiﬁcation can be\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.2\\nRocchio classiﬁcation\\n295\\nTRAINROCCHIO(C, D)\\n1\\nfor each cj ∈C\\n2\\ndo Dj ←{d : ⟨d, cj⟩∈D}\\n3\\n⃗µj ←\\n1\\n|Dj| ∑d∈Dj ⃗v(d)\\n4\\nreturn {⃗µ1, . . . ,⃗µJ}\\nAPPLYROCCHIO({⃗µ1, . . . ,⃗µJ}, d)\\n1\\nreturn arg minj |⃗µj −⃗v(d)|\\n◮Figure 14.4\\nRocchio classiﬁcation: Training and testing.\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\na\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nX\\nX\\nA\\nB\\no\\n◮Figure 14.5\\nThe multimodal class “a” consists of two different clusters (small\\nupper circles centered on X’s).\\nRocchio classiﬁcation will misclassify “o” as “a”\\nbecause it is closer to the centroid A of the “a” class than to the centroid B of the “b”\\nclass.\\napplied to J > 2 classes whereas Rocchio relevance feedback is designed to\\ndistinguish only two classes, relevant and nonrelevant.\\nIn addition to respecting contiguity, the classes in Rocchio classiﬁcation\\nmust be approximate spheres with similar radii. In Figure 14.3, the solid\\nsquare just below the boundary between UK and Kenya is a better ﬁt for the\\nclass UK since UK is more scattered than Kenya. But Rocchio assigns it to\\nKenya because it ignores details of the distribution of points in a class and\\nonly uses distance from the centroid for classiﬁcation.\\nThe assumption of sphericity also does not hold in Figure 14.5. We can-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n296\\n14\\nVector space classiﬁcation\\nmode\\ntime complexity\\ntraining\\nΘ(|D|Lave + |C||V|)\\ntesting\\nΘ(La + |C|Ma) = Θ(|C|Ma)\\n◮Table 14.2\\nTraining and test times for Rocchio classiﬁcation.\\nLave is the average\\nnumber of tokens per document. La and Ma are the numbers of tokens and types,\\nrespectively, in the test document. Computing Euclidean distance between the class\\ncentroids and a document is Θ(|C|Ma).\\nnot represent the “a” class well with a single prototype because it has two\\nclusters. Rocchio often misclassiﬁes this type of multimodal class. A text clas-\\nMULTIMODAL CLASS\\nsiﬁcation example for multimodality is a country like Burma, which changed\\nits name to Myanmar in 1989. The two clusters before and after the name\\nchange need not be close to each other in space. We also encountered the\\nproblem of multimodality in relevance feedback (Section 9.1.2, page 184).\\nTwo-class classiﬁcation is another case where classes are rarely distributed\\nlike spheres with similar radii. Most two-class classiﬁers distinguish between\\na class like China that occupies a small region of the space and its widely\\nscattered complement. Assuming equal radii will result in a large number\\nof false positives. Most two-class classiﬁcation problems therefore require a\\nmodiﬁed decision rule of the form:\\nAssign d to class c iff |⃗µ(c) −⃗v(d)| < |⃗µ(c) −⃗v(d)| −b\\nfor a positive constant b. As in Rocchio relevance feedback, the centroid of\\nthe negative documents is often not used at all, so that the decision criterion\\nsimpliﬁes to |⃗µ(c) −⃗v(d)| < b′ for a positive constant b′.\\nTable 14.2 gives the time complexity of Rocchio classiﬁcation.2 Adding all\\ndocuments to their respective (unnormalized) centroid is Θ(|D|Lave) (as op-\\nposed to Θ(|D||V|)) since we need only consider non-zero entries. Dividing\\neach vector sum by the size of its class to compute the centroid is Θ(|V|).\\nOverall, training time is linear in the size of the collection (cf. Exercise 13.1).\\nThus, Rocchio classiﬁcation and Naive Bayes have the same linear training\\ntime complexity.\\nIn the next section, we will introduce another vector space classiﬁcation\\nmethod, kNN, that deals better with classes that have non-spherical, discon-\\nnected or other irregular shapes.\\n?\\nExercise 14.2\\n[⋆]\\nShow that Rocchio classiﬁcation can assign a label to a document that is different from\\nits training set label.\\n2. We write Θ(|D|Lave) for Θ(T) and assume that the length of test documents is bounded as\\nwe did on page 262.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.3\\nk nearest neighbor\\n297\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\nx\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋄\\n⋆\\n◮Figure 14.6\\nVoronoi tessellation and decision boundaries (double lines) in 1NN\\nclassiﬁcation. The three classes are: X, circle and diamond.\\n14.3\\nk nearest neighbor\\nUnlike Rocchio, k nearest neighbor or kNN classiﬁcation determines the deci-\\nk NEAREST NEIGHBOR\\nCLASSIFICATION\\nsion boundary locally. For 1NN we assign each document to the class of its\\nclosest neighbor. For kNN we assign each document to the majority class of\\nits k closest neighbors where k is a parameter. The rationale of kNN classiﬁ-\\ncation is that, based on the contiguity hypothesis, we expect a test document\\nd to have the same label as the training documents located in the local region\\nsurrounding d.\\nDecision boundaries in 1NN are concatenated segments of the Voronoi tes-\\nVORONOI\\nTESSELLATION\\nsellation as shown in Figure 14.6. The Voronoi tessellation of a set of objects\\ndecomposes space into Voronoi cells, where each object’s cell consists of all\\npoints that are closer to the object than to other objects. In our case, the ob-\\njects are documents. The Voronoi tessellation then partitions the plane into\\n|D| convex polygons, each containing its corresponding document (and no\\nother) as shown in Figure 14.6, where a convex polygon is a convex region in\\n2-dimensional space bounded by lines.\\nFor general k ∈N in kNN, consider the region in the space for which the\\nset of k nearest neighbors is the same. This again is a convex polygon and the\\nspace is partitioned into convex polygons, within each of which the set of k\\n', 'Online edition (c)\\n2009 Cambridge UP\\n298\\n14\\nVector space classiﬁcation\\nTRAIN-KNN(C, D)\\n1\\nD′ ←PREPROCESS(D)\\n2\\nk ←SELECT-K(C, D′)\\n3\\nreturn D′, k\\nAPPLY-KNN(C, D′, k, d)\\n1\\nSk ←COMPUTENEARESTNEIGHBORS(D′, k, d)\\n2\\nfor each cj ∈C\\n3\\ndo pj ←|Sk ∩cj|/k\\n4\\nreturn arg maxj pj\\n◮Figure 14.7\\nkNN training (with preprocessing) and testing. pj is an estimate for\\nP(cj|Sk) = P(cj|d). cj denotes the set of all documents in the class cj.\\nnearest neighbors is invariant (Exercise 14.11).3\\n1NN is not very robust. The classiﬁcation decision of each test document\\nrelies on the class of a single training document, which may be incorrectly\\nlabeled or atypical. kNN for k > 1 is more robust. It assigns documents to\\nthe majority class of their k closest neighbors, with ties broken randomly.\\nThere is a probabilistic version of this kNN classiﬁcation algorithm. We\\ncan estimate the probability of membership in class c as the proportion of the\\nk nearest neighbors in c. Figure 14.6 gives an example for k = 3. Probabil-\\nity estimates for class membership of the star are ˆP(circle class|star) = 1/3,\\nˆP(X class|star) = 2/3, and ˆP(diamond class|star) = 0. The 3nn estimate\\n( ˆP1(circle class|star) = 1/3) and the 1nn estimate ( ˆP1(circle class|star) = 1)\\ndiffer with 3nn preferring the X class and 1nn preferring the circle class .\\nThe parameter k in kNN is often chosen based on experience or knowledge\\nabout the classiﬁcation problem at hand. It is desirable for k to be odd to\\nmake ties less likely. k = 3 and k = 5 are common choices, but much larger\\nvalues between 50 and 100 are also used. An alternative way of setting the\\nparameter is to select the k that gives best results on a held-out portion of the\\ntraining set.\\nWe can also weight the “votes” of the k nearest neighbors by their cosine\\n3. The generalization of a polygon to higher dimensions is a polytope. A polytope is a region\\nin M-dimensional space bounded by (M −1)-dimensional hyperplanes. In M dimensions, the\\ndecision boundaries for kNN consist of segments of (M −1)-dimensional hyperplanes that form\\nthe Voronoi tessellation into convex polytopes for the training set of documents. The decision\\ncriterion of assigning a document to the majority class of its k nearest neighbors applies equally\\nto M = 2 (tessellation into polygons) and M > 2 (tessellation into polytopes).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.3\\nk nearest neighbor\\n299\\nkNN with preprocessing of training set\\ntraining\\nΘ(|D|Lave)\\ntesting\\nΘ(La + |D|MaveMa) = Θ(|D|MaveMa)\\nkNN without preprocessing of training set\\ntraining\\nΘ(1)\\ntesting\\nΘ(La + |D|LaveMa) = Θ(|D|LaveMa)\\n◮Table 14.3\\nTraining and test times for kNN classiﬁcation. Mave is the average size\\nof the vocabulary of documents in the collection.\\nsimilarity. In this scheme, a class’s score is computed as:\\nscore(c, d) =\\n∑\\nd′∈Sk(d)\\nIc(d′) cos(⃗v(d′),⃗v(d))\\nwhere Sk(d) is the set of d’s k nearest neighbors and Ic(d′) = 1 iff d′ is in class\\nc and 0 otherwise. We then assign the document to the class with the highest\\nscore. Weighting by similarities is often more accurate than simple voting.\\nFor example, if two classes have the same number of neighbors in the top k,\\nthe class with the more similar neighbors wins.\\nFigure 14.7 summarizes the kNN algorithm.\\n\\x0f\\nExample 14.2:\\nThe distances of the test document from the four training docu-\\nments in Table 14.1 are |⃗d1 −⃗d5| = |⃗d2 −⃗d5| = |⃗d3 −⃗d5| ≈1.41 and |⃗d4 −⃗d5| = 0.0.\\nd5’s nearest neighbor is therefore d4 and 1NN assigns d5 to d4’s class, c.\\n$\\n14.3.1\\nTime complexity and optimality of kNN\\nTable 14.3 gives the time complexity of kNN. kNN has properties that are\\nquite different from most other classiﬁcation algorithms. Training a kNN\\nclassiﬁer simply consists of determining k and preprocessing documents. In\\nfact, if we preselect a value for k and do not preprocess, then kNN requires\\nno training at all. In practice, we have to perform preprocessing steps like\\ntokenization. It makes more sense to preprocess training documents once\\nas part of the training phase rather than repeatedly every time we classify a\\nnew test document.\\nTest time is Θ(|D|MaveMa) for kNN. It is linear in the size of the training\\nset as we need to compute the distance of each training document from the\\ntest document. Test time is independent of the number of classes J. kNN\\ntherefore has a potential advantage for problems with large J.\\nIn kNN classiﬁcation, we do not perform any estimation of parameters as\\nwe do in Rocchio classiﬁcation (centroids) or in Naive Bayes (priors and con-\\nditional probabilities). kNN simply memorizes all examples in the training\\n', 'Online edition (c)\\n2009 Cambridge UP\\n300\\n14\\nVector space classiﬁcation\\nset and then compares the test document to them. For this reason, kNN is\\nalso called memory-based learning or instance-based learning. It is usually desir-\\nMEMORY-BASED\\nLEARNING\\nable to have as much training data as possible in machine learning. But in\\nkNN large training sets come with a severe efﬁciency penalty in classiﬁca-\\ntion.\\nCan kNN testing be made more efﬁcient than Θ(|D|MaveMa) or, ignoring\\nthe length of documents, more efﬁcient than Θ(|D|)? There are fast kNN\\nalgorithms for small dimensionality M (Exercise 14.12). There are also ap-\\nproximations for large M that give error bounds for speciﬁc efﬁciency gains\\n(see Section 14.7). These approximations have not been extensively tested\\nfor text classiﬁcation applications, so it is not clear whether they can achieve\\nmuch better efﬁciency than Θ(|D|) without a signiﬁcant loss of accuracy.\\nThe reader may have noticed the similarity between the problem of ﬁnding\\nnearest neighbors of a test document and ad hoc retrieval, where we search\\nfor the documents with the highest similarity to the query (Section 6.3.2,\\npage 123). In fact, the two problems are both k nearest neighbor problems\\nand only differ in the relative density of (the vector of) the test document\\nin kNN (10s or 100s of non-zero entries) versus the sparseness of (the vec-\\ntor of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries).\\nWe introduced the inverted index for efﬁcient ad hoc retrieval in Section 1.1\\n(page 6). Is the inverted index also the solution for efﬁcient kNN?\\nAn inverted index restricts a search to those documents that have at least\\none term in common with the query. Thus in the context of kNN, the in-\\nverted index will be efﬁcient if the test document has no term overlap with a\\nlarge number of training documents. Whether this is the case depends on the\\nclassiﬁcation problem. If documents are long and no stop list is used, then\\nless time will be saved. But with short documents and a large stop list, an\\ninverted index may well cut the average test time by a factor of 10 or more.\\nThe search time in an inverted index is a function of the length of the post-\\nings lists of the terms in the query. Postings lists grow sublinearly with the\\nlength of the collection since the vocabulary increases according to Heaps’\\nlaw – if the probability of occurrence of some terms increases, then the prob-\\nability of occurrence of others must decrease. However, most new terms are\\ninfrequent. We therefore take the complexity of inverted index search to be\\nΘ(T) (as discussed in Section 2.4.2, page 41) and, assuming average docu-\\nment length does not change over time, Θ(T) = Θ(|D|).\\nAs we will see in the next chapter, kNN’s effectiveness is close to that of the\\nmost accurate learning methods in text classiﬁcation (Table 15.2, page 334). A\\nmeasure of the quality of a learning method is its Bayes error rate, the average\\nBAYES ERROR RATE\\nerror rate of classiﬁers learned by it for a particular problem. kNN is not\\noptimal for problems with a non-zero Bayes error rate – that is, for problems\\nwhere even the best possible classiﬁer has a non-zero classiﬁcation error. The\\nerror of 1NN is asymptotically (as the training set increases) bounded by\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n301\\n◮Figure 14.8\\nThere are an inﬁnite number of hyperplanes that separate two linearly\\nseparable classes.\\ntwice the Bayes error rate. That is, if the optimal classiﬁer has an error rate\\nof x, then 1NN has an asymptotic error rate of less than 2x. This is due to the\\neffect of noise – we already saw one example of noise in the form of noisy\\nfeatures in Section 13.5 (page 271), but noise can also take other forms as we\\nwill discuss in the next section. Noise affects two components of kNN: the\\ntest document and the closest training document. The two sources of noise\\nare additive, so the overall error of 1NN is twice the optimal error rate. For\\nproblems with Bayes error rate 0, the error rate of 1NN will approach 0 as\\nthe size of the training set increases.\\n?\\nExercise 14.3\\nExplain why kNN handles multimodal classes better than Rocchio.\\n14.4\\nLinear versus nonlinear classiﬁers\\nIn this section, we show that the two learning methods Naive Bayes and\\nRocchio are instances of linear classiﬁers, the perhaps most important group\\nof text classiﬁers, and contrast them with nonlinear classiﬁers. To simplify\\nthe discussion, we will only consider two-class classiﬁers in this section and\\ndeﬁne a linear classiﬁer as a two-class classiﬁer that decides class membership\\nLINEAR CLASSIFIER\\nby comparing a linear combination of the features to a threshold.\\nIn two dimensions, a linear classiﬁer is a line. Five examples are shown\\nin Figure 14.8. These lines have the functional form w1x1 + w2x2 = b. The\\nclassiﬁcation rule of a linear classiﬁer is to assign a document to c if w1x1 +\\nw2x2 > b and to c if w1x1 + w2x2 ≤b. Here, (x1, x2)T is the two-dimensional\\nvector representation of the document and (w1, w2)T is the parameter vector\\n', 'Online edition (c)\\n2009 Cambridge UP\\n302\\n14\\nVector space classiﬁcation\\nAPPLYLINEARCLASSIFIER(⃗w, b,⃗x)\\n1\\nscore ←∑M\\ni=1 wixi\\n2\\nif score > b\\n3\\nthen return 1\\n4\\nelse return 0\\n◮Figure 14.9\\nLinear classiﬁcation algorithm.\\nthat deﬁnes (together with b) the decision boundary. An alternative geomet-\\nric interpretation of a linear classiﬁer is provided in Figure 15.7 (page 343).\\nWe can generalize this 2D linear classiﬁer to higher dimensions by deﬁning\\na hyperplane as we did in Equation (14.2), repeated here as Equation (14.3):\\n⃗wT⃗x = b\\n(14.3)\\nThe assignment criterion then is: assign to c if ⃗wT⃗x > b and to c if ⃗wT⃗x ≤b.\\nWe call a hyperplane that we use as a linear classiﬁer a decision hyperplane.\\nDECISION HYPERPLANE\\nThe corresponding algorithm for linear classiﬁcation in M dimensions is\\nshown in Figure 14.9. Linear classiﬁcation at ﬁrst seems trivial given the\\nsimplicity of this algorithm. However, the difﬁculty is in training the lin-\\near classiﬁer, that is, in determining the parameters ⃗w and b based on the\\ntraining set. In general, some learning methods compute much better param-\\neters than others where our criterion for evaluating the quality of a learning\\nmethod is the effectiveness of the learned linear classiﬁer on new data.\\nWe now show that Rocchio and Naive Bayes are linear classiﬁers. To see\\nthis for Rocchio, observe that a vector ⃗x is on the decision boundary if it has\\nequal distance to the two class centroids:\\n|⃗µ(c1) −⃗x| = |⃗µ(c2) −⃗x|\\n(14.4)\\nSome basic arithmetic shows that this corresponds to a linear classiﬁer with\\nnormal vector ⃗w = ⃗µ(c1) −⃗µ(c2) and b = 0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2) (Exer-\\ncise 14.15).\\nWe can derive the linearity of Naive Bayes from its decision rule, which\\nchooses the category c with the largest ˆP(c|d) (Figure 13.2, page 260) where:\\nˆP(c|d) ∝ˆP(c) ∏\\n1≤k≤nd\\nˆP(tk|c)\\nand nd is the number of tokens in the document that are part of the vocabu-\\nlary. Denoting the complement category as ¯c, we obtain for the log odds:\\nlog\\nˆP(c|d)\\nˆP(¯c|d) = log\\nˆP(c)\\nˆP(¯c) + ∑\\n1≤k≤nd\\nlog\\nˆP(tk|c)\\nˆP(tk|¯c)\\n(14.5)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n303\\nti\\nwi\\nd1i\\nd2i\\nti\\nwi\\nd1i\\nd2i\\nprime\\n0.70\\n0\\n1\\ndlrs\\n-0.71\\n1\\n1\\nrate\\n0.67\\n1\\n0\\nworld\\n-0.35\\n1\\n0\\ninterest\\n0.63\\n0\\n0\\nsees\\n-0.33\\n0\\n0\\nrates\\n0.60\\n0\\n0\\nyear\\n-0.25\\n0\\n0\\ndiscount\\n0.46\\n1\\n0\\ngroup\\n-0.24\\n0\\n0\\nbundesbank\\n0.43\\n0\\n0\\ndlr\\n-0.24\\n0\\n0\\n◮Table 14.4\\nA linear classiﬁer. The dimensions ti and parameters wi of a linear\\nclassiﬁer for the class interest (as in interest rate) in Reuters-21578. The threshold is\\nb = 0. Terms like dlr and world have negative weights because they are indicators for\\nthe competing class currency.\\nWe choose class c if the odds are greater than 1 or, equivalently, if the log\\nodds are greater than 0. It is easy to see that Equation (14.5) is an instance\\nof Equation (14.3) for wi = log[ ˆP(ti|c)/ ˆP(ti|¯c)], xi = number of occurrences\\nof ti in d, and b = −log[ ˆP(c)/ ˆP(¯c)]. Here, the index i, 1 ≤i ≤M, refers\\nto terms of the vocabulary (not to positions in d as k does; cf. Section 13.4.1,\\npage 270) and ⃗x and ⃗w are M-dimensional vectors. So in log space, Naive\\nBayes is a linear classiﬁer.\\n\\x0f\\nExample 14.3:\\nTable 14.4 deﬁnes a linear classiﬁer for the category interest in\\nReuters-21578 (see Section 13.6, page 279). We assign document ⃗d1 “rate discount\\ndlrs world” to interest since ⃗wT⃗d1 = 0.67 · 1 + 0.46 · 1 + (−0.71) · 1 + (−0.35) · 1 =\\n0.07 > 0 = b. We assign ⃗d2 “prime dlrs” to the complement class (not in interest) since\\n⃗wT⃗d2 = −0.01 ≤b. For simplicity, we assume a simple binary vector representation\\nin this example: 1 for occurring terms, 0 for non-occurring terms.\\nFigure 14.10 is a graphical example of a linear problem, which we deﬁne to\\nmean that the underlying distributions P(d|c) and P(d|c) of the two classes\\nare separated by a line. We call this separating line the class boundary. It is\\nCLASS BOUNDARY\\nthe “true” boundary of the two classes and we distinguish it from the deci-\\nsion boundary that the learning method computes to approximate the class\\nboundary.\\nAs is typical in text classiﬁcation, there are some noise documents in Fig-\\nNOISE DOCUMENT\\nure 14.10 (marked with arrows) that do not ﬁt well into the overall distri-\\nbution of the classes. In Section 13.5 (page 271), we deﬁned a noise feature\\nas a misleading feature that, when included in the document representation,\\non average increases the classiﬁcation error. Analogously, a noise document\\nis a document that, when included in the training set, misleads the learn-\\ning method and increases classiﬁcation error.\\nIntuitively, the underlying\\ndistribution partitions the representation space into areas with mostly ho-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n304\\n14\\nVector space classiﬁcation\\n◮Figure 14.10\\nA linear problem with noise. In this hypothetical web page classiﬁ-\\ncation scenario, Chinese-only web pages are solid circles and mixed Chinese-English\\nweb pages are squares. The two classes are separated by a linear class boundary\\n(dashed line, short dashes), except for three noise documents (marked with arrows).\\nmogeneous class assignments. A document that does not conform with the\\ndominant class in its area is a noise document.\\nNoise documents are one reason why training a linear classiﬁer is hard. If\\nwe pay too much attention to noise documents when choosing the decision\\nhyperplane of the classiﬁer, then it will be inaccurate on new data. More\\nfundamentally, it is usually difﬁcult to determine which documents are noise\\ndocuments and therefore potentially misleading.\\nIf there exists a hyperplane that perfectly separates the two classes, then\\nwe call the two classes linearly separable. In fact, if linear separability holds,\\nLINEAR SEPARABILITY\\nthen there is an inﬁnite number of linear separators (Exercise 14.4) as illus-\\ntrated by Figure 14.8, where the number of possible separating hyperplanes\\nis inﬁnite.\\nFigure 14.8 illustrates another challenge in training a linear classiﬁer. If we\\nare dealing with a linearly separable problem, then we need a criterion for\\nselecting among all decision hyperplanes that perfectly separate the training\\ndata. In general, some of these hyperplanes will do well on new data, some\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.4\\nLinear versus nonlinear classiﬁers\\n305\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n◮Figure 14.11\\nA nonlinear problem.\\nwill not.\\nAn example of a nonlinear classiﬁer is kNN. The nonlinearity of kNN is\\nNONLINEAR\\nCLASSIFIER\\nintuitively clear when looking at examples like Figure 14.6. The decision\\nboundaries of kNN (the double lines in Figure 14.6) are locally linear seg-\\nments, but in general have a complex shape that is not equivalent to a line in\\n2D or a hyperplane in higher dimensions.\\nFigure 14.11 is another example of a nonlinear problem: there is no good\\nlinear separator between the distributions P(d|c) and P(d|c) because of the\\ncircular “enclave” in the upper left part of the graph. Linear classiﬁers mis-\\nclassify the enclave, whereas a nonlinear classiﬁer like kNN will be highly\\naccurate for this type of problem if the training set is large enough.\\nIf a problem is nonlinear and its class boundaries cannot be approximated\\nwell with linear hyperplanes, then nonlinear classiﬁers are often more accu-\\nrate than linear classiﬁers. If a problem is linear, it is best to use a simpler\\nlinear classiﬁer.\\n?\\nExercise 14.4\\nProve that the number of linear separators of two classes is either inﬁnite or zero.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n306\\n14\\nVector space classiﬁcation\\n14.5\\nClassiﬁcation with more than two classes\\nWe can extend two-class linear classiﬁers to J > 2 classes. The method to use\\ndepends on whether the classes are mutually exclusive or not.\\nClassiﬁcation for classes that are not mutually exclusive is called any-of,\\nANY-OF\\nCLASSIFICATION\\nmultilabel, or multivalue classiﬁcation. In this case, a document can belong to\\nseveral classes simultaneously, or to a single class, or to none of the classes.\\nA decision on one class leaves all options open for the others. It is some-\\ntimes said that the classes are independent of each other, but this is misleading\\nsince the classes are rarely statistically independent in the sense deﬁned on\\npage 275. In terms of the formal deﬁnition of the classiﬁcation problem in\\nEquation (13.1) (page 256), we learn J different classiﬁers γj in any-of classi-\\nﬁcation, each returning either cj or cj: γj(d) ∈{cj, cj}.\\nSolving an any-of classiﬁcation task with linear classiﬁers is straightfor-\\nward:\\n1. Build a classiﬁer for each class, where the training set consists of the set\\nof documents in the class (positive labels) and its complement (negative\\nlabels).\\n2. Given the test document, apply each classiﬁer separately. The decision of\\none classiﬁer has no inﬂuence on the decisions of the other classiﬁers.\\nThe second type of classiﬁcation with more than two classes is one-of clas-\\nONE-OF\\nCLASSIFICATION\\nsiﬁcation. Here, the classes are mutually exclusive. Each document must\\nbelong to exactly one of the classes. One-of classiﬁcation is also called multi-\\nnomial, polytomous4, multiclass, or single-label classiﬁcation. Formally, there is a\\nsingle classiﬁcation function γ in one-of classiﬁcation whose range is C, i.e.,\\nγ(d) ∈{c1, . . . , cJ}. kNN is a (nonlinear) one-of classiﬁer.\\nTrue one-of problems are less common in text classiﬁcation than any-of\\nproblems. With classes like UK, China, poultry, or coffee, a document can be\\nrelevant to many topics simultaneously – as when the prime minister of the\\nUK visits China to talk about the coffee and poultry trade.\\nNevertheless, we will often make a one-of assumption, as we did in Fig-\\nure 14.1, even if classes are not really mutually exclusive. For the classiﬁca-\\ntion problem of identifying the language of a document, the one-of assump-\\ntion is a good approximation as most text is written in only one language.\\nIn such cases, imposing a one-of constraint can increase the classiﬁer’s ef-\\nfectiveness because errors that are due to the fact that the any-of classiﬁers\\nassigned a document to either no class or more than one class are eliminated.\\nJ hyperplanes do not divide R|V| into J distinct regions as illustrated in\\nFigure 14.12. Thus, we must use a combination method when using two-\\nclass linear classiﬁers for one-of classiﬁcation. The simplest method is to\\n4. A synonym of polytomous is polychotomous.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.5\\nClassiﬁcation with more than two classes\\n307\\n?\\n◮Figure 14.12\\nJ hyperplanes do not divide space into J disjoint regions.\\nrank classes and then select the top-ranked class. Geometrically, the ranking\\ncan be with respect to the distances from the J linear separators. Documents\\nclose to a class’s separator are more likely to be misclassiﬁed, so the greater\\nthe distance from the separator, the more plausible it is that a positive clas-\\nsiﬁcation decision is correct. Alternatively, we can use a direct measure of\\nconﬁdence to rank classes, e.g., probability of class membership. We can\\nstate this algorithm for one-of classiﬁcation with linear classiﬁers as follows:\\n1. Build a classiﬁer for each class, where the training set consists of the set\\nof documents in the class (positive labels) and its complement (negative\\nlabels).\\n2. Given the test document, apply each classiﬁer separately.\\n3. Assign the document to the class with\\n• the maximum score,\\n• the maximum conﬁdence value,\\n• or the maximum probability.\\nAn important tool for analyzing the performance of a classiﬁer for J > 2\\nclasses is the confusion matrix. The confusion matrix shows for each pair of\\nCONFUSION MATRIX\\nclasses ⟨c1, c2⟩, how many documents from c1 were incorrectly assigned to c2.\\nIn Table 14.5, the classiﬁer manages to distinguish the three ﬁnancial classes\\nmoney-fx, trade, and interest from the three agricultural classes wheat, corn,\\nand grain, but makes many errors within these two groups. The confusion\\nmatrix can help pinpoint opportunities for improving the accuracy of the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n308\\n14\\nVector space classiﬁcation\\nassigned class\\nmoney-fx\\ntrade\\ninterest\\nwheat\\ncorn\\ngrain\\ntrue class\\nmoney-fx\\n95\\n0\\n10\\n0\\n0\\n0\\ntrade\\n1\\n1\\n90\\n0\\n1\\n0\\ninterest\\n13\\n0\\n0\\n0\\n0\\n0\\nwheat\\n0\\n0\\n1\\n34\\n3\\n7\\ncorn\\n1\\n0\\n2\\n13\\n26\\n5\\ngrain\\n0\\n0\\n2\\n14\\n5\\n10\\n◮Table 14.5\\nA confusion matrix for Reuters-21578. For example, 14 documents\\nfrom grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006).\\nsystem. For example, to address the second largest error in Table 14.5 (14 in\\nthe row grain), one could attempt to introduce features that distinguish wheat\\ndocuments from grain documents.\\n?\\nExercise 14.5\\nCreate a training set of 300 documents, 100 each from three different languages (e.g.,\\nEnglish, French, Spanish). Create a test set by the same procedure, but also add 100\\ndocuments from a fourth language. Train (i) a one-of classiﬁer (ii) an any-of classi-\\nﬁer on this training set and evaluate it on the test set. (iii) Are there any interesting\\ndifferences in how the two classiﬁers behave on this task?\\n$\\n14.6\\nThe bias-variance tradeoff\\nNonlinear classiﬁers are more powerful than linear classiﬁers.\\nFor some\\nproblems, there exists a nonlinear classiﬁer with zero classiﬁcation error, but\\nno such linear classiﬁer. Does that mean that we should always use nonlinear\\nclassiﬁers for optimal effectiveness in statistical text classiﬁcation?\\nTo answer this question, we introduce the bias-variance tradeoff in this sec-\\ntion, one of the most important concepts in machine learning. The tradeoff\\nhelps explain why there is no universally optimal learning method. Selecting\\nan appropriate learning method is therefore an unavoidable part of solving\\na text classiﬁcation problem.\\nThroughout this section, we use linear and nonlinear classiﬁers as proto-\\ntypical examples of “less powerful” and “more powerful” learning, respec-\\ntively. This is a simpliﬁcation for a number of reasons. First, many nonlinear\\nmodels subsume linear models as a special case. For instance, a nonlinear\\nlearning method like kNN will in some cases produce a linear classiﬁer. Sec-\\nond, there are nonlinear models that are less complex than linear models.\\nFor instance, a quadratic polynomial with two parameters is less powerful\\nthan a 10,000-dimensional linear classiﬁer. Third, the complexity of learn-\\ning is not really a property of the classiﬁer because there are many aspects\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n309\\nof learning (such as feature selection, cf. (Section 13.5, page 271), regulariza-\\ntion, and constraints such as margin maximization in Chapter 15) that make\\na learning method either more powerful or less powerful without affecting\\nthe type of classiﬁer that is the ﬁnal result of learning – regardless of whether\\nthat classiﬁer is linear or nonlinear. We refer the reader to the publications\\nlisted in Section 14.7 for a treatment of the bias-variance tradeoff that takes\\ninto account these complexities. In this section, linear and nonlinear classi-\\nﬁers will simply serve as proxies for weaker and stronger learning methods\\nin text classiﬁcation.\\nWe ﬁrst need to state our objective in text classiﬁcation more precisely. In\\nSection 13.1 (page 256), we said that we want to minimize classiﬁcation er-\\nror on the test set. The implicit assumption was that training documents\\nand test documents are generated according to the same underlying distri-\\nbution. We will denote this distribution P(⟨d, c⟩) where d is the document\\nand c its label or class. Figures 13.4 and 13.5 were examples of generative\\nmodels that decompose P(⟨d, c⟩) into the product of P(c) and P(d|c). Fig-\\nures 14.10 and 14.11 depict generative models for ⟨d, c⟩with d ∈R2 and\\nc ∈{square, solid circle}.\\nIn this section, instead of using the number of correctly classiﬁed test doc-\\numents (or, equivalently, the error rate on test documents) as evaluation\\nmeasure, we adopt an evaluation measure that addresses the inherent un-\\ncertainty of labeling. In many text classiﬁcation problems, a given document\\nrepresentation can arise from documents belonging to different classes. This\\nis because documents from different classes can be mapped to the same doc-\\nument representation. For example, the one-sentence documents China sues\\nFrance and France sues China are mapped to the same document representa-\\ntion d′ = {China, France, sues} in a bag of words model. But only the latter\\ndocument is relevant to the class c′ = legal actions brought by France (which\\nmight be deﬁned, for example, as a standing query by an international trade\\nlawyer).\\nTo simplify the calculations in this section, we do not count the number\\nof errors on the test set when evaluating a classiﬁer, but instead look at how\\nwell the classiﬁer estimates the conditional probability P(c|d) of a document\\nbeing in a class. In the above example, we might have P(c′|d′) = 0.5.\\nOur goal in text classiﬁcation then is to ﬁnd a classiﬁer γ such that, aver-\\naged over documents d, γ(d) is as close as possible to the true probability\\nP(c|d). We measure this using mean squared error:\\nMSE(γ) = Ed[γ(d) −P(c|d)]2\\n(14.6)\\nwhere Ed is the expectation with respect to P(d). The mean squared error\\nterm gives partial credit for decisions by γ that are close if not completely\\nright.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n310\\n14\\nVector space classiﬁcation\\nE[x −α]2\\n=\\nEx2 −2Exα + α2\\n(14.8)\\n=\\n(Ex)2 −2Exα + α2\\n+Ex2 −2(Ex)2 + (Ex)2\\n=\\n[Ex −α]2\\n+Ex2 −E2x(Ex) + E(Ex)2\\n=\\n[Ex −α]2 + E[x −Ex]2\\nEDEd[ΓD(d) −P(c|d)]2\\n=\\nEdED[ΓD(d) −P(c|d)]2\\n(14.9)\\n=\\nEd[ [EDΓD(d) −P(c|d)]2\\n+ED[ΓD(d) −EDΓD(d)]2 ]\\n◮Figure 14.13\\nArithmetic transformations for the bias-variance decomposition.\\nFor the derivation of Equation (14.9), we set α = P(c|d) and x = ΓD(d) in Equa-\\ntion (14.8).\\nWe deﬁne a classiﬁer γ to be optimal for a distribution P(⟨d, c⟩) if it mini-\\nOPTIMAL CLASSIFIER\\nmizes MSE(γ).\\nMinimizing MSE is a desideratum for classiﬁers. We also need a criterion\\nfor learning methods. Recall that we deﬁned a learning method Γ as a function\\nthat takes a labeled training set D as input and returns a classiﬁer γ.\\nFor learning methods, we adopt as our goal to ﬁnd a Γ that, averaged over\\ntraining sets, learns classiﬁers γ with minimal MSE. We can formalize this as\\nminimizing learning error:\\nLEARNING ERROR\\nlearning-error(Γ) = ED[MSE(Γ(D))]\\n(14.7)\\nwhere ED is the expectation over labeled training sets. To keep things simple,\\nwe can assume that training sets have a ﬁxed size – the distribution P(⟨d, c⟩)\\nthen deﬁnes a distribution P(D) over training sets.\\nWe can use learning error as a criterion for selecting a learning method in\\nstatistical text classiﬁcation. A learning method Γ is optimal for a distribution\\nOPTIMAL LEARNING\\nMETHOD\\nP(D) if it minimizes the learning error.\\nWriting ΓD for Γ(D) for better readability, we can transform Equation (14.7)\\nas follows:\\nlearning-error(Γ)\\n=\\nED[MSE(ΓD)]\\n=\\nEDEd[ΓD(d) −P(c|d)]2\\n(14.10)\\n=\\nEd[bias(Γ, d) + variance(Γ, d)]\\n(14.11)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n311\\nbias(Γ, d)\\n=\\n[P(c|d) −EDΓD(d)]2\\n(14.12)\\nvariance(Γ, d)\\n=\\nED[ΓD(d) −EDΓD(d)]2\\n(14.13)\\nwhere the equivalence between Equations (14.10) and (14.11) is shown in\\nEquation (14.9) in Figure 14.13. Note that d and D are independent of each\\nother. In general, for a random document d and a random training set D, D\\ndoes not contain a labeled instance of d.\\nBias is the squared difference between P(c|d), the true conditional prob-\\nBIAS\\nability of d being in c, and ΓD(d), the prediction of the learned classiﬁer,\\naveraged over training sets. Bias is large if the learning method produces\\nclassiﬁers that are consistently wrong. Bias is small if (i) the classiﬁers are\\nconsistently right or (ii) different training sets cause errors on different docu-\\nments or (iii) different training sets cause positive and negative errors on the\\nsame documents, but that average out to close to 0. If one of these three con-\\nditions holds, then EDΓD(d), the expectation over all training sets, is close to\\nP(c|d).\\nLinear methods like Rocchio and Naive Bayes have a high bias for non-\\nlinear problems because they can only model one type of class boundary, a\\nlinear hyperplane. If the generative model P(⟨d, c⟩) has a complex nonlinear\\nclass boundary, the bias term in Equation (14.11) will be high because a large\\nnumber of points will be consistently misclassiﬁed. For example, the circular\\nenclave in Figure 14.11 does not ﬁt a linear model and will be misclassiﬁed\\nconsistently by linear classiﬁers.\\nWe can think of bias as resulting from our domain knowledge (or lack\\nthereof) that we build into the classiﬁer. If we know that the true boundary\\nbetween the two classes is linear, then a learning method that produces linear\\nclassiﬁers is more likely to succeed than a nonlinear method. But if the true\\nclass boundary is not linear and we incorrectly bias the classiﬁer to be linear,\\nthen classiﬁcation accuracy will be low on average.\\nNonlinear methods like kNN have low bias. We can see in Figure 14.6 that\\nthe decision boundaries of kNN are variable – depending on the distribu-\\ntion of documents in the training set, learned decision boundaries can vary\\ngreatly. As a result, each document has a chance of being classiﬁed correctly\\nfor some training sets. The average prediction EDΓD(d) is therefore closer to\\nP(c|d) and bias is smaller than for a linear learning method.\\nVariance is the variation of the prediction of learned classiﬁers: the aver-\\nVARIANCE\\nage squared difference between ΓD(d) and its average EDΓD(d). Variance is\\nlarge if different training sets D give rise to very different classiﬁers ΓD. It is\\nsmall if the training set has a minor effect on the classiﬁcation decisions ΓD\\nmakes, be they correct or incorrect. Variance measures how inconsistent the\\ndecisions are, not whether they are correct or incorrect.\\nLinear learning methods have low variance because most randomly drawn\\ntraining sets produce similar decision hyperplanes. The decision lines pro-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n312\\n14\\nVector space classiﬁcation\\nduced by linear learning methods in Figures 14.10 and 14.11 will deviate\\nslightly from the main class boundaries, depending on the training set, but\\nthe class assignment for the vast majority of documents (with the exception\\nof those close to the main boundary) will not be affected. The circular enclave\\nin Figure 14.11 will be consistently misclassiﬁed.\\nNonlinear methods like kNN have high variance. It is apparent from Fig-\\nure 14.6 that kNN can model very complex boundaries between two classes.\\nIt is therefore sensitive to noise documents of the sort depicted in Figure 14.10.\\nAs a result the variance term in Equation (14.11) is large for kNN: Test doc-\\numents are sometimes misclassiﬁed – if they happen to be close to a noise\\ndocument in the training set – and sometimes correctly classiﬁed – if there\\nare no noise documents in the training set near them. This results in high\\nvariation from training set to training set.\\nHigh-variance learning methods are prone to overﬁtting the training data.\\nOVERFITTING\\nThe goal in classiﬁcation is to ﬁt the training data to the extent that we cap-\\nture true properties of the underlying distribution P(⟨d, c⟩). In overﬁtting,\\nthe learning method also learns from noise. Overﬁtting increases MSE and\\nfrequently is a problem for high-variance learning methods.\\nWe can also think of variance as the model complexity or, equivalently, mem-\\nMEMORY CAPACITY\\nory capacity of the learning method – how detailed a characterization of the\\ntraining set it can remember and then apply to new data. This capacity corre-\\nsponds to the number of independent parameters available to ﬁt the training\\nset. Each kNN neighborhood Sk makes an independent classiﬁcation deci-\\nsion. The parameter in this case is the estimate ˆP(c|Sk) from Figure 14.7.\\nThus, kNN’s capacity is only limited by the size of the training set. It can\\nmemorize arbitrarily large training sets. In contrast, the number of parame-\\nters of Rocchio is ﬁxed – J parameters per dimension, one for each centroid\\n– and independent of the size of the training set. The Rocchio classiﬁer (in\\nform of the centroids deﬁning it) cannot “remember” ﬁne-grained details of\\nthe distribution of the documents in the training set.\\nAccording to Equation (14.7), our goal in selecting a learning method is to\\nminimize learning error. The fundamental insight captured by Equation (14.11),\\nwhich we can succinctly state as: learning-error = bias + variance, is that the\\nlearning error has two components, bias and variance, which in general can-\\nnot be minimized simultaneously. When comparing two learning methods\\nΓ1 and Γ2, in most cases the comparison comes down to one method having\\nhigher bias and lower variance and the other lower bias and higher variance.\\nThe decision for one learning method vs. another is then not simply a mat-\\nter of selecting the one that reliably produces good classiﬁers across training\\nsets (small variance) or the one that can learn classiﬁcation problems with\\nvery difﬁcult decision boundaries (small bias). Instead, we have to weigh\\nthe respective merits of bias and variance in our application and choose ac-\\ncordingly. This tradeoff is called the bias-variance tradeoff.\\nBIAS-VARIANCE\\nTRADEOFF\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.6\\nThe bias-variance tradeoff\\n313\\nFigure 14.10 provides an illustration, which is somewhat contrived, but\\nwill be useful as an example for the tradeoff. Some Chinese text contains\\nEnglish words written in the Roman alphabet like CPU, ONLINE, and GPS.\\nConsider the task of distinguishing Chinese-only web pages from mixed\\nChinese-English web pages. A search engine might offer Chinese users with-\\nout knowledge of English (but who understand loanwords like CPU) the op-\\ntion of ﬁltering out mixed pages. We use two features for this classiﬁcation\\ntask: number of Roman alphabet characters and number of Chinese char-\\nacters on the web page. As stated earlier, the distribution P(⟨d, c⟩) of the\\ngenerative model generates most mixed (respectively, Chinese) documents\\nabove (respectively, below) the short-dashed line, but there are a few noise\\ndocuments.\\nIn Figure 14.10, we see three classiﬁers:\\n• One-feature classiﬁer. Shown as a dotted horizontal line. This classiﬁer\\nuses only one feature, the number of Roman alphabet characters. Assum-\\ning a learning method that minimizes the number of misclassiﬁcations\\nin the training set, the position of the horizontal decision boundary is\\nnot greatly affected by differences in the training set (e.g., noise docu-\\nments). So a learning method producing this type of classiﬁer has low\\nvariance. But its bias is high since it will consistently misclassify squares\\nin the lower left corner and “solid circle” documents with more than 50\\nRoman characters.\\n• Linear classiﬁer. Shown as a dashed line with long dashes. Learning lin-\\near classiﬁers has less bias since only noise documents and possibly a few\\ndocuments close to the boundary between the two classes are misclassi-\\nﬁed. The variance is higher than for the one-feature classiﬁers, but still\\nsmall: The dashed line with long dashes deviates only slightly from the\\ntrue boundary between the two classes, and so will almost all linear de-\\ncision boundaries learned from training sets. Thus, very few documents\\n(documents close to the class boundary) will be inconsistently classiﬁed.\\n• “Fit-training-set-perfectly” classiﬁer. Shown as a solid line. Here, the\\nlearning method constructs a decision boundary that perfectly separates\\nthe classes in the training set. This method has the lowest bias because\\nthere is no document that is consistently misclassiﬁed – the classiﬁers\\nsometimes even get noise documents in the test set right. But the variance\\nof this learning method is high. Because noise documents can move the\\ndecision boundary arbitrarily, test documents close to noise documents\\nin the training set will be misclassiﬁed – something that a linear learning\\nmethod is unlikely to do.\\nIt is perhaps surprising that so many of the best-known text classiﬁcation\\nalgorithms are linear. Some of these methods, in particular linear SVMs, reg-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n314\\n14\\nVector space classiﬁcation\\nularized logistic regression and regularized linear regression, are among the\\nmost effective known methods. The bias-variance tradeoff provides insight\\ninto their success. Typical classes in text classiﬁcation are complex and seem\\nunlikely to be modeled well linearly. However, this intuition is misleading\\nfor the high-dimensional spaces that we typically encounter in text appli-\\ncations. With increased dimensionality, the likelihood of linear separability\\nincreases rapidly (Exercise 14.17). Thus, linear models in high-dimensional\\nspaces are quite powerful despite their linearity. Even more powerful nonlin-\\near learning methods can model decision boundaries that are more complex\\nthan a hyperplane, but they are also more sensitive to noise in the training\\ndata. Nonlinear learning methods sometimes perform better if the training\\nset is large, but by no means in all cases.\\n14.7\\nReferences and further reading\\nAs discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio\\n(1971). Joachims (1997) presents a probabilistic analysis of the method. Roc-\\nchio classiﬁcation was widely used as a classiﬁcation method in TREC in the\\n1990s (Buckley et al. 1994a;b, Voorhees and Harman 2005). Initially, it was\\nused as a form of routing. Routing merely ranks documents according to rel-\\nROUTING\\nevance to a class without assigning them. Early work on ﬁltering, a true clas-\\nFILTERING\\nsiﬁcation approach that makes an assignment decision on each document,\\nwas published by Ittner et al. (1995) and Schapire et al. (1998). The deﬁnition\\nof routing we use here should not be confused with another sense. Routing\\ncan also refer to the electronic distribution of documents to subscribers, the\\nso-called push model of document distribution. In a pull model, each transfer\\nPUSH MODEL\\nPULL MODEL\\nof a document to the user is initiated by the user – for example, by means\\nof search or by selecting it from a list of documents on a news aggregation\\nwebsite.\\nSome authors restrict the name Roccchio classiﬁcation to two-class problems\\nand use the terms cluster-based (Iwayama and Tokunaga 1995) and centroid-\\nCENTROID-BASED\\nCLASSIFICATION\\nbased classiﬁcation (Han and Karypis 2000, Tan and Cheng 2007) for Rocchio\\nclassiﬁcation with J > 2.\\nA more detailed treatment of kNN can be found in (Hastie et al. 2001), in-\\ncluding methods for tuning the parameter k. An example of an approximate\\nfast kNN algorithm is locality-based hashing (Andoni et al. 2006). Klein-\\nberg (1997) presents an approximate Θ((M log2 M)(M + log N)) kNN algo-\\nrithm (where M is the dimensionality of the space and N the number of data\\npoints), but at the cost of exponential storage requirements: Θ((N log M)2M).\\nIndyk (2004) surveys nearest neighbor methods in high-dimensional spaces.\\nEarly work on kNN in text classiﬁcation was motivated by the availability\\nof massively parallel hardware architectures (Creecy et al. 1992). Yang (1994)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.8\\nExercises\\n315\\nuses an inverted index to speed up kNN classiﬁcation. The optimality result\\nfor 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart\\n(1967).\\nThe effectiveness of Rocchio classiﬁcation and kNN is highly dependent\\non careful parameter tuning (in particular, the parameters b′ for Rocchio on\\npage 296 and k for kNN), feature engineering (Section 15.3, page 334) and\\nfeature selection (Section 13.5, page 271). Buckley and Salton (1995), Schapire\\net al. (1998), Yang and Kisiel (2003) and Moschitti (2003) address these issues\\nfor Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al.\\n(2000) compare feature selection methods for kNN.\\nThe bias-variance tradeoff was introduced by Geman et al. (1992). The\\nderivation in Section 14.6 is for MSE(γ), but the tradeoff applies to many\\nloss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995)\\nand Lewis et al. (1996) discuss linear classiﬁers for text and Hastie et al. (2001)\\nlinear classiﬁers in general. Readers interested in the algorithms mentioned,\\nbut not described in this chapter may wish to consult Bishop (2006) for neu-\\nral networks, Hastie et al. (2001) for linear and logistic regression, and Min-\\nsky and Papert (1988) for the perceptron algorithm. Anagnostopoulos et al.\\n(2006) show that an inverted index can be used for highly efﬁcient document\\nclassiﬁcation with any linear classiﬁer, provided that the classiﬁer is still ef-\\nfective when trained on a modest number of features via feature selection.\\nWe have only presented the simplest method for combining two-class clas-\\nsiﬁers into a one-of classiﬁer. Another important method is the use of error-\\ncorrecting codes, where a vector of decisions of different two-class classiﬁers\\nis constructed for each document. A test document’s decision vector is then\\n“corrected” based on the distribution of decision vectors in the training set,\\na procedure that incorporates information from all two-class classiﬁers and\\ntheir correlations into the ﬁnal classiﬁcation decision (Dietterich and Bakiri\\n1995). Ghamrawi and McCallum (2005) also exploit dependencies between\\nclasses in any-of classiﬁcation. Allwein et al. (2000) propose a general frame-\\nwork for combining two-class classiﬁers.\\n14.8\\nExercises\\n?\\nExercise 14.6\\nIn Figure 14.14, which of the three vectors⃗a,⃗b, and⃗c is (i) most similar to ⃗x according\\nto dot product similarity, (ii) most similar to ⃗x according to cosine similarity, (iii)\\nclosest to ⃗x according to Euclidean distance?\\nExercise 14.7\\nDownload Reuters-21578 and train and test Rocchio and kNN classiﬁers for the classes\\nacquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Use the\\nModApte split. You may want to use one of a number of software packages that im-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n316\\n14\\nVector space classiﬁcation\\n0 1 2 3 4 5 6 7 8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\na\\nx\\nb\\nc\\n◮Figure 14.14\\nExample for differences between Euclidean distance, dot product\\nsimilarity and cosine similarity. The vectors are ⃗a = (0.5 1.5)T, ⃗x = (2 2)T, ⃗b =\\n(4 4)T, and⃗c = (8 6)T.\\nplement Rocchio classiﬁcation and kNN classiﬁcation, for example, the Bow toolkit\\n(McCallum 1996).\\nExercise 14.8\\nDownload 20 Newgroups (page 154) and train and test Rocchio and kNN classiﬁers\\nfor its 20 classes.\\nExercise 14.9\\nShow that the decision boundaries in Rocchio classiﬁcation are, as in kNN, given by\\nthe Voronoi tessellation.\\nExercise 14.10\\n[⋆]\\nComputing the distance between a dense centroid and a sparse vector is Θ(M) for\\na naive implementation that iterates over all M dimensions. Based on the equality\\n∑(xi −µi)2 = 1.0 + ∑µ2\\ni −2 ∑xiµi and assuming that ∑µ2\\ni has been precomputed,\\nwrite down an algorithm that is Θ(Ma) instead, where Ma is the number of distinct\\nterms in the test document.\\nExercise 14.11\\n[⋆⋆⋆]\\nProve that the region of the plane consisting of all points with the same k nearest\\nneighbors is a convex polygon.\\nExercise 14.12\\nDesign an algorithm that performs an efﬁcient 1NN search in 1 dimension (where\\nefﬁciency is with respect to the number of documents N). What is the time complexity\\nof the algorithm?\\nExercise 14.13\\n[⋆⋆⋆]\\nDesign an algorithm that performs an efﬁcient 1NN search in 2 dimensions with at\\nmost polynomial (in N) preprocessing time.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n14.8\\nExercises\\n317\\nb\\nb\\n◮Figure 14.15\\nA simple non-separable set of points.\\nExercise 14.14\\n[⋆⋆⋆]\\nCan one design an exact efﬁcient algorithm for 1NN for very large M along the ideas\\nyou used to solve the last exercise?\\nExercise 14.15\\nShow that Equation (14.4) deﬁnes a hyperplane with ⃗w = ⃗µ(c1) −⃗µ(c2) and b =\\n0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2).\\nExercise 14.16\\nWe can easily construct non-separable data sets in high dimensions by embedding\\na non-separable set like the one shown in Figure 14.15. Consider embedding Fig-\\nure 14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a small\\ndistance in a random direction). Why would you expect the resulting conﬁguration\\nto be linearly separable? How likely is then a non-separable set of m ≪M points in\\nM-dimensional space?\\nExercise 14.17\\nAssuming two classes, show that the percentage of non-separable assignments of the\\nvertices of a hypercube decreases with dimensionality M for M > 1. For example,\\nfor M = 1 the proportion of non-separable assignments is 0, for M = 2, it is 2/16.\\nOne of the two non-separable cases for M = 2 is shown in Figure 14.15, the other is\\nits mirror image. Solve the exercise either analytically or by simulation.\\nExercise 14.18\\nAlthough we point out the similarities of Naive Bayes with linear vector space classi-\\nﬁers, it does not make sense to represent count vectors (the document representations\\nin NB) in a continuous vector space. There is however a formalization of NB that is\\nanalogous to Rocchio. Show that NB assigns a document to the class (represented as\\na parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4, page 251)\\nto the document (represented as a count vector as in Section 13.4.1 (page 270), nor-\\nmalized to sum to 1) is smallest.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n319\\n15\\nSupport vector machines and\\nmachine learning on documents\\nImproving classiﬁer effectiveness has been an area of intensive machine-\\nlearning research over the last two decades, and this work has led to a new\\ngeneration of state-of-the-art classiﬁers, such as support vector machines,\\nboosted decision trees, regularized logistic regression, neural networks, and\\nrandom forests. Many of these methods, including support vector machines\\n(SVMs), the main topic of this chapter, have been applied with success to\\ninformation retrieval problems, particularly text classiﬁcation. An SVM is a\\nkind of large-margin classiﬁer: it is a vector space based machine learning\\nmethod where the goal is to ﬁnd a decision boundary between two classes\\nthat is maximally far from any point in the training data (possibly discount-\\ning some points as outliers or noise).\\nWe will initially motivate and develop SVMs for the case of two-class data\\nsets that are separable by a linear classiﬁer (Section 15.1), and then extend the\\nmodel in Section 15.2 to non-separable data, multi-class problems, and non-\\nlinear models, and also present some additional discussion of SVM perfor-\\nmance. The chapter then moves to consider the practical deployment of text\\nclassiﬁers in Section 15.3: what sorts of classiﬁers are appropriate when, and\\nhow can you exploit domain-speciﬁc text features in classiﬁcation? Finally,\\nwe will consider how the machine learning technology that we have been\\nbuilding for text classiﬁcation can be applied back to the problem of learning\\nhow to rank documents in ad hoc retrieval (Section 15.4). While several ma-\\nchine learning methods have been applied to this task, use of SVMs has been\\nprominent. Support vector machines are not necessarily better than other\\nmachine learning methods (except perhaps in situations with little training\\ndata), but they perform at the state-of-the-art level and have much current\\ntheoretical and empirical appeal.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n320\\n15\\nSupport vector machines and machine learning on documents\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nSupport vectors\\nMaximum\\nmargin\\ndecision\\nhyperplane\\nMargin is\\nmaximized\\n◮Figure 15.1\\nThe support vectors are the 5 points right up against the margin of\\nthe classiﬁer.\\n15.1\\nSupport vector machines: The linearly separable case\\nFor two-class, separable training data sets, such as the one in Figure 14.8\\n(page 301), there are lots of possible linear separators. Intuitively, a decision\\nboundary drawn in the middle of the void between data items of the two\\nclasses seems better than one which approaches very close to examples of\\none or both classes. While some learning methods such as the perceptron\\nalgorithm (see references in Section 14.7, page 314) ﬁnd just any linear sepa-\\nrator, others, like Naive Bayes, search for the best linear separator according\\nto some criterion. The SVM in particular deﬁnes the criterion to be looking\\nfor a decision surface that is maximally far away from any data point. This\\ndistance from the decision surface to the closest data point determines the\\nmargin of the classiﬁer. This method of construction necessarily means that\\nMARGIN\\nthe decision function for an SVM is fully speciﬁed by a (usually small) sub-\\nset of the data which deﬁnes the position of the separator. These points are\\nreferred to as the support vectors (in a vector space, a point can be thought of\\nSUPPORT VECTOR\\nas a vector between the origin and that point). Figure 15.1 shows the margin\\nand support vectors for a sample problem. Other data points play no part in\\ndetermining the decision surface that is chosen.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n321\\n◮Figure 15.2\\nAn intuition for large-margin classiﬁcation. Insisting on a large mar-\\ngin reduces the capacity of the model: the range of angles at which the fat deci-\\nsion surface can be placed is smaller than for a decision hyperplane (cf. Figure 14.8,\\npage 301).\\nMaximizing the margin seems good because points near the decision sur-\\nface represent very uncertain classiﬁcation decisions: there is almost a 50%\\nchance of the classiﬁer deciding either way. A classiﬁer with a large margin\\nmakes no low certainty classiﬁcation decisions. This gives you a classiﬁca-\\ntion safety margin: a slight error in measurement or a slight document vari-\\nation will not cause a misclassiﬁcation. Another intuition motivating SVMs\\nis shown in Figure 15.2. By construction, an SVM classiﬁer insists on a large\\nmargin around the decision boundary. Compared to a decision hyperplane,\\nif you have to place a fat separator between classes, you have fewer choices\\nof where it can be put. As a result of this, the memory capacity of the model\\nhas been decreased, and hence we expect that its ability to correctly general-\\nize to test data is increased (cf. the discussion of the bias-variance tradeoff in\\nChapter 14, page 312).\\nLet us formalize an SVM with algebra. A decision hyperplane (page 302)\\ncan be deﬁned by an intercept term b and a decision hyperplane normal vec-\\ntor ⃗w which is perpendicular to the hyperplane. This vector is commonly\\n', 'Online edition (c)\\n2009 Cambridge UP\\n322\\n15\\nSupport vector machines and machine learning on documents\\nreferred to in the machine learning literature as the weight vector. To choose\\nWEIGHT VECTOR\\namong all the hyperplanes that are perpendicular to the normal vector, we\\nspecify the intercept term b. Because the hyperplane is perpendicular to the\\nnormal vector, all points ⃗x on the hyperplane satisfy ⃗wT⃗x = −b. Now sup-\\npose that we have a set of training data points D = {(⃗xi, yi)}, where each\\nmember is a pair of a point ⃗xi and a class label yi corresponding to it.1 For\\nSVMs, the two data classes are always named +1 and −1 (rather than 1 and\\n0), and the intercept term is always explicitly represented as b (rather than\\nbeing folded into the weight vector ⃗w by adding an extra always-on feature).\\nThe math works out much more cleanly if you do things this way, as we will\\nsee almost immediately in the deﬁnition of functional margin. The linear\\nclassiﬁer is then:\\nf(⃗x) = sign(⃗wT⃗x + b)\\n(15.1)\\nA value of −1 indicates one class, and a value of +1 the other class.\\nWe are conﬁdent in the classiﬁcation of a point if it is far away from the\\ndecision boundary. For a given data set and decision hyperplane, we deﬁne\\nthe functional margin of the ith example ⃗xi with respect to a hyperplane ⟨⃗w, b⟩\\nFUNCTIONAL MARGIN\\nas the quantity yi(⃗wT⃗xi + b). The functional margin of a data set with re-\\nspect to a decision surface is then twice the functional margin of any of the\\npoints in the data set with minimal functional margin (the factor of 2 comes\\nfrom measuring across the whole width of the margin, as in Figure 15.3).\\nHowever, there is a problem with using this deﬁnition as is: the value is un-\\nderconstrained, because we can always make the functional margin as big\\nas we wish by simply scaling up ⃗w and b. For example, if we replace ⃗w by\\n5⃗w and b by 5b then the functional margin yi(5⃗wT⃗xi + 5b) is ﬁve times as\\nlarge. This suggests that we need to place some constraint on the size of the\\n⃗w vector. To get a sense of how to do that, let us look at the actual geometry.\\nWhat is the Euclidean distance from a point ⃗x to the decision boundary? In\\nFigure 15.3, we denote by r this distance. We know that the shortest distance\\nbetween a point and a hyperplane is perpendicular to the plane, and hence,\\nparallel to ⃗w. A unit vector in this direction is ⃗w/|⃗w|. The dotted line in the\\ndiagram is then a translation of the vector r⃗w/|⃗w|. Let us label the point on\\nthe hyperplane closest to ⃗x as ⃗x′. Then:\\n⃗x′ = ⃗x −yr ⃗w\\n|⃗w|\\n(15.2)\\nwhere multiplying by y just changes the sign for the two cases of ⃗x being on\\neither side of the decision surface. Moreover,⃗x′ lies on the decision boundary\\n1. As discussed in Section 14.1 (page 291), we present the general case of points in a vector\\nspace, but if the points are length normalized document vectors, then all the action is taking\\nplace on the surface of a unit sphere, and the decision surface intersects the sphere’s surface.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n323\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nu\\nt\\nu\\nt\\nu\\nt⃗x\\n+\\n⃗x′\\nr\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nρ\\n⃗w\\n◮Figure 15.3\\nThe geometric margin of a point (r) and a decision boundary (ρ).\\nand so satisﬁes ⃗wT⃗x′ + b = 0. Hence:\\n⃗wT\\x00⃗x −yr ⃗w\\n|⃗w|\\n\\x01 + b = 0\\n(15.3)\\nSolving for r gives:2\\nr = y ⃗wT⃗x + b\\n|⃗w|\\n(15.4)\\nAgain, the points closest to the separating hyperplane are support vectors.\\nThe geometric margin of the classiﬁer is the maximum width of the band that\\nGEOMETRIC MARGIN\\ncan be drawn separating the support vectors of the two classes. That is, it is\\ntwice the minimum value over data points for r given in Equation (15.4), or,\\nequivalently, the maximal width of one of the fat separators shown in Fig-\\nure 15.2. The geometric margin is clearly invariant to scaling of parameters:\\nif we replace ⃗w by 5⃗w and b by 5b, then the geometric margin is the same, be-\\ncause it is inherently normalized by the length of ⃗w. This means that we can\\nimpose any scaling constraint we wish on ⃗w without affecting the geometric\\nmargin. Among other choices, we could use unit vectors, as in Chapter 6, by\\n2. Recall that |⃗w| =\\n√\\n⃗wT⃗w.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n324\\n15\\nSupport vector machines and machine learning on documents\\nrequiring that |⃗w| = 1. This would have the effect of making the geometric\\nmargin the same as the functional margin.\\nSince we can scale the functional margin as we please, for convenience in\\nsolving large SVMs, let us choose to require that the functional margin of all\\ndata points is at least 1 and that it is equal to 1 for at least one data vector.\\nThat is, for all items in the data:\\nyi(⃗wT⃗xi + b) ≥1\\n(15.5)\\nand there exist support vectors for which the inequality is an equality. Since\\neach example’s distance from the hyperplane is ri = yi(⃗wT⃗xi + b)/|⃗w|, the\\ngeometric margin is ρ = 2/|⃗w|. Our desire is still to maximize this geometric\\nmargin. That is, we want to ﬁnd ⃗w and b such that:\\n• ρ = 2/|⃗w| is maximized\\n• For all (⃗xi, yi) ∈D, yi(⃗wT⃗xi + b) ≥1\\nMaximizing 2/|⃗w| is the same as minimizing |⃗w|/2. This gives the ﬁnal stan-\\ndard formulation of an SVM as a minimization problem:\\n(15.6)\\nFind ⃗w and b such that:\\n•\\n1\\n2 ⃗wT⃗w is minimized, and\\n• for all {(⃗xi, yi)}, yi(⃗wT⃗xi + b) ≥1\\nWe are now optimizing a quadratic function subject to linear constraints.\\nQuadratic optimization problems are a standard, well-known class of mathe-\\nQUADRATIC\\nPROGRAMMING\\nmatical optimization problems, and many algorithms exist for solving them.\\nWe could in principle build our SVM using standard quadratic programming\\n(QP) libraries, but there has been much recent research in this area aiming to\\nexploit the structure of the kind of QP that emerges from an SVM. As a result,\\nthere are more intricate but much faster and more scalable libraries available\\nespecially for building SVMs, which almost everyone uses to build models.\\nWe will not present the details of such algorithms here.\\nHowever, it will be helpful to what follows to understand the shape of the\\nsolution of such an optimization problem. The solution involves construct-\\ning a dual problem where a Lagrange multiplier αi is associated with each\\nconstraint yi(⃗wT⃗xi + b) ≥1 in the primal problem:\\n(15.7)\\nFind α1, . . . αN such that ∑αi −1\\n2 ∑i ∑j αiαjyiyj⃗xi\\nT⃗xj is maximized, and\\n• ∑i αiyi = 0\\n• αi ≥0 for all 1 ≤i ≤N\\nThe solution is then of the form:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.1\\nSupport vector machines: The linearly separable case\\n325\\n0\\n1\\n2\\n3\\n0\\n1\\n2\\n3\\nb\\nb\\nu\\nt\\n◮Figure 15.4\\nA tiny 3 data point training set for an SVM.\\n(15.8)\\n⃗w = ∑αiyi⃗xi\\nb = yk −⃗wT⃗xk for any ⃗xk such that αk ̸= 0\\nIn the solution, most of the αi are zero. Each non-zero αi indicates that the\\ncorresponding ⃗xi is a support vector. The classiﬁcation function is then:\\nf(⃗x) = sign(∑i αiyi⃗xiT⃗x + b)\\n(15.9)\\nBoth the term to be maximized in the dual problem and the classifying func-\\ntion involve a dot product between pairs of points (⃗x and ⃗xi or ⃗xi and ⃗xj), and\\nthat is the only way the data are used – we will return to the signiﬁcance of\\nthis later.\\nTo recap, we start with a training data set. The data set uniquely deﬁnes\\nthe best separating hyperplane, and we feed the data through a quadratic\\noptimization procedure to ﬁnd this plane. Given a new point ⃗x to classify,\\nthe classiﬁcation function f(⃗x) in either Equation (15.1) or Equation (15.9) is\\ncomputing the projection of the point onto the hyperplane normal. The sign\\nof this function determines the class to assign to the point. If the point is\\nwithin the margin of the classiﬁer (or another conﬁdence threshold t that we\\nmight have determined to minimize classiﬁcation mistakes) then the classi-\\nﬁer can return “don’t know” rather than one of the two classes. The value\\nof f(⃗x) may also be transformed into a probability of classiﬁcation; ﬁtting\\na sigmoid to transform the values is standard (Platt 2000). Also, since the\\nmargin is constant, if the model includes dimensions from various sources,\\ncareful rescaling of some dimensions may be required. However, this is not\\na problem if our documents (points) are on the unit hypersphere.\\n\\x0f\\nExample 15.1:\\nConsider building an SVM over the (very little) data set shown in\\nFigure 15.4. Working geometrically, for an example like this, the maximum margin\\nweight vector will be parallel to the shortest line connecting points of the two classes,\\nthat is, the line between (1, 1) and (2, 3), giving a weight vector of (1, 2). The opti-\\nmal decision surface is orthogonal to that line and intersects it at the halfway point.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n326\\n15\\nSupport vector machines and machine learning on documents\\nTherefore, it passes through (1.5, 2). So, the SVM decision boundary is:\\ny = x1 + 2x2 −5.5\\nWorking algebraically, with the standard constraint that sign(yi(⃗wT⃗xi + b)) ≥1,\\nwe seek to minimize |⃗w|. This happens when this constraint is satisﬁed with equality\\nby the two support vectors. Further we know that the solution is ⃗w = (a, 2a) for some\\na. So we have that:\\na + 2a + b\\n=\\n−1\\n2a + 6a + b\\n=\\n1\\nTherefore, a = 2/5 and b = −11/5. So the optimal hyperplane is given by ⃗w =\\n(2/5, 4/5) and b = −11/5.\\nThe margin ρ is 2/|⃗w| = 2/√\\n4/25 + 16/25 = 2/(2\\n√\\n5/5) =\\n√\\n5. This answer can\\nbe conﬁrmed geometrically by examining Figure 15.4.\\n?\\nExercise 15.1\\n[⋆]\\nWhat is the minimum number of support vectors that there can be for a data set\\n(which contains instances of each class)?\\nExercise 15.2\\n[⋆⋆]\\nThe basis of being able to use kernels in SVMs (see Section 15.2.3) is that the classiﬁca-\\ntion function can be written in the form of Equation (15.9) (where, for large problems,\\nmost αi are 0). Show explicitly how the classiﬁcation function could be written in this\\nform for the data set from Example 15.1. That is, write f as a function where the data\\npoints appear and the only variable is ⃗x.\\nExercise 15.3\\n[⋆⋆]\\nInstall an SVM package such as SVMlight (http://svmlight.joachims.org/), and build an\\nSVM for the data set discussed in Example 15.1. Conﬁrm that the program gives the\\nsame solution as the text. For SVMlight, or another package that accepts the same\\ntraining data format, the training ﬁle would be:\\n+1 1:2 2:3\\n−1 1:2 2:0\\n−1 1:1 2:1\\nThe training command for SVMlight is then:\\nsvm_learn -c 1 -a alphas.dat train.dat model.dat\\nThe -c 1 option is needed to turn off use of the slack variables that we discuss in\\nSection 15.2.1. Check that the norm of the weight vector agrees with what we found\\nin Example 15.1. Examine the ﬁle alphas.dat which contains the αi values, and check\\nthat they agree with your answers in Exercise 15.2.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n327\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n⃗xi\\nξi\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\nu\\nt\\n⃗xj\\nξj\\n◮Figure 15.5\\nLarge margin classiﬁcation with slack variables.\\n15.2\\nExtensions to the SVM model\\n15.2.1\\nSoft margin classiﬁcation\\nFor the very high dimensional problems common in text classiﬁcation, some-\\ntimes the data are linearly separable. But in the general case they are not, and\\neven if they are, we might prefer a solution that better separates the bulk of\\nthe data while ignoring a few weird noise documents.\\nIf the training set D is not linearly separable, the standard approach is to\\nallow the fat decision margin to make a few mistakes (some points – outliers\\nor noisy examples – are inside or on the wrong side of the margin). We then\\npay a cost for each misclassiﬁed example, which depends on how far it is\\nfrom meeting the margin requirement given in Equation (15.5). To imple-\\nment this, we introduce slack variables ξi. A non-zero value for ξi allows ⃗xi to\\nSLACK VARIABLES\\nnot meet the margin requirement at a cost proportional to the value of ξi. See\\nFigure 15.5.\\nThe formulation of the SVM optimization problem with slack variables is:\\n(15.10)\\nFind ⃗w, b, and ξi ≥0 such that:\\n•\\n1\\n2⃗wT⃗w + C ∑i ξi is minimized\\n• and for all {(⃗xi, yi)}, yi(⃗wT⃗xi + b) ≥1 −ξi\\n', 'Online edition (c)\\n2009 Cambridge UP\\n328\\n15\\nSupport vector machines and machine learning on documents\\nThe optimization problem is then trading off how fat it can make the margin\\nversus how many points have to be moved around to allow this margin.\\nThe margin can be less than 1 for a point ⃗xi by setting ξi > 0, but then one\\npays a penalty of Cξi in the minimization for having done that. The sum of\\nthe ξi gives an upper bound on the number of training errors. Soft-margin\\nSVMs minimize training error traded off against margin. The parameter C\\nis a regularization term, which provides a way to control overﬁtting: as C\\nREGULARIZATION\\nbecomes large, it is unattractive to not respect the data at the cost of reducing\\nthe geometric margin; when it is small, it is easy to account for some data\\npoints with the use of slack variables and to have a fat margin placed so it\\nmodels the bulk of the data.\\nThe dual problem for soft margin classiﬁcation becomes:\\n(15.11)\\nFind α1, . . . αN such that ∑αi −1\\n2 ∑i ∑j αiαjyiyj⃗xi\\nT⃗xj is maximized, and\\n• ∑i αiyi = 0\\n• 0 ≤αi ≤C for all 1 ≤i ≤N\\nNeither the slack variables ξi nor Lagrange multipliers for them appear in the\\ndual problem. All we are left with is the constant C bounding the possible\\nsize of the Lagrange multipliers for the support vector data points. As before,\\nthe ⃗xi with non-zero αi will be the support vectors. The solution of the dual\\nproblem is of the form:\\n(15.12)\\n⃗w = ∑αyi⃗xi\\nb = yk(1 −ξk) −⃗wT⃗xk for k = arg maxk αk\\nAgain ⃗w is not needed explicitly for classiﬁcation, which can be done in terms\\nof dot products with data points, as in Equation (15.9).\\nTypically, the support vectors will be a small proportion of the training\\ndata. However, if the problem is non-separable or with small margin, then\\nevery data point which is misclassiﬁed or within the margin will have a non-\\nzero αi. If this set of points becomes large, then, for the nonlinear case which\\nwe turn to in Section 15.2.3, this can be a major slowdown for using SVMs at\\ntest time.\\nThe complexity of training and testing with linear SVMs is shown in Ta-\\nble 15.1.3 The time for training an SVM is dominated by the time for solving\\nthe underlying QP, and so the theoretical and empirical complexity varies de-\\npending on the method used to solve it. The standard result for solving QPs\\nis that it takes time cubic in the size of the data set (Kozlov et al. 1979). All the\\nrecent work on SVM training has worked to reduce that complexity, often by\\n3. We write Θ(|D|Lave) for Θ(T) (page 262) and assume that the length of test documents is\\nbounded as we did on page 262.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n329\\nClassiﬁer\\nMode\\nMethod\\nTime complexity\\nNB\\ntraining\\nΘ(|D|Lave + |C||V|)\\nNB\\ntesting\\nΘ(|C|Ma)\\nRocchio\\ntraining\\nΘ(|D|Lave + |C||V|)\\nRocchio\\ntesting\\nΘ(|C|Ma)\\nkNN\\ntraining\\npreprocessing\\nΘ(|D|Lave)\\nkNN\\ntesting\\npreprocessing\\nΘ(|D|MaveMa)\\nkNN\\ntraining\\nno preprocessing\\nΘ(1)\\nkNN\\ntesting\\nno preprocessing\\nΘ(|D|LaveMa)\\nSVM\\ntraining\\nconventional\\nO(|C||D|3Mave);\\n≈O(|C||D|1.7Mave), empirically\\nSVM\\ntraining\\ncutting planes\\nO(|C||D|Mave)\\nSVM\\ntesting\\nO(|C|Ma)\\n◮Table 15.1\\nTraining and testing complexity of various classiﬁers including SVMs.\\nTraining is the time the learning method takes to learn a classiﬁer over D, while test-\\ning is the time it takes a classiﬁer to classify one document. For SVMs, multiclass\\nclassiﬁcation is assumed to be done by a set of |C| one-versus-rest classiﬁers. Lave is\\nthe average number of tokens per document, while Mave is the average vocabulary\\n(number of non-zero features) of a document. La and Ma are the numbers of tokens\\nand types, respectively, in the test document.\\nbeing satisﬁed with approximate solutions. Standardly, empirical complex-\\nity is about O(|D|1.7) (Joachims 2006a). Nevertheless, the super-linear train-\\ning time of traditional SVM algorithms makes them difﬁcult or impossible\\nto use on very large training data sets. Alternative traditional SVM solu-\\ntion algorithms which are linear in the number of training examples scale\\nbadly with a large number of features, which is another standard attribute\\nof text problems. However, a new training algorithm based on cutting plane\\ntechniques gives a promising answer to this issue by having running time\\nlinear in the number of training examples and the number of non-zero fea-\\ntures in examples (Joachims 2006a). Nevertheless, the actual speed of doing\\nquadratic optimization remains much slower than simply counting terms as\\nis done in a Naive Bayes model. Extending SVM algorithms to nonlinear\\nSVMs, as in the next section, standardly increases training complexity by a\\nfactor of |D| (since dot products between examples need to be calculated),\\nmaking them impractical. In practice it can often be cheaper to materialize\\n', 'Online edition (c)\\n2009 Cambridge UP\\n330\\n15\\nSupport vector machines and machine learning on documents\\nthe higher-order features and to train a linear SVM.4\\n15.2.2\\nMulticlass SVMs\\nSVMs are inherently two-class classiﬁers. The traditional way to do mul-\\nticlass classiﬁcation with SVMs is to use one of the methods discussed in\\nSection 14.5 (page 306). In particular, the most common technique in prac-\\ntice has been to build |C| one-versus-rest classiﬁers (commonly referred to as\\n“one-versus-all” or OVA classiﬁcation), and to choose the class which classi-\\nﬁes the test datum with greatest margin. Another strategy is to build a set\\nof one-versus-one classiﬁers, and to choose the class that is selected by the\\nmost classiﬁers. While this involves building |C|(|C| −1)/2 classiﬁers, the\\ntime for training classiﬁers may actually decrease, since the training data set\\nfor each classiﬁer is much smaller.\\nHowever, these are not very elegant approaches to solving multiclass prob-\\nlems. A better alternative is provided by the construction of multiclass SVMs,\\nwhere we build a two-class classiﬁer over a feature vector Φ(⃗x, y) derived\\nfrom the pair consisting of the input features and the class of the datum. At\\ntest time, the classiﬁer chooses the class y = arg maxy′ ⃗wTΦ(⃗x, y′). The mar-\\ngin during training is the gap between this value for the correct class and\\nfor the nearest other class, and so the quadratic program formulation will\\nrequire that ∀i ∀y ̸= yi ⃗wTΦ(⃗xi, yi) −⃗wTΦ(⃗xi, y) ≥1 −ξi. This general\\nmethod can be extended to give a multiclass formulation of various kinds of\\nlinear classiﬁers. It is also a simple instance of a generalization of classiﬁca-\\ntion where the classes are not just a set of independent, categorical labels, but\\nmay be arbitrary structured objects with relationships deﬁned between them.\\nIn the SVM world, such work comes under the label of structural SVMs. We\\nSTRUCTURAL SVMS\\nmention them again in Section 15.4.2.\\n15.2.3\\nNonlinear SVMs\\nWith what we have presented so far, data sets that are linearly separable (per-\\nhaps with a few exceptions or some noise) are well-handled. But what are\\nwe going to do if the data set just doesn’t allow classiﬁcation by a linear clas-\\nsiﬁer? Let us look at a one-dimensional case. The top data set in Figure 15.6\\nis straightforwardly classiﬁed by a linear classiﬁer but the middle data set is\\nnot. We instead need to be able to pick out an interval. One way to solve this\\nproblem is to map the data on to a higher dimensional space and then to use\\na linear classiﬁer in the higher dimensional space. For example, the bottom\\npart of the ﬁgure shows that a linear separator can easily classify the data\\n4. Materializing the features refers to directly calculating higher order and interaction terms\\nand then putting them into a linear model.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n331\\n◮Figure 15.6\\nProjecting data that is not linearly separable into a higher dimensional\\nspace can make it linearly separable.\\nif we use a quadratic function to map the data into two dimensions (a po-\\nlar coordinates projection would be another possibility). The general idea is\\nto map the original feature space to some higher-dimensional feature space\\nwhere the training set is separable. Of course, we would want to do so in\\nways that preserve relevant dimensions of relatedness between data points,\\nso that the resultant classiﬁer should still generalize well.\\nSVMs, and also a number of other linear classiﬁers, provide an easy and\\nefﬁcient way of doing this mapping to a higher dimensional space, which is\\nreferred to as “the kernel trick”. It’s not really a trick: it just exploits the math\\nKERNEL TRICK\\nthat we have seen. The SVM linear classiﬁer relies on a dot product between\\ndata point vectors. Let K(⃗xi,⃗xj) = ⃗xiT⃗xj. Then the classiﬁer we have seen so\\n', 'Online edition (c)\\n2009 Cambridge UP\\n332\\n15\\nSupport vector machines and machine learning on documents\\nfar is:\\nf(⃗x) = sign(∑\\ni\\nαiyiK(⃗xi,⃗x) + b)\\n(15.13)\\nNow suppose we decide to map every data point into a higher dimensional\\nspace via some transformation Φ:⃗x 7→φ(⃗x). Then the dot product becomes\\nφ(⃗xi)Tφ(⃗xj). If it turned out that this dot product (which is just a real num-\\nber) could be computed simply and efﬁciently in terms of the original data\\npoints, then we wouldn’t have to actually map from ⃗x 7→φ(⃗x). Rather, we\\ncould simply compute the quantity K(⃗xi,⃗xj) = φ(⃗xi)Tφ(⃗xj), and then use the\\nfunction’s value in Equation (15.13). A kernel function K is such a function\\nKERNEL FUNCTION\\nthat corresponds to a dot product in some expanded feature space.\\n\\x0f\\nExample 15.2: The quadratic kernel in two dimensions.\\nFor 2-dimensional\\nvectors ⃗u = (u1\\nu2), ⃗v = (v1\\nv2), consider K(⃗u,⃗v) = (1 + ⃗uT⃗v)2. We wish to\\nshow that this is a kernel, i.e., that K(⃗u,⃗v) = φ(⃗u)Tφ(⃗v) for some φ. Consider φ(⃗u) =\\n(1 u2\\n1\\n√\\n2u1u2 u2\\n2\\n√\\n2u1\\n√\\n2u2). Then:\\nK(⃗u,⃗v)\\n=\\n(1 +⃗uT⃗v)2\\n(15.14)\\n=\\n1 + u2\\n1v2\\n1 + 2u1v1u2v2 + u2\\n2v2\\n2 + 2u1v1 + 2u2v2\\n=\\n(1 u2\\n1\\n√\\n2u1u2 u2\\n2\\n√\\n2u1\\n√\\n2u2)T(1 v2\\n1\\n√\\n2v1v2 v2\\n2\\n√\\n2v1\\n√\\n2v2)\\n=\\nφ(⃗u)Tφ(⃗v)\\nIn the language of functional analysis, what kinds of functions are valid\\nkernel functions? Kernel functions are sometimes more precisely referred to\\nKERNEL\\nas Mercer kernels, because they must satisfy Mercer’s condition: for any g(⃗x)\\nMERCER KERNEL\\nsuch that R\\ng(⃗x)2d⃗x is ﬁnite, we must have that:\\nZ\\nK(⃗x,⃗z)g(⃗x)g(⃗z)d⃗xd⃗z ≥0 .\\n(15.15)\\nA kernel function K must be continuous, symmetric, and have a positive def-\\ninite gram matrix. Such a K means that there exists a mapping to a reproduc-\\ning kernel Hilbert space (a Hilbert space is a vector space closed under dot\\nproducts) such that the dot product there gives the same value as the function\\nK. If a kernel does not satisfy Mercer’s condition, then the corresponding QP\\nmay have no solution. If you would like to better understand these issues,\\nyou should consult the books on SVMs mentioned in Section 15.5. Other-\\nwise, you can content yourself with knowing that 90% of work with kernels\\nuses one of two straightforward families of functions of two vectors, which\\nwe deﬁne below, and which deﬁne valid kernels.\\nThe two commonly used families of kernels are polynomial kernels and\\nradial basis functions. Polynomial kernels are of the form K(⃗x,⃗z) = (1 +\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.2\\nExtensions to the SVM model\\n333\\n⃗xT⃗z)d. The case of d = 1 is a linear kernel, which is what we had before the\\nstart of this section (the constant 1 just changing the threshold). The case of\\nd = 2 gives a quadratic kernel, and is very commonly used. We illustrated\\nthe quadratic kernel in Example 15.2.\\nThe most common form of radial basis function is a Gaussian distribution,\\ncalculated as:\\nK(⃗x,⃗z) = e−(⃗x−⃗z)2/(2σ2)\\n(15.16)\\nA radial basis function (rbf) is equivalent to mapping the data into an inﬁ-\\nnite dimensional Hilbert space, and so we cannot illustrate the radial basis\\nfunction concretely, as we did a quadratic kernel. Beyond these two families,\\nthere has been interesting work developing other kernels, some of which is\\npromising for text applications. In particular, there has been investigation of\\nstring kernels (see Section 15.5).\\nThe world of SVMs comes with its own language, which is rather different\\nfrom the language otherwise used in machine learning. The terminology\\ndoes have deep roots in mathematics, but it’s important not to be too awed\\nby that terminology. Really, we are talking about some quite simple things. A\\npolynomial kernel allows us to model feature conjunctions (up to the order of\\nthe polynomial). That is, if we want to be able to model occurrences of pairs\\nof words, which give distinctive information about topic classiﬁcation, not\\ngiven by the individual words alone, like perhaps operating AND system or\\nethnic AND cleansing, then we need to use a quadratic kernel. If occurrences\\nof triples of words give distinctive information, then we need to use a cubic\\nkernel. Simultaneously you also get the powers of the basic features – for\\nmost text applications, that probably isn’t useful, but just comes along with\\nthe math and hopefully doesn’t do harm. A radial basis function allows you\\nto have features that pick out circles (hyperspheres) – although the decision\\nboundaries become much more complex as multiple such features interact. A\\nstring kernel lets you have features that are character subsequences of terms.\\nAll of these are straightforward notions which have also been used in many\\nother places under different names.\\n15.2.4\\nExperimental results\\nWe presented results in Section 13.6 showing that an SVM is a very effec-\\ntive text classiﬁer. The results of Dumais et al. (1998) given in Table 13.9\\nshow SVMs clearly performing the best. This was one of several pieces of\\nwork from this time that established the strong reputation of SVMs for text\\nclassiﬁcation. Another pioneering work on scaling and evaluating SVMs\\nfor text classiﬁcation was (Joachims 1998). We present some of his results\\n', 'Online edition (c)\\n2009 Cambridge UP\\n334\\n15\\nSupport vector machines and machine learning on documents\\nRoc-\\nDec.\\nlinear SVM\\nrbf-SVM\\nNB\\nchio\\nTrees\\nkNN\\nC = 0.5\\nC = 1.0\\nσ ≈7\\nearn\\n96.0\\n96.1\\n96.1\\n97.8\\n98.0\\n98.2\\n98.1\\nacq\\n90.7\\n92.1\\n85.3\\n91.8\\n95.5\\n95.6\\n94.7\\nmoney-fx\\n59.6\\n67.6\\n69.4\\n75.4\\n78.8\\n78.5\\n74.3\\ngrain\\n69.8\\n79.5\\n89.1\\n82.6\\n91.9\\n93.1\\n93.4\\ncrude\\n81.2\\n81.5\\n75.5\\n85.8\\n89.4\\n89.4\\n88.7\\ntrade\\n52.2\\n77.4\\n59.2\\n77.9\\n79.2\\n79.2\\n76.6\\ninterest\\n57.6\\n72.5\\n49.1\\n76.7\\n75.6\\n74.8\\n69.1\\nship\\n80.9\\n83.1\\n80.9\\n79.8\\n87.4\\n86.5\\n85.8\\nwheat\\n63.4\\n79.4\\n85.5\\n72.9\\n86.6\\n86.8\\n82.4\\ncorn\\n45.2\\n62.2\\n87.7\\n71.4\\n87.5\\n87.8\\n84.6\\nmicroavg.\\n72.3\\n79.9\\n79.4\\n82.6\\n86.7\\n87.5\\n86.4\\n◮Table 15.2\\nSVM classiﬁer break-even F1 from (Joachims 2002a, p. 114).\\nResults\\nare shown for the 10 largest categories and for microaveraged performance over all\\n90 categories on the Reuters-21578 data set.\\nfrom (Joachims 2002a) in Table 15.2.5 Joachims used a large number of term\\nfeatures in contrast to Dumais et al. (1998), who used MI feature selection\\n(Section 13.5.1, page 272) to build classiﬁers with a much more limited num-\\nber of features. The success of the linear SVM mirrors the results discussed\\nin Section 14.6 (page 308) on other linear approaches like Naive Bayes. It\\nseems that working with simple term features can get one a long way. It is\\nagain noticeable the extent to which different papers’ results for the same ma-\\nchine learning methods differ. In particular, based on replications by other\\nresearchers, the Naive Bayes results of (Joachims 1998) appear too weak, and\\nthe results in Table 13.9 should be taken as representative.\\n15.3\\nIssues in the classiﬁcation of text documents\\nThere are lots of applications of text classiﬁcation in the commercial world;\\nemail spam ﬁltering is perhaps now the most ubiquitous. Jackson and Mou-\\nlinier (2002) write: “There is no question concerning the commercial value of\\nbeing able to classify documents automatically by content. There are myriad\\n5. These results are in terms of the break-even F1 (see Section 8.4). Many researchers disprefer\\nthis measure for text classiﬁcation evaluation, since its calculation may involve interpolation\\nrather than an actual parameter setting of the system and it is not clear why this value should\\nbe reported rather than maximal F1 or another point on the precision/recall curve motivated by\\nthe task at hand. While earlier results in (Joachims 1998) suggested notable gains on this task\\nfrom the use of higher order polynomial or rbf kernels, this was with hard-margin SVMs. With\\nsoft-margin SVMs, a simple linear SVM with the default C = 1 performs best.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n335\\npotential applications of such a capability for corporate Intranets, govern-\\nment departments, and Internet publishers.”\\nMost of our discussion of classiﬁcation has focused on introducing various\\nmachine learning methods rather than discussing particular features of text\\ndocuments relevant to classiﬁcation. This bias is appropriate for a textbook,\\nbut is misplaced for an application developer. It is frequently the case that\\ngreater performance gains can be achieved from exploiting domain-speciﬁc\\ntext features than from changing from one machine learning method to an-\\nother. Jackson and Moulinier (2002) suggest that “Understanding the data\\nis one of the keys to successful categorization, yet this is an area in which\\nmost categorization tool vendors are extremely weak. Many of the ‘one size\\nﬁts all’ tools on the market have not been tested on a wide range of content\\ntypes.” In this section we wish to step back a little and consider the applica-\\ntions of text classiﬁcation, the space of possible solutions, and the utility of\\napplication-speciﬁc heuristics.\\n15.3.1\\nChoosing what kind of classiﬁer to use\\nWhen confronted with a need to build a text classiﬁer, the ﬁrst question to\\nask is how much training data is there currently available? None? Very little?\\nQuite a lot? Or a huge amount, growing every day? Often one of the biggest\\npractical challenges in ﬁelding a machine learning classiﬁer in real applica-\\ntions is creating or obtaining enough training data. For many problems and\\nalgorithms, hundreds or thousands of examples from each class are required\\nto produce a high performance classiﬁer and many real world contexts in-\\nvolve large sets of categories. We will initially assume that the classiﬁer is\\nneeded as soon as possible; if a lot of time is available for implementation,\\nmuch of it might be spent on assembling data resources.\\nIf you have no labeled training data, and especially if there are existing\\nstaff knowledgeable about the domain of the data, then you should never\\nforget the solution of using hand-written rules. That is, you write standing\\nqueries, as we touched on at the beginning of Chapter 13. For example:\\nIF (wheat OR grain) AND NOT (whole OR bread) THEN c = grain\\nIn practice, rules get a lot bigger than this, and can be phrased using more\\nsophisticated query languages than just Boolean expressions, including the\\nuse of numeric scores. With careful crafting (that is, by humans tuning the\\nrules on development data), the accuracy of such rules can become very high.\\nJacobs and Rau (1990) report identifying articles about takeovers with 92%\\nprecision and 88.5% recall, and Hayes and Weinstein (1990) report 94% re-\\ncall and 84% precision over 675 categories on Reuters newswire documents.\\nNevertheless the amount of work to create such well-tuned rules is very\\nlarge. A reasonable estimate is 2 days per class, and extra time has to go\\n', 'Online edition (c)\\n2009 Cambridge UP\\n336\\n15\\nSupport vector machines and machine learning on documents\\ninto maintenance of rules, as the content of documents in classes drifts over\\ntime (cf. page 269).\\nIf you have fairly little data and you are going to train a supervised clas-\\nsiﬁer, then machine learning theory says you should stick to a classiﬁer with\\nhigh bias, as we discussed in Section 14.6 (page 308). For example, there\\nare theoretical and empirical results that Naive Bayes does well in such cir-\\ncumstances (Ng and Jordan 2001, Forman and Cohen 2004), although this\\neffect is not necessarily observed in practice with regularized models over\\ntextual data (Klein and Manning 2002). At any rate, a very low bias model\\nlike a nearest neighbor model is probably counterindicated. Regardless, the\\nquality of the model will be adversely affected by the limited training data.\\nHere, the theoretically interesting answer is to try to apply semi-supervised\\nSEMI-SUPERVISED\\nLEARNING\\ntraining methods. This includes methods such as bootstrapping or the EM\\nalgorithm, which we will introduce in Section 16.5 (page 368). In these meth-\\nods, the system gets some labeled documents, and a further large supply\\nof unlabeled documents over which it can attempt to learn. One of the big\\nadvantages of Naive Bayes is that it can be straightforwardly extended to\\nbe a semi-supervised learning algorithm, but for SVMs, there is also semi-\\nsupervised learning work which goes under the title of transductive SVMs.\\nTRANSDUCTIVE SVMS\\nSee the references for pointers.\\nOften, the practical answer is to work out how to get more labeled data as\\nquickly as you can. The best way to do this is to insert yourself into a process\\nwhere humans will be willing to label data for you as part of their natural\\ntasks. For example, in many cases humans will sort or route email for their\\nown purposes, and these actions give information about classes. The alter-\\nnative of getting human labelers expressly for the task of training classiﬁers\\nis often difﬁcult to organize, and the labeling is often of lower quality, be-\\ncause the labels are not embedded in a realistic task context. Rather than\\ngetting people to label all or a random sample of documents, there has also\\nbeen considerable research on active learning, where a system is built which\\nACTIVE LEARNING\\ndecides which documents a human should label. Usually these are the ones\\non which a classiﬁer is uncertain of the correct classiﬁcation. This can be ef-\\nfective in reducing annotation costs by a factor of 2–4, but has the problem\\nthat the good documents to label to train one type of classiﬁer often are not\\nthe good documents to label to train a different type of classiﬁer.\\nIf there is a reasonable amount of labeled data, then you are in the per-\\nfect position to use everything that we have presented about text classiﬁ-\\ncation. For instance, you may wish to use an SVM. However, if you are\\ndeploying a linear classiﬁer such as an SVM, you should probably design\\nan application that overlays a Boolean rule-based classiﬁer over the machine\\nlearning classiﬁer. Users frequently like to adjust things that do not come\\nout quite right, and if management gets on the phone and wants the classi-\\nﬁcation of a particular document ﬁxed right now, then this is much easier to\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n337\\ndo by hand-writing a rule than by working out how to adjust the weights\\nof an SVM without destroying the overall classiﬁcation accuracy. This is one\\nreason why machine learning models like decision trees which produce user-\\ninterpretable Boolean-like models retain considerable popularity.\\nIf a huge amount of data are available, then the choice of classiﬁer probably\\nhas little effect on your results and the best choice may be unclear (cf. Banko\\nand Brill 2001). It may be best to choose a classiﬁer based on the scalability\\nof training or even runtime efﬁciency. To get to this point, you need to have\\nhuge amounts of data. The general rule of thumb is that each doubling of\\nthe training data size produces a linear increase in classiﬁer performance,\\nbut with very large amounts of data, the improvement becomes sub-linear.\\n15.3.2\\nImproving classiﬁer performance\\nFor any particular application, there is usually signiﬁcant room for improv-\\ning classiﬁer effectiveness through exploiting features speciﬁc to the domain\\nor document collection. Often documents will contain zones which are espe-\\ncially useful for classiﬁcation. Often there will be particular subvocabularies\\nwhich demand special treatment for optimal classiﬁcation effectiveness.\\nLarge and difﬁcult category taxonomies\\nIf a text classiﬁcation problem consists of a small number of well-separated\\ncategories, then many classiﬁcation algorithms are likely to work well. But\\nmany real classiﬁcation problems consist of a very large number of often\\nvery similar categories. The reader might think of examples like web direc-\\ntories (the Yahoo! Directory or the Open Directory Project), library classi-\\nﬁcation schemes (Dewey Decimal or Library of Congress) or the classiﬁca-\\ntion schemes used in legal or medical applications. For instance, the Yahoo!\\nDirectory consists of over 200,000 categories in a deep hierarchy. Accurate\\nclassiﬁcation over large sets of closely related classes is inherently difﬁcult.\\nMost large sets of categories have a hierarchical structure, and attempting\\nto exploit the hierarchy by doing hierarchical classiﬁcation is a promising ap-\\nHIERARCHICAL\\nCLASSIFICATION\\nproach. However, at present the effectiveness gains from doing this rather\\nthan just working with the classes that are the leaves of the hierarchy re-\\nmain modest.6 But the technique can be very useful simply to improve the\\nscalability of building classiﬁers over large hierarchies. Another simple way\\nto improve the scalability of classiﬁers over large hierarchies is the use of\\naggressive feature selection. We provide references to some work on hierar-\\nchical classiﬁcation in Section 15.5.\\n6. Using the small hierarchy in Figure 13.1 (page 257) as an example, the leaf classes are ones\\nlike poultry and coffee, as opposed to higher-up classes like industries.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n338\\n15\\nSupport vector machines and machine learning on documents\\nA general result in machine learning is that you can always get a small\\nboost in classiﬁcation accuracy by combining multiple classiﬁers, provided\\nonly that the mistakes that they make are at least somewhat independent.\\nThere is now a large literature on techniques such as voting, bagging, and\\nboosting multiple classiﬁers. Again, there are some pointers in the refer-\\nences. Nevertheless, ultimately a hybrid automatic/manual solution may be\\nneeded to achieve sufﬁcient classiﬁcation accuracy. A common approach in\\nsuch situations is to run a classiﬁer ﬁrst, and to accept all its high conﬁdence\\ndecisions, but to put low conﬁdence decisions in a queue for manual review.\\nSuch a process also automatically leads to the production of new training\\ndata which can be used in future versions of the machine learning classiﬁer.\\nHowever, note that this is a case in point where the resulting training data is\\nclearly not randomly sampled from the space of documents.\\nFeatures for text\\nThe default in both ad hoc retrieval and text classiﬁcation is to use terms\\nas features. However, for text classiﬁcation, a great deal of mileage can be\\nachieved by designing additional features which are suited to a speciﬁc prob-\\nlem. Unlike the case of IR query languages, since these features are internal\\nto the classiﬁer, there is no problem of communicating these features to an\\nend user. This process is generally referred to as feature engineering. At pre-\\nFEATURE ENGINEERING\\nsent, feature engineering remains a human craft, rather than something done\\nby machine learning. Good feature engineering can often markedly improve\\nthe performance of a text classiﬁer. It is especially beneﬁcial in some of the\\nmost important applications of text classiﬁcation, like spam and porn ﬁlter-\\ning.\\nClassiﬁcation problems will often contain large numbers of terms which\\ncan be conveniently grouped, and which have a similar vote in text classi-\\nﬁcation problems. Typical examples might be year mentions or strings of\\nexclamation marks. Or they may be more specialized tokens like ISBNs or\\nchemical formulas. Often, using them directly in a classiﬁer would greatly in-\\ncrease the vocabulary without providing classiﬁcatory power beyond know-\\ning that, say, a chemical formula is present. In such cases, the number of\\nfeatures and feature sparseness can be reduced by matching such items with\\nregular expressions and converting them into distinguished tokens. Con-\\nsequently, effectiveness and classiﬁer speed are normally enhanced. Some-\\ntimes all numbers are converted into a single feature, but often some value\\ncan be had by distinguishing different kinds of numbers, such as four digit\\nnumbers (which are usually years) versus other cardinal numbers versus real\\nnumbers with a decimal point. Similar techniques can be applied to dates,\\nISBN numbers, sports game scores, and so on.\\nGoing in the other direction, it is often useful to increase the number of fea-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.3\\nIssues in the classiﬁcation of text documents\\n339\\ntures by matching parts of words, and by matching selected multiword pat-\\nterns that are particularly discriminative. Parts of words are often matched\\nby character k-gram features. Such features can be particularly good at pro-\\nviding classiﬁcation clues for otherwise unknown words when the classiﬁer\\nis deployed. For instance, an unknown word ending in -rase is likely to be an\\nenzyme, even if it wasn’t seen in the training data. Good multiword patterns\\nare often found by looking for distinctively common word pairs (perhaps\\nusing a mutual information criterion between words, in a similar way to\\nits use in Section 13.5.1 (page 272) for feature selection) and then using fea-\\nture selection methods evaluated against classes. They are useful when the\\ncomponents of a compound would themselves be misleading as classiﬁca-\\ntion cues. For instance, this would be the case if the keyword ethnic was\\nmost indicative of the categories food and arts, the keyword cleansing was\\nmost indicative of the category home, but the collocation ethnic cleansing in-\\nstead indicates the category world news. Some text classiﬁers also make use\\nof features from named entity recognizers (cf. page 195).\\nDo techniques like stemming and lowercasing (Section 2.2, page 22) help\\nfor text classiﬁcation? As always, the ultimate test is empirical evaluations\\nconducted on an appropriate test collection. But it is nevertheless useful to\\nnote that such techniques have a more restricted chance of being useful for\\nclassiﬁcation. For IR, you often need to collapse forms of a word like oxy-\\ngenate and oxygenation, because the appearance of either in a document is a\\ngood clue that the document will be relevant to a query about oxygenation.\\nGiven copious training data, stemming necessarily delivers no value for text\\nclassiﬁcation. If several forms that stem together have a similar signal, the\\nparameters estimated for all of them will have similar weights. Techniques\\nlike stemming help only in compensating for data sparseness. This can be\\na useful role (as noted at the start of this section), but often different forms\\nof a word can convey signiﬁcantly different cues about the correct document\\nclassiﬁcation. Overly aggressive stemming can easily degrade classiﬁcation\\nperformance.\\nDocument zones in text classiﬁcation\\nAs already discussed in Section 6.1, documents usually have zones, such as\\nmail message headers like the subject and author, or the title and keywords\\nof a research article. Text classiﬁers can usually gain from making use of\\nthese zones during training and classiﬁcation.\\nUpweighting document zones.\\nIn text classiﬁcation problems, you can fre-\\nquently get a nice boost to effectiveness by differentially weighting contri-\\nbutions from different document zones. Often, upweighting title words is\\nparticularly effective (Cohen and Singer 1999, p. 163). As a rule of thumb,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n340\\n15\\nSupport vector machines and machine learning on documents\\nit is often effective to double the weight of title words in text classiﬁcation\\nproblems. You can also get value from upweighting words from pieces of\\ntext that are not so much clearly deﬁned zones, but where nevertheless evi-\\ndence from document structure or content suggests that they are important.\\nMurata et al. (2000) suggest that you can also get value (in an ad hoc retrieval\\ncontext) from upweighting the ﬁrst sentence of a (newswire) document.\\nSeparate feature spaces for document zones.\\nThere are two strategies that\\ncan be used for document zones. Above we upweighted words that appear\\nin certain zones. This means that we are using the same features (that is, pa-\\nrameters are “tied” across different zones), but we pay more attention to the\\nPARAMETER TYING\\noccurrence of terms in particular zones. An alternative strategy is to have a\\ncompletely separate set of features and corresponding parameters for words\\noccurring in different zones. This is in principle more powerful: a word\\ncould usually indicate the topic Middle East when in the title but Commodities\\nwhen in the body of a document. But, in practice, tying parameters is usu-\\nally more successful. Having separate feature sets means having two or more\\ntimes as many parameters, many of which will be much more sparsely seen\\nin the training data, and hence with worse estimates, whereas upweighting\\nhas no bad effects of this sort. Moreover, it is quite uncommon for words to\\nhave different preferences when appearing in different zones; it is mainly the\\nstrength of their vote that should be adjusted. Nevertheless, ultimately this\\nis a contingent result, depending on the nature and quantity of the training\\ndata.\\nConnections to text summarization.\\nIn Section 8.7, we mentioned the ﬁeld\\nof text summarization, and how most work in that ﬁeld has adopted the\\nlimited goal of extracting and assembling pieces of the original text that are\\njudged to be central based on features of sentences that consider the sen-\\ntence’s position and content. Much of this work can be used to suggest zones\\nthat may be distinctively useful for text classiﬁcation. For example Kołcz\\net al. (2000) consider a form of feature selection where you classify docu-\\nments based only on words in certain zones. Based on text summarization\\nresearch, they consider using (i) only the title, (ii) only the ﬁrst paragraph,\\n(iii) only the paragraph with the most title words or keywords, (iv) the ﬁrst\\ntwo paragraphs or the ﬁrst and last paragraph, or (v) all sentences with a\\nminimum number of title words or keywords. In general, these positional\\nfeature selection methods produced as good results as mutual information\\n(Section 13.5.1), and resulted in quite competitive classiﬁers. Ko et al. (2004)\\nalso took inspiration from text summarization research to upweight sen-\\ntences with either words from the title or words that are central to the doc-\\nument’s content, leading to classiﬁcation accuracy gains of almost 1%. This\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n341\\npresumably works because most such sentences are somehow more central\\nto the concerns of the document.\\n?\\nExercise 15.4\\n[⋆⋆]\\nSpam email often makes use of various cloaking techniques to try to get through. One\\nmethod is to pad or substitute characters so as to defeat word-based text classiﬁers.\\nFor example, you see terms like the following in spam email:\\nRep1icaRolex\\nbonmus\\nViiiaaaagra\\npi11z\\nPHARlbdMACY\\n[LEV]i[IT]l[RA]\\nse∧xual\\nClAfLlS\\nDiscuss how you could engineer features that would largely defeat this strategy.\\nExercise 15.5\\n[⋆⋆]\\nAnother strategy often used by purveyors of email spam is to follow the message\\nthey wish to send (such as buying a cheap stock or whatever) with a paragraph of\\ntext from another innocuous source (such as a news article). Why might this strategy\\nbe effective? How might it be addressed by a text classiﬁer?\\nExercise 15.6\\n[⋆]\\nWhat other kinds of features appear as if they would be useful in an email spam\\nclassiﬁer?\\n15.4\\nMachine learning methods in ad hoc information retrieval\\nRather than coming up with term and document weighting functions by\\nhand, as we primarily did in Chapter 6, we can view different sources of rele-\\nvance signal (cosine score, title match, etc.) as features in a learning problem.\\nA classiﬁer that has been fed examples of relevant and nonrelevant docu-\\nments for each of a set of queries can then ﬁgure out the relative weights\\nof these signals. If we conﬁgure the problem so that there are pairs of a\\ndocument and a query which are assigned a relevance judgment of relevant\\nor nonrelevant, then we can think of this problem too as a text classiﬁcation\\nproblem. Taking such a classiﬁcation approach is not necessarily best, and\\nwe present an alternative in Section 15.4.2. Nevertheless, given the material\\nwe have covered, the simplest place to start is to approach this problem as\\na classiﬁcation problem, by ordering the documents according to the conﬁ-\\ndence of a two-class classiﬁer in its relevance decision. And this move is not\\npurely pedagogical; exactly this approach is sometimes used in practice.\\n15.4.1\\nA simple example of machine-learned scoring\\nIn this section we generalize the methodology of Section 6.1.2 (page 113) to\\nmachine learning of the scoring function. In Section 6.1.2 we considered a\\ncase where we had to combine Boolean indicators of relevance; here we con-\\nsider more general factors to further develop the notion of machine-learned\\n', 'Online edition (c)\\n2009 Cambridge UP\\n342\\n15\\nSupport vector machines and machine learning on documents\\nExample\\nDocID\\nQuery\\nCosine score\\nω\\nJudgment\\nΦ1\\n37\\nlinux operating system\\n0.032\\n3\\nrelevant\\nΦ2\\n37\\npenguin logo\\n0.02\\n4\\nnonrelevant\\nΦ3\\n238\\noperating system\\n0.043\\n2\\nrelevant\\nΦ4\\n238\\nruntime environment\\n0.004\\n2\\nnonrelevant\\nΦ5\\n1741\\nkernel layer\\n0.022\\n3\\nrelevant\\nΦ6\\n2094\\ndevice driver\\n0.03\\n2\\nrelevant\\nΦ7\\n3191\\ndevice driver\\n0.027\\n5\\nnonrelevant\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n· · ·\\n◮Table 15.3\\nTraining examples for machine-learned scoring.\\nrelevance. In particular, the factors we now consider go beyond Boolean\\nfunctions of query term presence in document zones, as in Section 6.1.2.\\nWe develop the ideas in a setting where the scoring function is a linear\\ncombination of two factors: (1) the vector space cosine similarity between\\nquery and document and (2) the minimum window width ω within which\\nthe query terms lie. As we noted in Section 7.2.2 (page 144), query term\\nproximity is often very indicative of a document being on topic, especially\\nwith longer documents and on the web. Among other things, this quantity\\ngives us an implementation of implicit phrases. Thus we have one factor that\\ndepends on the statistics of query terms in the document as a bag of words,\\nand another that depends on proximity weighting. We consider only two\\nfeatures in the development of the ideas because a two-feature exposition\\nremains simple enough to visualize. The technique can be generalized to\\nmany more features.\\nAs in Section 6.1.2, we are provided with a set of training examples, each\\nof which is a pair consisting of a query and a document, together with a\\nrelevance judgment for that document on that query that is either relevant or\\nnonrelevant. For each such example we can compute the vector space cosine\\nsimilarity, as well as the window width ω. The result is a training set as\\nshown in Table 15.3, which resembles Figure 6.5 (page 115) from Section 6.1.2.\\nHere, the two features (cosine score denoted α and window width ω) are\\nreal-valued predictors. If we once again quantify the judgment relevant as 1\\nand nonrelevant as 0, we seek a scoring function that combines the values of\\nthe features to generate a value that is (close to) 0 or 1. We wish this func-\\ntion to be in agreement with our set of training examples as far as possible.\\nWithout loss of generality, a linear classiﬁer will use a linear combination of\\nfeatures of the form\\nScore(d, q) = Score(α, ω) = aα + bω + c,\\n(15.17)\\nwith the coefﬁcients a, b, c to be learned from the training data. While it is\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n343\\n0\\n2\\n3\\n4\\n5\\n0\\n.\\n0\\n5\\n0\\n.\\n0\\n2\\n5\\nc\\no\\ns\\ni\\nn\\ne\\ns\\nc\\no\\nr\\ne\\n\\r\\nT\\ne\\nr\\nm\\np\\nr\\no\\nx\\ni\\nm\\ni\\nt\\ny\\n\\x18\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nR\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\nN\\n◮Figure 15.7\\nA collection of training examples. Each R denotes a training example\\nlabeled relevant, while each N is a training example labeled nonrelevant.\\npossible to formulate this as an error minimization problem as we did in\\nSection 6.1.2, it is instructive to visualize the geometry of Equation (15.17).\\nThe examples in Table 15.3 can be plotted on a two-dimensional plane with\\naxes corresponding to the cosine score α and the window width ω. This is\\ndepicted in Figure 15.7.\\nIn this setting, the function Score(α, ω) from Equation (15.17) represents\\na plane “hanging above” Figure 15.7. Ideally this plane (in the direction\\nperpendicular to the page containing Figure 15.7) assumes values close to\\n1 above the points marked R, and values close to 0 above the points marked\\nN. Since a plane is unlikely to assume only values close to 0 or 1 above the\\ntraining sample points, we make use of thresholding: given any query and\\ndocument for which we wish to determine relevance, we pick a value θ and\\nif Score(α, ω) > θ we declare the document to be relevant, else we declare\\nthe document to be nonrelevant. As we know from Figure 14.8 (page 301),\\nall points that satisfy Score(α, ω) = θ form a line (shown as a dashed line\\nin Figure 15.7) and we thus have a linear classiﬁer that separates relevant\\n', 'Online edition (c)\\n2009 Cambridge UP\\n344\\n15\\nSupport vector machines and machine learning on documents\\nfrom nonrelevant instances. Geometrically, we can ﬁnd the separating line\\nas follows. Consider the line passing through the plane Score(α, ω) whose\\nheight is θ above the page containing Figure 15.7. Project this line down onto\\nFigure 15.7; this will be the dashed line in Figure 15.7. Then, any subse-\\nquent query/document pair that falls below the dashed line in Figure 15.7 is\\ndeemed nonrelevant; above the dashed line, relevant.\\nThus, the problem of making a binary relevant/nonrelevant judgment given\\ntraining examples as above turns into one of learning the dashed line in Fig-\\nure 15.7 separating relevant training examples from the nonrelevant ones. Be-\\ning in the α-ω plane, this line can be written as a linear equation involving\\nα and ω, with two parameters (slope and intercept). The methods of lin-\\near classiﬁcation that we have already looked at in Chapters 13–15 provide\\nmethods for choosing this line. Provided we can build a sufﬁciently rich col-\\nlection of training samples, we can thus altogether avoid hand-tuning score\\nfunctions as in Section 7.2.3 (page 145). The bottleneck of course is the ability\\nto maintain a suitably representative set of training examples, whose rele-\\nvance assessments must be made by experts.\\n15.4.2\\nResult ranking by machine learning\\nThe above ideas can be readily generalized to functions of many more than\\ntwo variables. There are lots of other scores that are indicative of the rel-\\nevance of a document to a query, including static quality (PageRank-style\\nmeasures, discussed in Chapter 21), document age, zone contributions, doc-\\nument length, and so on. Providing that these measures can be calculated\\nfor a training document collection with relevance judgments, any number\\nof such measures can be used to train a machine learning classiﬁer. For in-\\nstance, we could train an SVM over binary relevance judgments, and order\\ndocuments based on their probability of relevance, which is monotonic with\\nthe documents’ signed distance from the decision boundary.\\nHowever, approaching IR result ranking like this is not necessarily the\\nright way to think about the problem.\\nStatisticians normally ﬁrst divide\\nproblems into classiﬁcation problems (where a categorical variable is pre-\\ndicted) versus regression problems (where a real number is predicted). In\\nREGRESSION\\nbetween is the specialized ﬁeld of ordinal regression where a ranking is pre-\\nORDINAL REGRESSION\\ndicted. Machine learning for ad hoc retrieval is most properly thought of as\\nan ordinal regression problem, where the goal is to rank a set of documents\\nfor a query, given training data of the same sort. This formulation gives\\nsome additional power, since documents can be evaluated relative to other\\ncandidate documents for the same query, rather than having to be mapped\\nto a global scale of goodness, while also weakening the problem space, since\\njust a ranking is required rather than an absolute measure of relevance. Is-\\nsues of ranking are especially germane in web search, where the ranking at\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.4\\nMachine learning methods in ad hoc information retrieval\\n345\\nthe very top of the results list is exceedingly important, whereas decisions\\nof relevance of a document to a query may be much less important. Such\\nwork can and has been pursued using the structural SVM framework which\\nwe mentioned in Section 15.2.2, where the class being predicted is a ranking\\nof results for a query, but here we will present the slightly simpler ranking\\nSVM.\\nThe construction of a ranking SVM proceeds as follows. We begin with a\\nRANKING SVM\\nset of judged queries. For each training query q, we have a set of documents\\nreturned in response to the query, which have been totally ordered by a per-\\nson for relevance to the query. We construct a vector of features ψj = ψ(dj, q)\\nfor each document/query pair, using features such as those discussed in Sec-\\ntion 15.4.1, and many more. For two documents di and dj, we then form the\\nvector of feature differences:\\nΦ(di, dj, q) = ψ(di, q) −ψ(dj, q)\\n(15.18)\\nBy hypothesis, one of di and dj has been judged more relevant. If di is\\njudged more relevant than dj, denoted di ≺dj (di should precede dj in the\\nresults ordering), then we will assign the vector Φ(di, dj, q) the class yijq =\\n+1; otherwise −1. The goal then is to build a classiﬁer which will return\\n⃗wTΦ(di, dj, q) > 0\\niff\\ndi ≺dj\\n(15.19)\\nThis SVM learning task is formalized in a manner much like the other exam-\\nples that we saw before:\\n(15.20)\\nFind ⃗w, and ξi,j ≥0 such that:\\n•\\n1\\n2⃗wT⃗w + C ∑i,j ξi,j is minimized\\n• and for all {Φ(di, dj, q) : di ≺dj}, ⃗wTΦ(di, dj, q) ≥1 −ξi,j\\nWe can leave out yijq in the statement of the constraint, since we only need\\nto consider the constraint for document pairs ordered in one direction, since\\n≺is antisymmetric. These constraints are then solved, as before, to give\\na linear classiﬁer which can rank pairs of documents. This approach has\\nbeen used to build ranking functions which outperform standard hand-built\\nranking functions in IR evaluations on standard data sets; see the references\\nfor papers that present such results.\\nBoth of the methods that we have just looked at use a linear weighting\\nof document features that are indicators of relevance, as has most work in\\nthis area. It is therefore perhaps interesting to note that much of traditional\\nIR weighting involves nonlinear scaling of basic measurements (such as log-\\nweighting of term frequency, or idf). At the present time, machine learning is\\nvery good at producing optimal weights for features in a linear combination\\n', 'Online edition (c)\\n2009 Cambridge UP\\n346\\n15\\nSupport vector machines and machine learning on documents\\n(or other similar restricted model classes), but it is not good at coming up\\nwith good nonlinear scalings of basic measurements. This area remains the\\ndomain of human feature engineering.\\nThe idea of learning ranking functions has been around for a number of\\nyears, but it is only very recently that sufﬁcient machine learning knowledge,\\ntraining document collections, and computational power have come together\\nto make this method practical and exciting. It is thus too early to write some-\\nthing deﬁnitive on machine learning approaches to ranking in information\\nretrieval, but there is every reason to expect the use and importance of ma-\\nchine learned ranking approaches to grow over time. While skilled humans\\ncan do a very good job at deﬁning ranking functions by hand, hand tuning\\nis difﬁcult, and it has to be done again for each new document collection and\\nclass of users.\\n?\\nExercise 15.7\\nPlot the ﬁrst 7 rows of Table 15.3 in the α-ω plane to produce a ﬁgure like that in\\nFigure 15.7.\\nExercise 15.8\\nWrite down the equation of a line in the α-ω plane separating the Rs from the Ns.\\nExercise 15.9\\nGive a training example (consisting of values for α, ω and the relevance judgment)\\nthat when added to the training set makes it impossible to separate the R’s from the\\nN’s using a line in the α-ω plane.\\n15.5\\nReferences and further reading\\nThe somewhat quirky name support vector machine originates in the neu-\\nral networks literature, where learning algorithms were thought of as ar-\\nchitectures, and often referred to as “machines”. The distinctive element of\\nthis model is that the decision boundary to use is completely decided (“sup-\\nported”) by a few training data points, the support vectors.\\nFor a more detailed presentation of SVMs, a good, well-known article-\\nlength introduction is (Burges 1998). Chen et al. (2005) introduce the more\\nrecent ν-SVM, which provides an alternative parameterization for dealing\\nwith inseparable problems, whereby rather than specifying a penalty C, you\\nspecify a parameter ν which bounds the number of examples which can ap-\\npear on the wrong side of the decision surface. There are now also several\\nbooks dedicated to SVMs, large margin learning, and kernels: (Cristianini\\nand Shawe-Taylor 2000) and (Schölkopf and Smola 2001) are more math-\\nematically oriented, while (Shawe-Taylor and Cristianini 2004) aims to be\\nmore practical. For the foundations by their originator, see (Vapnik 1998).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n15.5\\nReferences and further reading\\n347\\nSome recent, more general books on statistical learning, such as (Hastie et al.\\n2001) also give thorough coverage of SVMs.\\nThe construction of multiclass SVMs is discussed in (Weston and Watkins\\n1999), (Crammer and Singer 2001), and (Tsochantaridis et al. 2005). The last\\nreference provides an introduction to the general framework of structural\\nSVMs.\\nThe kernel trick was ﬁrst presented in (Aizerman et al. 1964). For more\\nabout string kernels and other kernels for structured data, see (Lodhi et al.\\n2002) and (Gaertner et al. 2002). The Advances in Neural Information Pro-\\ncessing (NIPS) conferences have become the premier venue for theoretical\\nmachine learning work, such as on SVMs. Other venues such as SIGIR are\\nmuch stronger on experimental methodology and using text-speciﬁc features\\nto improve classiﬁer effectiveness.\\nA recent comparison of most current machine learning classiﬁers (though\\non problems rather different from typical text problems) can be found in\\n(Caruana and Niculescu-Mizil 2006). (Li and Yang 2003), discussed in Sec-\\ntion 13.6, is the most recent comparative evaluation of machine learning clas-\\nsiﬁers on text classiﬁcation. Older examinations of classiﬁers on text prob-\\nlems can be found in (Yang 1999, Yang and Liu 1999, Dumais et al. 1998).\\nJoachims (2002a) presents his work on SVMs applied to text problems in de-\\ntail. Zhang and Oles (2001) present an insightful comparison of Naive Bayes,\\nregularized logistic regression and SVM classiﬁers.\\nJoachims (1999) discusses methods of making SVM learning practical over\\nlarge text data sets. Joachims (2006a) improves on this work.\\nA number of approaches to hierarchical classiﬁcation have been developed\\nin order to deal with the common situation where the classes to be assigned\\nhave a natural hierarchical organization (Koller and Sahami 1997, McCal-\\nlum et al. 1998, Weigend et al. 1999, Dumais and Chen 2000). In a recent\\nlarge study on scaling SVMs to the entire Yahoo! directory, Liu et al. (2005)\\nconclude that hierarchical classiﬁcation noticeably if still modestly outper-\\nforms ﬂat classiﬁcation. Classiﬁer effectiveness remains limited by the very\\nsmall number of training documents for many classes. For a more general\\napproach that can be applied to modeling relations between classes, which\\nmay be arbitrary rather than simply the case of a hierarchy, see Tsochan-\\ntaridis et al. (2005).\\nMoschitti and Basili (2004) investigate the use of complex nominals, proper\\nnouns and word senses as features in text classiﬁcation.\\nDietterich (2002) overviews ensemble methods for classiﬁer combination,\\nwhile Schapire (2003) focuses particularly on boosting, which is applied to\\ntext classiﬁcation in (Schapire and Singer 2000).\\nChapelle et al. (2006) present an introduction to work in semi-supervised\\nmethods, including in particular chapters on using EM for semi-supervised\\ntext classiﬁcation (Nigam et al. 2006) and on transductive SVMs (Joachims\\n', 'Online edition (c)\\n2009 Cambridge UP\\n348\\n15\\nSupport vector machines and machine learning on documents\\n2006b). Sindhwani and Keerthi (2006) present a more efﬁcient implementa-\\ntion of a transductive SVM for large data sets.\\nTong and Koller (2001) explore active learning with SVMs for text classi-\\nﬁcation; Baldridge and Osborne (2004) point out that examples selected for\\nannotation with one classiﬁer in an active learning context may be no better\\nthan random examples when used with another classiﬁer.\\nMachine learning approaches to ranking for ad hoc retrieval were pio-\\nneered in (Wong et al. 1988), (Fuhr 1992), and (Gey 1994). But limited training\\ndata and poor machine learning techniques meant that these pieces of work\\nachieved only middling results, and hence they only had limited impact at\\nthe time.\\nTaylor et al. (2006) study using machine learning to tune the parameters\\nof the BM25 family of ranking functions (Section 11.4.3, page 232) so as to\\nmaximize NDCG (Section 8.4, page 163). Machine learning approaches to\\nordinal regression appear in (Herbrich et al. 2000) and (Burges et al. 2005),\\nand are applied to clickstream data in (Joachims 2002b). Cao et al. (2006)\\nstudy how to make this approach effective in IR, and Qin et al. (2007) suggest\\nan extension involving using multiple hyperplanes. Yue et al. (2007) study\\nhow to do ranking with a structural SVM approach, and in particular show\\nhow this construction can be effectively used to directly optimize for MAP\\n(Section 8.4, page 158), rather than using surrogate measures like accuracy or\\narea under the ROC curve. Geng et al. (2007) study feature selection for the\\nranking problem.\\nOther approaches to learning to rank have also been shown to be effective\\nfor web search, such as (Burges et al. 2005, Richardson et al. 2006).\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n349\\n16\\nFlat clustering\\nClustering algorithms group a set of documents into subsets or clusters. The\\nCLUSTER\\nalgorithms’ goal is to create clusters that are coherent internally, but clearly\\ndifferent from each other. In other words, documents within a cluster should\\nbe as similar as possible; and documents in one cluster should be as dissimi-\\nlar as possible from documents in other clusters.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n◮Figure 16.1\\nAn example of a data set with a clear cluster structure.\\nClustering is the most common form of unsupervised learning. No super-\\nUNSUPERVISED\\nLEARNING\\nvision means that there is no human expert who has assigned documents\\nto classes. In clustering, it is the distribution and makeup of the data that\\nwill determine cluster membership. A simple example is Figure 16.1. It is\\nvisually clear that there are three distinct clusters of points. This chapter and\\nChapter 17 introduce algorithms that ﬁnd such clusters in an unsupervised\\nfashion.\\nThe difference between clustering and classiﬁcation may not seem great\\nat ﬁrst. After all, in both cases we have a partition of a set of documents\\ninto groups. But as we will see the two problems are fundamentally differ-\\nent. Classiﬁcation is a form of supervised learning (Chapter 13, page 256):\\nour goal is to replicate a categorical distinction that a human supervisor im-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n350\\n16\\nFlat clustering\\nposes on the data. In unsupervised learning, of which clustering is the most\\nimportant example, we have no such teacher to guide us.\\nThe key input to a clustering algorithm is the distance measure. In Fig-\\nure 16.1, the distance measure is distance in the 2D plane. This measure sug-\\ngests three different clusters in the ﬁgure. In document clustering, the dis-\\ntance measure is often also Euclidean distance. Different distance measures\\ngive rise to different clusterings. Thus, the distance measure is an important\\nmeans by which we can inﬂuence the outcome of clustering.\\nFlat clustering creates a ﬂat set of clusters without any explicit structure that\\nFLAT CLUSTERING\\nwould relate clusters to each other. Hierarchical clustering creates a hierarchy\\nof clusters and will be covered in Chapter 17. Chapter 17 also addresses the\\ndifﬁcult problem of labeling clusters automatically.\\nA second important distinction can be made between hard and soft cluster-\\ning algorithms. Hard clustering computes a hard assignment – each document\\nHARD CLUSTERING\\nis a member of exactly one cluster. The assignment of soft clustering algo-\\nSOFT CLUSTERING\\nrithms is soft – a document’s assignment is a distribution over all clusters.\\nIn a soft assignment, a document has fractional membership in several clus-\\nters. Latent semantic indexing, a form of dimensionality reduction, is a soft\\nclustering algorithm (Chapter 18, page 417).\\nThis chapter motivates the use of clustering in information retrieval by\\nintroducing a number of applications (Section 16.1), deﬁnes the problem\\nwe are trying to solve in clustering (Section 16.2) and discusses measures\\nfor evaluating cluster quality (Section 16.3). It then describes two ﬂat clus-\\ntering algorithms, K-means (Section 16.4), a hard clustering algorithm, and\\nthe Expectation-Maximization (or EM) algorithm (Section 16.5), a soft clus-\\ntering algorithm. K-means is perhaps the most widely used ﬂat clustering\\nalgorithm due to its simplicity and efﬁciency. The EM algorithm is a gen-\\neralization of K-means and can be applied to a large variety of document\\nrepresentations and distributions.\\n16.1\\nClustering in information retrieval\\nThe cluster hypothesis states the fundamental assumption we make when us-\\nCLUSTER HYPOTHESIS\\ning clustering in information retrieval.\\nCluster hypothesis. Documents in the same cluster behave similarly\\nwith respect to relevance to information needs.\\nThe hypothesis states that if there is a document from a cluster that is rele-\\nvant to a search request, then it is likely that other documents from the same\\ncluster are also relevant. This is because clustering puts together documents\\nthat share many terms. The cluster hypothesis essentially is the contiguity\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.1\\nClustering in information retrieval\\n351\\nApplication\\nWhat is\\nBeneﬁt\\nExample\\nclustered?\\nSearch result clustering\\nsearch\\nresults\\nmore effective information\\npresentation to user\\nFigure 16.2\\nScatter-Gather\\n(subsets\\nof)\\ncollection\\nalternative user interface:\\n“search without typing”\\nFigure 16.3\\nCollection clustering\\ncollection\\neffective information pre-\\nsentation for exploratory\\nbrowsing\\nMcKeown et al. (2002),\\nhttp://news.google.com\\nLanguage modeling\\ncollection\\nincreased precision and/or\\nrecall\\nLiu and Croft (2004)\\nCluster-based retrieval\\ncollection\\nhigher efﬁciency:\\nfaster\\nsearch\\nSalton (1971a)\\n◮Table 16.1\\nSome applications of clustering in information retrieval.\\nhypothesis in Chapter 14 (page 289). In both cases, we posit that similar\\ndocuments behave similarly with respect to relevance.\\nTable 16.1 shows some of the main applications of clustering in informa-\\ntion retrieval. They differ in the set of documents that they cluster – search\\nresults, collection or subsets of the collection – and the aspect of an informa-\\ntion retrieval system they try to improve – user experience, user interface,\\neffectiveness or efﬁciency of the search system. But they are all based on the\\nbasic assumption stated by the cluster hypothesis.\\nThe ﬁrst application mentioned in Table 16.1 is search result clustering where\\nSEARCH RESULT\\nCLUSTERING\\nby search results we mean the documents that were returned in response to\\na query. The default presentation of search results in information retrieval is\\na simple list. Users scan the list from top to bottom until they have found\\nthe information they are looking for. Instead, search result clustering clus-\\nters the search results, so that similar documents appear together. It is often\\neasier to scan a few coherent groups than many individual documents. This\\nis particularly useful if a search term has different word senses. The example\\nin Figure 16.2 is jaguar. Three frequent senses on the web refer to the car, the\\nanimal and an Apple operating system. The Clustered Results panel returned\\nby the Vivísimo search engine (http://vivisimo.com) can be a more effective user\\ninterface for understanding what is in the search results than a simple list of\\ndocuments.\\nA better user interface is also the goal of Scatter-Gather, the second ap-\\nSCATTER-GATHER\\nplication in Table 16.1. Scatter-Gather clusters the whole collection to get\\ngroups of documents that the user can select or gather. The selected groups\\nare merged and the resulting set is again clustered. This process is repeated\\nuntil a cluster of interest is found. An example is shown in Figure 16.3.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n352\\n16\\nFlat clustering\\n◮Figure 16.2\\nClustering of search results to improve recall.\\nNone of the top hits\\ncover the animal sense of jaguar, but users can easily access it by clicking on the cat\\ncluster in the Clustered Results panel on the left (third arrow from the top).\\nAutomatically generated clusters like those in Figure 16.3 are not as neatly\\norganized as a manually constructed hierarchical tree like the Open Direc-\\ntory at http://dmoz.org. Also, ﬁnding descriptive labels for clusters automati-\\ncally is a difﬁcult problem (Section 17.7, page 396). But cluster-based navi-\\ngation is an interesting alternative to keyword searching, the standard infor-\\nmation retrieval paradigm. This is especially true in scenarios where users\\nprefer browsing over searching because they are unsure about which search\\nterms to use.\\nAs an alternative to the user-mediated iterative clustering in Scatter-Gather,\\nwe can also compute a static hierarchical clustering of a collection that is\\nnot inﬂuenced by user interactions (“Collection clustering” in Table 16.1).\\nGoogle News and its precursor, the Columbia NewsBlaster system, are ex-\\namples of this approach. In the case of news, we need to frequently recom-\\npute the clustering to make sure that users can access the latest breaking\\nstories. Clustering is well suited for access to a collection of news stories\\nsince news reading is not really search, but rather a process of selecting a\\nsubset of stories about recent events.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.1\\nClustering in information retrieval\\n353\\n◮Figure 16.3\\nAn example of a user session in Scatter-Gather. A collection of New\\nYork Times news stories is clustered (“scattered”) into eight clusters (top row). The\\nuser manually gathers three of these into a smaller collection International Stories and\\nperforms another scattering operation. This process repeats until a small cluster with\\nrelevant documents is found (e.g., Trinidad).\\nThe fourth application of clustering exploits the cluster hypothesis directly\\nfor improving search results, based on a clustering of the entire collection.\\nWe use a standard inverted index to identify an initial set of documents that\\nmatch the query, but we then add other documents from the same clusters\\neven if they have low similarity to the query. For example, if the query is car\\nand several car documents are taken from a cluster of automobile documents,\\nthen we can add documents from this cluster that use terms other than car\\n(automobile, vehicle etc). This can increase recall since a group of documents\\nwith high mutual similarity is often relevant as a whole.\\nMore recently this idea has been used for language modeling. Equation (12.10),\\npage 245, showed that to avoid sparse data problems in the language mod-\\neling approach to IR, the model of document d can be interpolated with a\\n', 'Online edition (c)\\n2009 Cambridge UP\\n354\\n16\\nFlat clustering\\ncollection model. But the collection contains many documents with terms\\nuntypical of d. By replacing the collection model with a model derived from\\nd’s cluster, we get more accurate estimates of the occurrence probabilities of\\nterms in d.\\nClustering can also speed up search. As we saw in Section 6.3.2 (page 123)\\nsearch in the vector space model amounts to ﬁnding the nearest neighbors\\nto the query. The inverted index supports fast nearest-neighbor search for\\nthe standard IR setting. However, sometimes we may not be able to use an\\ninverted index efﬁciently, e.g., in latent semantic indexing (Chapter 18). In\\nsuch cases, we could compute the similarity of the query to every document,\\nbut this is slow. The cluster hypothesis offers an alternative: Find the clus-\\nters that are closest to the query and only consider documents from these\\nclusters. Within this much smaller set, we can compute similarities exhaus-\\ntively and rank documents in the usual way. Since there are many fewer\\nclusters than documents, ﬁnding the closest cluster is fast; and since the doc-\\numents matching a query are all similar to each other, they tend to be in\\nthe same clusters. While this algorithm is inexact, the expected decrease in\\nsearch quality is small. This is essentially the application of clustering that\\nwas covered in Section 7.1.6 (page 141).\\n?\\nExercise 16.1\\nDeﬁne two documents as similar if they have at least two proper names like Clinton\\nor Sarkozy in common. Give an example of an information need and two documents,\\nfor which the cluster hypothesis does not hold for this notion of similarity.\\nExercise 16.2\\nMake up a simple one-dimensional example (i.e. points on a line) with two clusters\\nwhere the inexactness of cluster-based retrieval shows up. In your example, retriev-\\ning clusters close to the query should do worse than direct nearest neighbor search.\\n16.2\\nProblem statement\\nWe can deﬁne the goal in hard ﬂat clustering as follows. Given (i) a set of\\ndocuments D = {d1, . . . , dN}, (ii) a desired number of clusters K, and (iii)\\nan objective function that evaluates the quality of a clustering, we want to\\nOBJECTIVE FUNCTION\\ncompute an assignment γ : D →{1, . . . , K} that minimizes (or, in other\\ncases, maximizes) the objective function. In most cases, we also demand that\\nγ is surjective, i.e., that none of the K clusters is empty.\\nThe objective function is often deﬁned in terms of similarity or distance\\nbetween documents. Below, we will see that the objective in K-means clus-\\ntering is to minimize the average distance between documents and their cen-\\ntroids or, equivalently, to maximize the similarity between documents and\\ntheir centroids. The discussion of similarity measures and distance metrics\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.2\\nProblem statement\\n355\\nin Chapter 14 (page 291) also applies to this chapter. As in Chapter 14, we use\\nboth similarity and distance to talk about relatedness between documents.\\nFor documents, the type of similarity we want is usually topic similarity\\nor high values on the same dimensions in the vector space model. For exam-\\nple, documents about China have high values on dimensions like Chinese,\\nBeijing, and Mao whereas documents about the UK tend to have high values\\nfor London, Britain and Queen. We approximate topic similarity with cosine\\nsimilarity or Euclidean distance in vector space (Chapter 6). If we intend to\\ncapture similarity of a type other than topic, for example, similarity of lan-\\nguage, then a different representation may be appropriate. When computing\\ntopic similarity, stop words can be safely ignored, but they are important\\ncues for separating clusters of English (in which the occurs frequently and la\\ninfrequently) and French documents (in which the occurs infrequently and la\\nfrequently).\\nA note on terminology.\\nAn alternative deﬁnition of hard clustering is that\\na document can be a full member of more than one cluster. Partitional clus-\\nPARTITIONAL\\nCLUSTERING\\ntering always refers to a clustering where each document belongs to exactly\\none cluster. (But in a partitional hierarchical clustering (Chapter 17) all mem-\\nbers of a cluster are of course also members of its parent.) On the deﬁnition\\nof hard clustering that permits multiple membership, the difference between\\nsoft clustering and hard clustering is that membership values in hard clus-\\ntering are either 0 or 1, whereas they can take on any non-negative value in\\nsoft clustering.\\nSome researchers distinguish between exhaustive clusterings that assign\\nEXHAUSTIVE\\neach document to a cluster and non-exhaustive clusterings, in which some\\ndocuments will be assigned to no cluster.\\nNon-exhaustive clusterings in\\nwhich each document is a member of either no cluster or one cluster are\\ncalled exclusive. We deﬁne clustering to be exhaustive in this book.\\nEXCLUSIVE\\n16.2.1\\nCardinality – the number of clusters\\nA difﬁcult issue in clustering is determining the number of clusters or cardi-\\nCARDINALITY\\nnality of a clustering, which we denote by K. Often K is nothing more than\\na good guess based on experience or domain knowledge. But for K-means,\\nwe will also introduce a heuristic method for choosing K and an attempt to\\nincorporate the selection of K into the objective function. Sometimes the ap-\\nplication puts constraints on the range of K. For example, the Scatter-Gather\\ninterface in Figure 16.3 could not display more than about K = 10 clusters\\nper layer because of the size and resolution of computer monitors in the early\\n1990s.\\nSince our goal is to optimize an objective function, clustering is essentially\\n', 'Online edition (c)\\n2009 Cambridge UP\\n356\\n16\\nFlat clustering\\na search problem. The brute force solution would be to enumerate all pos-\\nsible clusterings and pick the best. However, there are exponentially many\\npartitions, so this approach is not feasible.1 For this reason, most ﬂat clus-\\ntering algorithms reﬁne an initial partitioning iteratively. If the search starts\\nat an unfavorable initial point, we may miss the global optimum. Finding a\\ngood starting point is therefore another important problem we have to solve\\nin ﬂat clustering.\\n16.3\\nEvaluation of clustering\\nTypical objective functions in clustering formalize the goal of attaining high\\nintra-cluster similarity (documents within a cluster are similar) and low inter-\\ncluster similarity (documents from different clusters are dissimilar). This is\\nan internal criterion for the quality of a clustering. But good scores on an\\nINTERNAL CRITERION\\nOF QUALITY\\ninternal criterion do not necessarily translate into good effectiveness in an\\napplication. An alternative to internal criteria is direct evaluation in the ap-\\nplication of interest. For search result clustering, we may want to measure\\nthe time it takes users to ﬁnd an answer with different clustering algorithms.\\nThis is the most direct evaluation, but it is expensive, especially if large user\\nstudies are necessary.\\nAs a surrogate for user judgments, we can use a set of classes in an evalua-\\ntion benchmark or gold standard (see Section 8.5, page 164, and Section 13.6,\\npage 279). The gold standard is ideally produced by human judges with a\\ngood level of inter-judge agreement (see Chapter 8, page 152). We can then\\ncompute an external criterion that evaluates how well the clustering matches\\nEXTERNAL CRITERION\\nOF QUALITY\\nthe gold standard classes. For example, we may want to say that the opti-\\nmal clustering of the search results for jaguar in Figure 16.2 consists of three\\nclasses corresponding to the three senses car, animal, and operating system.\\nIn this type of evaluation, we only use the partition provided by the gold\\nstandard, not the class labels.\\nThis section introduces four external criteria of clustering quality. Purity is\\na simple and transparent evaluation measure. Normalized mutual information\\ncan be information-theoretically interpreted. The Rand index penalizes both\\nfalse positive and false negative decisions during clustering. The F measure\\nin addition supports differential weighting of these two types of errors.\\nTo compute purity, each cluster is assigned to the class which is most fre-\\nPURITY\\nquent in the cluster, and then the accuracy of this assignment is measured\\nby counting the number of correctly assigned documents and dividing by N.\\n1. An upper bound on the number of clusterings is KN/K!. The exact number of different\\npartitions of N documents into K clusters is the Stirling number of the second kind.\\nSee\\nhttp://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html or Comtet (1974).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.3\\nEvaluation of clustering\\n357\\nx\\no\\nx\\nx\\nx\\nx\\no\\nx\\no\\no ⋄\\no\\nx\\n⋄\\n⋄\\n⋄\\nx\\ncluster 1\\ncluster 2\\ncluster 3\\n◮Figure 16.4\\nPurity as an external evaluation criterion for cluster quality. Majority\\nclass and number of members of the majority class for the three clusters are: x, 5\\n(cluster 1); o, 4 (cluster 2); and ⋄, 3 (cluster 3). Purity is (1/17) × (5 + 4 + 3) ≈0.71.\\npurity\\nNMI\\nRI\\nF5\\nlower bound\\n0.0\\n0.0\\n0.0\\n0.0\\nmaximum\\n1\\n1\\n1\\n1\\nvalue for Figure 16.4\\n0.71\\n0.36\\n0.68\\n0.46\\n◮Table 16.2\\nThe four external evaluation measures applied to the clustering in\\nFigure 16.4.\\nFormally:\\npurity(Ω, C) = 1\\nN ∑\\nk\\nmax\\nj\\n|ωk ∩cj|\\n(16.1)\\nwhere Ω= {ω1, ω2, . . . , ωK} is the set of clusters and C = {c1, c2, . . . , cJ} is\\nthe set of classes. We interpret ωk as the set of documents in ωk and cj as the\\nset of documents in cj in Equation (16.1).\\nWe present an example of how to compute purity in Figure 16.4.2 Bad\\nclusterings have purity values close to 0, a perfect clustering has a purity of\\n1. Purity is compared with the other three measures discussed in this chapter\\nin Table 16.2.\\nHigh purity is easy to achieve when the number of clusters is large – in\\nparticular, purity is 1 if each document gets its own cluster. Thus, we cannot\\nuse purity to trade off the quality of the clustering against the number of\\nclusters.\\nA measure that allows us to make this tradeoff is normalized mutual infor-\\nNORMALIZED MUTUAL\\nINFORMATION\\n2. Recall our note of caution from Figure 14.2 (page 291) when looking at this and other 2D\\nﬁgures in this and the following chapter: these illustrations can be misleading because 2D pro-\\njections of length-normalized vectors distort similarities and distances between points.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n358\\n16\\nFlat clustering\\nmation or NMI:\\nNMI(Ω, C) =\\nI(Ω; C)\\n[H(Ω) + H(C)]/2\\n(16.2)\\nI is mutual information (cf. Chapter 13, page 272):\\nI(Ω; C)\\n= ∑\\nk ∑\\nj\\nP(ωk ∩cj) log P(ωk ∩cj)\\nP(ωk)P(cj)\\n(16.3)\\n= ∑\\nk ∑\\nj\\n|ωk ∩cj|\\nN\\nlog N|ωk ∩cj|\\n|ωk||cj|\\n(16.4)\\nwhere P(ωk), P(cj), and P(ωk ∩cj) are the probabilities of a document being\\nin cluster ωk, class cj, and in the intersection of ωk and cj, respectively. Equa-\\ntion (16.4) is equivalent to Equation (16.3) for maximum likelihood estimates\\nof the probabilities (i.e., the estimate of each probability is the corresponding\\nrelative frequency).\\nH is entropy as deﬁned in Chapter 5 (page 99):\\nH(Ω)\\n=\\n−∑\\nk\\nP(ωk) log P(ωk)\\n(16.5)\\n=\\n−∑\\nk\\n|ωk|\\nN log |ωk|\\nN\\n(16.6)\\nwhere, again, the second equation is based on maximum likelihood estimates\\nof the probabilities.\\nI(Ω; C) in Equation (16.3) measures the amount of information by which\\nour knowledge about the classes increases when we are told what the clusters\\nare. The minimum of I(Ω; C) is 0 if the clustering is random with respect to\\nclass membership. In that case, knowing that a document is in a particular\\ncluster does not give us any new information about what its class might be.\\nMaximum mutual information is reached for a clustering Ωexact that perfectly\\nrecreates the classes – but also if clusters in Ωexact are further subdivided into\\nsmaller clusters (Exercise 16.7). In particular, a clustering with K = N one-\\ndocument clusters has maximum MI. So MI has the same problem as purity:\\nit does not penalize large cardinalities and thus does not formalize our bias\\nthat, other things being equal, fewer clusters are better.\\nThe normalization by the denominator [H(Ω) + H(C)]/2 in Equation (16.2)\\nﬁxes this problem since entropy tends to increase with the number of clus-\\nters. For example, H(Ω) reaches its maximum log N for K = N, which en-\\nsures that NMI is low for K = N. Because NMI is normalized, we can use\\nit to compare clusterings with different numbers of clusters. The particular\\nform of the denominator is chosen because [H(Ω) + H(C)]/2 is a tight upper\\nbound on I(Ω; C) (Exercise 16.8). Thus, NMI is always a number between 0\\nand 1.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.3\\nEvaluation of clustering\\n359\\nAn alternative to this information-theoretic interpretation of clustering is\\nto view it as a series of decisions, one for each of the N(N −1)/2 pairs of\\ndocuments in the collection. We want to assign two documents to the same\\ncluster if and only if they are similar. A true positive (TP) decision assigns\\ntwo similar documents to the same cluster, a true negative (TN) decision as-\\nsigns two dissimilar documents to different clusters. There are two types\\nof errors we can commit. A false positive (FP) decision assigns two dissim-\\nilar documents to the same cluster. A false negative (FN) decision assigns\\ntwo similar documents to different clusters. The Rand index (RI) measures\\nRAND INDEX\\nRI\\nthe percentage of decisions that are correct. That is, it is simply accuracy\\n(Section 8.3, page 155).\\nRI =\\nTP + TN\\nTP + FP + FN + TN\\nAs an example, we compute RI for Figure 16.4. We ﬁrst compute TP + FP.\\nThe three clusters contain 6, 6, and 5 points, respectively, so the total number\\nof “positives” or pairs of documents that are in the same cluster is:\\nTP + FP =\\n\\x12\\n6\\n2\\n\\x13\\n+\\n\\x12\\n6\\n2\\n\\x13\\n+\\n\\x12\\n5\\n2\\n\\x13\\n= 40\\nOf these, the x pairs in cluster 1, the o pairs in cluster 2, the ⋄pairs in cluster 3,\\nand the x pair in cluster 3 are true positives:\\nTP =\\n\\x12\\n5\\n2\\n\\x13\\n+\\n\\x12\\n4\\n2\\n\\x13\\n+\\n\\x12\\n3\\n2\\n\\x13\\n+\\n\\x12\\n2\\n2\\n\\x13\\n= 20\\nThus, FP = 40 −20 = 20.\\nFN and TN are computed similarly, resulting in the following contingency\\ntable:\\nSame cluster\\nDifferent clusters\\nSame class\\nTP = 20\\nFN = 24\\nDifferent classes\\nFP = 20\\nTN = 72\\nRI is then (20 + 72)/(20 + 20 + 24 + 72) ≈0.68.\\nThe Rand index gives equal weight to false positives and false negatives.\\nSeparating similar documents is sometimes worse than putting pairs of dis-\\nsimilar documents in the same cluster. We can use the F measure (Section 8.3,\\nF MEASURE\\npage 154) to penalize false negatives more strongly than false positives by\\nselecting a value β > 1, thus giving more weight to recall.\\nP =\\nTP\\nTP + FP\\nR =\\nTP\\nTP + FN\\nFβ = (β2 + 1)PR\\nβ2P + R\\n', 'Online edition (c)\\n2009 Cambridge UP\\n360\\n16\\nFlat clustering\\nBased on the numbers in the contingency table, P = 20/40 = 0.5 and R =\\n20/44 ≈0.455. This gives us F1 ≈0.48 for β = 1 and F5 ≈0.456 for β = 5.\\nIn information retrieval, evaluating clustering with F has the advantage that\\nthe measure is already familiar to the research community.\\n?\\nExercise 16.3\\nReplace every point d in Figure 16.4 with two identical copies of d in the same class.\\n(i) Is it less difﬁcult, equally difﬁcult or more difﬁcult to cluster this set of 34 points\\nas opposed to the 17 points in Figure 16.4? (ii) Compute purity, NMI, RI, and F5 for\\nthe clustering with 34 points. Which measures increase and which stay the same after\\ndoubling the number of points? (iii) Given your assessment in (i) and the results in\\n(ii), which measures are best suited to compare the quality of the two clusterings?\\n16.4\\nK-means\\nK-means is the most important ﬂat clustering algorithm. Its objective is to\\nminimize the average squared Euclidean distance (Chapter 6, page 131) of\\ndocuments from their cluster centers where a cluster center is deﬁned as the\\nmean or centroid ⃗µ of the documents in a cluster ω:\\nCENTROID\\n⃗µ(ω) =\\n1\\n|ω| ∑\\n⃗x∈ω\\n⃗x\\nThe deﬁnition assumes that documents are represented as length-normalized\\nvectors in a real-valued space in the familiar way. We used centroids for Roc-\\nchio classiﬁcation in Chapter 14 (page 292). They play a similar role here.\\nThe ideal cluster in K-means is a sphere with the centroid as its center of\\ngravity. Ideally, the clusters should not overlap. Our desiderata for classes\\nin Rocchio classiﬁcation were the same. The difference is that we have no la-\\nbeled training set in clustering for which we know which documents should\\nbe in the same cluster.\\nA measure of how well the centroids represent the members of their clus-\\nters is the residual sum of squares or RSS, the squared distance of each vector\\nRESIDUAL SUM OF\\nSQUARES\\nfrom its centroid summed over all vectors:\\nRSSk = ∑\\n⃗x∈ωk\\n|⃗x −⃗µ(ωk)|2\\nRSS =\\nK\\n∑\\nk=1\\nRSSk\\n(16.7)\\nRSS is the objective function in K-means and our goal is to minimize it. Since\\nN is ﬁxed, minimizing RSS is equivalent to minimizing the average squared\\ndistance, a measure of how well centroids represent their documents.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n361\\nK-MEANS({⃗x1, . . . ,⃗xN}, K)\\n1\\n(⃗s1,⃗s2, . . . ,⃗sK) ←SELECTRANDOMSEEDS({⃗x1, . . . ,⃗xN}, K)\\n2\\nfor k ←1 to K\\n3\\ndo ⃗µk ←⃗sk\\n4\\nwhile stopping criterion has not been met\\n5\\ndo for k ←1 to K\\n6\\ndo ωk ←{}\\n7\\nfor n ←1 to N\\n8\\ndo j ←arg minj′ |⃗µj′ −⃗xn|\\n9\\nωj ←ωj ∪{⃗xn} (reassignment of vectors)\\n10\\nfor k ←1 to K\\n11\\ndo ⃗µk ←\\n1\\n|ωk| ∑⃗x∈ωk ⃗x (recomputation of centroids)\\n12\\nreturn {⃗µ1, . . . ,⃗µK}\\n◮Figure 16.5\\nThe K-means algorithm.\\nFor most IR applications, the vectors\\n⃗xn ∈RM should be length-normalized. Alternative methods of seed selection and\\ninitialization are discussed on page 364.\\nThe ﬁrst step of K-means is to select as initial cluster centers K randomly\\nselected documents, the seeds. The algorithm then moves the cluster centers\\nSEED\\naround in space in order to minimize RSS. As shown in Figure 16.5, this is\\ndone iteratively by repeating two steps until a stopping criterion is met: reas-\\nsigning documents to the cluster with the closest centroid; and recomputing\\neach centroid based on the current members of its cluster. Figure 16.6 shows\\nsnapshots from nine iterations of the K-means algorithm for a set of points.\\nThe “centroid” column of Table 17.2 (page 397) shows examples of centroids.\\nWe can apply one of the following termination conditions.\\n• A ﬁxed number of iterations I has been completed. This condition limits\\nthe runtime of the clustering algorithm, but in some cases the quality of\\nthe clustering will be poor because of an insufﬁcient number of iterations.\\n• Assignment of documents to clusters (the partitioning function γ) does\\nnot change between iterations. Except for cases with a bad local mini-\\nmum, this produces a good clustering, but runtimes may be unacceptably\\nlong.\\n• Centroids⃗µk do not change between iterations. This is equivalent to γ not\\nchanging (Exercise 16.5).\\n• Terminate when RSS falls below a threshold. This criterion ensures that\\nthe clustering is of a desired quality after termination. In practice, we\\n', 'Online edition (c)\\n2009 Cambridge UP\\n362\\n16\\nFlat clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n××\\nselection of seeds\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nbb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\nb\\n××\\nassignment of documents (iter. 1)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+ +\\no\\no\\n+\\no\\n+\\n+\\n++\\n+\\n+\\n+\\n+ o\\n+\\n+\\no\\n+\\n+\\n+\\n+\\no\\no\\n+\\no\\n+\\n+\\no\\n+\\no\\n×\\n×\\n×\\n×\\nrecomputation/movement of ⃗µ’s (iter. 1)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+\\n+ +\\n+\\n+\\n+\\no\\n+\\n+\\n++\\n+\\no\\n+\\no o\\no\\no\\n+\\no\\n+\\no\\n+\\no\\n+\\no\\no\\no\\n+\\no\\n+\\no\\n×\\n×\\n⃗µ’s after convergence (iter. 9)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n1\\n2\\n3\\n4\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n..\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nmovement of ⃗µ’s in 9 iterations\\n◮Figure 16.6\\nA K-means example for K = 2 in R2.\\nThe position of the two cen-\\ntroids (⃗µ’s shown as X’s in the top four panels) converges after nine iterations.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n363\\nneed to combine it with a bound on the number of iterations to guarantee\\ntermination.\\n• Terminate when the decrease in RSS falls below a threshold θ. For small θ,\\nthis indicates that we are close to convergence. Again, we need to combine\\nit with a bound on the number of iterations to prevent very long runtimes.\\nWe now show that K-means converges by proving that RSS monotonically\\ndecreases in each iteration. We will use decrease in the meaning decrease or does\\nnot change in this section. First, RSS decreases in the reassignment step since\\neach vector is assigned to the closest centroid, so the distance it contributes\\nto RSS decreases. Second, it decreases in the recomputation step because the\\nnew centroid is the vector ⃗v for which RSSk reaches its minimum.\\nRSSk(⃗v)\\n=\\n∑\\n⃗x∈ωk\\n|⃗v −⃗x|2 = ∑\\n⃗x∈ωk\\nM\\n∑\\nm=1\\n(vm −xm)2\\n(16.8)\\n∂RSSk(⃗v)\\n∂vm\\n=\\n∑\\n⃗x∈ωk\\n2(vm −xm)\\n(16.9)\\nwhere xm and vm are the mth components of their respective vectors. Setting\\nthe partial derivative to zero, we get:\\nvm =\\n1\\n|ωk| ∑\\n⃗x∈ωk\\nxm\\n(16.10)\\nwhich is the componentwise deﬁnition of the centroid. Thus, we minimize\\nRSSk when the old centroid is replaced with the new centroid. RSS, the sum\\nof the RSSk, must then also decrease during recomputation.\\nSince there is only a ﬁnite set of possible clusterings, a monotonically de-\\ncreasing algorithm will eventually arrive at a (local) minimum. Take care,\\nhowever, to break ties consistently, e.g., by assigning a document to the clus-\\nter with the lowest index if there are several equidistant centroids. Other-\\nwise, the algorithm can cycle forever in a loop of clusterings that have the\\nsame cost.\\nWhile this proves the convergence of K-means, there is unfortunately no\\nguarantee that a global minimum in the objective function will be reached.\\nThis is a particular problem if a document set contains many outliers, doc-\\nOUTLIER\\numents that are far from any other documents and therefore do not ﬁt well\\ninto any cluster. Frequently, if an outlier is chosen as an initial seed, then no\\nother vector is assigned to it during subsequent iterations. Thus, we end up\\nwith a singleton cluster (a cluster with only one document) even though there\\nSINGLETON CLUSTER\\nis probably a clustering with lower RSS. Figure 16.7 shows an example of a\\nsuboptimal clustering resulting from a bad choice of initial seeds.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n364\\n16\\nFlat clustering\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\n×\\n×\\n×\\n×\\n×\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n◮Figure 16.7\\nThe outcome of clustering in K-means depends on the initial seeds.\\nFor seeds d2 and d5, K-means converges to {{d1, d2, d3}, {d4, d5, d6}}, a suboptimal\\nclustering. For seeds d2 and d3, it converges to {{d1, d2, d4, d5}, {d3, d6}}, the global\\noptimum for K = 2.\\nAnother type of suboptimal clustering that frequently occurs is one with\\nempty clusters (Exercise 16.11).\\nEffective heuristics for seed selection include (i) excluding outliers from\\nthe seed set; (ii) trying out multiple starting points and choosing the cluster-\\ning with lowest cost; and (iii) obtaining seeds from another method such as\\nhierarchical clustering. Since deterministic hierarchical clustering methods\\nare more predictable than K-means, a hierarchical clustering of a small ran-\\ndom sample of size iK (e.g., for i = 5 or i = 10) often provides good seeds\\n(see the description of the Buckshot algorithm, Chapter 17, page 399).\\nOther initialization methods compute seeds that are not selected from the\\nvectors to be clustered. A robust method that works well for a large variety\\nof document distributions is to select i (e.g., i = 10) random vectors for each\\ncluster and use their centroid as the seed for this cluster. See Section 16.6 for\\nmore sophisticated initializations.\\nWhat is the time complexity of K-means? Most of the time is spent on com-\\nputing vector distances. One such operation costs Θ(M). The reassignment\\nstep computes KN distances, so its overall complexity is Θ(KNM). In the\\nrecomputation step, each vector gets added to a centroid once, so the com-\\nplexity of this step is Θ(NM). For a ﬁxed number of iterations I, the overall\\ncomplexity is therefore Θ(IKNM). Thus, K-means is linear in all relevant\\nfactors: iterations, number of clusters, number of vectors and dimensionality\\nof the space. This means that K-means is more efﬁcient than the hierarchical\\nalgorithms in Chapter 17. We had to ﬁx the number of iterations I, which can\\nbe tricky in practice. But in most cases, K-means quickly reaches either com-\\nplete convergence or a clustering that is close to convergence. In the latter\\ncase, a few documents would switch membership if further iterations were\\ncomputed, but this has a small effect on the overall quality of the clustering.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n365\\nThere is one subtlety in the preceding argument. Even a linear algorithm\\ncan be quite slow if one of the arguments of Θ(. . .) is large, and M usually is\\nlarge. High dimensionality is not a problem for computing the distance be-\\ntween two documents. Their vectors are sparse, so that only a small fraction\\nof the theoretically possible M componentwise differences need to be com-\\nputed. Centroids, however, are dense since they pool all terms that occur in\\nany of the documents of their clusters. As a result, distance computations are\\ntime consuming in a naive implementation of K-means. However, there are\\nsimple and effective heuristics for making centroid-document similarities as\\nfast to compute as document-document similarities. Truncating centroids to\\nthe most signiﬁcant k terms (e.g., k = 1000) hardly decreases cluster quality\\nwhile achieving a signiﬁcant speedup of the reassignment step (see refer-\\nences in Section 16.6).\\nThe same efﬁciency problem is addressed by K-medoids, a variant of K-\\nK-MEDOIDS\\nmeans that computes medoids instead of centroids as cluster centers. We\\ndeﬁne the medoid of a cluster as the document vector that is closest to the\\nMEDOID\\ncentroid. Since medoids are sparse document vectors, distance computations\\nare fast.\\n$\\n16.4.1\\nCluster cardinality in K-means\\nWe stated in Section 16.2 that the number of clusters K is an input to most ﬂat\\nclustering algorithms. What do we do if we cannot come up with a plausible\\nguess for K?\\nA naive approach would be to select the optimal value of K according to\\nthe objective function, namely the value of K that minimizes RSS. Deﬁning\\nRSSmin(K) as the minimal RSS of all clusterings with K clusters, we observe\\nthat RSSmin(K) is a monotonically decreasing function in K (Exercise 16.13),\\nwhich reaches its minimum 0 for K = N where N is the number of doc-\\numents. We would end up with each document being in its own cluster.\\nClearly, this is not an optimal clustering.\\nA heuristic method that gets around this problem is to estimate RSSmin(K)\\nas follows. We ﬁrst perform i (e.g., i = 10) clusterings with K clusters (each\\nwith a different initialization) and compute the RSS of each. Then we take the\\nminimum of the i RSS values. We denote this minimum by d\\nRSSmin(K). Now\\nwe can inspect the values d\\nRSSmin(K) as K increases and ﬁnd the “knee” in the\\ncurve – the point where successive decreases in d\\nRSSmin become noticeably\\nsmaller. There are two such points in Figure 16.8, one at K = 4, where the\\ngradient ﬂattens slightly, and a clearer ﬂattening at K = 9. This is typical:\\nthere is seldom a single best number of clusters. We still need to employ an\\nexternal constraint to choose from a number of possible values of K (4 and 9\\nin this case).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n366\\n16\\nFlat clustering\\n2\\n4\\n6\\n8\\n10\\n1750\\n1800\\n1850\\n1900\\n1950\\nnumber of clusters\\nresidual sum of squares\\n◮Figure 16.8\\nEstimated minimal residual sum of squares as a function of the num-\\nber of clusters in K-means. In this clustering of 1203 Reuters-RCV1 documents, there\\nare two points where the d\\nRSSmin curve ﬂattens: at 4 clusters and at 9 clusters. The\\ndocuments were selected from the categories China, Germany, Russia and Sports, so\\nthe K = 4 clustering is closest to the Reuters classiﬁcation.\\nA second type of criterion for cluster cardinality imposes a penalty for each\\nnew cluster – where conceptually we start with a single cluster containing all\\ndocuments and then search for the optimal number of clusters K by succes-\\nsively incrementing K by one. To determine the cluster cardinality in this\\nway, we create a generalized objective function that combines two elements:\\ndistortion, a measure of how much documents deviate from the prototype of\\nDISTORTION\\ntheir clusters (e.g., RSS for K-means); and a measure of model complexity. We\\nMODEL COMPLEXITY\\ninterpret a clustering here as a model of the data. Model complexity in clus-\\ntering is usually the number of clusters or a function thereof. For K-means,\\nwe then get this selection criterion for K:\\nK = arg min\\nK\\n[RSSmin(K) + λK]\\n(16.11)\\nwhere λ is a weighting factor. A large value of λ favors solutions with few\\nclusters. For λ = 0, there is no penalty for more clusters and K = N is the\\nbest solution.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.4\\nK-means\\n367\\nThe obvious difﬁculty with Equation (16.11) is that we need to determine\\nλ. Unless this is easier than determining K directly, then we are back to\\nsquare one. In some cases, we can choose values of λ that have worked well\\nfor similar data sets in the past. For example, if we periodically cluster news\\nstories from a newswire, there is likely to be a ﬁxed value of λ that gives us\\nthe right K in each successive clustering. In this application, we would not\\nbe able to determine K based on past experience since K changes.\\nA theoretical justiﬁcation for Equation (16.11) is the Akaike Information Cri-\\nAKAIKE INFORMATION\\nCRITERION\\nterion or AIC, an information-theoretic measure that trades off distortion\\nagainst model complexity. The general form of AIC is:\\nAIC:\\nK = arg min\\nK\\n[−2L(K) + 2q(K)]\\n(16.12)\\nwhere −L(K), the negative maximum log-likelihood of the data for K clus-\\nters, is a measure of distortion and q(K), the number of parameters of a\\nmodel with K clusters, is a measure of model complexity. We will not at-\\ntempt to derive the AIC here, but it is easy to understand intuitively. The\\nﬁrst property of a good model of the data is that each data point is modeled\\nwell by the model. This is the goal of low distortion. But models should\\nalso be small (i.e., have low model complexity) since a model that merely\\ndescribes the data (and therefore has zero distortion) is worthless. AIC pro-\\nvides a theoretical justiﬁcation for one particular way of weighting these two\\nfactors, distortion and model complexity, when selecting a model.\\nFor K-means, the AIC can be stated as follows:\\nAIC:\\nK = arg min\\nK\\n[RSSmin(K) + 2MK]\\n(16.13)\\nEquation (16.13) is a special case of Equation (16.11) for λ = 2M.\\nTo derive Equation (16.13) from Equation (16.12) observe that q(K) = KM\\nin K-means since each element of the K centroids is a parameter that can be\\nvaried independently; and that L(K) = −(1/2)RSSmin(K) (modulo a con-\\nstant) if we view the model underlying K-means as a Gaussian mixture with\\nhard assignment, uniform cluster priors and identical spherical covariance\\nmatrices (see Exercise 16.19).\\nThe derivation of AIC is based on a number of assumptions, e.g., that the\\ndata are independent and identically distributed. These assumptions are\\nonly approximately true for data sets in information retrieval. As a conse-\\nquence, the AIC can rarely be applied without modiﬁcation in text clustering.\\nIn Figure 16.8, the dimensionality of the vector space is M ≈50,000. Thus,\\n2MK > 50,000 dominates the smaller RSS-based term (d\\nRSSmin(1) < 5000,\\nnot shown in the ﬁgure) and the minimum of the expression is reached for\\nK = 1. But as we know, K = 4 (corresponding to the four classes China,\\n', 'Online edition (c)\\n2009 Cambridge UP\\n368\\n16\\nFlat clustering\\nGermany, Russia and Sports) is a better choice than K = 1. In practice, Equa-\\ntion (16.11) is often more useful than Equation (16.13) – with the caveat that\\nwe need to come up with an estimate for λ.\\n?\\nExercise 16.4\\nWhy are documents that do not use the same term for the concept car likely to end\\nup in the same cluster in K-means clustering?\\nExercise 16.5\\nTwo of the possible termination conditions for K-means were (1) assignment does not\\nchange, (2) centroids do not change (page 361). Do these two conditions imply each\\nother?\\n$\\n16.5\\nModel-based clustering\\nIn this section, we describe a generalization of K-means, the EM algorithm.\\nIt can be applied to a larger variety of document representations and distri-\\nbutions than K-means.\\nIn K-means, we attempt to ﬁnd centroids that are good representatives. We\\ncan view the set of K centroids as a model that generates the data. Generating\\na document in this model consists of ﬁrst picking a centroid at random and\\nthen adding some noise. If the noise is normally distributed, this procedure\\nwill result in clusters of spherical shape. Model-based clustering assumes that\\nMODEL-BASED\\nCLUSTERING\\nthe data were generated by a model and tries to recover the original model\\nfrom the data. The model that we recover from the data then deﬁnes clusters\\nand an assignment of documents to clusters.\\nA commonly used criterion for estimating the model parameters is maxi-\\nmum likelihood. In K-means, the quantity exp(−RSS) is proportional to the\\nlikelihood that a particular model (i.e., a set of centroids) generated the data.\\nFor K-means, maximum likelihood and minimal RSS are equivalent criteria.\\nWe denote the model parameters by Θ. In K-means, Θ = {⃗µ1, . . . ,⃗µK}.\\nMore generally, the maximum likelihood criterion is to select the parame-\\nters Θ that maximize the log-likelihood of generating the data D:\\nΘ = arg max\\nΘ\\nL(D|Θ) = arg max\\nΘ\\nlog\\nN\\n∏\\nn=1\\nP(dn|Θ) = arg max\\nΘ\\nN\\n∑\\nn=1\\nlog P(dn|Θ)\\nL(D|Θ) is the objective function that measures the goodness of the cluster-\\ning. Given two clusterings with the same number of clusters, we prefer the\\none with higher L(D|Θ).\\nThis is the same approach we took in Chapter 12 (page 237) for language\\nmodeling and in Section 13.1 (page 265) for text classiﬁcation. In text clas-\\nsiﬁcation, we chose the class that maximizes the likelihood of generating a\\nparticular document. Here, we choose the clustering Θ that maximizes the\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.5\\nModel-based clustering\\n369\\nlikelihood of generating a given set of documents. Once we have Θ, we can\\ncompute an assignment probability P(d|ωk; Θ) for each document-cluster\\npair. This set of assignment probabilities deﬁnes a soft clustering.\\nAn example of a soft assignment is that a document about Chinese cars\\nmay have a fractional membership of 0.5 in each of the two clusters China\\nand automobiles, reﬂecting the fact that both topics are pertinent. A hard clus-\\ntering like K-means cannot model this simultaneous relevance to two topics.\\nModel-based clustering provides a framework for incorporating our know-\\nledge about a domain. K-means and the hierarchical algorithms in Chap-\\nter 17 make fairly rigid assumptions about the data. For example, clusters\\nin K-means are assumed to be spheres. Model-based clustering offers more\\nﬂexibility. The clustering model can be adapted to what we know about\\nthe underlying distribution of the data, be it Bernoulli (as in the example\\nin Table 16.3), Gaussian with non-spherical variance (another model that is\\nimportant in document clustering) or a member of a different family.\\nA commonly used algorithm for model-based clustering is the Expectation-\\nEXPECTATION-\\nMAXIMIZATION\\nALGORITHM\\nMaximization algorithm or EM algorithm. EM clustering is an iterative algo-\\nrithm that maximizes L(D|Θ). EM can be applied to many different types of\\nprobabilistic modeling. We will work with a mixture of multivariate Bernoulli\\ndistributions here, the distribution we know from Section 11.3 (page 222) and\\nSection 13.3 (page 263):\\nP(d|ωk; Θ) =\\n \\n∏\\ntm∈d\\nqmk\\n!  \\n∏\\ntm/∈d\\n(1 −qmk)\\n!\\n(16.14)\\nwhere Θ = {Θ1, . . . , ΘK}, Θk = (αk, q1k, . . . , qMk), and qmk = P(Um = 1|ωk)\\nare the parameters of the model.3 P(Um = 1|ωk) is the probability that a\\ndocument from cluster ωk contains term tm. The probability αk is the prior of\\ncluster ωk: the probability that a document d is in ωk if we have no informa-\\ntion about d.\\nThe mixture model then is:\\nP(d|Θ) =\\nK\\n∑\\nk=1\\nαk\\n \\n∏\\ntm∈d\\nqmk\\n!  \\n∏\\ntm/∈d\\n(1 −qmk)\\n!\\n(16.15)\\nIn this model, we generate a document by ﬁrst picking a cluster k with prob-\\nability αk and then generating the terms of the document according to the\\nparameters qmk. Recall that the document representation of the multivariate\\nBernoulli is a vector of M Boolean values (and not a real-valued vector).\\n3. Um is the random variable we deﬁned in Section 13.3 (page 266) for the Bernoulli Naive Bayes\\nmodel. It takes the values 1 (term tm is present in the document) and 0 (term tm is absent in the\\ndocument).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n370\\n16\\nFlat clustering\\nHow do we use EM to infer the parameters of the clustering from the data?\\nThat is, how do we choose parameters Θ that maximize L(D|Θ)? EM is simi-\\nlar to K-means in that it alternates between an expectation step, corresponding\\nEXPECTATION STEP\\nto reassignment, and a maximization step, corresponding to recomputation of\\nMAXIMIZATION STEP\\nthe parameters of the model. The parameters of K-means are the centroids,\\nthe parameters of the instance of EM in this section are the αk and qmk.\\nThe maximization step recomputes the conditional parameters qmk and the\\npriors αk as follows:\\nMaximization step:\\nqmk = ∑N\\nn=1 rnkI(tm ∈dn)\\n∑N\\nn=1 rnk\\nαk = ∑N\\nn=1 rnk\\nN\\n(16.16)\\nwhere I(tm ∈dn) = 1 if tm ∈dn and 0 otherwise and rnk is the soft as-\\nsignment of document dn to cluster k as computed in the preceding iteration.\\n(We’ll address the issue of initialization in a moment.) These are the max-\\nimum likelihood estimates for the parameters of the multivariate Bernoulli\\nfrom Table 13.3 (page 268) except that documents are assigned fractionally to\\nclusters here. These maximum likelihood estimates maximize the likelihood\\nof the data given the model.\\nThe expectation step computes the soft assignment of documents to clus-\\nters given the current parameters qmk and αk:\\nExpectation step :\\nrnk =\\nαk(∏tm∈dn qmk)(∏tm/∈dn(1 −qmk))\\n∑K\\nk=1 αk(∏tm∈dn qmk)(∏tm/∈dn(1 −qmk))\\n(16.17)\\nThis expectation step applies Equations (16.14) and (16.15) to computing the\\nlikelihood that ωk generated document dn. It is the classiﬁcation procedure\\nfor the multivariate Bernoulli in Table 13.3. Thus, the expectation step is\\nnothing else but Bernoulli Naive Bayes classiﬁcation (including normaliza-\\ntion, i.e. dividing by the denominator, to get a probability distribution over\\nclusters).\\nWe clustered a set of 11 documents into two clusters using EM in Ta-\\nble 16.3. After convergence in iteration 25, the ﬁrst 5 documents are assigned\\nto cluster 1 (ri,1 = 1.00) and the last 6 to cluster 2 (ri,1 = 0.00). Somewhat\\natypically, the ﬁnal assignment is a hard assignment here. EM usually con-\\nverges to a soft assignment.\\nIn iteration 25, the prior α1 for cluster 1 is\\n5/11 ≈0.45 because 5 of the 11 documents are in cluster 1. Some terms\\nare quickly associated with one cluster because the initial assignment can\\n“spread” to them unambiguously. For example, membership in cluster 2\\nspreads from document 7 to document 8 in the ﬁrst iteration because they\\nshare sugar (r8,1 = 0 in iteration 1).\\nFor parameters of terms occurring\\nin ambiguous contexts, convergence takes longer. Seed documents 6 and 7\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.5\\nModel-based clustering\\n371\\n(a)\\ndocID\\ndocument text\\ndocID\\ndocument text\\n1\\nhot chocolate cocoa beans\\n7\\nsweet sugar\\n2\\ncocoa ghana africa\\n8\\nsugar cane brazil\\n3\\nbeans harvest ghana\\n9\\nsweet sugar beet\\n4\\ncocoa butter\\n10\\nsweet cake icing\\n5\\nbutter trufﬂes\\n11\\ncake black forest\\n6\\nsweet chocolate\\n(b)\\nParameter\\nIteration of clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n15\\n25\\nα1\\n0.50\\n0.45\\n0.53\\n0.57\\n0.58\\n0.54\\n0.45\\nr1,1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\nr2,1\\n0.50\\n0.79\\n0.99\\n1.00\\n1.00\\n1.00\\n1.00\\nr3,1\\n0.50\\n0.84\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\nr4,1\\n0.50\\n0.75\\n0.94\\n1.00\\n1.00\\n1.00\\n1.00\\nr5,1\\n0.50\\n0.52\\n0.66\\n0.91\\n1.00\\n1.00\\n1.00\\nr6,1\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n1.00\\n0.83\\n0.00\\nr7,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr8,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr9,1\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nr10,1\\n0.50\\n0.40\\n0.14\\n0.01\\n0.00\\n0.00\\n0.00\\nr11,1\\n0.50\\n0.57\\n0.58\\n0.41\\n0.07\\n0.00\\n0.00\\nqafrica,1\\n0.000\\n0.100\\n0.134\\n0.158\\n0.158\\n0.169\\n0.200\\nqafrica,2\\n0.000\\n0.083\\n0.042\\n0.001\\n0.000\\n0.000\\n0.000\\nqbrazil,1\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\nqbrazil,2\\n0.000\\n0.167\\n0.195\\n0.213\\n0.214\\n0.196\\n0.167\\nqcocoa,1\\n0.000\\n0.400\\n0.432\\n0.465\\n0.474\\n0.508\\n0.600\\nqcocoa,2\\n0.000\\n0.167\\n0.090\\n0.014\\n0.001\\n0.000\\n0.000\\nqsugar,1\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\n0.000\\nqsugar,2\\n1.000\\n0.500\\n0.585\\n0.640\\n0.642\\n0.589\\n0.500\\nqsweet,1\\n1.000\\n0.300\\n0.238\\n0.180\\n0.159\\n0.153\\n0.000\\nqsweet,2\\n1.000\\n0.417\\n0.507\\n0.610\\n0.640\\n0.608\\n0.667\\n◮Table 16.3\\nThe EM clustering algorithm. The table shows a set of documents\\n(a) and parameter values for selected iterations during EM clustering (b). Parameters\\nshown are prior α1, soft assignment scores rn,1 (both omitted for cluster 2), and lexical\\nparameters qm,k for a few terms. The authors initially assigned document 6 to clus-\\nter 1 and document 7 to cluster 2 (iteration 0). EM converges after 25 iterations. For\\nsmoothing, the rnk in Equation (16.16) were replaced with rnk + ǫ where ǫ = 0.0001.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n372\\n16\\nFlat clustering\\nboth contain sweet. As a result, it takes 25 iterations for the term to be unam-\\nbiguously associated with cluster 2. (qsweet,1 = 0 in iteration 25.)\\nFinding good seeds is even more critical for EM than for K-means. EM is\\nprone to get stuck in local optima if the seeds are not chosen well. This is a\\ngeneral problem that also occurs in other applications of EM.4 Therefore, as\\nwith K-means, the initial assignment of documents to clusters is often com-\\nputed by a different algorithm. For example, a hard K-means clustering may\\nprovide the initial assignment, which EM can then “soften up.”\\n?\\nExercise 16.6\\nWe saw above that the time complexity of K-means is Θ(IKNM). What is the time\\ncomplexity of EM?\\n16.6\\nReferences and further reading\\nBerkhin (2006b) gives a general up-to-date survey of clustering methods with\\nspecial attention to scalability. The classic reference for clustering in pat-\\ntern recognition, covering both K-means and EM, is (Duda et al. 2000). Ras-\\nmussen (1992) introduces clustering from an information retrieval perspec-\\ntive. Anderberg (1973) provides a general introduction to clustering for ap-\\nplications. In addition to Euclidean distance and cosine similarity, Kullback-\\nLeibler divergence is often used in clustering as a measure of how (dis)similar\\ndocuments and clusters are (Xu and Croft 1999, Muresan and Harper 2004,\\nKurland and Lee 2004).\\nThe cluster hypothesis is due to Jardine and van Rijsbergen (1971) who\\nstate it as follows: Associations between documents convey information about the\\nrelevance of documents to requests. Salton (1971a; 1975), Croft (1978), Voorhees\\n(1985a), Can and Ozkarahan (1990), Cacheda et al. (2003), Can et al. (2004),\\nSingitham et al. (2004) and Altingövde et al. (2008) investigate the efﬁciency\\nand effectiveness of cluster-based retrieval.\\nWhile some of these studies\\nshow improvements in effectiveness, efﬁciency or both, there is no consensus\\nthat cluster-based retrieval works well consistently across scenarios. Cluster-\\nbased language modeling was pioneered by Liu and Croft (2004).\\nThere is good evidence that clustering of search results improves user ex-\\nperience and search result quality (Hearst and Pedersen 1996, Zamir and Et-\\nzioni 1999, Tombros et al. 2002, Käki 2005, Toda and Kataoka 2005), although\\nnot as much as search result structuring based on carefully edited category\\nhierarchies (Hearst 2006). The Scatter-Gather interface for browsing collec-\\ntions was presented by Cutting et al. (1992). A theoretical framework for an-\\n4. For example, this problem is common when EM is used to estimate parameters of hidden\\nMarkov models, probabilistic grammars, and machine translation models in natural language\\nprocessing (Manning and Schütze 1999).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.6\\nReferences and further reading\\n373\\nalyzing the properties of Scatter/Gather and other information seeking user\\ninterfaces is presented by Pirolli (2007). Schütze and Silverstein (1997) eval-\\nuate LSI (Chapter 18) and truncated representations of centroids for efﬁcient\\nK-means clustering.\\nThe Columbia NewsBlaster system (McKeown et al. 2002), a forerunner to\\nthe now much more famous and reﬁned Google News (http://news.google.com),\\nused hierarchical clustering (Chapter 17) to give two levels of news topic\\ngranularity. See Hatzivassiloglou et al. (2000) for details, and Chen and Lin\\n(2000) and Radev et al. (2001) for related systems.\\nOther applications of\\nclustering in information retrieval are duplicate detection (Yang and Callan\\n(2006), Section 19.6, page 438), novelty detection (see references in Section 17.9,\\npage 399) and metadata discovery on the semantic web (Alonso et al. 2006).\\nThe discussion of external evaluation measures is partially based on Strehl\\n(2002). Dom (2002) proposes a measure Q0 that is better motivated theoret-\\nically than NMI. Q0 is the number of bits needed to transmit class member-\\nships assuming cluster memberships are known. The Rand index is due to\\nRand (1971). Hubert and Arabie (1985) propose an adjusted Rand index that\\nADJUSTED RAND INDEX\\nranges between −1 and 1 and is 0 if there is only chance agreement between\\nclusters and classes (similar to κ in Chapter 8, page 165). Basu et al. (2004) ar-\\ngue that the three evaluation measures NMI, Rand index and F measure give\\nvery similar results. Stein et al. (2003) propose expected edge density as an in-\\nternal measure and give evidence that it is a good predictor of the quality of a\\nclustering. Kleinberg (2002) and Meil˘a (2005) present axiomatic frameworks\\nfor comparing clusterings.\\nAuthors that are often credited with the invention of the K-means algo-\\nrithm include Lloyd (1982) (ﬁrst distributed in 1957), Ball (1965), MacQueen\\n(1967), and Hartigan and Wong (1979). Arthur and Vassilvitskii (2006) in-\\nvestigate the worst-case complexity of K-means. Bradley and Fayyad (1998),\\nPelleg and Moore (1999) and Davidson and Satyanarayana (2003) investi-\\ngate the convergence properties of K-means empirically and how it depends\\non initial seed selection. Dhillon and Modha (2001) compare K-means clus-\\nters with SVD-based clusters (Chapter 18). The K-medoid algorithm was\\npresented by Kaufman and Rousseeuw (1990). The EM algorithm was orig-\\ninally introduced by Dempster et al. (1977). An in-depth treatment of EM is\\n(McLachlan and Krishnan 1996). See Section 18.5 (page 417) for publications\\non latent analysis, which can also be viewed as soft clustering.\\nAIC is due to Akaike (1974) (see also Burnham and Anderson (2002)). An\\nalternative to AIC is BIC, which can be motivated as a Bayesian model se-\\nlection procedure (Schwarz 1978). Fraley and Raftery (1998) show how to\\nchoose an optimal number of clusters based on BIC. An application of BIC to\\nK-means is (Pelleg and Moore 2000). Hamerly and Elkan (2003) propose an\\nalternative to BIC that performs better in their experiments. Another inﬂu-\\nential Bayesian approach for determining the number of clusters (simultane-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n374\\n16\\nFlat clustering\\nously with cluster assignment) is described by Cheeseman and Stutz (1996).\\nTwo methods for determining cardinality without external criteria are pre-\\nsented by Tibshirani et al. (2001).\\nWe only have space here for classical completely unsupervised clustering.\\nAn important current topic of research is how to use prior knowledge to\\nguide clustering (e.g., Ji and Xu (2006)) and how to incorporate interactive\\nfeedback during clustering (e.g., Huang and Mitchell (2006)). Fayyad et al.\\n(1998) propose an initialization for EM clustering. For algorithms that can\\ncluster very large data sets in one scan through the data see Bradley et al.\\n(1998).\\nThe applications in Table 16.1 all cluster documents. Other information re-\\ntrieval applications cluster words (e.g., Crouch 1988), contexts of words (e.g.,\\nSchütze and Pedersen 1995) or words and documents simultaneously (e.g.,\\nTishby and Slonim 2000, Dhillon 2001, Zha et al. 2001). Simultaneous clus-\\ntering of words and documents is an example of co-clustering or biclustering.\\nCO-CLUSTERING\\n16.7\\nExercises\\n?\\nExercise 16.7\\nLet Ωbe a clustering that exactly reproduces a class structure C and Ω′ a clustering\\nthat further subdivides some clusters in Ω. Show that I(Ω; C) = I(Ω′; C).\\nExercise 16.8\\nShow that I(Ω; C) ≤[H(Ω) + H(C)]/2.\\nExercise 16.9\\nMutual information is symmetric in the sense that its value does not change if the\\nroles of clusters and classes are switched: I(Ω; C) = I(C; Ω). Which of the other\\nthree evaluation measures are symmetric in this sense?\\nExercise 16.10\\nCompute RSS for the two clusterings in Figure 16.7.\\nExercise 16.11\\n(i) Give an example of a set of points and three initial centroids (which need not be\\nmembers of the set of points) for which 3-means converges to a clustering with an\\nempty cluster. (ii) Can a clustering with an empty cluster be the global optimum with\\nrespect to RSS?\\nExercise 16.12\\nDownload Reuters-21578.\\nDiscard documents that do not occur in one of the 10\\nclasses acquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat.\\nDiscard documents that occur in two of these 10 classes. (i) Compute a K-means clus-\\ntering of this subset into 10 clusters. There are a number of software packages that\\nimplement K-means, such as WEKA (Witten and Frank 2005) and R (R Development\\nCore Team 2005). (ii) Compute purity, normalized mutual information, F1 and RI for\\n', 'Online edition (c)\\n2009 Cambridge UP\\n16.7\\nExercises\\n375\\nthe clustering with respect to the 10 classes. (iii) Compile a confusion matrix (Ta-\\nble 14.5, page 308) for the 10 classes and 10 clusters. Identify classes that give rise to\\nfalse positives and false negatives.\\nExercise 16.13\\nProve that RSSmin(K) is monotonically decreasing in K.\\nExercise 16.14\\nThere is a soft version of K-means that computes the fractional membership of a doc-\\nument in a cluster as a monotonically decreasing function of the distance ∆from its\\ncentroid, e.g., as e−∆. Modify reassignment and recomputation steps of hard K-means\\nfor this soft version.\\nExercise 16.15\\nIn the last iteration in Table 16.3, document 6 is in cluster 2 even though it was the\\ninitial seed for cluster 1. Why does the document change membership?\\nExercise 16.16\\nThe values of the parameters qmk in iteration 25 in Table 16.3 are rounded. What are\\nthe exact values that EM will converge to?\\nExercise 16.17\\nPerform a K-means clustering for the documents in Table 16.3.\\nAfter how many\\niterations does K-means converge? Compare the result with the EM clustering in\\nTable 16.3 and discuss the differences.\\nExercise 16.18\\n[⋆⋆⋆]\\nModify the expectation and maximization steps of EM for a Gaussian mixture. The\\nmaximization step computes the maximum likelihood parameter estimates αk, ⃗µk,\\nand Σk for each of the clusters. The expectation step computes for each vector a soft\\nassignment to clusters (Gaussians) based on their current parameters. Write down\\nthe equations for Gaussian mixtures corresponding to Equations (16.16) and (16.17).\\nExercise 16.19\\n[⋆⋆⋆]\\nShow that K-means can be viewed as the limiting case of EM for Gaussian mixtures\\nif variance is very small and all covariances are 0.\\nExercise 16.20\\n[⋆⋆⋆]\\nThe within-point scatter of a clustering is deﬁned as ∑k\\n1\\n2 ∑⃗xi∈ωk ∑⃗xj∈ωk |⃗xi −⃗xj|2. Show\\nWITHIN-POINT\\nSCATTER\\nthat minimizing RSS and minimizing within-point scatter are equivalent.\\nExercise 16.21\\n[⋆⋆⋆]\\nDerive an AIC criterion for the multivariate Bernoulli mixture model from Equa-\\ntion (16.12).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n377\\n17\\nHierarchical clustering\\nFlat clustering is efﬁcient and conceptually simple, but as we saw in Chap-\\nter 16 it has a number of drawbacks. The algorithms introduced in Chap-\\nter 16 return a ﬂat unstructured set of clusters, require a prespeciﬁed num-\\nber of clusters as input and are nondeterministic. Hierarchical clustering (or\\nHIERARCHICAL\\nCLUSTERING\\nhierarchic clustering) outputs a hierarchy, a structure that is more informative\\nthan the unstructured set of clusters returned by ﬂat clustering.1 Hierarchical\\nclustering does not require us to prespecify the number of clusters and most\\nhierarchical algorithms that have been used in IR are deterministic. These ad-\\nvantages of hierarchical clustering come at the cost of lower efﬁciency. The\\nmost common hierarchical clustering algorithms have a complexity that is at\\nleast quadratic in the number of documents compared to the linear complex-\\nity of K-means and EM (cf. Section 16.4, page 364).\\nThis chapter ﬁrst introduces agglomerative hierarchical clustering (Section 17.1)\\nand presents four different agglomerative algorithms, in Sections 17.2–17.4,\\nwhich differ in the similarity measures they employ: single-link, complete-\\nlink, group-average, and centroid similarity. We then discuss the optimality\\nconditions of hierarchical clustering in Section 17.5. Section 17.6 introduces\\ntop-down (or divisive) hierarchical clustering. Section 17.7 looks at labeling\\nclusters automatically, a problem that must be solved whenever humans in-\\nteract with the output of clustering. We discuss implementation issues in\\nSection 17.8. Section 17.9 provides pointers to further reading, including ref-\\nerences to soft hierarchical clustering, which we do not cover in this book.\\nThere are few differences between the applications of ﬂat and hierarchi-\\ncal clustering in information retrieval. In particular, hierarchical clustering\\nis appropriate for any of the applications shown in Table 16.1 (page 351; see\\nalso Section 16.6, page 372). In fact, the example we gave for collection clus-\\ntering is hierarchical. In general, we select ﬂat clustering when efﬁciency\\nis important and hierarchical clustering when one of the potential problems\\n1. In this chapter, we only consider hierarchies that are binary trees like the one shown in Fig-\\nure 17.1 – but hierarchical clustering can be easily extended to other types of trees.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n378\\n17\\nHierarchical clustering\\nof ﬂat clustering (not enough structure, predetermined number of clusters,\\nnon-determinism) is a concern. In addition, many researchers believe that hi-\\nerarchical clustering produces better clusters than ﬂat clustering. However,\\nthere is no consensus on this issue (see references in Section 17.9).\\n17.1\\nHierarchical agglomerative clustering\\nHierarchical clustering algorithms are either top-down or bottom-up. Bottom-\\nup algorithms treat each document as a singleton cluster at the outset and\\nthen successively merge (or agglomerate) pairs of clusters until all clusters\\nhave been merged into a single cluster that contains all documents. Bottom-\\nup hierarchical clustering is therefore called hierarchical agglomerative cluster-\\nHIERARCHICAL\\nAGGLOMERATIVE\\nCLUSTERING\\ning or HAC. Top-down clustering requires a method for splitting a cluster.\\nHAC\\nIt proceeds by splitting clusters recursively until individual documents are\\nreached. See Section 17.6. HAC is more frequently used in IR than top-down\\nclustering and is the main subject of this chapter.\\nBefore looking at speciﬁc similarity measures used in HAC in Sections\\n17.2–17.4, we ﬁrst introduce a method for depicting hierarchical clusterings\\ngraphically, discuss a few key properties of HACs and present a simple algo-\\nrithm for computing an HAC.\\nAn HAC clustering is typically visualized as a dendrogram as shown in\\nDENDROGRAM\\nFigure 17.1. Each merge is represented by a horizontal line. The y-coordinate\\nof the horizontal line is the similarity of the two clusters that were merged,\\nwhere documents are viewed as singleton clusters. We call this similarity the\\ncombination similarity of the merged cluster. For example, the combination\\nCOMBINATION\\nSIMILARITY\\nsimilarity of the cluster consisting of Lloyd’s CEO questioned and Lloyd’s chief\\n/ U.S. grilling in Figure 17.1 is ≈0.56. We deﬁne the combination similarity\\nof a singleton cluster as its document’s self-similarity (which is 1.0 for cosine\\nsimilarity).\\nBy moving up from the bottom layer to the top node, a dendrogram al-\\nlows us to reconstruct the history of merges that resulted in the depicted\\nclustering. For example, we see that the two documents entitled War hero\\nColin Powell were merged ﬁrst in Figure 17.1 and that the last merge added\\nAg trade reform to a cluster consisting of the other 29 documents.\\nA fundamental assumption in HAC is that the merge operation is mono-\\nMONOTONICITY\\ntonic. Monotonic means that if s1, s2, . . . , sK−1 are the combination similarities\\nof the successive merges of an HAC, then s1 ≥s2 ≥. . . ≥sK−1 holds. A non-\\nmonotonic hierarchical clustering contains at least one inversion si < si+1\\nINVERSION\\nand contradicts the fundamental assumption that we chose the best merge\\navailable at each step. We will see an example of an inversion in Figure 17.12.\\nHierarchical clustering does not require a prespeciﬁed number of clusters.\\nHowever, in some applications we want a partition of disjoint clusters just as\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.1\\nHierarchical agglomerative clustering\\n379\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\nAg trade reform.\\nBack−to−school spending is up\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nViag stays positive\\nChrysler / Latin America\\nOhio Blue Cross\\nJapanese prime minister / Mexico\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nGerman unions split\\nWar hero Colin Powell\\nWar hero Colin Powell\\nOil prices slip\\nChains may raise prices\\nClinton signs law\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nMost active stocks\\nMexican markets\\nHog prices tumble\\nNYSE closing averages\\nBritish FTSE index\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\n◮Figure 17.1\\nA dendrogram of a single-link clustering of 30 documents from\\nReuters-RCV1. Two possible cuts of the dendrogram are shown: at 0.4 into 24 clusters\\nand at 0.1 into 12 clusters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n380\\n17\\nHierarchical clustering\\nin ﬂat clustering. In those cases, the hierarchy needs to be cut at some point.\\nA number of criteria can be used to determine the cutting point:\\n• Cut at a prespeciﬁed level of similarity. For example, we cut the dendro-\\ngram at 0.4 if we want clusters with a minimum combination similarity\\nof 0.4. In Figure 17.1, cutting the diagram at y = 0.4 yields 24 clusters\\n(grouping only documents with high similarity together) and cutting it at\\ny = 0.1 yields 12 clusters (one large ﬁnancial news cluster and 11 smaller\\nclusters).\\n• Cut the dendrogram where the gap between two successive combination\\nsimilarities is largest. Such large gaps arguably indicate “natural” clus-\\nterings. Adding one more cluster decreases the quality of the clustering\\nsigniﬁcantly, so cutting before this steep decrease occurs is desirable. This\\nstrategy is analogous to looking for the knee in the K-means graph in Fig-\\nure 16.8 (page 366).\\n• Apply Equation (16.11) (page 366):\\nK = arg min\\nK′\\n[RSS(K′) + λK′]\\nwhere K′ refers to the cut of the hierarchy that results in K′ clusters, RSS is\\nthe residual sum of squares and λ is a penalty for each additional cluster.\\nInstead of RSS, another measure of distortion can be used.\\n• As in ﬂat clustering, we can also prespecify the number of clusters K and\\nselect the cutting point that produces K clusters.\\nA simple, naive HAC algorithm is shown in Figure 17.2. We ﬁrst compute\\nthe N × N similarity matrix C. The algorithm then executes N −1 steps\\nof merging the currently most similar clusters. In each iteration, the two\\nmost similar clusters are merged and the rows and columns of the merged\\ncluster i in C are updated.2 The clustering is stored as a list of merges in\\nA. I indicates which clusters are still available to be merged. The function\\nSIM(i, m, j) computes the similarity of cluster j with the merge of clusters i\\nand m. For some HAC algorithms, SIM(i, m, j) is simply a function of C[j][i]\\nand C[j][m], for example, the maximum of these two values for single-link.\\nWe will now reﬁne this algorithm for the different similarity measures\\nof single-link and complete-link clustering (Section 17.2) and group-average\\nand centroid clustering (Sections 17.3 and 17.4). The merge criteria of these\\nfour variants of HAC are shown in Figure 17.3.\\n2. We assume that we use a deterministic method for breaking ties, such as always choose the\\nmerge that is the ﬁrst cluster with respect to a total ordering of the subsets of the document set\\nD.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.1\\nHierarchical agglomerative clustering\\n381\\nSIMPLEHAC(d1, . . . , dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i] ←SIM(dn, di)\\n4\\nI[n] ←1 (keeps track of active clusters)\\n5\\nA ←[] (assembles clustering as a sequence of merges)\\n6\\nfor k ←1 to N −1\\n7\\ndo ⟨i, m⟩←arg max{⟨i,m⟩:i̸=m∧I[i]=1∧I[m]=1} C[i][m]\\n8\\nA.APPEND(⟨i, m⟩) (store merge)\\n9\\nfor j ←1 to N\\n10\\ndo C[i][j] ←SIM(i, m, j)\\n11\\nC[j][i] ←SIM(i, m, j)\\n12\\nI[m] ←0 (deactivate cluster)\\n13\\nreturn A\\n◮Figure 17.2\\nA simple, but inefﬁcient HAC algorithm.\\nb\\nb\\nb\\nb\\n(a) single-link: maximum similarity\\nb\\nb\\nb\\nb\\n(b) complete-link: minimum similarity\\nb\\nb\\nb\\nb\\n(c) centroid: average inter-similarity\\nb\\nb\\nb\\nb\\n(d) group-average: average of all similarities\\n◮Figure 17.3\\nThe different notions of cluster similarity used by the four HAC al-\\ngorithms. An inter-similarity is a similarity between two documents from different\\nclusters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n382\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\nd5\\n×\\nd6\\n×\\nd7\\n×\\nd8\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n0\\n1\\n2\\n3\\n4\\n0\\n1\\n2\\n3\\n×\\nd5\\n×\\nd6\\n×\\nd7\\n×\\nd8\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n◮Figure 17.4\\nA single-link (left) and complete-link (right) clustering of eight doc-\\numents. The ellipses correspond to successive clustering stages. Left: The single-link\\nsimilarity of the two upper two-point clusters is the similarity of d2 and d3 (solid\\nline), which is greater than the single-link similarity of the two left two-point clusters\\n(dashed line). Right: The complete-link similarity of the two upper two-point clusters\\nis the similarity of d1 and d4 (dashed line), which is smaller than the complete-link\\nsimilarity of the two left two-point clusters (solid line).\\n17.2\\nSingle-link and complete-link clustering\\nIn single-link clustering or single-linkage clustering, the similarity of two clus-\\nSINGLE-LINK\\nCLUSTERING\\nters is the similarity of their most similar members (see Figure 17.3, (a))3. This\\nsingle-link merge criterion is local. We pay attention solely to the area where\\nthe two clusters come closest to each other. Other, more distant parts of the\\ncluster and the clusters’ overall structure are not taken into account.\\nIn complete-link clustering or complete-linkage clustering, the similarity of two\\nCOMPLETE-LINK\\nCLUSTERING\\nclusters is the similarity of their most dissimilar members (see Figure 17.3, (b)).\\nThis is equivalent to choosing the cluster pair whose merge has the smallest\\ndiameter. This complete-link merge criterion is non-local; the entire structure\\nof the clustering can inﬂuence merge decisions. This results in a preference\\nfor compact clusters with small diameters over long, straggly clusters, but\\nalso causes sensitivity to outliers. A single document far from the center can\\nincrease diameters of candidate merge clusters dramatically and completely\\nchange the ﬁnal clustering.\\nFigure 17.4 depicts a single-link and a complete-link clustering of eight\\ndocuments. The ﬁrst four steps, each producing a cluster consisting of a pair\\nof two documents, are identical. Then single-link clustering joins the up-\\nper two pairs (and after that the lower two pairs) because on the maximum-\\nsimilarity deﬁnition of cluster similarity, those two clusters are closest. Complete-\\n3. Throughout this chapter, we equate similarity with proximity in 2D depictions of clustering.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n383\\n1.0\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\nNYSE closing averages\\nHog prices tumble\\nOil prices slip\\nAg trade reform.\\nChrysler / Latin America\\nJapanese prime minister / Mexico\\nFed holds interest rates steady\\nFed to keep interest rates steady\\nFed keeps interest rates steady\\nFed keeps interest rates steady\\nMexican markets\\nBritish FTSE index\\nWar hero Colin Powell\\nWar hero Colin Powell\\nLloyd’s CEO questioned\\nLloyd’s chief / U.S. grilling\\nOhio Blue Cross\\nLawsuit against tobacco companies\\nsuits against tobacco firms\\nIndiana tobacco lawsuit\\nViag stays positive\\nMost active stocks\\nCompuServe reports loss\\nSprint / Internet access service\\nPlanet Hollywood\\nTrocadero: tripling of revenues\\nBack−to−school spending is up\\nGerman unions split\\nChains may raise prices\\nClinton signs law\\n◮Figure 17.5\\nA dendrogram of a complete-link clustering. The same 30 documents\\nwere clustered with single-link clustering in Figure 17.1.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n384\\n17\\nHierarchical clustering\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n◮Figure 17.6\\nChaining in single-link clustering. The local criterion in single-link\\nclustering can cause undesirable elongated clusters.\\nlink clustering joins the left two pairs (and then the right two pairs) because\\nthose are the closest pairs according to the minimum-similarity deﬁnition of\\ncluster similarity.4\\nFigure 17.1 is an example of a single-link clustering of a set of documents\\nand Figure 17.5 is the complete-link clustering of the same set. When cutting\\nthe last merge in Figure 17.5, we obtain two clusters of similar size (doc-\\numents 1–16, from NYSE closing averages to Lloyd’s chief / U.S. grilling, and\\ndocuments 17–30, from Ohio Blue Cross to Clinton signs law). There is no cut\\nof the dendrogram in Figure 17.1 that would give us an equally balanced\\nclustering.\\nBoth single-link and complete-link clustering have graph-theoretic inter-\\npretations. Deﬁne sk to be the combination similarity of the two clusters\\nmerged in step k, and G(sk) the graph that links all data points with a similar-\\nity of at least sk. Then the clusters after step k in single-link clustering are the\\nconnected components of G(sk) and the clusters after step k in complete-link\\nclustering are maximal cliques of G(sk). A connected component is a maximal\\nCONNECTED\\nCOMPONENT\\nset of connected points such that there is a path connecting each pair. A clique\\nCLIQUE\\nis a set of points that are completely linked with each other.\\nThese graph-theoretic interpretations motivate the terms single-link and\\ncomplete-link clustering. Single-link clusters at step k are maximal sets of\\npoints that are linked via at least one link (a single link) of similarity s ≥sk;\\ncomplete-link clusters at step k are maximal sets of points that are completely\\nlinked with each other via links of similarity s ≥sk.\\nSingle-link and complete-link clustering reduce the assessment of cluster\\nquality to a single similarity between a pair of documents: the two most sim-\\nilar documents in single-link clustering and the two most dissimilar docu-\\nments in complete-link clustering. A measurement based on one pair cannot\\nfully reﬂect the distribution of documents in a cluster. It is therefore not sur-\\nprising that both algorithms often produce undesirable clusters. Single-link\\nclustering can produce straggling clusters as shown in Figure 17.6. Since the\\nmerge criterion is strictly local, a chain of points can be extended for long\\n4. If you are bothered by the possibility of ties, assume that d1 has coordinates (1 + ǫ, 3 −ǫ) and\\nthat all other points have integer coordinates.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n385\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n×\\nd5\\n◮Figure 17.7\\nOutliers in complete-link clustering.\\nThe ﬁve documents have\\nthe x-coordinates 1 + 2ǫ, 4, 5 + 2ǫ, 6 and 7 −ǫ.\\nComplete-link clustering cre-\\nates the two clusters shown as ellipses.\\nThe most intuitive two-cluster cluster-\\ning is {{d1}, {d2, d3, d4, d5}}, but in complete-link clustering, the outlier d1 splits\\n{d2, d3, d4, d5} as shown.\\ndistances without regard to the overall shape of the emerging cluster. This\\neffect is called chaining.\\nCHAINING\\nThe chaining effect is also apparent in Figure 17.1. The last eleven merges\\nof the single-link clustering (those above the 0.1 line) add on single docu-\\nments or pairs of documents, corresponding to a chain. The complete-link\\nclustering in Figure 17.5 avoids this problem. Documents are split into two\\ngroups of roughly equal size when we cut the dendrogram at the last merge.\\nIn general, this is a more useful organization of the data than a clustering\\nwith chains.\\nHowever, complete-link clustering suffers from a different problem. It\\npays too much attention to outliers, points that do not ﬁt well into the global\\nstructure of the cluster. In the example in Figure 17.7 the four documents\\nd2, d3, d4, d5 are split because of the outlier d1 at the left edge (Exercise 17.1).\\nComplete-link clustering does not ﬁnd the most intuitive cluster structure in\\nthis example.\\n17.2.1\\nTime complexity of HAC\\nThe complexity of the naive HAC algorithm in Figure 17.2 is Θ(N3) because\\nwe exhaustively scan the N × N matrix C for the largest similarity in each of\\nN −1 iterations.\\nFor the four HAC methods discussed in this chapter a more efﬁcient algo-\\nrithm is the priority-queue algorithm shown in Figure 17.8. Its time complex-\\nity is Θ(N2 log N). The rows C[k] of the N × N similarity matrix C are sorted\\nin decreasing order of similarity in the priority queues P. P[k].MAX() then\\nreturns the cluster in P[k] that currently has the highest similarity with ωk,\\nwhere we use ωk to denote the kth cluster as in Chapter 16. After creating the\\nmerged cluster of ωk1 and ωk2, ωk1 is used as its representative. The function\\nSIM computes the similarity function for potential merge pairs: largest simi-\\nlarity for single-link, smallest similarity for complete-link, average similarity\\nfor GAAC (Section 17.3), and centroid similarity for centroid clustering (Sec-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n386\\n17\\nHierarchical clustering\\nEFFICIENTHAC(⃗d1, . . . , ⃗dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i].sim ←⃗dn · ⃗di\\n4\\nC[n][i].index ←i\\n5\\nI[n] ←1\\n6\\nP[n] ←priority queue for C[n] sorted on sim\\n7\\nP[n].DELETE(C[n][n]) (don’t want self-similarities)\\n8\\nA ←[]\\n9\\nfor k ←1 to N −1\\n10\\ndo k1 ←arg max{k:I[k]=1} P[k].MAX().sim\\n11\\nk2 ←P[k1].MAX().index\\n12\\nA.APPEND(⟨k1, k2⟩)\\n13\\nI[k2] ←0\\n14\\nP[k1] ←[]\\n15\\nfor each i with I[i] = 1 ∧i ̸= k1\\n16\\ndo P[i].DELETE(C[i][k1])\\n17\\nP[i].DELETE(C[i][k2])\\n18\\nC[i][k1].sim ←SIM(i, k1, k2)\\n19\\nP[i].INSERT(C[i][k1])\\n20\\nC[k1][i].sim ←SIM(i, k1, k2)\\n21\\nP[k1].INSERT(C[k1][i])\\n22\\nreturn A\\nclustering algorithm\\nSIM(i, k1, k2)\\nsingle-link\\nmax(SIM(i, k1), SIM(i, k2))\\ncomplete-link\\nmin(SIM(i, k1), SIM(i, k2))\\ncentroid\\n( 1\\nNm⃗vm) · ( 1\\nNi⃗vi)\\ngroup-average\\n1\\n(Nm+Ni)(Nm+Ni−1)[(⃗vm +⃗vi)2 −(Nm + Ni)]\\ncompute C[5]\\n1\\n2\\n3\\n4\\n5\\n0.2\\n0.8\\n0.6\\n0.4\\n1.0\\ncreate P[5] (by sorting)\\n2\\n3\\n4\\n1\\n0.8\\n0.6\\n0.4\\n0.2\\nmerge 2 and 3, update\\nsimilarity of 2, delete 3\\n2\\n4\\n1\\n0.3\\n0.4\\n0.2\\ndelete and reinsert 2\\n4\\n2\\n1\\n0.4\\n0.3\\n0.2\\n◮Figure 17.8\\nThe priority-queue algorithm for HAC. Top: The algorithm. Center:\\nFour different similarity measures. Bottom: An example for processing steps 6 and\\n16–19. This is a made up example showing P[5] for a 5 × 5 matrix C.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.2\\nSingle-link and complete-link clustering\\n387\\nSINGLELINKCLUSTERING(d1, . . . , dN)\\n1\\nfor n ←1 to N\\n2\\ndo for i ←1 to N\\n3\\ndo C[n][i].sim ←SIM(dn, di)\\n4\\nC[n][i].index ←i\\n5\\nI[n] ←n\\n6\\nNBM[n] ←arg maxX∈{C[n][i]:n̸=i} X.sim\\n7\\nA ←[]\\n8\\nfor n ←1 to N −1\\n9\\ndo i1 ←arg max{i:I[i]=i} NBM[i].sim\\n10\\ni2 ←I[NBM[i1].index]\\n11\\nA.APPEND(⟨i1, i2⟩)\\n12\\nfor i ←1 to N\\n13\\ndo if I[i] = i ∧i ̸= i1 ∧i ̸= i2\\n14\\nthen C[i1][i].sim ←C[i][i1].sim ←max(C[i1][i].sim, C[i2][i].sim)\\n15\\nif I[i] = i2\\n16\\nthen I[i] ←i1\\n17\\nNBM[i1] ←arg maxX∈{C[i1][i]:I[i]=i∧i̸=i1} X.sim\\n18\\nreturn A\\n◮Figure 17.9\\nSingle-link clustering algorithm using an NBM array. After merging\\ntwo clusters i1 and i2, the ﬁrst one (i1) represents the merged cluster. If I[i] = i, then i\\nis the representative of its current cluster. If I[i] ̸= i, then i has been merged into the\\ncluster represented by I[i] and will therefore be ignored when updating NBM[i1].\\ntion 17.4). We give an example of how a row of C is processed (Figure 17.8,\\nbottom panel). The loop in lines 1–7 is Θ(N2) and the loop in lines 9–21 is\\nΘ(N2 log N) for an implementation of priority queues that supports deletion\\nand insertion in Θ(log N). The overall complexity of the algorithm is there-\\nfore Θ(N2 log N). In the deﬁnition of the function SIM, ⃗vm and ⃗vi are the\\nvector sums of ωk1 ∪ωk2 and ωi, respectively, and Nm and Ni are the number\\nof documents in ωk1 ∪ωk2 and ωi, respectively.\\nThe argument of EFFICIENTHAC in Figure 17.8 is a set of vectors (as op-\\nposed to a set of generic documents) because GAAC and centroid clustering\\n(Sections 17.3 and 17.4) require vectors as input. The complete-link version\\nof EFFICIENTHAC can also be applied to documents that are not represented\\nas vectors.\\nFor single-link, we can introduce a next-best-merge array (NBM) as a fur-\\nther optimization as shown in Figure 17.9. NBM keeps track of what the best\\nmerge is for each cluster. Each of the two top level for-loops in Figure 17.9\\nare Θ(N2), thus the overall complexity of single-link clustering is Θ(N2).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n388\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10\\n0\\n1\\n×\\nd1\\n×\\nd2\\n×\\nd3\\n×\\nd4\\n◮Figure 17.10\\nComplete-link clustering is not best-merge persistent. At ﬁrst, d2 is\\nthe best-merge cluster for d3. But after merging d1 and d2, d4 becomes d3’s best-merge\\ncandidate. In a best-merge persistent algorithm like single-link, d3’s best-merge clus-\\nter would be {d1, d2}.\\nCan we also speed up the other three HAC algorithms with an NBM ar-\\nray? We cannot because only single-link clustering is best-merge persistent.\\nBEST-MERGE\\nPERSISTENCE\\nSuppose that the best merge cluster for ωk is ωj in single-link clustering.\\nThen after merging ωj with a third cluster ωi ̸= ωk, the merge of ωi and ωj\\nwill be ωk’s best merge cluster (Exercise 17.6). In other words, the best-merge\\ncandidate for the merged cluster is one of the two best-merge candidates of\\nits components in single-link clustering. This means that C can be updated\\nin Θ(N) in each iteration – by taking a simple max of two values on line 14\\nin Figure 17.9 for each of the remaining ≤N clusters.\\nFigure 17.10 demonstrates that best-merge persistence does not hold for\\ncomplete-link clustering, which means that we cannot use an NBM array to\\nspeed up clustering. After merging d3’s best merge candidate d2 with cluster\\nd1, an unrelated cluster d4 becomes the best merge candidate for d3. This is\\nbecause the complete-link merge criterion is non-local and can be affected by\\npoints at a great distance from the area where two merge candidates meet.\\nIn practice, the efﬁciency penalty of the Θ(N2 log N) algorithm is small\\ncompared with the Θ(N2) single-link algorithm since computing the similar-\\nity between two documents (e.g., as a dot product) is an order of magnitude\\nslower than comparing two scalars in sorting. All four HAC algorithms in\\nthis chapter are Θ(N2) with respect to similarity computations. So the differ-\\nence in complexity is rarely a concern in practice when choosing one of the\\nalgorithms.\\n?\\nExercise 17.1\\nShow that complete-link clustering creates the two-cluster clustering depicted in Fig-\\nure 17.7.\\n17.3\\nGroup-average agglomerative clustering\\nGroup-average agglomerative clustering or GAAC (see Figure 17.3, (d)) evaluates\\nGROUP-AVERAGE\\nAGGLOMERATIVE\\nCLUSTERING\\ncluster quality based on all similarities between documents, thus avoiding\\nthe pitfalls of the single-link and complete-link criteria, which equate cluster\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.3\\nGroup-average agglomerative clustering\\n389\\nsimilarity with the similarity of a single pair of documents. GAAC is also\\ncalled group-average clustering and average-link clustering. GAAC computes\\nthe average similarity SIM-GA of all pairs of documents, including pairs from\\nthe same cluster. But self-similarities are not included in the average:\\nSIM-GA(ωi, ωj) =\\n1\\n(Ni + Nj)(Ni + Nj −1)\\n∑\\ndm∈ωi∪ωj\\n∑\\ndn∈ωi∪ωj,dn̸=dm\\n⃗dm · ⃗dn\\n(17.1)\\nwhere ⃗d is the length-normalized vector of document d, · denotes the dot\\nproduct, and Ni and Nj are the number of documents in ωi and ωj, respec-\\ntively.\\nThe motivation for GAAC is that our goal in selecting two clusters ωi\\nand ωj as the next merge in HAC is that the resulting merge cluster ωk =\\nωi ∪ωj should be coherent. To judge the coherence of ωk, we need to look\\nat all document-document similarities within ωk, including those that occur\\nwithin ωi and those that occur within ωj.\\nWe can compute the measure SIM-GA efﬁciently because the sum of indi-\\nvidual vector similarities is equal to the similarities of their sums:\\n∑\\ndm∈ωi ∑\\ndn∈ωj\\n(⃗dm · ⃗dn) = ( ∑\\ndm∈ωi\\n⃗dm) · ( ∑\\ndn∈ωj\\n⃗dn)\\n(17.2)\\nWith (17.2), we have:\\nSIM-GA(ωi, ωj)\\n=\\n1\\n(Ni + Nj)(Ni + Nj −1)[(\\n∑\\ndm∈ωi∪ωj\\n⃗dm)2 −(Ni + Nj)]\\n(17.3)\\nThe term (Ni + Nj) on the right is the sum of Ni + Nj self-similarities of value\\n1.0. With this trick we can compute cluster similarity in constant time (as-\\nsuming we have available the two vector sums ∑dm∈ωi ⃗dm and ∑dm∈ωj ⃗dm)\\ninstead of in Θ(NiNj). This is important because we need to be able to com-\\npute the function SIM on lines 18 and 20 in EFFICIENTHAC (Figure 17.8)\\nin constant time for efﬁcient implementations of GAAC. Note that for two\\nsingleton clusters, Equation (17.3) is equivalent to the dot product.\\nEquation (17.2) relies on the distributivity of the dot product with respect\\nto vector addition. Since this is crucial for the efﬁcient computation of a\\nGAAC clustering, the method cannot be easily applied to representations of\\ndocuments that are not real-valued vectors. Also, Equation (17.2) only holds\\nfor the dot product. While many algorithms introduced in this book have\\nnear-equivalent descriptions in terms of dot product, cosine similarity and\\nEuclidean distance (cf. Section 14.1, page 291), Equation (17.2) can only be\\nexpressed using the dot product. This is a fundamental difference between\\nsingle-link/complete-link clustering and GAAC. The ﬁrst two only require a\\n', 'Online edition (c)\\n2009 Cambridge UP\\n390\\n17\\nHierarchical clustering\\nsquare matrix of similarities as input and do not care how these similarities\\nwere computed.\\nTo summarize, GAAC requires (i) documents represented as vectors, (ii)\\nlength normalization of vectors, so that self-similarities are 1.0, and (iii) the\\ndot product as the measure of similarity between vectors and sums of vec-\\ntors.\\nThe merge algorithms for GAAC and complete-link clustering are the same\\nexcept that we use Equation (17.3) as similarity function in Figure 17.8. There-\\nfore, the overall time complexity of GAAC is the same as for complete-link\\nclustering: Θ(N2 log N). Like complete-link clustering, GAAC is not best-\\nmerge persistent (Exercise 17.6). This means that there is no Θ(N2) algorithm\\nfor GAAC that would be analogous to the Θ(N2) algorithm for single-link in\\nFigure 17.9.\\nWe can also deﬁne group-average similarity as including self-similarities:\\nSIM-GA′(ωi, ωj) =\\n1\\n(Ni+Nj)2 (\\n∑\\ndm∈ωi∪ωj\\n⃗dm)2 =\\n1\\nNi+Nj\\n∑\\ndm∈ωi∪ωj\\n[⃗dm ·⃗µ(ωi∪ωj)]\\n(17.4)\\nwhere the centroid ⃗µ(ω) is deﬁned as in Equation (14.1) (page 292). This\\ndeﬁnition is equivalent to the intuitive deﬁnition of cluster quality as average\\nsimilarity of documents ⃗dm to the cluster’s centroid ⃗µ.\\nSelf-similarities are always equal to 1.0, the maximum possible value for\\nlength-normalized vectors. The proportion of self-similarities in Equation (17.4)\\nis i/i2 = 1/i for a cluster of size i. This gives an unfair advantage to small\\nclusters since they will have proportionally more self-similarities. For two\\ndocuments d1, d2 with a similarity s, we have SIM-GA′(d1, d2) = (1 + s)/2.\\nIn contrast, SIM-GA(d1, d2) = s ≤(1 + s)/2. This similarity SIM-GA(d1, d2)\\nof two documents is the same as in single-link, complete-link and centroid\\nclustering. We prefer the deﬁnition in Equation (17.3), which excludes self-\\nsimilarities from the average, because we do not want to penalize large clus-\\nters for their smaller proportion of self-similarities and because we want a\\nconsistent similarity value s for document pairs in all four HAC algorithms.\\n?\\nExercise 17.2\\nApply group-average clustering to the points in Figures 17.6 and 17.7. Map them onto\\nthe surface of the unit sphere in a three-dimensional space to get length-normalized\\nvectors. Is the group-average clustering different from the single-link and complete-\\nlink clusterings?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.4\\nCentroid clustering\\n391\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n1\\n2\\n3\\n4\\n5\\n× d1\\n× d2\\n× d3\\n× d4\\n×\\nd5\\n× d6\\nb\\nc\\nµ1\\nb\\nc\\nµ3\\nb\\nc µ2\\n◮Figure 17.11\\nThree iterations of centroid clustering. Each iteration merges the\\ntwo clusters whose centroids are closest.\\n17.4\\nCentroid clustering\\nIn centroid clustering, the similarity of two clusters is deﬁned as the similar-\\nity of their centroids:\\nSIM-CENT(ωi, ωj)\\n=\\n⃗µ(ωi) ·⃗µ(ωj)\\n(17.5)\\n=\\n( 1\\nNi ∑\\ndm∈ωi\\n⃗dm) · ( 1\\nNj ∑\\ndn∈ωj\\n⃗dn)\\n=\\n1\\nNiNj ∑\\ndm∈ωi ∑\\ndn∈ωj\\n⃗dm · ⃗dn\\n(17.6)\\nEquation (17.5) is centroid similarity. Equation (17.6) shows that centroid\\nsimilarity is equivalent to average similarity of all pairs of documents from\\ndifferent clusters. Thus, the difference between GAAC and centroid clustering\\nis that GAAC considers all pairs of documents in computing average pair-\\nwise similarity (Figure 17.3, (d)) whereas centroid clustering excludes pairs\\nfrom the same cluster (Figure 17.3, (c)).\\nFigure 17.11 shows the ﬁrst three steps of a centroid clustering. The ﬁrst\\ntwo iterations form the clusters {d5, d6} with centroid µ1 and {d1, d2} with\\ncentroid µ2 because the pairs ⟨d5, d6⟩and ⟨d1, d2⟩have the highest centroid\\nsimilarities. In the third iteration, the highest centroid similarity is between\\nµ1 and d4 producing the cluster {d4, d5, d6} with centroid µ3.\\nLike GAAC, centroid clustering is not best-merge persistent and therefore\\nΘ(N2 log N) (Exercise 17.6).\\nIn contrast to the other three HAC algorithms, centroid clustering is not\\nmonotonic. So-called inversions can occur: Similarity can increase during\\nINVERSION\\n', 'Online edition (c)\\n2009 Cambridge UP\\n392\\n17\\nHierarchical clustering\\n0\\n1\\n2\\n3\\n4\\n5\\n0\\n1\\n2\\n3\\n4\\n5\\n×\\n×\\n×\\nb\\nc\\nd1\\nd2\\nd3\\n−4\\n−3\\n−2\\n−1\\n0\\nd1\\nd2\\nd3\\n◮Figure 17.12\\nCentroid clustering is not monotonic. The documents d1 at (1+ ǫ, 1),\\nd2 at (5, 1), and d3 at (3, 1 + 2\\n√\\n3) are almost equidistant, with d1 and d2 closer to\\neach other than to d3. The non-monotonic inversion in the hierarchical clustering\\nof the three points appears as an intersecting merge line in the dendrogram. The\\nintersection is circled.\\nclustering as in the example in Figure 17.12, where we deﬁne similarity as\\nnegative distance. In the ﬁrst merge, the similarity of d1 and d2 is −(4 −ǫ). In\\nthe second merge, the similarity of the centroid of d1 and d2 (the circle) and d3\\nis ≈−cos(π/6) × 4 = −\\n√\\n3/2 × 4 ≈−3.46 > −(4 −ǫ). This is an example\\nof an inversion: similarity increases in this sequence of two clustering steps.\\nIn a monotonic HAC algorithm, similarity is monotonically decreasing from\\niteration to iteration.\\nIncreasing similarity in a series of HAC clustering steps contradicts the\\nfundamental assumption that small clusters are more coherent than large\\nclusters. An inversion in a dendrogram shows up as a horizontal merge line\\nthat is lower than the previous merge line. All merge lines in Figures 17.1\\nand 17.5 are higher than their predecessors because single-link and complete-\\nlink clustering are monotonic clustering algorithms.\\nDespite its non-monotonicity, centroid clustering is often used because its\\nsimilarity measure – the similarity of two centroids – is conceptually simpler\\nthan the average of all pairwise similarities in GAAC. Figure 17.11 is all one\\nneeds to understand centroid clustering. There is no equally simple graph\\nthat would explain how GAAC works.\\n?\\nExercise 17.3\\nFor a ﬁxed set of N documents there are up to N2 distinct similarities between clusters\\nin single-link and complete-link clustering. How many distinct cluster similarities are\\nthere in GAAC and centroid clustering?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.5\\nOptimality of HAC\\n393\\n$\\n17.5\\nOptimality of HAC\\nTo state the optimality conditions of hierarchical clustering precisely, we ﬁrst\\ndeﬁne the combination similarity COMB-SIM of a clustering Ω= {ω1, . . . , ωK}\\nas the smallest combination similarity of any of its K clusters:\\nCOMB-SIM({ω1, . . . , ωK}) = min\\nk\\nCOMB-SIM(ωk)\\nRecall that the combination similarity of a cluster ω that was created as the\\nmerge of ω1 and ω2 is the similarity of ω1 and ω2 (page 378).\\nWe then deﬁne Ω= {ω1, . . . , ωK} to be optimal if all clusterings Ω′ with k\\nOPTIMAL CLUSTERING\\nclusters, k ≤K, have lower combination similarities:\\n|Ω′| ≤|Ω| ⇒COMB-SIM(Ω′) ≤COMB-SIM(Ω)\\nFigure 17.12 shows that centroid clustering is not optimal. The cluster-\\ning {{d1, d2}, {d3}} (for K = 2) has combination similarity −(4 −ǫ) and\\n{{d1, d2, d3}} (for K = 1) has combination similarity -3.46. So the cluster-\\ning {{d1, d2}, {d3}} produced in the ﬁrst merge is not optimal since there is\\na clustering with fewer clusters ({{d1, d2, d3}}) that has higher combination\\nsimilarity. Centroid clustering is not optimal because inversions can occur.\\nThe above deﬁnition of optimality would be of limited use if it was only\\napplicable to a clustering together with its merge history. However, we can\\nshow (Exercise 17.4) that combination similarity for the three non-inversion\\nCOMBINATION\\nSIMILARITY\\nalgorithms can be read off from the cluster without knowing its history. These\\ndirect deﬁnitions of combination similarity are as follows.\\nsingle-link The combination similarity of a cluster ω is the smallest similar-\\nity of any bipartition of the cluster, where the similarity of a bipartition is\\nthe largest similarity between any two documents from the two parts:\\nCOMB-SIM(ω) =\\nmin\\n{ω′:ω′⊂ω} max\\ndi∈ω′\\nmax\\ndj∈ω−ω′ SIM(di, dj)\\nwhere each ⟨ω′, ω −ω′⟩is a bipartition of ω.\\ncomplete-link The combination similarity of a cluster ω is the smallest sim-\\nilarity of any two points in ω: mindi∈ω mindj∈ω SIM(di, dj).\\nGAAC The combination similarity of a cluster ω is the average of all pair-\\nwise similarities in ω (where self-similarities are not included in the aver-\\nage): Equation (17.3).\\nIf we use these deﬁnitions of combination similarity, then optimality is a\\nproperty of a set of clusters and not of a process that produces a set of clus-\\nters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n394\\n17\\nHierarchical clustering\\nWe can now prove the optimality of single-link clustering by induction\\nover the number of clusters K. We will give a proof for the case where no two\\npairs of documents have the same similarity, but it can easily be extended to\\nthe case with ties.\\nThe inductive basis of the proof is that a clustering with K = N clusters has\\ncombination similarity 1.0, which is the largest value possible. The induc-\\ntion hypothesis is that a single-link clustering ΩK with K clusters is optimal:\\nCOMB-SIM(ΩK) ≥COMB-SIM(Ω′\\nK) for all Ω′\\nK. Assume for contradiction that\\nthe clustering ΩK−1 we obtain by merging the two most similar clusters in\\nΩK is not optimal and that instead a different sequence of merges Ω′\\nK, Ω′\\nK−1\\nleads to the optimal clustering with K −1 clusters. We can write the as-\\nsumption that Ω′\\nK−1 is optimal and that ΩK−1 is not as COMB-SIM(Ω′\\nK−1) >\\nCOMB-SIM(ΩK−1).\\nCase 1: The two documents linked by s = COMB-SIM(Ω′\\nK−1) are in the\\nsame cluster in ΩK. They can only be in the same cluster if a merge with sim-\\nilarity smaller than s has occurred in the merge sequence producing ΩK. This\\nimplies s > COMB-SIM(ΩK). Thus, COMB-SIM(Ω′\\nK−1) = s > COMB-SIM(ΩK) >\\nCOMB-SIM(Ω′\\nK) > COMB-SIM(Ω′\\nK−1). Contradiction.\\nCase 2: The two documents linked by s = COMB-SIM(Ω′\\nK−1) are not in\\nthe same cluster in ΩK. But s = COMB-SIM(Ω′\\nK−1) > COMB-SIM(ΩK−1), so\\nthe single-link merging rule should have merged these two clusters when\\nprocessing ΩK. Contradiction.\\nThus, ΩK−1 is optimal.\\nIn contrast to single-link clustering, complete-link clustering and GAAC\\nare not optimal as this example shows:\\n×\\n×\\n×\\n×\\n1\\n3\\n3\\nd1\\nd2\\nd3\\nd4\\nBoth algorithms merge the two points with distance 1 (d2 and d3) ﬁrst and\\nthus cannot ﬁnd the two-cluster clustering {{d1, d2}, {d3, d4}}. But {{d1, d2}, {d3, d4}}\\nis optimal on the optimality criteria of complete-link clustering and GAAC.\\nHowever, the merge criteria of complete-link clustering and GAAC ap-\\nproximate the desideratum of approximate sphericity better than the merge\\ncriterion of single-link clustering. In many applications, we want spheri-\\ncal clusters. Thus, even though single-link clustering may seem preferable at\\nﬁrst because of its optimality, it is optimal with respect to the wrong criterion\\nin many document clustering applications.\\nTable 17.1 summarizes the properties of the four HAC algorithms intro-\\nduced in this chapter. We recommend GAAC for document clustering be-\\ncause it is generally the method that produces the clustering with the best\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.6\\nDivisive clustering\\n395\\nmethod\\ncombination similarity\\ntime compl.\\noptimal?\\ncomment\\nsingle-link\\nmax inter-similarity of any 2 docs\\nΘ(N2)\\nyes\\nchaining effect\\ncomplete-link\\nmin inter-similarity of any 2 docs\\nΘ(N2 log N)\\nno\\nsensitive to outliers\\ngroup-average\\naverage of all sims\\nΘ(N2 log N)\\nno\\nbest choice for\\nmost applications\\ncentroid\\naverage inter-similarity\\nΘ(N2 log N)\\nno\\ninversions can occur\\n◮Table 17.1\\nComparison of HAC algorithms.\\nproperties for applications. It does not suffer from chaining, from sensitivity\\nto outliers and from inversions.\\nThere are two exceptions to this recommendation. First, for non-vector\\nrepresentations, GAAC is not applicable and clustering should typically be\\nperformed with the complete-link method.\\nSecond, in some applications the purpose of clustering is not to create a\\ncomplete hierarchy or exhaustive partition of the entire document set. For\\ninstance, ﬁrst story detection or novelty detection is the task of detecting the ﬁrst\\nFIRST STORY\\nDETECTION\\noccurrence of an event in a stream of news stories. One approach to this task\\nis to ﬁnd a tight cluster within the documents that were sent across the wire\\nin a short period of time and are dissimilar from all previous documents. For\\nexample, the documents sent over the wire in the minutes after the World\\nTrade Center attack on September 11, 2001 form such a cluster. Variations of\\nsingle-link clustering can do well on this task since it is the structure of small\\nparts of the vector space – and not global structure – that is important in this\\ncase.\\nSimilarly, we will describe an approach to duplicate detection on the web\\nin Section 19.6 (page 440) where single-link clustering is used in the guise of\\nthe union-ﬁnd algorithm. Again, the decision whether a group of documents\\nare duplicates of each other is not inﬂuenced by documents that are located\\nfar away and single-link clustering is a good choice for duplicate detection.\\n?\\nExercise 17.4\\nShow the equivalence of the two deﬁnitions of combination similarity: the process\\ndeﬁnition on page 378 and the static deﬁnition on page 393.\\n17.6\\nDivisive clustering\\nSo far we have only looked at agglomerative clustering, but a cluster hierar-\\nchy can also be generated top-down. This variant of hierarchical clustering\\nis called top-down clustering or divisive clustering. We start at the top with all\\nTOP-DOWN\\nCLUSTERING\\ndocuments in one cluster. The cluster is split using a ﬂat clustering algo-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n396\\n17\\nHierarchical clustering\\nrithm. This procedure is applied recursively until each document is in its\\nown singleton cluster.\\nTop-down clustering is conceptually more complex than bottom-up clus-\\ntering since we need a second, ﬂat clustering algorithm as a “subroutine”. It\\nhas the advantage of being more efﬁcient if we do not generate a complete\\nhierarchy all the way down to individual document leaves. For a ﬁxed num-\\nber of top levels, using an efﬁcient ﬂat algorithm like K-means, top-down\\nalgorithms are linear in the number of documents and clusters. So they run\\nmuch faster than HAC algorithms, which are at least quadratic.\\nThere is evidence that divisive algorithms produce more accurate hierar-\\nchies than bottom-up algorithms in some circumstances. See the references\\non bisecting K-means in Section 17.9.\\nBottom-up methods make cluster-\\ning decisions based on local patterns without initially taking into account\\nthe global distribution. These early decisions cannot be undone. Top-down\\nclustering beneﬁts from complete information about the global distribution\\nwhen making top-level partitioning decisions.\\n17.7\\nCluster labeling\\nIn many applications of ﬂat clustering and hierarchical clustering, particu-\\nlarly in analysis tasks and in user interfaces (see applications in Table 16.1,\\npage 351), human users interact with clusters. In such settings, we must label\\nclusters, so that users can see what a cluster is about.\\nDifferential cluster labeling selects cluster labels by comparing the distribu-\\nDIFFERENTIAL CLUSTER\\nLABELING\\ntion of terms in one cluster with that of other clusters. The feature selection\\nmethods we introduced in Section 13.5 (page 271) can all be used for differen-\\ntial cluster labeling.5 In particular, mutual information (MI) (Section 13.5.1,\\npage 272) or, equivalently, information gain and the χ2-test (Section 13.5.2,\\npage 275) will identify cluster labels that characterize one cluster in contrast\\nto other clusters. A combination of a differential test with a penalty for rare\\nterms often gives the best labeling results because rare terms are not neces-\\nsarily representative of the cluster as a whole.\\nWe apply three labeling methods to a K-means clustering in Table 17.2. In\\nthis example, there is almost no difference between MI and χ2. We therefore\\nomit the latter.\\nCluster-internal labeling computes a label that solely depends on the cluster\\nCLUSTER-INTERNAL\\nLABELING\\nitself, not on other clusters. Labeling a cluster with the title of the document\\nclosest to the centroid is one cluster-internal method. Titles are easier to read\\nthan a list of terms. A full title can also contain important context that didn’t\\nmake it into the top 10 terms selected by MI. On the web, anchor text can\\n5. Selecting the most frequent terms is a non-differential feature selection technique we dis-\\ncussed in Section 13.5. It can also be used for labeling clusters.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.7\\nCluster labeling\\n397\\nlabeling method\\n# docs\\ncentroid\\nmutual information\\ntitle\\n4\\n622\\noil plant mexico pro-\\nduction crude power\\n000 reﬁnery gas bpd\\nplant\\noil\\nproduction\\nbarrels\\ncrude\\nbpd\\nmexico dolly capacity\\npetroleum\\nMEXICO:\\nHurri-\\ncane Dolly heads for\\nMexico coast\\n9\\n1017\\npolice security russian\\npeople military peace\\nkilled\\ntold\\ngrozny\\ncourt\\npolice killed military\\nsecurity\\npeace\\ntold\\ntroops\\nforces rebels\\npeople\\nRUSSIA:\\nRussia’s\\nLebed\\nmeets\\nrebel\\nchief in Chechnya\\n10\\n1259\\n00 000 tonnes traders\\nfutures wheat prices\\ncents\\nseptember\\ntonne\\ndelivery\\ntraders\\nfutures tonne tonnes\\ndesk wheat prices 000\\n00\\nUSA: Export Business\\n- Grain/oilseeds com-\\nplex\\n◮Table 17.2\\nAutomatically computed cluster labels. This is for three of ten clusters\\n(4, 9, and 10) in a K-means clustering of the ﬁrst 10,000 documents in Reuters-RCV1.\\nThe last three columns show cluster summaries computed by three labeling methods:\\nmost highly weighted terms in centroid (centroid), mutual information, and the title\\nof the document closest to the centroid of the cluster (title). Terms selected by only\\none of the ﬁrst two methods are in bold.\\nplay a role similar to a title since the anchor text pointing to a page can serve\\nas a concise summary of its contents.\\nIn Table 17.2, the title for cluster 9 suggests that many of its documents are\\nabout the Chechnya conﬂict, a fact the MI terms do not reveal. However, a\\nsingle document is unlikely to be representative of all documents in a cluster.\\nAn example is cluster 4, whose selected title is misleading. The main topic of\\nthe cluster is oil. Articles about hurricane Dolly only ended up in this cluster\\nbecause of its effect on oil prices.\\nWe can also use a list of terms with high weights in the centroid of the clus-\\nter as a label. Such highly weighted terms (or, even better, phrases, especially\\nnoun phrases) are often more representative of the cluster than a few titles\\ncan be, even if they are not ﬁltered for distinctiveness as in the differential\\nmethods. However, a list of phrases takes more time to digest for users than\\na well crafted title.\\nCluster-internal methods are efﬁcient, but they fail to distinguish terms\\nthat are frequent in the collection as a whole from those that are frequent only\\nin the cluster. Terms like year or Tuesday may be among the most frequent in\\na cluster, but they are not helpful in understanding the contents of a cluster\\nwith a speciﬁc topic like oil.\\nIn Table 17.2, the centroid method selects a few more uninformative terms\\n(000, court, cents, september) than MI (forces, desk), but most of the terms se-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n398\\n17\\nHierarchical clustering\\nlected by either method are good descriptors. We get a good sense of the\\ndocuments in a cluster from scanning the selected terms.\\nFor hierarchical clustering, additional complications arise in cluster label-\\ning. Not only do we need to distinguish an internal node in the tree from\\nits siblings, but also from its parent and its children. Documents in child\\nnodes are by deﬁnition also members of their parent node, so we cannot use\\na naive differential method to ﬁnd labels that distinguish the parent from\\nits children. However, more complex criteria, based on a combination of\\noverall collection frequency and prevalence in a given cluster, can determine\\nwhether a term is a more informative label for a child node or a parent node\\n(see Section 17.9).\\n17.8\\nImplementation notes\\nMost problems that require the computation of a large number of dot prod-\\nucts beneﬁt from an inverted index. This is also the case for HAC clustering.\\nComputational savings due to the inverted index are large if there are many\\nzero similarities – either because many documents do not share any terms or\\nbecause an aggressive stop list is used.\\nIn low dimensions, more aggressive optimizations are possible that make\\nthe computation of most pairwise similarities unnecessary (Exercise 17.10).\\nHowever, no such algorithms are known in higher dimensions. We encoun-\\ntered the same problem in kNN classiﬁcation (see Section 14.7, page 314).\\nWhen using GAAC on a large document set in high dimensions, we have\\nto take care to avoid dense centroids. For dense centroids, clustering can\\ntake time Θ(MN2 log N) where M is the size of the vocabulary, whereas\\ncomplete-link clustering is Θ(MaveN2 log N) where Mave is the average size\\nof the vocabulary of a document. So for large vocabularies complete-link\\nclustering can be more efﬁcient than an unoptimized implementation of GAAC.\\nWe discussed this problem in the context of K-means clustering in Chap-\\nter 16 (page 365) and suggested two solutions: truncating centroids (keeping\\nonly highly weighted terms) and representing clusters by means of sparse\\nmedoids instead of dense centroids. These optimizations can also be applied\\nto GAAC and centroid clustering.\\nEven with these optimizations, HAC algorithms are all Θ(N2) or Θ(N2 log N)\\nand therefore infeasible for large sets of 1,000,000 or more documents. For\\nsuch large sets, HAC can only be used in combination with a ﬂat clustering\\nalgorithm like K-means. Recall that K-means requires a set of seeds as initial-\\nization (Figure 16.5, page 361). If these seeds are badly chosen, then the re-\\nsulting clustering will be of poor quality. We can employ an HAC algorithm\\nto compute seeds of high quality. If the HAC algorithm is applied to a docu-\\nment subset of size\\n√\\nN, then the overall runtime of K-means cum HAC seed\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.9\\nReferences and further reading\\n399\\ngeneration is Θ(N). This is because the application of a quadratic algorithm\\nto a sample of size\\n√\\nN has an overall complexity of Θ(N). An appropriate\\nadjustment can be made for an Θ(N2 log N) algorithm to guarantee linear-\\nity. This algorithm is referred to as the Buckshot algorithm. It combines the\\nBUCKSHOT\\nALGORITHM\\ndeterminism and higher reliability of HAC with the efﬁciency of K-means.\\n17.9\\nReferences and further reading\\nAn excellent general review of clustering is (Jain et al. 1999). Early references\\nfor speciﬁc HAC algorithms are (King 1967) (single-link), (Sneath and Sokal\\n1973) (complete-link, GAAC) and (Lance and Williams 1967) (discussing a\\nlarge variety of hierarchical clustering algorithms). The single-link algorithm\\nin Figure 17.9 is similar to Kruskal’s algorithm for constructing a minimum\\nKRUSKAL’S\\nALGORITHM\\nspanning tree. A graph-theoretical proof of the correctness of Kruskal’s al-\\ngorithm (which is analogous to the proof in Section 17.5) is provided by Cor-\\nmen et al. (1990, Theorem 23.1). See Exercise 17.5 for the connection between\\nminimum spanning trees and single-link clusterings.\\nIt is often claimed that hierarchical clustering algorithms produce better\\nclusterings than ﬂat algorithms (Jain and Dubes (1988, p. 140), Cutting et al.\\n(1992), Larsen and Aone (1999)) although more recently there have been ex-\\nperimental results suggesting the opposite (Zhao and Karypis 2002). Even\\nwithout a consensus on average behavior, there is no doubt that results of\\nEM and K-means are highly variable since they will often converge to a local\\noptimum of poor quality. The HAC algorithms we have presented here are\\ndeterministic and thus more predictable.\\nThe complexity of complete-link, group-average and centroid clustering\\nis sometimes given as Θ(N2) (Day and Edelsbrunner 1984, Voorhees 1985b,\\nMurtagh 1983) because a document similarity computation is an order of\\nmagnitude more expensive than a simple comparison, the main operation\\nexecuted in the merging steps after the N × N similarity matrix has been\\ncomputed.\\nThe centroid algorithm described here is due to Voorhees (1985b). Voorhees\\nrecommends complete-link and centroid clustering over single-link for a re-\\ntrieval application. The Buckshot algorithm was originally published by Cut-\\nting et al. (1993). Allan et al. (1998) apply single-link clustering to ﬁrst story\\ndetection.\\nAn important HAC technique not discussed here is Ward’s method (Ward\\nWARD’S METHOD\\nJr. 1963, El-Hamdouchi and Willett 1986), also called minimum variance clus-\\ntering. In each step, it selects the merge with the smallest RSS (Chapter 16,\\npage 360). The merge criterion in Ward’s method (a function of all individual\\ndistances from the centroid) is closely related to the merge criterion in GAAC\\n(a function of all individual similarities to the centroid).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n400\\n17\\nHierarchical clustering\\nDespite its importance for making the results of clustering useful, compar-\\natively little work has been done on labeling clusters. Popescul and Ungar\\n(2000) obtain good results with a combination of χ2 and collection frequency\\nof a term. Glover et al. (2002b) use information gain for labeling clusters of\\nweb pages. Stein and zu Eissen’s approach is ontology-based (2004). The\\nmore complex problem of labeling nodes in a hierarchy (which requires dis-\\ntinguishing more general labels for parents from more speciﬁc labels for chil-\\ndren) is tackled by Glover et al. (2002a) and Treeratpituk and Callan (2006).\\nSome clustering algorithms attempt to ﬁnd a set of labels ﬁrst and then build\\n(often overlapping) clusters around the labels, thereby avoiding the problem\\nof labeling altogether (Zamir and Etzioni 1999, Käki 2005, Osi´nski and Weiss\\n2005). We know of no comprehensive study that compares the quality of\\nsuch “label-based” clustering to the clustering algorithms discussed in this\\nchapter and in Chapter 16. In principle, work on multi-document summa-\\nrization (McKeown and Radev 1995) is also applicable to cluster labeling, but\\nmulti-document summaries are usually longer than the short text fragments\\nneeded when labeling clusters (cf. Section 8.7, page 170). Presenting clusters\\nin a way that users can understand is a UI problem. We recommend read-\\ning (Baeza-Yates and Ribeiro-Neto 1999, ch. 10) for an introduction to user\\ninterfaces in IR.\\nAn example of an efﬁcient divisive algorithm is bisecting K-means (Stein-\\nbach et al. 2000). Spectral clustering algorithms (Kannan et al. 2000, Dhillon\\nSPECTRAL CLUSTERING\\n2001, Zha et al. 2001, Ng et al. 2001a), including principal direction divisive\\npartitioning (PDDP) (whose bisecting decisions are based on SVD, see Chap-\\nter 18) (Boley 1998, Savaresi and Boley 2004), are computationally more ex-\\npensive than bisecting K-means, but have the advantage of being determin-\\nistic.\\nUnlike K-means and EM, most hierarchical clustering algorithms do not\\nhave a probabilistic interpretation. Model-based hierarchical clustering (Vaithyanathan\\nand Dom 2000, Kamvar et al. 2002, Castro et al. 2004) is an exception.\\nThe evaluation methodology described in Section 16.3 (page 356) is also\\napplicable to hierarchical clustering. Specialized evaluation measures for hi-\\nerarchies are discussed by Fowlkes and Mallows (1983), Larsen and Aone\\n(1999) and Sahoo et al. (2006).\\nThe R environment (R Development Core Team 2005) offers good support\\nfor hierarchical clustering. The R function hclust implements single-link,\\ncomplete-link, group-average, and centroid clustering; and Ward’s method.\\nAnother option provided is median clustering which represents each cluster\\nby its medoid (cf. k-medoids in Chapter 16, page 365). Support for cluster-\\ning vectors in high-dimensional spaces is provided by the software package\\nCLUTO (http://glaros.dtc.umn.edu/gkhome/views/cluto).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n17.10\\nExercises\\n401\\n17.10\\nExercises\\n?\\nExercise 17.5\\nA single-link clustering can also be computed from the minimum spanning tree of a\\nMINIMUM SPANNING\\nTREE\\ngraph. The minimum spanning tree connects the vertices of a graph at the smallest\\npossible cost, where cost is deﬁned as the sum over all edges of the graph. In our\\ncase the cost of an edge is the distance between two documents. Show that if ∆k−1 >\\n∆k > . . . > ∆1 are the costs of the edges of a minimum spanning tree, then these\\nedges correspond to the k −1 merges in constructing a single-link clustering.\\nExercise 17.6\\nShow that single-link clustering is best-merge persistent and that GAAC and centroid\\nclustering are not best-merge persistent.\\nExercise 17.7\\na. Consider running 2-means clustering on a collection with documents from two\\ndifferent languages. What result would you expect?\\nb. Would you expect the same result when running an HAC algorithm?\\nExercise 17.8\\nDownload Reuters-21578. Keep only documents that are in the classes crude, inter-\\nest, and grain. Discard documents that are members of more than one of these three\\nclasses. Compute a (i) single-link, (ii) complete-link, (iii) GAAC, (iv) centroid cluster-\\ning of the documents. (v) Cut each dendrogram at the second branch from the top to\\nobtain K = 3 clusters. Compute the Rand index for each of the 4 clusterings. Which\\nclustering method performs best?\\nExercise 17.9\\nSuppose a run of HAC ﬁnds the clustering with K = 7 to have the highest value on\\nsome prechosen goodness measure of clustering. Have we found the highest-value\\nclustering among all clusterings with K = 7?\\nExercise 17.10\\nConsider the task of producing a single-link clustering of N points on a line:\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nShow that we only need to compute a total of about N similarities. What is the overall\\ncomplexity of single-link clustering for a set of points on a line?\\nExercise 17.11\\nProve that single-link, complete-link, and group-average clustering are monotonic in\\nthe sense deﬁned on page 378.\\nExercise 17.12\\nFor N points, there are ≤NK different ﬂat clusterings into K clusters (Section 16.2,\\npage 356). What is the number of different hierarchical clusterings (or dendrograms)\\nof N documents? Are there more ﬂat clusterings or more hierarchical clusterings for\\ngiven K and N?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n403\\n18\\nMatrix decompositions and latent\\nsemantic indexing\\nOn page 123 we introduced the notion of a term-document matrix: an M × N\\nmatrix C, each of whose rows represents a term and each of whose columns\\nrepresents a document in the collection. Even for a collection of modest size,\\nthe term-document matrix C is likely to have several tens of thousands of\\nrows and columns. In Section 18.1.1 we ﬁrst develop a class of operations\\nfrom linear algebra, known as matrix decomposition. In Section 18.2 we use a\\nspecial form of matrix decomposition to construct a low-rank approximation\\nto the term-document matrix. In Section 18.3 we examine the application\\nof such low-rank approximations to indexing and retrieving documents, a\\ntechnique referred to as latent semantic indexing. While latent semantic in-\\ndexing has not been established as a signiﬁcant force in scoring and ranking\\nfor information retrieval, it remains an intriguing approach to clustering in a\\nnumber of domains including for collections of text documents (Section 16.6,\\npage 372). Understanding its full potential remains an area of active research.\\nReaders who do not require a refresher on linear algebra may skip Sec-\\ntion 18.1, although Example 18.1 is especially recommended as it highlights\\na property of eigenvalues that we exploit later in the chapter.\\n18.1\\nLinear algebra review\\nWe brieﬂy review some necessary background in linear algebra. Let C be\\nan M × N matrix with real-valued entries; for a term-document matrix, all\\nentries are in fact non-negative. The rank of a matrix is the number of linearly\\nRANK\\nindependent rows (or columns) in it; thus, rank(C) ≤min{M, N}. A square\\nr × r matrix all of whose off-diagonal entries are zero is called a diagonal\\nmatrix; its rank is equal to the number of non-zero diagonal entries. If all\\nr diagonal entries of such a diagonal matrix are 1, it is called the identity\\nmatrix of dimension r and represented by Ir.\\nFor a square M × M matrix C and a vector ⃗x that is not all zeros, the values\\n', 'Online edition (c)\\n2009 Cambridge UP\\n404\\n18\\nMatrix decompositions and latent semantic indexing\\nof λ satisfying\\nC⃗x = λ⃗x\\n(18.1)\\nare called the eigenvalues of C . The N-vector ⃗x satisfying Equation (18.1)\\nEIGENVALUE\\nfor an eigenvalue λ is the corresponding right eigenvector. The eigenvector\\ncorresponding to the eigenvalue of largest magnitude is called the principal\\neigenvector. In a similar fashion, the left eigenvectors of C are the M-vectors y\\nsuch that\\n⃗yT C = λ⃗yT.\\n(18.2)\\nThe number of non-zero eigenvalues of C is at most rank(C).\\nThe eigenvalues of a matrix are found by solving the characteristic equation,\\nwhich is obtained by rewriting Equation (18.1) in the form (C −λIM)⃗x = 0.\\nThe eigenvalues of C are then the solutions of |(C −λIM)| = 0, where |S|\\ndenotes the determinant of a square matrix S. The equation |(C −λIM)| = 0\\nis an Mth order polynomial equation in λ and can have at most M roots,\\nwhich are the eigenvalues of C. These eigenvalues can in general be complex,\\neven if all entries of C are real.\\nWe now examine some further properties of eigenvalues and eigenvectors,\\nto set up the central idea of singular value decompositions in Section 18.2 be-\\nlow. First, we look at the relationship between matrix-vector multiplication\\nand eigenvalues.\\n\\x0f\\nExample 18.1:\\nConsider the matrix\\nS =\\n\\uf8eb\\n\\uf8ed\\n30\\n0\\n0\\n0\\n20\\n0\\n0\\n0\\n1\\n\\uf8f6\\n\\uf8f8.\\nClearly the matrix has rank 3, and has 3 non-zero eigenvalues λ1 = 30, λ2 = 20 and\\nλ3 = 1, with the three corresponding eigenvectors\\n⃗x1 =\\n\\uf8eb\\n\\uf8ed\\n1\\n0\\n0\\n\\uf8f6\\n\\uf8f8, ⃗x2 =\\n\\uf8eb\\n\\uf8ed\\n0\\n1\\n0\\n\\uf8f6\\n\\uf8f8and ⃗x3 =\\n\\uf8eb\\n\\uf8ed\\n0\\n0\\n1\\n\\uf8f6\\n\\uf8f8.\\nFor each of the eigenvectors, multiplication by S acts as if we were multiplying the\\neigenvector by a multiple of the identity matrix; the multiple is different for each\\neigenvector. Now, consider an arbitrary vector, such as ⃗v =\\n\\uf8eb\\n\\uf8ed\\n2\\n4\\n6\\n\\uf8f6\\n\\uf8f8. We can always\\nexpress⃗v as a linear combination of the three eigenvectors of S; in the current example\\nwe have\\n⃗v =\\n\\uf8eb\\n\\uf8ed\\n2\\n4\\n6\\n\\uf8f6\\n\\uf8f8= 2⃗x1 + 4⃗x2 + 6⃗x3.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.1\\nLinear algebra review\\n405\\nSuppose we multiply⃗v by S:\\nS⃗v\\n=\\nS(2⃗x1 + 4⃗x2 + 6⃗x3)\\n=\\n2S⃗x1 + 4S⃗x2 + 6S⃗x3\\n=\\n2λ1⃗x1 + 4λ2⃗x2 + 6λ3⃗x3\\n=\\n60⃗x1 + 80⃗x2 + 6⃗x3.\\n(18.3)\\nExample 18.1 shows that even though ⃗v is an arbitrary vector, the effect of\\nmultiplication by S is determined by the eigenvalues and eigenvectors of S.\\nFurthermore, it is intuitively apparent from Equation (18.3) that the product\\nS⃗v is relatively unaffected by terms arising from the small eigenvalues of S;\\nin our example, since λ3 = 1, the contribution of the third term on the right\\nhand side of Equation (18.3) is small. In fact, if we were to completely ignore\\nthe contribution in Equation (18.3) from the third eigenvector corresponding\\nto λ3 = 1, then the product S⃗v would be computed to be\\n\\uf8eb\\n\\uf8ed\\n60\\n80\\n0\\n\\uf8f6\\n\\uf8f8rather than\\nthe correct product which is\\n\\uf8eb\\n\\uf8ed\\n60\\n80\\n6\\n\\uf8f6\\n\\uf8f8; these two vectors are relatively close\\nto each other by any of various metrics one could apply (such as the length\\nof their vector difference).\\nThis suggests that the effect of small eigenvalues (and their eigenvectors)\\non a matrix-vector product is small. We will carry forward this intuition\\nwhen studying matrix decompositions and low-rank approximations in Sec-\\ntion 18.2. Before doing so, we examine the eigenvectors and eigenvalues of\\nspecial forms of matrices that will be of particular interest to us.\\nFor a symmetric matrix S, the eigenvectors corresponding to distinct eigen-\\nvalues are orthogonal. Further, if S is both real and symmetric, the eigenvalues\\nare all real.\\n\\x0f\\nExample 18.2:\\nConsider the real, symmetric matrix\\nS =\\n\\x12 2\\n1\\n1\\n2\\n\\x13\\n.\\n(18.4)\\nFrom the characteristic equation |S −λI| = 0, we have the quadratic (2 −λ)2 −1 =\\n0, whose solutions yield the eigenvalues 3 and 1. The corresponding eigenvectors\\n\\x12\\n1\\n−1\\n\\x13\\nand\\n\\x12 1\\n1\\n\\x13\\nare orthogonal.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n406\\n18\\nMatrix decompositions and latent semantic indexing\\n18.1.1\\nMatrix decompositions\\nIn this section we examine ways in which a square matrix can be factored\\ninto the product of matrices derived from its eigenvectors; we refer to this\\nprocess as matrix decomposition. Matrix decompositions similar to the ones\\nMATRIX\\nDECOMPOSITION\\nin this section will form the basis of our principal text-analysis technique\\nin Section 18.3, where we will look at decompositions of non-square term-\\ndocument matrices. The square decompositions in this section are simpler\\nand can be treated with sufﬁcient mathematical rigor to help the reader un-\\nderstand how such decompositions work. The detailed mathematical deriva-\\ntion of the more complex decompositions in Section 18.2 are beyond the\\nscope of this book.\\nWe begin by giving two theorems on the decomposition of a square ma-\\ntrix into the product of three matrices of a special form. The ﬁrst of these,\\nTheorem 18.1, gives the basic factorization of a square real-valued matrix\\ninto three factors. The second, Theorem 18.2, applies to square symmetric\\nmatrices and is the basis of the singular value decomposition described in\\nTheorem 18.3.\\nTheorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valued\\nM × M matrix with M linearly independent eigenvectors. Then there exists an\\neigen decomposition\\nEIGEN DECOMPOSITION\\nS = UΛU−1,\\n(18.5)\\nwhere the columns of U are the eigenvectors of S and Λ is a diagonal matrix whose\\ndiagonal entries are the eigenvalues of S in decreasing order\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nλ1\\nλ2\\n· · ·\\nλM\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8, λi ≥λi+1.\\n(18.6)\\nIf the eigenvalues are distinct, then this decomposition is unique.\\nTo understand how Theorem 18.1 works, we note that U has the eigenvec-\\ntors of S as columns\\nU = (⃗u1 ⃗u2 · · · ⃗uM) .\\n(18.7)\\nThen we have\\nSU\\n=\\nS (⃗u1 ⃗u2 · · · ⃗uM)\\n=\\n(λ1⃗u1 λ2⃗u2 · · · λM ⃗uM)\\n=\\n(⃗u1 ⃗u2 · · · ⃗\\nuM)\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nλ1\\nλ2\\n· · ·\\nλM\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.2\\nTerm-document matrices and singular value decompositions\\n407\\nThus, we have SU = UΛ, or S = UΛU−1.\\nWe next state a closely related decomposition of a symmetric square matrix\\ninto the product of matrices derived from its eigenvectors. This will pave the\\nway for the development of our main tool for text analysis, the singular value\\ndecomposition (Section 18.2).\\nTheorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-\\nmetric real-valued M × M matrix with M linearly independent eigenvectors. Then\\nthere exists a symmetric diagonal decomposition\\nSYMMETRIC DIAGONAL\\nDECOMPOSITION\\nS = QΛQT,\\n(18.8)\\nwhere the columns of Q are the orthogonal and normalized (unit length, real) eigen-\\nvectors of S, and Λ is the diagonal matrix whose entries are the eigenvalues of S.\\nFurther, all entries of Q are real and we have Q−1 = QT.\\nWe will build on this symmetric diagonal decomposition to build low-rank\\napproximations to term-document matrices.\\n?\\nExercise 18.1\\nWhat is the rank of the 3 × 3 diagonal matrix below?\\n\\uf8eb\\n\\uf8ed\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n2\\n1\\n\\uf8f6\\n\\uf8f8\\nExercise 18.2\\nShow that λ = 2 is an eigenvalue of\\nC =\\n\\x12 6\\n−2\\n4\\n0\\n\\x13\\n.\\nFind the corresponding eigenvector.\\nExercise 18.3\\nCompute the unique eigen decomposition of the 2 × 2 matrix in (18.4).\\n18.2\\nTerm-document matrices and singular value decompositions\\nThe decompositions we have been studying thus far apply to square matri-\\nces. However, the matrix we are interested in is the M × N term-document\\nmatrix C where (barring a rare coincidence) M ̸= N; furthermore, C is very\\nunlikely to be symmetric. To this end we ﬁrst describe an extension of the\\nsymmetric diagonal decomposition known as the singular value decomposi-\\nSINGULAR VALUE\\nDECOMPOSITION\\ntion. We then show in Section 18.3 how this can be used to construct an ap-\\nproximate version of C. It is beyond the scope of this book to develop a full\\n', 'Online edition (c)\\n2009 Cambridge UP\\n408\\n18\\nMatrix decompositions and latent semantic indexing\\ntreatment of the mathematics underlying singular value decompositions; fol-\\nlowing the statement of Theorem 18.3 we relate the singular value decompo-\\nsition to the symmetric diagonal decompositions from Section 18.1.1. Given\\nSYMMETRIC DIAGONAL\\nDECOMPOSITION\\nC, let U be the M × M matrix whose columns are the orthogonal eigenvec-\\ntors of CCT, and V be the N × N matrix whose columns are the orthogonal\\neigenvectors of CTC. Denote by CT the transpose of a matrix C.\\nTheorem 18.3. Let r be the rank of the M × N matrix C. Then, there is a singular-\\nvalue decomposition (SVD for short) of C of the form\\nSVD\\nC = UΣVT,\\n(18.9)\\nwhere\\n1. The eigenvalues λ1, . . . , λr of CCT are the same as the eigenvalues of CTC;\\n2. For 1 ≤i ≤r, let σi = √λi, with λi ≥λi+1. Then the M × N matrix Σ is\\ncomposed by setting Σii = σi for 1 ≤i ≤r, and zero otherwise.\\nThe values σi are referred to as the singular values of C. It is instructive to\\nexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this rather\\nthan derive the general proof of Theorem 18.3, which is beyond the scope of\\nthis book.\\nBy multiplying Equation (18.9) by its transposed version, we have\\nCCT = UΣVT VΣUT = UΣ2UT.\\n(18.10)\\nNote now that in Equation (18.10), the left-hand side is a square symmetric\\nmatrix real-valued matrix, and the right-hand side represents its symmetric\\ndiagonal decomposition as in Theorem 18.2. What does the left-hand side\\nCCT represent? It is a square matrix with a row and a column correspond-\\ning to each of the M terms. The entry (i, j) in the matrix is a measure of the\\noverlap between the ith and jth terms, based on their co-occurrence in docu-\\nments. The precise mathematical meaning depends on the manner in which\\nC is constructed based on term weighting. Consider the case where C is the\\nterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then the\\nentry (i, j) in CCT is the number of documents in which both term i and term\\nj occur.\\nWhen writing down the numerical values of the SVD, it is conventional\\nto represent Σ as an r × r matrix with the singular values on the diagonals,\\nsince all its entries outside this sub-matrix are zeros. Accordingly, it is con-\\nventional to omit the rightmost M −r columns of U corresponding to these\\nomitted rows of Σ; likewise the rightmost N −r columns of V are omitted\\nsince they correspond in VT to the rows that will be multiplied by the N −r\\ncolumns of zeros in Σ. This written form of the SVD is sometimes known\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.2\\nTerm-document matrices and singular value decompositions\\n409\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nC\\n=\\nU\\nΣ\\nVT\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\n◮Figure 18.1\\nIllustration of the singular-value decomposition. In this schematic\\nillustration of (18.9), we see two cases illustrated. In the top half of the ﬁgure, we\\nhave a matrix C for which M > N. The lower half illustrates the case M < N.\\nas the reduced SVD or truncated SVD and we will encounter it again in Ex-\\nREDUCED SVD\\nTRUNCATED SVD\\nercise 18.9. Henceforth, our numerical examples and exercises will use this\\nreduced form.\\n\\x0f\\nExample 18.3:\\nWe now illustrate the singular-value decomposition of a 4 × 2 ma-\\ntrix of rank 2; the singular values are Σ11 = 2.236 and Σ22 = 1.\\nC =\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n1\\n−1\\n0\\n1\\n1\\n0\\n−1\\n1\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8=\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n−0.632\\n0.000\\n0.316\\n−0.707\\n−0.316\\n−0.707\\n0.632\\n0.000\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n\\x12\\n2.236\\n0.000\\n0.000\\n1.000\\n\\x13 \\x12 −0.707\\n0.707\\n−0.707\\n−0.707\\n\\x13\\n.\\n(18.11)\\nAs with the matrix decompositions deﬁned in Section 18.1.1, the singu-\\nlar value decomposition of a matrix can be computed by a variety of algo-\\nrithms, many of which have been publicly available software implementa-\\ntions; pointers to these are given in Section 18.5.\\n?\\nExercise 18.4\\nLet\\nC =\\n\\uf8eb\\n\\uf8ed\\n1\\n1\\n0\\n1\\n1\\n0\\n\\uf8f6\\n\\uf8f8\\n(18.12)\\nbe the term-document incidence matrix for a collection. Compute the co-occurrence\\nmatrix CCT. What is the interpretation of the diagonal entries of CCT when C is a\\nterm-document incidence matrix?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n410\\n18\\nMatrix decompositions and latent semantic indexing\\nExercise 18.5\\nVerify that the SVD of the matrix in Equation (18.12) is\\nU =\\n\\uf8eb\\n\\uf8ed\\n−0.816\\n0.000\\n−0.408\\n−0.707\\n−0.408\\n0.707\\n\\uf8f6\\n\\uf8f8, Σ =\\n\\x12 1.732\\n0.000\\n0.000\\n1.000\\n\\x13\\nand VT =\\n\\x12 −0.707\\n−0.707\\n0.707\\n−0.707\\n\\x13\\n,\\n(18.13)\\nby verifying all of the properties in the statement of Theorem 18.3.\\nExercise 18.6\\nSuppose that C is a binary term-document incidence matrix. What do the entries of\\nCTC represent?\\nExercise 18.7\\nLet\\nC =\\n\\uf8eb\\n\\uf8ed\\n0\\n2\\n1\\n0\\n3\\n0\\n2\\n1\\n0\\n\\uf8f6\\n\\uf8f8\\n(18.14)\\nbe a term-document matrix whose entries are term frequencies; thus term 1 occurs 2\\ntimes in document 2 and once in document 3. Compute CCT; observe that its entries\\nare largest where two terms have their most frequent occurrences together in the same\\ndocument.\\n18.3\\nLow-rank approximations\\nWe next state a matrix approximation problem that at ﬁrst seems to have\\nlittle to do with information retrieval. We describe a solution to this matrix\\nproblem using singular-value decompositions, then develop its application\\nto information retrieval.\\nGiven an M × N matrix C and a positive integer k, we wish to ﬁnd an\\nM × N matrix Ck of rank at most k, so as to minimize the Frobenius norm of\\nFROBENIUS NORM\\nthe matrix difference X = C −Ck, deﬁned to be\\n∥X∥F =\\nv\\nu\\nu\\nt\\nM\\n∑\\ni=1\\nN\\n∑\\nj=1\\nX2\\nij.\\n(18.15)\\nThus, the Frobenius norm of X measures the discrepancy between Ck and C;\\nour goal is to ﬁnd a matrix Ck that minimizes this discrepancy, while con-\\nstraining Ck to have rank at most k. If r is the rank of C, clearly Cr = C\\nand the Frobenius norm of the discrepancy is zero in this case. When k is far\\nsmaller than r, we refer to Ck as a low-rank approximation.\\nLOW-RANK\\nAPPROXIMATION\\nThe singular value decomposition can be used to solve the low-rank ma-\\ntrix approximation problem. We then derive from it an application to ap-\\nproximating term-document matrices. We invoke the following three-step\\nprocedure to this end:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.3\\nLow-rank approximations\\n411\\nCk\\n=\\nU\\nΣk\\nVT\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\nr\\n◮Figure 18.2\\nIllustration of low rank approximation using the singular-value de-\\ncomposition. The dashed boxes indicate the matrix entries affected by “zeroing out”\\nthe smallest singular values.\\n1. Given C, construct its SVD in the form shown in (18.9); thus, C = UΣVT.\\n2. Derive from Σ the matrix Σk formed by replacing by zeros the r −k small-\\nest singular values on the diagonal of Σ.\\n3. Compute and output Ck = UΣkVT as the rank-k approximation to C.\\nThe rank of Ck is at most k: this follows from the fact that Σk has at most\\nk non-zero values. Next, we recall the intuition of Example 18.1: the effect\\nof small eigenvalues on matrix products is small. Thus, it seems plausible\\nthat replacing these small eigenvalues by zero will not substantially alter the\\nproduct, leaving it “close” to C. The following theorem due to Eckart and\\nYoung tells us that, in fact, this procedure yields the matrix of rank k with\\nthe lowest possible Frobenius error.\\nTheorem 18.4.\\nmin\\nZ| rank(Z)=k\\n∥C −Z∥F = ∥C −Ck∥F = σk+1.\\n(18.16)\\nRecalling that the singular values are in decreasing order σ1 ≥σ2 ≥· · ·,\\nwe learn from Theorem 18.4 that Ck is the best rank-k approximation to C,\\nincurring an error (measured by the Frobenius norm of C −Ck) equal to σk+1.\\nThus the larger k is, the smaller this error (and in particular, for k = r, the\\nerror is zero since Σr = Σ; provided r < M, N, then σr+1 = 0 and thus\\nCr = C).\\nTo derive further insight into why the process of truncating the smallest\\nr −k singular values in Σ helps generate a rank-k approximation of low error,\\nwe examine the form of Ck:\\nCk\\n=\\nUΣkVT\\n(18.17)\\n', 'Online edition (c)\\n2009 Cambridge UP\\n412\\n18\\nMatrix decompositions and latent semantic indexing\\n=\\nU\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nσ1\\n0\\n0\\n0\\n0\\n0\\n· · ·\\n0\\n0\\n0\\n0\\n0\\nσk\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n· · ·\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\nVT\\n(18.18)\\n=\\nk\\n∑\\ni=1\\nσi⃗ui⃗vT\\ni ,\\n(18.19)\\nwhere ⃗ui and ⃗vi are the ith columns of U and V, respectively. Thus, ⃗ui⃗vT\\ni is\\na rank-1 matrix, so that we have just expressed Ck as the sum of k rank-1\\nmatrices each weighted by a singular value. As i increases, the contribution\\nof the rank-1 matrix ⃗ui⃗vT\\ni is weighted by a sequence of shrinking singular\\nvalues σi.\\n?\\nExercise 18.8\\nCompute a rank 1 approximation C1 to the matrix C in Example 18.12, using the SVD\\nas in Exercise 18.13. What is the Frobenius norm of the error of this approximation?\\nExercise 18.9\\nConsider now the computation in Exercise 18.8.\\nFollowing the schematic in Fig-\\nure 18.2, notice that for a rank 1 approximation we have σ1 being a scalar. Denote\\nby U1 the ﬁrst column of U and by V1 the ﬁrst column of V. Show that the rank-1\\napproximation to C can then be written as U1σ1VT\\n1 = σ1U1VT\\n1 .\\nExercise 18.10\\nExercise 18.9 can be generalized to rank k approximations: we let U′\\nk and V′\\nk denote\\nthe “reduced” matrices formed by retaining only the ﬁrst k columns of U and V,\\nrespectively. Thus U′\\nk is an M × k matrix while V′T\\nk is a k × N matrix. Then, we have\\nCk = U′\\nkΣ′\\nkV′T\\nk ,\\n(18.20)\\nwhere Σ′\\nk is the square k × k submatrix of Σk with the singular values σ1, . . . , σk on\\nthe diagonal. The primary advantage of using (18.20) is to eliminate a lot of redun-\\ndant columns of zeros in U and V, thereby explicitly eliminating multiplication by\\ncolumns that do not affect the low-rank approximation; this version of the SVD is\\nsometimes known as the reduced SVD or truncated SVD and is a computationally\\nsimpler representation from which to compute the low rank approximation.\\nFor the matrix C in Example 18.3, write down both Σ2 and Σ′\\n2.\\n18.4\\nLatent semantic indexing\\nWe now discuss the approximation of a term-document matrix C by one of\\nlower rank using the SVD. The low-rank approximation to C yields a new\\nrepresentation for each document in the collection.\\nWe will cast queries\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.4\\nLatent semantic indexing\\n413\\ninto this low-rank representation as well, enabling us to compute query-\\ndocument similarity scores in this low-rank representation. This process is\\nknown as latent semantic indexing (generally abbreviated LSI).\\nLATENT SEMANTIC\\nINDEXING\\nBut ﬁrst, we motivate such an approximation. Recall the vector space rep-\\nresentation of documents and queries introduced in Section 6.3 (page 120).\\nThis vector space representation enjoys a number of advantages including\\nthe uniform treatment of queries and documents as vectors, the induced\\nscore computation based on cosine similarity, the ability to weight differ-\\nent terms differently, and its extension beyond document retrieval to such\\napplications as clustering and classiﬁcation. The vector space representa-\\ntion suffers, however, from its inability to cope with two classic problems\\narising in natural languages: synonymy and polysemy. Synonymy refers to a\\ncase where two different words (say car and automobile) have the same mean-\\ning. Because the vector space representation fails to capture the relationship\\nbetween synonymous terms such as car and automobile – according each a\\nseparate dimension in the vector space. Consequently the computed simi-\\nlarity⃗q · ⃗d between a query⃗q (say, car) and a document ⃗d containing both car\\nand automobile underestimates the true similarity that a user would perceive.\\nPolysemy on the other hand refers to the case where a term such as charge\\nhas multiple meanings, so that the computed similarity ⃗q · ⃗d overestimates\\nthe similarity that a user would perceive. Could we use the co-occurrences\\nof terms (whether, for instance, charge occurs in a document containing steed\\nversus in a document containing electron) to capture the latent semantic as-\\nsociations of terms and alleviate these problems?\\nEven for a collection of modest size, the term-document matrix C is likely\\nto have several tens of thousand of rows and columns, and a rank in the\\ntens of thousands as well. In latent semantic indexing (sometimes referred\\nto as latent semantic analysis (LSA)), we use the SVD to construct a low-rank\\nLSA\\napproximation Ck to the term-document matrix, for a value of k that is far\\nsmaller than the original rank of C. In the experimental work cited later\\nin this section, k is generally chosen to be in the low hundreds. We thus\\nmap each row/column (respectively corresponding to a term/document) to\\na k-dimensional space; this space is deﬁned by the k principal eigenvectors\\n(corresponding to the largest eigenvalues) of CCT and CTC. Note that the\\nmatrix Ck is itself still an M × N matrix, irrespective of k.\\nNext, we use the new k-dimensional LSI representation as we did the orig-\\ninal representation – to compute similarities between vectors. A query vector\\n⃗q is mapped into its representation in the LSI space by the transformation\\n⃗qk = Σ−1\\nk UT\\nk ⃗q.\\n(18.21)\\nNow, we may use cosine similarities as in Section 6.3.1 (page 120) to com-\\npute the similarity between a query and a document, between two docu-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n414\\n18\\nMatrix decompositions and latent semantic indexing\\nments, or between two terms. Note especially that Equation (18.21) does not\\nin any way depend on ⃗q being a query; it is simply a vector in the space of\\nterms. This means that if we have an LSI representation of a collection of\\ndocuments, a new document not in the collection can be “folded in” to this\\nrepresentation using Equation (18.21). This allows us to incrementally add\\ndocuments to an LSI representation. Of course, such incremental addition\\nfails to capture the co-occurrences of the newly added documents (and even\\nignores any new terms they contain). As such, the quality of the LSI rep-\\nresentation will degrade as more documents are added and will eventually\\nrequire a recomputation of the LSI representation.\\nThe ﬁdelity of the approximation of Ck to C leads us to hope that the rel-\\native values of cosine similarities are preserved: if a query is close to a doc-\\nument in the original space, it remains relatively close in the k-dimensional\\nspace. But this in itself is not sufﬁciently interesting, especially given that\\nthe sparse query vector ⃗q turns into a dense query vector ⃗qk in the low-\\ndimensional space. This has a signiﬁcant computational cost, when com-\\npared with the cost of processing⃗q in its native form.\\n\\x0f\\nExample 18.4:\\nConsider the term-document matrix C =\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\nship\\n1\\n0\\n1\\n0\\n0\\n0\\nboat\\n0\\n1\\n0\\n0\\n0\\n0\\nocean\\n1\\n1\\n0\\n0\\n0\\n0\\nvoyage\\n1\\n0\\n0\\n1\\n1\\n0\\ntrip\\n0\\n0\\n0\\n1\\n0\\n1\\nIts singular value decomposition is the product of three matrices as below. First we\\nhave U which in this example is:\\n1\\n2\\n3\\n4\\n5\\nship\\n−0.44\\n−0.30\\n0.57\\n0.58\\n0.25\\nboat\\n−0.13\\n−0.33\\n−0.59\\n0.00\\n0.73\\nocean\\n−0.48\\n−0.51\\n−0.37\\n0.00\\n−0.61\\nvoyage\\n−0.70\\n0.35\\n0.15\\n−0.58\\n0.16\\ntrip\\n−0.26\\n0.65\\n−0.41\\n0.58\\n−0.09\\nWhen applying the SVD to a term-document matrix, U is known as the SVD term\\nmatrix. The singular values are Σ =\\n2.16\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.59\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.28\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.39\\nFinally we have VT, which in the context of a term-document matrix is known as\\nthe SVD document matrix:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.4\\nLatent semantic indexing\\n415\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−0.75\\n−0.28\\n−0.20\\n−0.45\\n−0.33\\n−0.12\\n2\\n−0.29\\n−0.53\\n−0.19\\n0.63\\n0.22\\n0.41\\n3\\n0.28\\n−0.75\\n0.45\\n−0.20\\n0.12\\n−0.33\\n4\\n0.00\\n0.00\\n0.58\\n0.00\\n−0.58\\n0.58\\n5\\n−0.53\\n0.29\\n0.63\\n0.19\\n0.41\\n−0.22\\nBy “zeroing out” all but the two largest singular values of Σ, we obtain Σ2 =\\n2.16\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n1.59\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nFrom this, we compute C2 =\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−1.62\\n−0.60\\n−0.44\\n−0.97\\n−0.70\\n−0.26\\n2\\n−0.46\\n−0.84\\n−0.30\\n1.00\\n0.35\\n0.65\\n3\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n4\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n5\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\nNotice that the low-rank approximation, unlike the original matrix C, can have\\nnegative entries.\\nExamination of C2 and Σ2 in Example 18.4 shows that the last 3 rows of\\neach of these matrices are populated entirely by zeros. This suggests that\\nthe SVD product UΣVT in Equation (18.18) can be carried out with only two\\nrows in the representations of Σ2 and VT; we may then replace these matrices\\nby their truncated versions Σ′\\n2 and (V′)T. For instance, the truncated SVD\\ndocument matrix (V′)T in this example is:\\nd1\\nd2\\nd3\\nd4\\nd5\\nd6\\n1\\n−1.62\\n−0.60\\n−0.44\\n−0.97\\n−0.70\\n−0.26\\n2\\n−0.46\\n−0.84\\n−0.30\\n1.00\\n0.35\\n0.65\\nFigure 18.3 illustrates the documents in (V′)T in two dimensions. Note\\nalso that C2 is dense relative to C.\\nWe may in general view the low-rank approximation of C by Ck as a con-\\nstrained optimization problem: subject to the constraint that Ck have rank at\\nmost k, we seek a representation of the terms and documents comprising C\\nwith low Frobenius norm for the error C −Ck. When forced to squeeze the\\nterms/documents down to a k-dimensional space, the SVD should bring to-\\ngether terms with similar co-occurrences. This intuition suggests, then, that\\nnot only should retrieval quality not suffer too much from the dimension\\nreduction, but in fact may improve.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n416\\n18\\nMatrix decompositions and latent semantic indexing\\n−0.5\\n−1.0\\n−1.5\\n0.5\\n1.0\\n−0.5\\n−1.0\\ndim 2\\ndim 1\\n×\\nd1\\n×\\nd2\\n× d3\\n×\\nd4\\n×\\nd5\\n× d6\\n◮Figure 18.3\\nThe documents of Example 18.4 reduced to two dimensions in (V′)T.\\nDumais (1993) and Dumais (1995) conducted experiments with LSI on\\nTREC documents and tasks, using the commonly-used Lanczos algorithm\\nto compute the SVD. At the time of their work in the early 1990’s, the LSI\\ncomputation on tens of thousands of documents took approximately a day\\non one machine. On these experiments, they achieved precision at or above\\nthat of the median TREC participant. On about 20% of TREC topics their\\nsystem was the top scorer, and reportedly slightly better on average than\\nstandard vector spaces for LSI at about 350 dimensions. Here are some con-\\nclusions on LSI ﬁrst suggested by their work, and subsequently veriﬁed by\\nmany other experiments.\\n• The computational cost of the SVD is signiﬁcant; at the time of this writ-\\ning, we know of no successful experiment with over one million docu-\\nments. This has been the biggest obstacle to the widespread adoption to\\nLSI. One approach to this obstacle is to build the LSI representation on a\\nrandomly sampled subset of the documents in the collection, following\\nwhich the remaining documents are “folded in” as detailed with Equa-\\ntion (18.21).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.5\\nReferences and further reading\\n417\\n• As we reduce k, recall tends to increase, as expected.\\n• Most surprisingly, a value of k in the low hundreds can actually increase\\nprecision on some query benchmarks. This appears to suggest that for a\\nsuitable value of k, LSI addresses some of the challenges of synonymy.\\n• LSI works best in applications where there is little overlap between queries\\nand documents.\\nThe experiments also documented some modes where LSI failed to match\\nthe effectiveness of more traditional indexes and score computations. Most\\nnotably (and perhaps obviously), LSI shares two basic drawbacks of vector\\nspace retrieval: there is no good way of expressing negations (ﬁnd docu-\\nments that contain german but not shepherd), and no way of enforcing Boolean\\nconditions.\\nLSI can be viewed as soft clustering by interpreting each dimension of the\\nSOFT CLUSTERING\\nreduced space as a cluster and the value that a document has on that dimen-\\nsion as its fractional membership in that cluster.\\n18.5\\nReferences and further reading\\nStrang (1986) provides an excellent introductory overview of matrix decom-\\npositions including the singular value decomposition. Theorem 18.4 is due\\nto Eckart and Young (1936). The connection between information retrieval\\nand low-rank approximations of the term-document matrix was introduced\\nin Deerwester et al. (1990), with a subsequent survey of results in Berry\\net al. (1995). Dumais (1993) and Dumais (1995) describe experiments on\\nTREC benchmarks giving evidence that at least on some benchmarks, LSI\\ncan produce better precision and recall than standard vector-space retrieval.\\nhttp://www.cs.utk.edu/˜berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.html\\noffer comprehensive pointers to the literature and software of LSI. Schütze\\nand Silverstein (1997) evaluate LSI and truncated representations of cen-\\ntroids for efﬁcient K-means clustering (Section 16.4). Bast and Majumdar\\n(2005) detail the role of the reduced dimension k in LSI and how different\\npairs of terms get coalesced together at differing values of k. Applications of\\nLSI to cross-language information retrieval (where documents in two or more\\nCROSS-LANGUAGE\\nINFORMATION\\nRETRIEVAL\\ndifferent languages are indexed, and a query posed in one language is ex-\\npected to retrieve documents in other languages) are developed in Berry and\\nYoung (1995) and Littman et al. (1998). LSI (referred to as LSA in more gen-\\neral settings) has been applied to host of other problems in computer science\\nranging from memory modeling to computer vision.\\nHofmann (1999a;b) provides an initial probabilistic extension of the basic\\nlatent semantic indexing technique. A more satisfactory formal basis for a\\n', 'Online edition (c)\\n2009 Cambridge UP\\n418\\n18\\nMatrix decompositions and latent semantic indexing\\nDocID\\nDocument text\\n1\\nhello\\n2\\nopen house\\n3\\nmi casa\\n4\\nhola Profesor\\n5\\nhola y bienvenido\\n6\\nhello and welcome\\n◮Figure 18.4\\nDocuments for Exercise 18.11.\\nSpanish\\nEnglish\\nmi\\nmy\\ncasa\\nhouse\\nhola\\nhello\\nprofesor\\nprofessor\\ny\\nand\\nbienvenido\\nwelcome\\n◮Figure 18.5\\nGlossary for Exercise 18.11.\\nprobabilistic latent variable model for dimensionality reduction is the Latent\\nDirichlet Allocation (LDA) model (Blei et al. 2003), which is generative and\\nassigns probabilities to documents outside of the training set. This model is\\nextended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft\\n(2006) present the ﬁrst large scale evaluation of LDA, ﬁnding it to signiﬁ-\\ncantly outperform the query likelihood model of Section 12.2 (page 242), but\\nto not perform quite as well as the relevance model mentioned in Section 12.4\\n(page 250) – but the latter does additional per-query processing unlike LDA.\\nTeh et al. (2006) generalize further by presenting Hierarchical Dirichlet Pro-\\ncesses, a probabilistic model which allows a group (for us, a document) to\\nbe drawn from an inﬁnite mixture of latent topics, while still allowing these\\ntopics to be shared across documents.\\n?\\nExercise 18.11\\nAssume you have a set of documents each of which is in either English or in Spanish.\\nThe collection is given in Figure 18.4.\\nFigure 18.5 gives a glossary relating the Spanish and English words above for your\\nown information. This glossary is NOT available to the retrieval system:\\n1. Construct the appropriate term-document matrix C to use for a collection con-\\nsisting of these documents. For simplicity, use raw term frequencies rather than\\nnormalized tf-idf weights. Make sure to clearly label the dimensions of your ma-\\ntrix.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n18.5\\nReferences and further reading\\n419\\n2. Write down the matrices U2, Σ′\\n2 and V2 and from these derive the rank 2 approxi-\\nmation C2.\\n3. State succinctly what the (i, j) entry in the matrix CTC represents.\\n4. State succinctly what the (i, j) entry in the matrix CT\\n2 C2 represents, and why it\\ndiffers from that in CTC.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n421\\n19\\nWeb search basics\\nIn this and the following two chapters, we consider web search engines. Sec-\\ntions 19.1–19.4 provide some background and history to help the reader ap-\\npreciate the forces that conspire to make the Web chaotic, fast-changing and\\n(from the standpoint of information retrieval) very different from the “tradi-\\ntional” collections studied thus far in this book. Sections 19.5–19.6 deal with\\nestimating the number of documents indexed by web search engines, and the\\nelimination of duplicate documents in web indexes, respectively. These two\\nlatter sections serve as background material for the following two chapters.\\n19.1\\nBackground and history\\nThe Web is unprecedented in many ways: unprecedented in scale, unprece-\\ndented in the almost-complete lack of coordination in its creation, and un-\\nprecedented in the diversity of backgrounds and motives of its participants.\\nEach of these contributes to making web search different – and generally far\\nharder – than searching “traditional” documents.\\nThe invention of hypertext, envisioned by Vannevar Bush in the 1940’s and\\nﬁrst realized in working systems in the 1970’s, signiﬁcantly precedes the for-\\nmation of the World Wide Web (which we will simply refer to as the Web), in\\nthe 1990’s. Web usage has shown tremendous growth to the point where it\\nnow claims a good fraction of humanity as participants, by relying on a sim-\\nple, open client-server design: (1) the server communicates with the client\\nvia a protocol (the http or hypertext transfer protocol) that is lightweight and\\nHTTP\\nsimple, asynchronously carrying a variety of payloads (text, images and –\\nover time – richer media such as audio and video ﬁles) encoded in a sim-\\nple markup language called HTML (for hypertext markup language); (2) the\\nHTML\\nclient – generally a browser, an application within a graphical user environ-\\nment – can ignore what it does not understand. Each of these seemingly\\ninnocuous features has contributed enormously to the growth of the Web, so\\nit is worthwhile to examine them further.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n422\\n19\\nWeb search basics\\nThe basic operation is as follows: a client (such as a browser) sends an http\\nrequest to a web server. The browser speciﬁes a URL (for Universal Resource Lo-\\nURL\\ncator) such as http://www.stanford.edu/home/atoz/contact.html.\\nIn this example URL, the string http refers to the protocol to be used for\\ntransmitting the data. The string www.stanford.edu is known as the do-\\nmain and speciﬁes the root of a hierarchy of web pages (typically mirroring a\\nﬁlesystem hierarchy underlying the web server). In this example, /home/atoz/contact.html\\nis a path in this hierarchy with a ﬁle contact.html that contains the infor-\\nmation to be returned by the web server at www.stanford.edu in response\\nto this request. The HTML-encoded ﬁle contact.html holds the hyper-\\nlinks and the content (in this instance, contact information for Stanford Uni-\\nversity), as well as formatting rules for rendering this content in a browser.\\nSuch an http request thus allows us to fetch the content of a page, some-\\nthing that will prove to be useful to us for crawling and indexing documents\\n(Chapter 20).\\nThe designers of the ﬁrst browsers made it easy to view the HTML markup\\ntags on the content of a URL. This simple convenience allowed new users to\\ncreate their own HTML content without extensive training or experience;\\nrather, they learned from example content that they liked. As they did so, a\\nsecond feature of browsers supported the rapid proliferation of web content\\ncreation and usage: browsers ignored what they did not understand. This\\ndid not, as one might fear, lead to the creation of numerous incompatible\\ndialects of HTML. What it did promote was amateur content creators who\\ncould freely experiment with and learn from their newly created web pages\\nwithout fear that a simple syntax error would “bring the system down.” Pub-\\nlishing on the Web became a mass activity that was not limited to a few\\ntrained programmers, but rather open to tens and eventually hundreds of\\nmillions of individuals. For most users and for most information needs, the\\nWeb quickly became the best way to supply and consume information on\\neverything from rare ailments to subway schedules.\\nThe mass publishing of information on the Web is essentially useless un-\\nless this wealth of information can be discovered and consumed by other\\nusers. Early attempts at making web information “discoverable” fell into two\\nbroad categories: (1) full-text index search engines such as Altavista, Excite\\nand Infoseek and (2) taxonomies populated with web pages in categories,\\nsuch as Yahoo! The former presented the user with a keyword search in-\\nterface supported by inverted indexes and ranking mechanisms building on\\nthose introduced in earlier chapters. The latter allowed the user to browse\\nthrough a hierarchical tree of category labels. While this is at ﬁrst blush a\\nconvenient and intuitive metaphor for ﬁnding web pages, it has a number of\\ndrawbacks: ﬁrst, accurately classifying web pages into taxonomy tree nodes\\nis for the most part a manual editorial process, which is difﬁcult to scale\\nwith the size of the Web. Arguably, we only need to have “high-quality”\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n423\\nweb pages in the taxonomy, with only the best web pages for each category.\\nHowever, just discovering these and classifying them accurately and consis-\\ntently into the taxonomy entails signiﬁcant human effort. Furthermore, in\\norder for a user to effectively discover web pages classiﬁed into the nodes of\\nthe taxonomy tree, the user’s idea of what sub-tree(s) to seek for a particu-\\nlar topic should match that of the editors performing the classiﬁcation. This\\nquickly becomes challenging as the size of the taxonomy grows; the Yahoo!\\ntaxonomy tree surpassed 1000 distinct nodes fairly early on. Given these\\nchallenges, the popularity of taxonomies declined over time, even though\\nvariants (such as About.com and the Open Directory Project) sprang up with\\nsubject-matter experts collecting and annotating web pages for each cate-\\ngory.\\nThe ﬁrst generation of web search engines transported classical search\\ntechniques such as those in the preceding chapters to the web domain, focus-\\ning on the challenge of scale. The earliest web search engines had to contend\\nwith indexes containing tens of millions of documents, which was a few or-\\nders of magnitude larger than any prior information retrieval system in the\\npublic domain. Indexing, query serving and ranking at this scale required\\nthe harnessing together of tens of machines to create highly available sys-\\ntems, again at scales not witnessed hitherto in a consumer-facing search ap-\\nplication. The ﬁrst generation of web search engines was largely successful\\nat solving these challenges while continually indexing a signiﬁcant fraction\\nof the Web, all the while serving queries with sub-second response times.\\nHowever, the quality and relevance of web search results left much to be\\ndesired owing to the idiosyncrasies of content creation on the Web that we\\ndiscuss in Section 19.2. This necessitated the invention of new ranking and\\nspam-ﬁghting techniques in order to ensure the quality of the search results.\\nWhile classical information retrieval techniques (such as those covered ear-\\nlier in this book) continue to be necessary for web search, they are not by\\nany means sufﬁcient. A key aspect (developed further in Chapter 21) is that\\nwhereas classical techniques measure the relevance of a document to a query,\\nthere remains a need to gauge the authoritativeness of a document based on\\ncues such as which website hosts it.\\n19.2\\nWeb characteristics\\nThe essential feature that led to the explosive growth of the web – decentral-\\nized content publishing with essentially no central control of authorship –\\nturned out to be the biggest challenge for web search engines in their quest to\\nindex and retrieve this content. Web page authors created content in dozens\\nof (natural) languages and thousands of dialects, thus demanding many dif-\\nferent forms of stemming and other linguistic operations. Because publish-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n424\\n19\\nWeb search basics\\ning was now open to tens of millions, web pages exhibited heterogeneity at a\\ndaunting scale, in many crucial aspects. First, content-creation was no longer\\nthe privy of editorially-trained writers; while this represented a tremendous\\ndemocratization of content creation, it also resulted in a tremendous varia-\\ntion in grammar and style (and in many cases, no recognizable grammar or\\nstyle). Indeed, web publishing in a sense unleashed the best and worst of\\ndesktop publishing on a planetary scale, so that pages quickly became rid-\\ndled with wild variations in colors, fonts and structure. Some web pages,\\nincluding the professionally created home pages of some large corporations,\\nconsisted entirely of images (which, when clicked, led to richer textual con-\\ntent) – and therefore, no indexable text.\\nWhat about the substance of the text in web pages? The democratization\\nof content creation on the web meant a new level of granularity in opinion on\\nvirtually any subject. This meant that the web contained truth, lies, contra-\\ndictions and suppositions on a grand scale. This gives rise to the question:\\nwhich web pages does one trust? In a simplistic approach, one might argue\\nthat some publishers are trustworthy and others not – begging the question\\nof how a search engine is to assign such a measure of trust to each website\\nor web page. In Chapter 21 we will examine approaches to understanding\\nthis question. More subtly, there may be no universal, user-independent no-\\ntion of trust; a web page whose contents are trustworthy to one user may\\nnot be so to another. In traditional (non-web) publishing this is not an issue:\\nusers self-select sources they ﬁnd trustworthy. Thus one reader may ﬁnd\\nthe reporting of The New York Times to be reliable, while another may prefer\\nThe Wall Street Journal. But when a search engine is the only viable means\\nfor a user to become aware of (let alone select) most content, this challenge\\nbecomes signiﬁcant.\\nWhile the question “how big is the Web?” has no easy answer (see Sec-\\ntion 19.5), the question “how many web pages are in a search engine’s index”\\nis more precise, although, even this question has issues. By the end of 1995,\\nAltavista reported that it had crawled and indexed approximately 30 million\\nstatic web pages. Static web pages are those whose content does not vary from\\nSTATIC WEB PAGES\\none request for that page to the next. For this purpose, a professor who man-\\nually updates his home page every week is considered to have a static web\\npage, but an airport’s ﬂight status page is considered to be dynamic. Dy-\\nnamic pages are typically mechanically generated by an application server\\nin response to a query to a database, as show in Figure 19.1. One sign of\\nsuch a page is that the URL has the character \"?\" in it. Since the number\\nof static web pages was believed to be doubling every few months in 1995,\\nearly web search engines such as Altavista had to constantly add hardware\\nand bandwidth for crawling and indexing web pages.\\n', \"Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n425\\n◮Figure 19.1\\nA dynamically generated web page. The browser sends a request for\\nﬂight information on ﬂight AA129 to the web application, that fetches the informa-\\ntion from back-end databases then creates a dynamic web page that it returns to the\\nbrowser.\\n&%\\n'$\\n&%\\n'$\\n-\\nanchor\\n◮Figure 19.2\\nTwo nodes of the web graph joined by a link.\\n19.2.1\\nThe web graph\\nWe can view the static Web consisting of static HTML pages together with\\nthe hyperlinks between them as a directed graph in which each web page is\\na node and each hyperlink a directed edge.\\nFigure 19.2 shows two nodes A and B from the web graph, each corre-\\nsponding to a web page, with a hyperlink from A to B. We refer to the set of\\nall such nodes and directed edges as the web graph. Figure 19.2 also shows\\nthat (as is the case with most links on web pages) there is some text surround-\\ning the origin of the hyperlink on page A. This text is generally encapsulated\\nin the href attribute of the <a> (for anchor) tag that encodes the hyperlink\\nin the HTML code of page A, and is referred to as anchor text. As one might\\nANCHOR TEXT\\nsuspect, this directed graph is not strongly connected: there are pairs of pages\\nsuch that one cannot proceed from one page of the pair to the other by follow-\\ning hyperlinks. We refer to the hyperlinks into a page as in-links and those\\nIN-LINKS\\nout of a page as out-links. The number of in-links to a page (also known as\\nOUT-LINKS\\nits in-degree) has averaged from roughly 8 to 15, in a range of studies. We\\nsimilarly deﬁne the out-degree of a web page to be the number of links out\\n\", 'Online edition (c)\\n2009 Cambridge UP\\n426\\n19\\nWeb search basics\\n◮Figure 19.3\\nA sample small web graph. In this example we have six pages labeled\\nA-F. Page B has in-degree 3 and out-degree 1. This example graph is not strongly\\nconnected: there is no path from any of pages B-F to page A.\\nof it. These notions are represented in Figure 19.3.\\nThere is ample evidence that these links are not randomly distributed; for\\none thing, the distribution of the number of links into a web page does not\\nfollow the Poisson distribution one would expect if every web page were\\nto pick the destinations of its links uniformly at random. Rather, this dis-\\ntribution is widely reported to be a power law, in which the total number of\\nPOWER LAW\\nweb pages with in-degree i is proportional to 1/iα; the value of α typically\\nreported by studies is 2.1.1 Furthermore, several studies have suggested that\\nthe directed graph connecting web pages has a bowtie shape: there are three\\nBOWTIE\\nmajor categories of web pages that are sometimes referred to as IN, OUT\\nand SCC. A web surfer can pass from any page in IN to any page in SCC, by\\nfollowing hyperlinks. Likewise, a surfer can pass from page in SCC to any\\npage in OUT. Finally, the surfer can surf from any page in SCC to any other\\npage in SCC. However, it is not possible to pass from a page in SCC to any\\npage in IN, or from a page in OUT to a page in SCC (or, consequently, IN).\\nNotably, in several studies IN and OUT are roughly equal in size, whereas\\n1. Cf. Zipf’s law of the distribution of words in text in Chapter 5 (page 90), which is a power\\nlaw with α = 1.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.2\\nWeb characteristics\\n427\\n◮Figure 19.4\\nThe bowtie structure of the Web. Here we show one tube and three\\ntendrils.\\nSCC is somewhat larger; most web pages fall into one of these three sets. The\\nremaining pages form into tubes that are small sets of pages outside SCC that\\nlead directly from IN to OUT, and tendrils that either lead nowhere from IN,\\nor from nowhere to OUT. Figure 19.4 illustrates this structure of the Web.\\n19.2.2\\nSpam\\nEarly in the history of web search, it became clear that web search engines\\nwere an important means for connecting advertisers to prospective buyers.\\nA user searching for maui golf real estate is not merely seeking news or en-\\ntertainment on the subject of housing on golf courses on the island of Maui,\\nbut instead likely to be seeking to purchase such a property. Sellers of such\\nproperty and their agents, therefore, have a strong incentive to create web\\npages that rank highly on this query. In a search engine whose scoring was\\nbased on term frequencies, a web page with numerous repetitions of maui golf\\nreal estate would rank highly. This led to the ﬁrst generation of spam, which\\nSPAM\\n(in the context of web search) is the manipulation of web page content for\\nthe purpose of appearing high up in search results for selected keywords.\\nTo avoid irritating users with these repetitions, sophisticated spammers re-\\nsorted to such tricks as rendering these repeated terms in the same color as\\nthe background. Despite these words being consequently invisible to the hu-\\nman user, a search engine indexer would parse the invisible words out of\\n', 'Online edition (c)\\n2009 Cambridge UP\\n428\\n19\\nWeb search basics\\n◮Figure 19.5\\nCloaking as used by spammers.\\nthe HTML representation of the web page and index these words as being\\npresent in the page.\\nAt its root, spam stems from the heterogeneity of motives in content cre-\\nation on the Web. In particular, many web content creators have commercial\\nmotives and therefore stand to gain from manipulating search engine results.\\nYou might argue that this is no different from a company that uses large fonts\\nto list its phone numbers in the yellow pages; but this generally costs the\\ncompany more and is thus a fairer mechanism. A more apt analogy, perhaps,\\nis the use of company names beginning with a long string of A’s to be listed\\nearly in a yellow pages category. In fact, the yellow pages’ model of com-\\npanies paying for larger/darker fonts has been replicated in web search: in\\nmany search engines, it is possible to pay to have one’s web page included\\nin the search engine’s index – a model known as paid inclusion. Different\\nPAID INCLUSION\\nsearch engines have different policies on whether to allow paid inclusion,\\nand whether such a payment has any effect on ranking in search results.\\nSearch engines soon became sophisticated enough in their spam detection\\nto screen out a large number of repetitions of particular keywords. Spam-\\nmers responded with a richer set of spam techniques, the best known of\\nwhich we now describe. The ﬁrst of these techniques is cloaking, shown in\\nFigure 19.5. Here, the spammer’s web server returns different pages depend-\\ning on whether the http request comes from a web search engine’s crawler\\n(the part of the search engine that gathers web pages, to be described in\\nChapter 20), or from a human user’s browser. The former causes the web\\npage to be indexed by the search engine under misleading keywords. When\\nthe user searches for these keywords and elects to view the page, he receives\\na web page that has altogether different content than that indexed by the\\nsearch engine. Such deception of search indexers is unknown in the tra-\\nditional world of information retrieval; it stems from the fact that the rela-\\ntionship between page publishers and web search engines is not completely\\ncollaborative.\\nA doorway page contains text and metadata carefully chosen to rank highly\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.3\\nAdvertising as the economic model\\n429\\non selected search keywords. When a browser requests the doorway page, it\\nis redirected to a page containing content of a more commercial nature. More\\ncomplex spamming techniques involve manipulation of the metadata related\\nto a page including (for reasons we will see in Chapter 21) the links into a\\nweb page. Given that spamming is inherently an economically motivated\\nactivity, there has sprung around it an industry of Search Engine Optimizers,\\nSEARCH ENGINE\\nOPTIMIZERS\\nor SEOs to provide consultancy services for clients who seek to have their\\nweb pages rank highly on selected keywords. Web search engines frown on\\nthis business of attempting to decipher and adapt to their proprietary rank-\\ning techniques and indeed announce policies on forms of SEO behavior they\\ndo not tolerate (and have been known to shut down search requests from cer-\\ntain SEOs for violation of these). Inevitably, the parrying between such SEOs\\n(who gradually infer features of each web search engine’s ranking methods)\\nand the web search engines (who adapt in response) is an unending struggle;\\nindeed, the research sub-area of adversarial information retrieval has sprung up\\nADVERSARIAL\\nINFORMATION\\nRETRIEVAL\\naround this battle. To combat spammers who manipulate the text of their\\nweb pages is the exploitation of the link structure of the Web – a technique\\nknown as link analysis. The ﬁrst web search engine known to apply link anal-\\nysis on a large scale (to be detailed in Chapter 21) was Google, although all\\nweb search engines currently make use of it (and correspondingly, spam-\\nmers now invest considerable effort in subverting it – this is known as link\\nLINK SPAM\\nspam).\\n?\\nExercise 19.1\\nIf the number of pages with in-degree i is proportional to 1/i2.1, what is the probabil-\\nity that a randomly chosen web page has in-degree 1?\\nExercise 19.2\\nIf the number of pages with in-degree i is proportional to 1/i2.1, what is the average\\nin-degree of a web page?\\nExercise 19.3\\nIf the number of pages with in-degree i is proportional to 1/i2.1, then as the largest\\nin-degree goes to inﬁnity, does the fraction of pages with in-degree i grow, stay the\\nsame, or diminish? How would your answer change for values of the exponent other\\nthan 2.1?\\nExercise 19.4\\nThe average in-degree of all nodes in a snapshot of the web graph is 9. What can we\\nsay about the average out-degree of all nodes in this snapshot?\\n19.3\\nAdvertising as the economic model\\nEarly in the history of the Web, companies used graphical banner advertise-\\nments on web pages at popular websites (news and entertainment sites such\\nas MSN, America Online, Yahoo! and CNN). The primary purpose of these\\nadvertisements was branding: to convey to the viewer a positive feeling about\\n', 'Online edition (c)\\n2009 Cambridge UP\\n430\\n19\\nWeb search basics\\nthe brand of the company placing the advertisement. Typically these adver-\\ntisements are priced on a cost per mil (CPM) basis: the cost to the company of\\nCPM\\nhaving its banner advertisement displayed 1000 times. Some websites struck\\ncontracts with their advertisers in which an advertisement was priced not by\\nthe number of times it is displayed (also known as impressions), but rather\\nby the number of times it was clicked on by the user. This pricing model is\\nknown as the cost per click (CPC) model. In such cases, clicking on the adver-\\nCPC\\ntisement leads the user to a web page set up by the advertiser, where the user\\nis induced to make a purchase. Here the goal of the advertisement is not so\\nmuch brand promotion as to induce a transaction. This distinction between\\nbrand and transaction-oriented advertising was already widely recognized\\nin the context of conventional media such as broadcast and print. The inter-\\nactivity of the web allowed the CPC billing model – clicks could be metered\\nand monitored by the website and billed to the advertiser.\\nThe pioneer in this direction was a company named Goto, which changed\\nits name to Overture prior to eventual acquisition by Yahoo! Goto was not,\\nin the traditional sense, a search engine; rather, for every query term q it ac-\\ncepted bids from companies who wanted their web page shown on the query\\nq. In response to the query q, Goto would return the pages of all advertisers\\nwho bid for q, ordered by their bids. Furthermore, when the user clicked\\non one of the returned results, the corresponding advertiser would make a\\npayment to Goto (in the initial implementation, this payment equaled the\\nadvertiser’s bid for q).\\nSeveral aspects of Goto’s model are worth highlighting. First, a user typing\\nthe query q into Goto’s search interface was actively expressing an interest\\nand intent related to the query q. For instance, a user typing golf clubs is more\\nlikely to be imminently purchasing a set than one who is simply browsing\\nnews on golf. Second, Goto only got compensated when a user actually ex-\\npressed interest in an advertisement – as evinced by the user clicking the ad-\\nvertisement. Taken together, these created a powerful mechanism by which\\nto connect advertisers to consumers, quickly raising the annual revenues of\\nGoto/Overture into hundreds of millions of dollars. This style of search en-\\ngine came to be known variously as sponsored search or search advertising.\\nSPONSORED SEARCH\\nSEARCH ADVERTISING\\nGiven these two kinds of search engines – the “pure” search engines such\\nas Google and Altavista, versus the sponsored search engines – the logi-\\ncal next step was to combine them into a single user experience. Current\\nsearch engines follow precisely this model: they provide pure search results\\n(generally known as algorithmic search results) as the primary response to a\\nALGORITHMIC SEARCH\\nuser’s search, together with sponsored search results displayed separately\\nand distinctively to the right of the algorithmic results. This is shown in Fig-\\nure 19.6. Retrieving sponsored search results and ranking them in response\\nto a query has now become considerably more sophisticated than the sim-\\nple Goto scheme; the process entails a blending of ideas from information\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.3\\nAdvertising as the economic model\\n431\\n◮Figure 19.6\\nSearch advertising triggered by query keywords. Here the query A320\\nreturns algorithmic search results about the Airbus aircraft, together with advertise-\\nments for various non-aircraft goods numbered A320, that advertisers seek to market\\nto those querying on this query. The lack of advertisements for the aircraft reﬂects the\\nfact that few marketers attempt to sell A320 aircraft on the web.\\nretrieval and microeconomics, and is beyond the scope of this book. For\\nadvertisers, understanding how search engines do this ranking and how to\\nallocate marketing campaign budgets to different keywords and to different\\nsponsored search engines has become a profession known as search engine\\nSEARCH ENGINE\\nMARKETING\\nmarketing (SEM).\\nThe inherently economic motives underlying sponsored search give rise\\nto attempts by some participants to subvert the system to their advantage.\\nThis can take many forms, one of which is known as click spam. There is\\nCLICK SPAM\\ncurrently no universally accepted deﬁnition of click spam. It refers (as the\\nname suggests) to clicks on sponsored search results that are not from bona\\nﬁde search users. For instance, a devious advertiser may attempt to exhaust\\nthe advertising budget of a competitor by clicking repeatedly (through the\\nuse of a robotic click generator) on that competitor’s sponsored search ad-\\nvertisements. Search engines face the challenge of discerning which of the\\nclicks they observe are part of a pattern of click spam, to avoid charging their\\nadvertiser clients for such clicks.\\n?\\nExercise 19.5\\nThe Goto method ranked advertisements matching a query by bid: the highest-bidding\\nadvertiser got the top position, the second-highest the next, and so on. What can go\\nwrong with this when the highest-bidding advertiser places an advertisement that is\\nirrelevant to the query? Why might an advertiser with an irrelevant advertisement\\nbid high in this manner?\\nExercise 19.6\\nSuppose that, in addition to bids, we had for each advertiser their click-through rate:\\nthe ratio of the historical number of times users click on their advertisement to the\\nnumber of times the advertisement was shown. Suggest a modiﬁcation of the Goto\\nscheme that exploits this data to avoid the problem in Exercise 19.5 above.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n432\\n19\\nWeb search basics\\n19.4\\nThe search user experience\\nIt is crucial that we understand the users of web search as well. This is\\nagain a signiﬁcant change from traditional information retrieval, where users\\nwere typically professionals with at least some training in the art of phrasing\\nqueries over a well-authored collection whose style and structure they un-\\nderstood well. In contrast, web search users tend to not know (or care) about\\nthe heterogeneity of web content, the syntax of query languages and the art\\nof phrasing queries; indeed, a mainstream tool (as web search has come to\\nbecome) should not place such onerous demands on billions of people. A\\nrange of studies has concluded that the average number of keywords in a\\nweb search is somewhere between 2 and 3. Syntax operators (Boolean con-\\nnectives, wildcards, etc.) are seldom used, again a result of the composition\\nof the audience – “normal” people, not information scientists.\\nIt is clear that the more user trafﬁc a web search engine can attract, the\\nmore revenue it stands to earn from sponsored search. How do search en-\\ngines differentiate themselves and grow their trafﬁc? Here Google identiﬁed\\ntwo principles that helped it grow at the expense of its competitors: (1) a\\nfocus on relevance, speciﬁcally precision rather than recall in the ﬁrst few re-\\nsults; (2) a user experience that is lightweight, meaning that both the search\\nquery page and the search results page are uncluttered and almost entirely\\ntextual, with very few graphical elements. The effect of the ﬁrst was simply\\nto save users time in locating the information they sought. The effect of the\\nsecond is to provide a user experience that is extremely responsive, or at any\\nrate not bottlenecked by the time to load the search query or results page.\\n19.4.1\\nUser query needs\\nThere appear to be three broad categories into which common web search\\nqueries can be grouped: (i) informational, (ii) navigational and (iii) transac-\\ntional. We now explain these categories; it should be clear that some queries\\nwill fall in more than one of these categories, while others will fall outside\\nthem.\\nInformational queries seek general information on a broad topic, such as\\nINFORMATIONAL\\nQUERIES\\nleukemia or Provence. There is typically not a single web page that con-\\ntains all the information sought; indeed, users with informational queries\\ntypically try to assimilate information from multiple web pages.\\nNavigational queries seek the website or home page of a single entity that the\\nNAVIGATIONAL\\nQUERIES\\nuser has in mind, say Lufthansa airlines. In such cases, the user’s expectation\\nis that the very ﬁrst search result should be the home page of Lufthansa.\\nThe user is not interested in a plethora of documents containing the term\\nLufthansa; for such a user, the best measure of user satisfaction is precision at\\n1.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.5\\nIndex size and estimation\\n433\\nA transactional query is one that is a prelude to the user performing a trans-\\nTRANSACTIONAL\\nQUERY\\naction on the Web – such as purchasing a product, downloading a ﬁle or\\nmaking a reservation. In such cases, the search engine should return results\\nlisting services that provide form interfaces for such transactions.\\nDiscerning which of these categories a query falls into can be challeng-\\ning. The category not only governs the algorithmic search results, but the\\nsuitability of the query for sponsored search results (since the query may re-\\nveal an intent to purchase). For navigational queries, some have argued that\\nthe search engine should return only a single result or even the target web\\npage directly. Nevertheless, web search engines have historically engaged in\\na battle of bragging rights over which one indexes more web pages. Does\\nthe user really care? Perhaps not, but the media does highlight estimates\\n(often statistically indefensible) of the sizes of various search engines. Users\\nare inﬂuenced by these reports and thus, search engines do have to pay at-\\ntention to how their index sizes compare to competitors’. For informational\\n(and to a lesser extent, transactional) queries, the user does care about the\\ncomprehensiveness of the search engine.\\nFigure 19.7 shows a composite picture of a web search engine including\\nthe crawler, as well as both the web page and advertisement indexes. The\\nportion of the ﬁgure under the curved dashed line is internal to the search\\nengine.\\n19.5\\nIndex size and estimation\\nTo a ﬁrst approximation, comprehensiveness grows with index size, although\\nit does matter which speciﬁc pages a search engine indexes – some pages are\\nmore informative than others. It is also difﬁcult to reason about the fraction\\nof the Web indexed by a search engine, because there is an inﬁnite number of\\ndynamic web pages; for instance, http://www.yahoo.com/any_string\\nreturns a valid HTML page rather than an error, politely informing the user\\nthat there is no such page at Yahoo! Such a \"soft 404 error\" is only one exam-\\nple of many ways in which web servers can generate an inﬁnite number of\\nvalid web pages. Indeed, some of these are malicious spider traps devised\\nto cause a search engine’s crawler (the component that systematically gath-\\ners web pages for the search engine’s index, described in Chapter 20) to stay\\nwithin a spammer’s website and index many pages from that site.\\nWe could ask the following better-deﬁned question: given two search en-\\ngines, what are the relative sizes of their indexes? Even this question turns\\nout to be imprecise, because:\\n1. In response to queries a search engine can return web pages whose con-\\ntents it has not (fully or even partially) indexed. For one thing, search\\nengines generally index only the ﬁrst few thousand words in a web page.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n434\\n19\\nWeb search basics\\nT\\nh\\ne\\nW\\ne\\nb\\nA\\nd\\ni\\nn\\nd\\ne\\nx\\ne\\ns\\nW\\ne\\nb\\nc\\nr\\na\\nw\\nl\\ne\\nr\\nI\\nn\\nd\\ne\\nx\\ne\\nr\\nI\\nn\\nd\\ne\\nx\\ne\\ns\\nS\\ne\\na\\nr\\nc\\nh\\nU\\ns\\ne\\nr\\n◮Figure 19.7\\nThe various components of a web search engine.\\nIn some cases, a search engine is aware of a page p that is linked to by pages\\nit has indexed, but has not indexed p itself. As we will see in Chapter 21,\\nit is still possible to meaningfully return p in search results.\\n2. Search engines generally organize their indexes in various tiers and parti-\\ntions, not all of which are examined on every search (recall tiered indexes\\nfrom Section 7.2.1). For instance, a web page deep inside a website may be\\nindexed but not retrieved on general web searches; it is however retrieved\\nas a result on a search that a user has explicitly restricted to that website\\n(such site-speciﬁc search is offered by most web search engines).\\nThus, search engine indexes include multiple classes of indexed pages, so\\nthat there is no single measure of index size. These issues notwithstanding,\\na number of techniques have been devised for crude estimates of the ratio of\\nthe index sizes of two search engines, E1 and E2. The basic hypothesis under-\\nlying these techniques is that each search engine indexes a fraction of the Web\\nchosen independently and uniformly at random. This involves some ques-\\ntionable assumptions: ﬁrst, that there is a ﬁnite size for the Web from which\\neach search engine chooses a subset, and second, that each engine chooses\\nan independent, uniformly chosen subset. As will be clear from the discus-\\nsion of crawling in Chapter 20, this is far from true. However, if we begin\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.5\\nIndex size and estimation\\n435\\nwith these assumptions, then we can invoke a classical estimation technique\\nknown as the capture-recapture method.\\nCAPTURE-RECAPTURE\\nMETHOD\\nSuppose that we could pick a random page from the index of E1 and test\\nwhether it is in E2’s index and symmetrically, test whether a random page\\nfrom E2 is in E1. These experiments give us fractions x and y such that our\\nestimate is that a fraction x of the pages in E1 are in E2, while a fraction y of\\nthe pages in E2 are in E1. Then, letting |Ei| denote the size of the index of\\nsearch engine Ei, we have\\nx|E1| ≈y|E2|,\\nfrom which we have the form we will use\\n|E1|\\n|E2| ≈y\\nx .\\n(19.1)\\nIf our assumption about E1 and E2 being independent and uniform random\\nsubsets of the Web were true, and our sampling process unbiased, then Equa-\\ntion (19.1) should give us an unbiased estimator for |E1|/|E2|. We distinguish\\nbetween two scenarios here. Either the measurement is performed by some-\\none with access to the index of one of the search engines (say an employee of\\nE1), or the measurement is performed by an independent party with no ac-\\ncess to the innards of either search engine. In the former case, we can simply\\npick a random document from one index. The latter case is more challeng-\\ning; by picking a random page from one search engine from outside the search\\nengine, then verify whether the random page is present in the other search\\nengine.\\nTo implement the sampling phase, we might generate a random page from\\nthe entire (idealized, ﬁnite) Web and test it for presence in each search engine.\\nUnfortunately, picking a web page uniformly at random is a difﬁcult prob-\\nlem. We brieﬂy outline several attempts to achieve such a sample, pointing\\nout the biases inherent to each; following this we describe in some detail one\\ntechnique that much research has built on.\\n1. Random searches: Begin with a search log of web searches; send a random\\nsearch from this log to E1 and a random page from the results. Since such\\nlogs are not widely available outside a search engine, one implementation\\nis to trap all search queries going out of a work group (say scientists in a\\nresearch center) that agrees to have all its searches logged. This approach\\nhas a number of issues, including the bias from the types of searches made\\nby the work group. Further, a random document from the results of such\\na random search to E1 is not the same as a random document from E1.\\n2. Random IP addresses: A second approach is to generate random IP ad-\\ndresses and send a request to a web server residing at the random ad-\\ndress, collecting all pages at that server. The biases here include the fact\\n', 'Online edition (c)\\n2009 Cambridge UP\\n436\\n19\\nWeb search basics\\nthat many hosts might share one IP (due to a practice known as virtual\\nhosting) or not accept http requests from the host where the experiment\\nis conducted. Furthermore, this technique is more likely to hit one of the\\nmany sites with few pages, skewing the document probabilities; we may\\nbe able to correct for this effect if we understand the distribution of the\\nnumber of pages on websites.\\n3. Random walks: If the web graph were a strongly connected directed graph,\\nwe could run a random walk starting at an arbitrary web page. This\\nwalk would converge to a steady state distribution (see Chapter 21, Sec-\\ntion 21.2.1 for more background material on this), from which we could in\\nprinciple pick a web page with a ﬁxed probability. This method, too has\\na number of biases. First, the Web is not strongly connected so that, even\\nwith various corrective rules, it is difﬁcult to argue that we can reach a\\nsteady state distribution starting from any page. Second, the time it takes\\nfor the random walk to settle into this steady state is unknown and could\\nexceed the length of the experiment.\\nClearly each of these approaches is far from perfect. We now describe a\\nfourth sampling approach, random queries. This approach is noteworthy for\\ntwo reasons: it has been successfully built upon for a series of increasingly\\nreﬁned estimates, and conversely it has turned out to be the approach most\\nlikely to be misinterpreted and carelessly implemented, leading to mislead-\\ning measurements. The idea is to pick a page (almost) uniformly at random\\nfrom a search engine’s index by posing a random query to it. It should be\\nclear that picking a set of random terms from (say) Webster’s dictionary is\\nnot a good way of implementing this idea. For one thing, not all vocabulary\\nterms occur equally often, so this approach will not result in documents be-\\ning chosen uniformly at random from the search engine. For another, there\\nare a great many terms in web documents that do not occur in a standard\\ndictionary such as Webster’s. To address the problem of vocabulary terms\\nnot in a standard dictionary, we begin by amassing a sample web dictionary.\\nThis could be done by crawling a limited portion of the Web, or by crawling a\\nmanually-assembled representative subset of the Web such as Yahoo! (as was\\ndone in the earliest experiments with this method). Consider a conjunctive\\nquery with two or more randomly chosen words from this dictionary.\\nOperationally, we proceed as follows: we use a random conjunctive query\\non E1 and pick from the top 100 returned results a page p at random. We\\nthen test p for presence in E2 by choosing 6-8 low-frequency terms in p and\\nusing them in a conjunctive query for E2. We can improve the estimate by\\nrepeating the experiment a large number of times. Both the sampling process\\nand the testing process have a number of issues.\\n1. Our sample is biased towards longer documents.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.6\\nNear-duplicates and shingling\\n437\\n2. Picking from the top 100 results of E1 induces a bias from the ranking\\nalgorithm of E1. Picking from all the results of E1 makes the experiment\\nslower. This is particularly so because most web search engines put up\\ndefenses against excessive robotic querying.\\n3. During the checking phase, a number of additional biases are introduced:\\nfor instance, E2 may not handle 8-word conjunctive queries properly.\\n4. Either E1 or E2 may refuse to respond to the test queries, treating them as\\nrobotic spam rather than as bona ﬁde queries.\\n5. There could be operational problems like connection time-outs.\\nA sequence of research has built on this basic paradigm to eliminate some\\nof these issues; there is no perfect solution yet, but the level of sophistica-\\ntion in statistics for understanding the biases is increasing. The main idea\\nis to address biases by estimating, for each document, the magnitude of the\\nbias. From this, standard statistical sampling methods can generate unbi-\\nased samples. In the checking phase, the newer work moves away from\\nconjunctive queries to phrase and other queries that appear to be better-\\nbehaved. Finally, newer experiments use other sampling methods besides\\nrandom queries. The best known of these is document random walk sampling,\\nin which a document is chosen by a random walk on a virtual graph de-\\nrived from documents. In this graph, nodes are documents; two documents\\nare connected by an edge if they share two or more words in common. The\\ngraph is never instantiated; rather, a random walk on it can be performed by\\nmoving from a document d to another by picking a pair of keywords in d,\\nrunning a query on a search engine and picking a random document from\\nthe results. Details may be found in the references in Section 19.7.\\n?\\nExercise 19.7\\nTwo web search engines A and B each generate a large number of pages uniformly at\\nrandom from their indexes. 30% of A’s pages are present in B’s index, while 50% of\\nB’s pages are present in A’s index. What is the number of pages in A’s index relative\\nto B’s?\\n19.6\\nNear-duplicates and shingling\\nOne aspect we have ignored in the discussion of index size in Section 19.5 is\\nduplication: the Web contains multiple copies of the same content. By some\\nestimates, as many as 40% of the pages on the Web are duplicates of other\\npages. Many of these are legitimate copies; for instance, certain information\\nrepositories are mirrored simply to provide redundancy and access reliabil-\\nity. Search engines try to avoid indexing multiple copies of the same content,\\nto keep down storage and processing overheads.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n438\\n19\\nWeb search basics\\nThe simplest approach to detecting duplicates is to compute, for each web\\npage, a ﬁngerprint that is a succinct (say 64-bit) digest of the characters on that\\npage. Then, whenever the ﬁngerprints of two web pages are equal, we test\\nwhether the pages themselves are equal and if so declare one of them to be a\\nduplicate copy of the other. This simplistic approach fails to capture a crucial\\nand widespread phenomenon on the Web: near duplication. In many cases,\\nthe contents of one web page are identical to those of another except for a\\nfew characters – say, a notation showing the date and time at which the page\\nwas last modiﬁed. Even in such cases, we want to be able to declare the two\\npages to be close enough that we only index one copy. Short of exhaustively\\ncomparing all pairs of web pages, an infeasible task at the scale of billions of\\npages, how can we detect and ﬁlter out such near duplicates?\\nWe now describe a solution to the problem of detecting near-duplicate web\\npages. The answer lies in a technique known as shingling. Given a positive\\nSHINGLING\\ninteger k and a sequence of terms in a document d, deﬁne the k-shingles of\\nd to be the set of all consecutive sequences of k terms in d. As an example,\\nconsider the following text: a rose is a rose is a rose. The 4-shingles for this text\\n(k = 4 is a typical value used in the detection of near-duplicate web pages)\\nare a rose is a, rose is a rose and is a rose is. The ﬁrst two of these shingles\\neach occur twice in the text. Intuitively, two documents are near duplicates if\\nthe sets of shingles generated from them are nearly the same. We now make\\nthis intuition precise, then develop a method for efﬁciently computing and\\ncomparing the sets of shingles for all web pages.\\nLet S(dj) denote the set of shingles of document dj. Recall the Jaccard\\ncoefﬁcient from page 61, which measures the degree of overlap between\\nthe sets S(d1) and S(d2) as |S(d1) ∩S(d2)|/|S(d1) ∪S(d2)|; denote this by\\nJ(S(d1), S(d2)). Our test for near duplication between d1 and d2 is to com-\\npute this Jaccard coefﬁcient; if it exceeds a preset threshold (say, 0.9), we\\ndeclare them near duplicates and eliminate one from indexing. However,\\nthis does not appear to have simpliﬁed matters: we still have to compute\\nJaccard coefﬁcients pairwise.\\nTo avoid this, we use a form of hashing. First, we map every shingle into\\na hash value over a large space, say 64 bits. For j = 1, 2, let H(dj) be the\\ncorresponding set of 64-bit hash values derived from S(dj). We now invoke\\nthe following trick to detect document pairs whose sets H() have large Jac-\\ncard overlaps. Let π be a random permutation from the 64-bit integers to the\\n64-bit integers. Denote by Π(dj) the set of permuted hash values in H(dj);\\nthus for each h ∈H(dj), there is a corresponding value π(h) ∈Π(dj).\\nLet xπ\\nj be the smallest integer in Π(dj). Then\\nTheorem 19.1.\\nJ(S(d1), S(d2)) = P(xπ\\n1 = xπ\\n2 ).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.6\\nNear-duplicates and shingling\\n439\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\n264 −1\\nDocument 1\\nDocument 2\\nH(d1)\\nH(d2)\\nu\\n1\\nu\\n1\\nu\\n2\\nu\\n2\\nu\\n3\\nu\\n3\\nu\\n4\\nu\\n4\\nH(d1) and Π(d1)\\nH(d2) and Π(d2)\\nu\\nu\\nu\\nu\\nu\\nu\\nu\\nu\\n3\\n3\\n1\\n1\\n4\\n4\\n2\\n2\\n3\\n3\\n1\\n1\\n4\\n4\\n2\\n2\\n3\\n3\\nΠ(d1)\\nΠ(d2)\\nxπ\\n1\\nxπ\\n2\\n◮Figure 19.8\\nIllustration of shingle sketches. We see two documents going through\\nfour stages of shingle sketch computation. In the ﬁrst step (top row), we apply a 64-bit\\nhash to each shingle from each document to obtain H(d1) and H(d2) (circles). Next,\\nwe apply a random permutation Π to permute H(d1) and H(d2), obtaining Π(d1)\\nand Π(d2) (squares). The third row shows only Π(d1) and Π(d2), while the bottom\\nrow shows the minimum values xπ\\n1 and xπ\\n2 for each document.\\nProof. We give the proof in a slightly more general setting: consider a family\\nof sets whose elements are drawn from a common universe. View the sets\\nas columns of a matrix A, with one row for each element in the universe.\\nThe element aij = 1 if element i is present in the set Sj that the jth column\\nrepresents.\\nLet Π be a random permutation of the rows of A; denote by Π(Sj) the\\ncolumn that results from applying Π to the jth column. Finally, let xπ\\nj be the\\nindex of the ﬁrst row in which the column Π(Sj) has a 1. We then prove that\\nfor any two columns j1, j2,\\nP(xπ\\nj1 = xπ\\nj2) = J(Sj1, Sj2).\\nIf we can prove this, the theorem follows.\\nConsider two columns j1, j2 as shown in Figure 19.9. The ordered pairs of\\nentries of Sj1 and Sj2 partition the rows into four types: those with 0’s in both\\nof these columns, those with a 0 in Sj1 and a 1 in Sj2, those with a 1 in Sj1\\nand a 0 in Sj2, and ﬁnally those with 1’s in both of these columns. Indeed,\\nthe ﬁrst four rows of Figure 19.9 exemplify all of these four types of rows.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n440\\n19\\nWeb search basics\\nSj1\\nSj2\\n0\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n1\\n◮Figure 19.9\\nTwo sets Sj1 and Sj2; their Jaccard coefﬁcient is 2/5.\\nDenote by C00 the number of rows with 0’s in both columns, C01 the second,\\nC10 the third and C11 the fourth. Then,\\nJ(Sj1, Sj2) =\\nC11\\nC01 + C10 + C11\\n.\\n(19.2)\\nTo complete the proof by showing that the right-hand side of Equation (19.2)\\nequals P(xπ\\nj1 = xπ\\nj2), consider scanning columns j1, j2 in increasing row in-\\ndex until the ﬁrst non-zero entry is found in either column. Because Π is a\\nrandom permutation, the probability that this smallest row has a 1 in both\\ncolumns is exactly the right-hand side of Equation (19.2).\\nThus, our test for the Jaccard coefﬁcient of the shingle sets is probabilis-\\ntic: we compare the computed values xπ\\ni from different documents. If a pair\\ncoincides, we have candidate near duplicates. Repeat the process indepen-\\ndently for 200 random permutations π (a choice suggested in the literature).\\nCall the set of the 200 resulting values of xπ\\ni the sketch ψ(di) of di. We can\\nthen estimate the Jaccard coefﬁcient for any pair of documents di, dj to be\\n|ψi ∩ψj|/200; if this exceeds a preset threshold, we declare that di and dj are\\nsimilar.\\nHow can we quickly compute |ψi ∩ψj|/200 for all pairs i, j? Indeed, how\\ndo we represent all pairs of documents that are similar, without incurring\\na blowup that is quadratic in the number of documents? First, we use ﬁn-\\ngerprints to remove all but one copy of identical documents. We may also\\nremove common HTML tags and integers from the shingle computation, to\\neliminate shingles that occur very commonly in documents without telling\\nus anything about duplication. Next we use a union-ﬁnd algorithm to create\\nclusters that contain documents that are similar. To do this, we must accom-\\nplish a crucial step: going from the set of sketches to the set of pairs i, j such\\nthat di and dj are similar.\\nTo this end, we compute the number of shingles in common for any pair of\\ndocuments whose sketches have any members in common. We begin with\\nthe list < xπ\\ni , di > sorted by xπ\\ni pairs. For each xπ\\ni , we can now generate\\n', 'Online edition (c)\\n2009 Cambridge UP\\n19.7\\nReferences and further reading\\n441\\nall pairs i, j for which xπ\\ni is present in both their sketches. From these we\\ncan compute, for each pair i, j with non-zero sketch overlap, a count of the\\nnumber of xπ\\ni values they have in common. By applying a preset threshold,\\nwe know which pairs i, j have heavily overlapping sketches. For instance, if\\nthe threshold were 80%, we would need the count to be at least 160 for any\\ni, j. As we identify such pairs, we run the union-ﬁnd to group documents\\ninto near-duplicate “syntactic clusters”. This is essentially a variant of the\\nsingle-link clustering algorithm introduced in Section 17.2 (page 382).\\nOne ﬁnal trick cuts down the space needed in the computation of |ψi ∩\\nψj|/200 for pairs i, j, which in principle could still demand space quadratic\\nin the number of documents. To remove from consideration those pairs i, j\\nwhose sketches have few shingles in common, we preprocess the sketch for\\neach document as follows: sort the xπ\\ni in the sketch, then shingle this sorted\\nsequence to generate a set of super-shingles for each document. If two docu-\\nments have a super-shingle in common, we proceed to compute the precise\\nvalue of |ψi ∩ψj|/200. This again is a heuristic but can be highly effective\\nin cutting down the number of i, j pairs for which we accumulate the sketch\\noverlap counts.\\n?\\nExercise 19.8\\nWeb search engines A and B each crawl a random subset of the same size of the Web.\\nSome of the pages crawled are duplicates – exact textual copies of each other at dif-\\nferent URLs. Assume that duplicates are distributed uniformly amongst the pages\\ncrawled by A and B. Further, assume that a duplicate is a page that has exactly two\\ncopies – no pages have more than two copies. A indexes pages without duplicate\\nelimination whereas B indexes only one copy of each duplicate page. The two ran-\\ndom subsets have the same size before duplicate elimination. If, 45% of A’s indexed\\nURLs are present in B’s index, while 50% of B’s indexed URLs are present in A’s\\nindex, what fraction of the Web consists of pages that do not have a duplicate?\\nExercise 19.9\\nInstead of using the process depicted in Figure 19.8, consider instead the following\\nprocess for estimating the Jaccard coefﬁcient of the overlap between two sets S1 and\\nS2. We pick a random subset of the elements of the universe from which S1 and S2\\nare drawn; this corresponds to picking a random subset of the rows of the matrix A in\\nthe proof. We exhaustively compute the Jaccard coefﬁcient of these random subsets.\\nWhy is this estimate an unbiased estimator of the Jaccard coefﬁcient for S1 and S2?\\nExercise 19.10\\nExplain why this estimator would be very difﬁcult to use in practice.\\n19.7\\nReferences and further reading\\nBush (1945) foreshadowed the Web when he described an information man-\\nagement system that he called memex. Berners-Lee et al. (1992) describes\\none of the earliest incarnations of the Web. Kumar et al. (2000) and Broder\\n', 'Online edition (c)\\n2009 Cambridge UP\\n442\\n19\\nWeb search basics\\net al. (2000) provide comprehensive studies of the Web as a graph. The use\\nof anchor text was ﬁrst described in McBryan (1994). The taxonomy of web\\nqueries in Section 19.4 is due to Broder (2002). The observation of the power\\nlaw with exponent 2.1 in Section 19.2.1 appeared in Kumar et al. (1999).\\nChakrabarti (2002) is a good reference for many aspects of web search and\\nanalysis.\\nThe estimation of web search index sizes has a long history of develop-\\nment covered by Bharat and Broder (1998), Lawrence and Giles (1998), Rus-\\nmevichientong et al. (2001), Lawrence and Giles (1999), Henzinger et al. (2000),\\nBar-Yossef and Gurevich (2006). The state of the art is Bar-Yossef and Gure-\\nvich (2006), including several of the bias-removal techniques mentioned at\\nthe end of Section 19.5. Shingling was introduced by Broder et al. (1997) and\\nused for detecting websites (rather than simply pages) that are identical by\\nBharat et al. (2000).\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n443\\n20\\nWeb crawling and indexes\\n20.1\\nOverview\\nWeb crawling is the process by which we gather pages from the Web, in\\norder to index them and support a search engine. The objective of crawling\\nis to quickly and efﬁciently gather as many useful web pages as possible,\\ntogether with the link structure that interconnects them. In Chapter 19 we\\nstudied the complexities of the Web stemming from its creation by millions of\\nuncoordinated individuals. In this chapter we study the resulting difﬁculties\\nfor crawling the Web. The focus of this chapter is the component shown in\\nFigure 19.7 as web crawler; it is sometimes referred to as a spider.\\nWEB CRAWLER\\nSPIDER\\nThe goal of this chapter is not to describe how to build the crawler for\\na full-scale commercial web search engine. We focus instead on a range of\\nissues that are generic to crawling from the student project scale to substan-\\ntial research projects. We begin (Section 20.1.1) by listing desiderata for web\\ncrawlers, and then discuss in Section 20.2 how each of these issues is ad-\\ndressed. The remainder of this chapter describes the architecture and some\\nimplementation details for a distributed web crawler that satisﬁes these fea-\\ntures. Section 20.3 discusses distributing indexes across many machines for\\na web-scale implementation.\\n20.1.1\\nFeatures a crawler must provide\\nWe list the desiderata for web crawlers in two categories: features that web\\ncrawlers must provide, followed by features they should provide.\\nRobustness: The Web contains servers that create spider traps, which are gen-\\nerators of web pages that mislead crawlers into getting stuck fetching an\\ninﬁnite number of pages in a particular domain. Crawlers must be de-\\nsigned to be resilient to such traps. Not all such traps are malicious; some\\nare the inadvertent side-effect of faulty website development.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n444\\n20\\nWeb crawling and indexes\\nPoliteness: Web servers have both implicit and explicit policies regulating\\nthe rate at which a crawler can visit them. These politeness policies must\\nbe respected.\\n20.1.2\\nFeatures a crawler should provide\\nDistributed: The crawler should have the ability to execute in a distributed\\nfashion across multiple machines.\\nScalable: The crawler architecture should permit scaling up the crawl rate\\nby adding extra machines and bandwidth.\\nPerformance and efﬁciency: The crawl system should make efﬁcient use of\\nvarious system resources including processor, storage and network band-\\nwidth.\\nQuality: Given that a signiﬁcant fraction of all web pages are of poor util-\\nity for serving user query needs, the crawler should be biased towards\\nfetching “useful” pages ﬁrst.\\nFreshness: In many applications, the crawler should operate in continuous\\nmode: it should obtain fresh copies of previously fetched pages. A search\\nengine crawler, for instance, can thus ensure that the search engine’s index\\ncontains a fairly current representation of each indexed web page. For\\nsuch continuous crawling, a crawler should be able to crawl a page with\\na frequency that approximates the rate of change of that page.\\nExtensible: Crawlers should be designed to be extensible in many ways –\\nto cope with new data formats, new fetch protocols, and so on. This de-\\nmands that the crawler architecture be modular.\\n20.2\\nCrawling\\nThe basic operation of any hypertext crawler (whether for the Web, an in-\\ntranet or other hypertext document collection) is as follows. The crawler\\nbegins with one or more URLs that constitute a seed set. It picks a URL from\\nthis seed set, then fetches the web page at that URL. The fetched page is then\\nparsed, to extract both the text and the links from the page (each of which\\npoints to another URL). The extracted text is fed to a text indexer (described\\nin Chapters 4 and 5). The extracted links (URLs) are then added to a URL\\nfrontier, which at all times consists of URLs whose corresponding pages have\\nyet to be fetched by the crawler. Initially, the URL frontier contains the seed\\nset; as pages are fetched, the corresponding URLs are deleted from the URL\\nfrontier. The entire process may be viewed as traversing the web graph (see\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n445\\nChapter 19). In continuous crawling, the URL of a fetched page is added\\nback to the frontier for fetching again in the future.\\nThis seemingly simple recursive traversal of the web graph is complicated\\nby the many demands on a practical web crawling system: the crawler has to\\nbe distributed, scalable, efﬁcient, polite, robust and extensible while fetching\\npages of high quality. We examine the effects of each of these issues. Our\\ntreatment follows the design of the Mercator crawler that has formed the ba-\\nMERCATOR\\nsis of a number of research and commercial crawlers. As a reference point,\\nfetching a billion pages (a small fraction of the static Web at present) in a\\nmonth-long crawl requires fetching several hundred pages each second. We\\nwill see how to use a multi-threaded design to address several bottlenecks in\\nthe overall crawler system in order to attain this fetch rate.\\nBefore proceeding to this detailed description, we reiterate for readers who\\nmay attempt to build crawlers of some basic properties any non-professional\\ncrawler should satisfy:\\n1. Only one connection should be open to any given host at a time.\\n2. A waiting time of a few seconds should occur between successive requests\\nto a host.\\n3. Politeness restrictions detailed in Section 20.2.1 should be obeyed.\\n20.2.1\\nCrawler architecture\\nThe simple scheme outlined above for crawling demands several modules\\nthat ﬁt together as shown in Figure 20.1.\\n1. The URL frontier, containing URLs yet to be fetched in the current crawl\\n(in the case of continuous crawling, a URL may have been fetched previ-\\nously but is back in the frontier for re-fetching). We describe this further\\nin Section 20.2.3.\\n2. A DNS resolution module that determines the web server from which to\\nfetch the page speciﬁed by a URL. We describe this further in Section 20.2.2.\\n3. A fetch module that uses the http protocol to retrieve the web page at a\\nURL.\\n4. A parsing module that extracts the text and set of links from a fetched web\\npage.\\n5. A duplicate elimination module that determines whether an extracted\\nlink is already in the URL frontier or has recently been fetched.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n446\\n20\\nWeb crawling and indexes\\nwww\\nFetch\\nDNS\\nParse\\nURL Frontier\\nContent\\nSeen?\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nDoc\\nFP’s\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nrobots\\ntemplates\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nURL\\nset\\nURL\\nFilter\\nDup\\nURL\\nElim\\n-\\n\\x1b\\n-\\n6\\n\\x1b-\\n?\\n6\\n-\\n-\\n-\\n\\x1b\\n6\\n?\\n6\\n?\\n6\\n?\\n◮Figure 20.1\\nThe basic crawler architecture.\\nCrawling is performed by anywhere from one to potentially hundreds of\\nthreads, each of which loops through the logical cycle in Figure 20.1. These\\nthreads may be run in a single process, or be partitioned amongst multiple\\nprocesses running at different nodes of a distributed system. We begin by\\nassuming that the URL frontier is in place and non-empty and defer our de-\\nscription of the implementation of the URL frontier to Section 20.2.3. We\\nfollow the progress of a single URL through the cycle of being fetched, pass-\\ning through various checks and ﬁlters, then ﬁnally (for continuous crawling)\\nbeing returned to the URL frontier.\\nA crawler thread begins by taking a URL from the frontier and fetching\\nthe web page at that URL, generally using the http protocol. The fetched\\npage is then written into a temporary store, where a number of operations\\nare performed on it. Next, the page is parsed and the text as well as the\\nlinks in it are extracted. The text (with any tag information – e.g., terms in\\nboldface) is passed on to the indexer. Link information including anchor text\\nis also passed on to the indexer for use in ranking in ways that are described\\nin Chapter 21. In addition, each extracted link goes through a series of tests\\nto determine whether the link should be added to the URL frontier.\\nFirst, the thread tests whether a web page with the same content has al-\\nready been seen at another URL. The simplest implementation for this would\\nuse a simple ﬁngerprint such as a checksum (placed in a store labeled \"Doc\\nFP’s\" in Figure 20.1). A more sophisticated test would use shingles instead\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n447\\nof ﬁngerprints, as described in Chapter 19.\\nNext, a URL ﬁlter is used to determine whether the extracted URL should\\nbe excluded from the frontier based on one of several tests. For instance, the\\ncrawl may seek to exclude certain domains (say, all .com URLs) – in this case\\nthe test would simply ﬁlter out the URL if it were from the .com domain.\\nA similar test could be inclusive rather than exclusive. Many hosts on the\\nWeb place certain portions of their websites off-limits to crawling, under a\\nstandard known as the Robots Exclusion Protocol. This is done by placing a\\nROBOTS EXCLUSION\\nPROTOCOL\\nﬁle with the name robots.txt at the root of the URL hierarchy at the site. Here\\nis an example robots.txt ﬁle that speciﬁes that no robot should visit any URL\\nwhose position in the ﬁle hierarchy starts with /yoursite/temp/, except for the\\nrobot called “searchengine”.\\nUser-agent: *\\nDisallow: /yoursite/temp/\\nUser-agent: searchengine\\nDisallow:\\nThe robots.txt ﬁle must be fetched from a website in order to test whether\\nthe URL under consideration passes the robot restrictions, and can there-\\nfore be added to the URL frontier. Rather than fetch it afresh for testing on\\neach URL to be added to the frontier, a cache can be used to obtain a re-\\ncently fetched copy of the ﬁle for the host. This is especially important since\\nmany of the links extracted from a page fall within the host from which the\\npage was fetched and therefore can be tested against the host’s robots.txt\\nﬁle. Thus, by performing the ﬁltering during the link extraction process, we\\nwould have especially high locality in the stream of hosts that we need to test\\nfor robots.txt ﬁles, leading to high cache hit rates. Unfortunately, this runs\\nafoul of webmasters’ politeness expectations. A URL (particularly one refer-\\nring to a low-quality or rarely changing document) may be in the frontier for\\ndays or even weeks. If we were to perform the robots ﬁltering before adding\\nsuch a URL to the frontier, its robots.txt ﬁle could have changed by the time\\nthe URL is dequeued from the frontier and fetched. We must consequently\\nperform robots-ﬁltering immediately before attempting to fetch a web page.\\nAs it turns out, maintaining a cache of robots.txt ﬁles is still highly effective;\\nthere is sufﬁcient locality even in the stream of URLs dequeued from the URL\\nfrontier.\\nNext, a URL should be normalized in the following sense: often the HTML\\nURL NORMALIZATION\\nencoding of a link from a web page p indicates the target of that link relative\\nto the page p. Thus, there is a relative link encoded thus in the HTML of the\\npage en.wikipedia.org/wiki/Main_Page:\\n', 'Online edition (c)\\n2009 Cambridge UP\\n448\\n20\\nWeb crawling and indexes\\n<a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General\\ndisclaimer\">Disclaimers</a>\\npoints to the URL http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer.\\nFinally, the URL is checked for duplicate elimination: if the URL is already\\nin the frontier or (in the case of a non-continuous crawl) already crawled,\\nwe do not add it to the frontier. When the URL is added to the frontier, it is\\nassigned a priority based on which it is eventually removed from the frontier\\nfor fetching. The details of this priority queuing are in Section 20.2.3.\\nCertain housekeeping tasks are typically performed by a dedicated thread.\\nThis thread is generally quiescent except that it wakes up once every few\\nseconds to log crawl progress statistics (URLs crawled, frontier size, etc.),\\ndecide whether to terminate the crawl, or (once every few hours of crawling)\\ncheckpoint the crawl. In checkpointing, a snapshot of the crawler’s state (say,\\nthe URL frontier) is committed to disk. In the event of a catastrophic crawler\\nfailure, the crawl is restarted from the most recent checkpoint.\\nDistributing the crawler\\nWe have mentioned that the threads in a crawler could run under different\\nprocesses, each at a different node of a distributed crawling system. Such\\ndistribution is essential for scaling; it can also be of use in a geographically\\ndistributed crawler system where each node crawls hosts “near” it. Parti-\\ntioning the hosts being crawled amongst the crawler nodes can be done by\\na hash function, or by some more speciﬁcally tailored policy. For instance,\\nwe may locate a crawler node in Europe to focus on European domains, al-\\nthough this is not dependable for several reasons – the routes that packets\\ntake through the internet do not always reﬂect geographic proximity, and in\\nany case the domain of a host does not always reﬂect its physical location.\\nHow do the various nodes of a distributed crawler communicate and share\\nURLs? The idea is to replicate the ﬂow of Figure 20.1 at each node, with one\\nessential difference: following the URL ﬁlter, we use a host splitter to dispatch\\neach surviving URL to the crawler node responsible for the URL; thus the set\\nof hosts being crawled is partitioned among the nodes. This modiﬁed ﬂow is\\nshown in Figure 20.2. The output of the host splitter goes into the Duplicate\\nURL Eliminator block of each other node in the distributed system.\\nThe “Content Seen?” module in the distributed architecture of Figure 20.2\\nis, however, complicated by several factors:\\n1. Unlike the URL frontier and the duplicate elimination module, document\\nﬁngerprints/shingles cannot be partitioned based on host name. There is\\nnothing preventing the same (or highly similar) content from appearing\\non different web servers. Consequently, the set of ﬁngerprints/shingles\\nmust be partitioned across the nodes based on some property of the ﬁn-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n449\\nwww\\nFetch\\nDNS\\nParse\\nURL Frontier\\nContent\\nSeen?\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nDoc\\nFP’s\\n\\x13\\n\\x12\\n\\x10\\n\\x11\\n\\x12\\x11\\nURL\\nset\\nURL\\nFilter\\nHost\\nsplitter\\nTo\\nother\\nnodes\\nFrom\\nother\\nnodes\\nDup\\nURL\\nElim\\n-\\n\\x1b\\n-\\n6\\n\\x1b-\\n?\\n6\\n-\\n-\\n-\\n-\\n\\x1b\\n6\\n?\\n6\\n?\\n666\\n-\\n-\\n-\\n◮Figure 20.2\\nDistributing the basic crawl architecture.\\ngerprint/shingle (say by taking the ﬁngerprint modulo the number of\\nnodes). The result of this locality-mismatch is that most “Content Seen?”\\ntests result in a remote procedure call (although it is possible to batch\\nlookup requests).\\n2. There is very little locality in the stream of document ﬁngerprints/shingles.\\nThus, caching popular ﬁngerprints does not help (since there are no pop-\\nular ﬁngerprints).\\n3. Documents change over time and so, in the context of continuous crawl-\\ning, we must be able to delete their outdated ﬁngerprints/shingles from\\nthe content-seen set(s). In order to do so, it is necessary to save the ﬁnger-\\nprint/shingle of the document in the URL frontier, along with the URL\\nitself.\\n20.2.2\\nDNS resolution\\nEach web server (and indeed any host connected to the internet) has a unique\\nIP address: a sequence of four bytes generally represented as four integers\\nIP ADDRESS\\nseparated by dots; for instance 207.142.131.248is the numerical IP address as-\\nsociated with the host www.wikipedia.org. Given a URL such as www.wikipedia.org\\nin textual form, translating it to an IP address (in this case, 207.142.131.248)is\\n', 'Online edition (c)\\n2009 Cambridge UP\\n450\\n20\\nWeb crawling and indexes\\na process known as DNS resolution or DNS lookup; here DNS stands for Do-\\nDNS RESOLUTION\\nmain Name Service. During DNS resolution, the program that wishes to per-\\nform this translation (in our case, a component of the web crawler) contacts a\\nDNS server that returns the translated IP address. (In practice the entire trans-\\nDNS SERVER\\nlation may not occur at a single DNS server; rather, the DNS server contacted\\ninitially may recursively call upon other DNS servers to complete the transla-\\ntion.) For a more complex URL such as en.wikipedia.org/wiki/Domain_Name_System,\\nthe crawler component responsible for DNS resolution extracts the host name\\n– in this case en.wikipedia.org – and looks up the IP address for the host\\nen.wikipedia.org.\\nDNS resolution is a well-known bottleneck in web crawling. Due to the\\ndistributed nature of the Domain Name Service, DNS resolution may entail\\nmultiple requests and round-trips across the internet, requiring seconds and\\nsometimes even longer. Right away, this puts in jeopardy our goal of fetching\\nseveral hundred documents a second. A standard remedy is to introduce\\ncaching: URLs for which we have recently performed DNS lookups are likely\\nto be found in the DNS cache, avoiding the need to go to the DNS servers\\non the internet. However, obeying politeness constraints (see Section 20.2.3)\\nlimits the of cache hit rate.\\nThere is another important difﬁculty in DNS resolution; the lookup imple-\\nmentations in standard libraries (likely to be used by anyone developing a\\ncrawler) are generally synchronous. This means that once a request is made\\nto the Domain Name Service, other crawler threads at that node are blocked\\nuntil the ﬁrst request is completed. To circumvent this, most web crawlers\\nimplement their own DNS resolver as a component of the crawler. Thread\\ni executing the resolver code sends a message to the DNS server and then\\nperforms a timed wait: it resumes either when being signaled by another\\nthread or when a set time quantum expires. A single, separate DNS thread\\nlistens on the standard DNS port (port 53) for incoming response packets\\nfrom the name service. Upon receiving a response, it signals the appropriate\\ncrawler thread (in this case, i) and hands it the response packet if i has not\\nyet resumed because its time quantum has expired. A crawler thread that re-\\nsumes because its wait time quantum has expired retries for a ﬁxed number\\nof attempts, sending out a new message to the DNS server and performing\\na timed wait each time; the designers of Mercator recommend of the order\\nof ﬁve attempts. The time quantum of the wait increases exponentially with\\neach of these attempts; Mercator started with one second and ended with\\nroughly 90 seconds, in consideration of the fact that there are host names\\nthat take tens of seconds to resolve.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n451\\n20.2.3\\nThe URL frontier\\nThe URL frontier at a node is given a URL by its crawl process (or by the\\nhost splitter of another crawl process). It maintains the URLs in the frontier\\nand regurgitates them in some order whenever a crawler thread seeks a URL.\\nTwo important considerations govern the order in which URLs are returned\\nby the frontier. First, high-quality pages that change frequently should be\\nprioritized for frequent crawling. Thus, the priority of a page should be a\\nfunction of both its change rate and its quality (using some reasonable quality\\nestimate). The combination is necessary because a large number of spam\\npages change completely on every fetch.\\nThe second consideration is politeness: we must avoid repeated fetch re-\\nquests to a host within a short time span. The likelihood of this is exacerbated\\nbecause of a form of locality of reference: many URLs link to other URLs at\\nthe same host. As a result, a URL frontier implemented as a simple priority\\nqueue might result in a burst of fetch requests to a host. This might occur\\neven if we were to constrain the crawler so that at most one thread could\\nfetch from any single host at any time. A common heuristic is to insert a\\ngap between successive fetch requests to a host that is an order of magnitude\\nlarger than the time taken for the most recent fetch from that host.\\nFigure 20.3 shows a polite and prioritizing implementation of a URL fron-\\ntier. Its goals are to ensure that (i) only one connection is open at a time to any\\nhost; (ii) a waiting time of a few seconds occurs between successive requests\\nto a host and (iii) high-priority pages are crawled preferentially.\\nThe two major sub-modules are a set of F front queues in the upper por-\\ntion of the ﬁgure, and a set of B back queues in the lower part; all of these are\\nFIFO queues. The front queues implement the prioritization, while the back\\nqueues implement politeness. In the ﬂow of a URL added to the frontier as\\nit makes its way through the front and back queues, a prioritizer ﬁrst assigns\\nto the URL an integer priority i between 1 and F based on its fetch history\\n(taking into account the rate at which the web page at this URL has changed\\nbetween previous crawls). For instance, a document that has exhibited fre-\\nquent change would be assigned a higher priority. Other heuristics could be\\napplication-dependent and explicit – for instance, URLs from news services\\nmay always be assigned the highest priority. Now that it has been assigned\\npriority i, the URL is now appended to the ith of the front queues.\\nEach of the B back queues maintains the following invariants: (i) it is non-\\nempty while the crawl is in progress and (ii) it only contains URLs from a\\nsingle host1. An auxiliary table T (Figure 20.4) is used to maintain the map-\\nping from hosts to back queues. Whenever a back-queue is empty and is\\nbeing re-ﬁlled from a front-queue, table T must be updated accordingly.\\n1. The number of hosts is assumed to far exceed B.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n452\\n20\\nWeb crawling and indexes\\nBack queue\\nselector\\n-\\n\\x1b\\nBiased front queue selector\\nBack queue router\\nPrioritizer\\nr\\nr\\nr\\nr\\nB back queues\\nSingle host on each\\nr\\nr\\nr\\nr\\nr\\nF front queues\\n1\\n2\\nF\\n1\\n2\\nB\\n?\\nXXXXXXXXXXXX\\nz\\nXXXXXXXXXXXX\\nz\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n\\x18\\n9\\nXXXXXXXXXXXXXX\\nz\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n)\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n\\x10\\n)\\nPPPPPPPPPPP\\nq\\n?\\nHHHHHHHHHHH\\nj\\nHHHHHHHHHHH\\nj\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x08\\n\\x19\\n@\\n@@\\n\\x00\\x00\\x00 Heap\\n◮Figure 20.3\\nThe URL frontier. URLs extracted from already crawled pages ﬂow in\\nat the top of the ﬁgure. A crawl thread requesting a URL extracts it from the bottom of\\nthe ﬁgure. En route, a URL ﬂows through one of several front queues that manage its\\npriority for crawling, followed by one of several back queues that manage the crawler’s\\npoliteness.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.2\\nCrawling\\n453\\nHost\\nBack queue\\nstanford.edu\\n23\\nmicrosoft.com\\n47\\nacm.org\\n12\\n◮Figure 20.4\\nExample of an auxiliary hosts-to-back queues table.\\nIn addition, we maintain a heap with one entry for each back queue, the\\nentry being the earliest time te at which the host corresponding to that queue\\ncan be contacted again.\\nA crawler thread requesting a URL from the frontier extracts the root of\\nthis heap and (if necessary) waits until the corresponding time entry te. It\\nthen takes the URL u at the head of the back queue j corresponding to the\\nextracted heap root, and proceeds to fetch the URL u. After fetching u, the\\ncalling thread checks whether j is empty. If so, it picks a front queue and\\nextracts from its head a URL v. The choice of front queue is biased (usually\\nby a random process) towards queues of higher priority, ensuring that URLs\\nof high priority ﬂow more quickly into the back queues. We examine v to\\ncheck whether there is already a back queue holding URLs from its host.\\nIf so, v is added to that queue and we reach back to the front queues to\\nﬁnd another candidate URL for insertion into the now-empty queue j. This\\nprocess continues until j is non-empty again. In any case, the thread inserts\\na heap entry for j with a new earliest time te based on the properties of the\\nURL in j that was last fetched (such as when its host was last contacted as\\nwell as the time taken for the last fetch), then continues with its processing.\\nFor instance, the new entry te could be the current time plus ten times the\\nlast fetch time.\\nThe number of front queues, together with the policy of assigning priori-\\nties and picking queues, determines the priority properties we wish to build\\ninto the system. The number of back queues governs the extent to which we\\ncan keep all crawl threads busy while respecting politeness. The designers\\nof Mercator recommend a rough rule of three times as many back queues as\\ncrawler threads.\\nOn a Web-scale crawl, the URL frontier may grow to the point where it\\ndemands more memory at a node than is available. The solution is to let\\nmost of the URL frontier reside on disk. A portion of each queue is kept in\\nmemory, with more brought in from disk as it is drained in memory.\\n?\\nExercise 20.1\\nWhy is it better to partition hosts (rather than individual URLs) between the nodes of\\na distributed crawl system?\\nExercise 20.2\\nWhy should the host splitter precede the Duplicate URL Eliminator?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n454\\n20\\nWeb crawling and indexes\\nExercise 20.3\\n[⋆⋆⋆]\\nIn the preceding discussion we encountered two recommended “hard constants” –\\nthe increment on te being ten times the last fetch time, and the number of back\\nqueues being three times the number of crawl threads. How are these two constants\\nrelated?\\n20.3\\nDistributing indexes\\nIn Section 4.4 we described distributed indexing. We now consider the distri-\\nbution of the index across a large computer cluster2 that supports querying.\\nTwo obvious alternative index implementations suggest themselves: parti-\\nTERM PARTITIONING\\ntioning by terms, also known as global index organization, and partitioning by\\nDOCUMENT\\nPARTITIONING\\ndocuments, also know as local index organization. In the former, the diction-\\nary of index terms is partitioned into subsets, each subset residing at a node.\\nAlong with the terms at a node, we keep the postings for those terms. A\\nquery is routed to the nodes corresponding to its query terms. In principle,\\nthis allows greater concurrency since a stream of queries with different query\\nterms would hit different sets of machines.\\nIn practice, partitioning indexes by vocabulary terms turns out to be non-\\ntrivial. Multi-word queries require the sending of long postings lists between\\nsets of nodes for merging, and the cost of this can outweigh the greater con-\\ncurrency. Load balancing the partition is governed not by an a priori analysis\\nof relative term frequencies, but rather by the distribution of query terms\\nand their co-occurrences, which can drift with time or exhibit sudden bursts.\\nAchieving good partitions is a function of the co-occurrences of query terms\\nand entails the clustering of terms to optimize objectives that are not easy to\\nquantify. Finally, this strategy makes implementation of dynamic indexing\\nmore difﬁcult.\\nA more common implementation is to partition by documents: each node\\ncontains the index for a subset of all documents. Each query is distributed to\\nall nodes, with the results from various nodes being merged before presenta-\\ntion to the user. This strategy trades more local disk seeks for less inter-node\\ncommunication. One difﬁculty in this approach is that global statistics used\\nin scoring – such as idf – must be computed across the entire document col-\\nlection even though the index at any single node only contains a subset of\\nthe documents. These are computed by distributed “background” processes\\nthat periodically refresh the node indexes with fresh global statistics.\\nHow do we decide the partition of documents to nodes? Based on our de-\\nvelopment of the crawler architecture in Section 20.2.1, one simple approach\\nwould be to assign all pages from a host to a single node. This partitioning\\n2. Please note the different usage of “clusters” elsewhere in this book, in the sense of Chapters\\n16 and 17.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.4\\nConnectivity servers\\n455\\ncould follow the partitioning of hosts to crawler nodes. A danger of such\\npartitioning is that on many queries, a preponderance of the results would\\ncome from documents at a small number of hosts (and hence a small number\\nof index nodes).\\nA hash of each URL into the space of index nodes results in a more uni-\\nform distribution of query-time computation across nodes. At query time,\\nthe query is broadcast to each of the nodes, with the top k results from each\\nnode being merged to ﬁnd the top k documents for the query. A common\\nimplementation heuristic is to partition the document collection into indexes\\nof documents that are more likely to score highly on most queries (using,\\nfor instance, techniques in Chapter 21) and low-scoring indexes with the re-\\nmaining documents. We only search the low-scoring indexes when there are\\ntoo few matches in the high-scoring indexes, as described in Section 7.2.1.\\n20.4\\nConnectivity servers\\nFor reasons to become clearer in Chapter 21, web search engines require a\\nconnectivity server that supports fast connectivity queries on the web graph.\\nCONNECTIVITY SERVER\\nCONNECTIVITY\\nQUERIES\\nTypical connectivity queries are which URLs link to a given URL? and which\\nURLs does a given URL link to? To this end, we wish to store mappings in\\nmemory from URL to out-links, and from URL to in-links. Applications in-\\nclude crawl control, web graph analysis, sophisticated crawl optimization\\nand link analysis (to be covered in Chapter 21).\\nSuppose that the Web had four billion pages, each with ten links to other\\npages. In the simplest form, we would require 32 bits or 4 bytes to specify\\neach end (source and destination) of each link, requiring a total of\\n4 × 109 × 10 × 8 = 3.2 × 1011\\nbytes of memory. Some basic properties of the web graph can be exploited to\\nuse well under 10% of this memory requirement. At ﬁrst sight, we appear to\\nhave a data compression problem – which is amenable to a variety of stan-\\ndard solutions. However, our goal is not to simply compress the web graph\\nto ﬁt into memory; we must do so in a way that efﬁciently supports connec-\\ntivity queries; this challenge is reminiscent of index compression (Chapter 5).\\nWe assume that each web page is represented by a unique integer; the\\nspeciﬁc scheme used to assign these integers is described below. We build\\nan adjacency table that resembles an inverted index: it has a row for each web\\npage, with the rows ordered by the corresponding integers. The row for any\\npage p contains a sorted list of integers, each corresponding to a web page\\nthat links to p. This table permits us to respond to queries of the form which\\npages link to p? In similar fashion we build a table whose entries are the pages\\nlinked to by p.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n456\\n20\\nWeb crawling and indexes\\n1: www.stanford.edu/alchemy\\n2: www.stanford.edu/biology\\n3: www.stanford.edu/biology/plant\\n4: www.stanford.edu/biology/plant/copyright\\n5: www.stanford.edu/biology/plant/people\\n6: www.stanford.edu/chemistry\\n◮Figure 20.5\\nA lexicographically ordered set of URLs.\\nThis table representation cuts the space taken by the naive representation\\n(in which we explicitly represent each link by its two end points, each a 32-bit\\ninteger) by 50%. Our description below will focus on the table for the links\\nfrom each page; it should be clear that the techniques apply just as well to\\nthe table of links to each page. To further reduce the storage for the table, we\\nexploit several ideas:\\n1. Similarity between lists: Many rows of the table have many entries in\\ncommon.\\nThus, if we explicitly represent a prototype row for several\\nsimilar rows, the remainder can be succinctly expressed in terms of the\\nprototypical row.\\n2. Locality: many links from a page go to “nearby” pages – pages on the\\nsame host, for instance. This suggests that in encoding the destination of\\na link, we can often use small integers and thereby save space.\\n3. We use gap encodings in sorted lists: rather than store the destination of\\neach link, we store the offset from the previous entry in the row.\\nWe now develop each of these techniques.\\nIn a lexicographic ordering of all URLs, we treat each URL as an alphanu-\\nmeric string and sort these strings. Figure 20.5 shows a segment of this sorted\\norder. For a true lexicographic sort of web pages, the domain name part of\\nthe URL should be inverted, so that www.stanford.edu becomes edu.stanford.www,\\nbut this is not necessary here since we are mainly concerned with links local\\nto a single host.\\nTo each URL, we assign its position in this ordering as the unique identi-\\nfying integer. Figure 20.6 shows an example of such a numbering and the\\nresulting table. In this example sequence, www.stanford.edu/biology\\nis assigned the integer 2 since it is second in the sequence.\\nWe next exploit a property that stems from the way most websites are\\nstructured to get similarity and locality. Most websites have a template with\\na set of links from each page in the site to a ﬁxed set of pages on the site (such\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.4\\nConnectivity servers\\n457\\n1: 1, 2, 4, 8, 16, 32, 64\\n2: 1, 4, 9, 16, 25, 36, 49, 64\\n3: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\\n4: 1, 4, 8, 16, 25, 36, 49, 64\\n◮Figure 20.6\\nA four-row segment of the table of links.\\nas its copyright notice, terms of use, and so on). In this case, the rows cor-\\nresponding to pages in a website will have many table entries in common.\\nMoreover, under the lexicographic ordering of URLs, it is very likely that the\\npages from a website appear as contiguous rows in the table.\\nWe adopt the following strategy: we walk down the table, encoding each\\ntable row in terms of the seven preceding rows. In the example of Figure 20.6,\\nwe could encode the fourth row as “the same as the row at offset 2 (mean-\\ning, two rows earlier in the table), with 9 replaced by 8”. This requires the\\nspeciﬁcation of the offset, the integer(s) dropped (in this case 9) and the in-\\nteger(s) added (in this case 8). The use of only the seven preceding rows has\\ntwo advantages: (i) the offset can be expressed with only 3 bits; this choice\\nis optimized empirically (the reason for seven and not eight preceding rows\\nis the subject of Exercise 20.4) and (ii) ﬁxing the maximum offset to a small\\nvalue like seven avoids having to perform an expensive search among many\\ncandidate prototypes in terms of which to express the current row.\\nWhat if none of the preceding seven rows is a good prototype for express-\\ning the current row? This would happen, for instance, at each boundary\\nbetween different websites as we walk down the rows of the table. In this\\ncase we simply express the row as starting from the empty set and “adding\\nin” each integer in that row. By using gap encodings to store the gaps (rather\\nthan the actual integers) in each row, and encoding these gaps tightly based\\non the distribution of their values, we obtain further space reduction. In ex-\\nperiments mentioned in Section 20.5, the series of techniques outlined here\\nappears to use as few as 3 bits per link, on average – a dramatic reduction\\nfrom the 64 required in the naive representation.\\nWhile these ideas give us a representation of sizable web graphs that com-\\nfortably ﬁt in memory, we still need to support connectivity queries. What\\nis entailed in retrieving from this representation the set of links from a page?\\nFirst, we need an index lookup from (a hash of) the URL to its row number\\nin the table. Next, we need to reconstruct these entries, which may be en-\\ncoded in terms of entries in other rows. This entails following the offsets to\\nreconstruct these other rows – a process that in principle could lead through\\nmany levels of indirection. In practice however, this does not happen very\\noften. A heuristic for controlling this can be introduced into the construc-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n458\\n20\\nWeb crawling and indexes\\ntion of the table: when examining the preceding seven rows as candidates\\nfrom which to model the current row, we demand a threshold of similarity\\nbetween the current row and the candidate prototype. This threshold must\\nbe chosen with care. If the threshold is set too high, we seldom use proto-\\ntypes and express many rows afresh. If the threshold is too low, most rows\\nget expressed in terms of prototypes, so that at query time the reconstruction\\nof a row leads to many levels of indirection through preceding prototypes.\\n?\\nExercise 20.4\\nWe noted that expressing a row in terms of one of seven preceding rows allowed us\\nto use no more than three bits to specify which of the preceding rows we are using\\nas prototype. Why seven and not eight preceding rows? (Hint: consider the case when\\nnone of the preceding seven rows is a good prototype.)\\nExercise 20.5\\nWe noted that for the scheme in Section 20.4, decoding the links incident on a URL\\ncould result in many levels of indirection. Construct an example in which the number\\nof levels of indirection grows linearly with the number of URLs.\\n20.5\\nReferences and further reading\\nThe ﬁrst web crawler appears to be Matthew Gray’s Wanderer, written in the\\nspring of 1993. The Mercator crawler is due to Najork and Heydon (Najork\\nand Heydon 2001; 2002); the treatment in this chapter follows their work.\\nOther classic early descriptions of web crawling include Burner (1997), Brin\\nand Page (1998), Cho et al. (1998) and the creators of the Webbase system\\nat Stanford (Hirai et al. 2000). Cho and Garcia-Molina (2002) give a taxon-\\nomy and comparative study of different modes of communication between\\nthe nodes of a distributed crawler. The Robots Exclusion Protocol standard\\nis described at http://www.robotstxt.org/wc/exclusion.html. Boldi et al. (2002) and\\nShkapenyuk and Suel (2002) provide more recent details of implementing\\nlarge-scale distributed web crawlers.\\nOur discussion of DNS resolution (Section 20.2.2) uses the current conven-\\ntion for internet addresses, known as IPv4 (for Internet Protocol version 4) –\\neach IP address is a sequence of four bytes. In the future, the convention for\\naddresses (collectively known as the internet address space) is likely to use a\\nnew standard known as IPv6 (http://www.ipv6.org/).\\nTomasic and Garcia-Molina (1993) and Jeong and Omiecinski (1995) are\\nkey early papers evaluating term partitioning versus document partitioning\\nfor distributed indexes. Document partitioning is found to be superior, at\\nleast when the distribution of terms is skewed, as it typically is in practice.\\nThis result has generally been conﬁrmed in more recent work (MacFarlane\\net al. 2000). But the outcome depends on the details of the distributed system;\\n', 'Online edition (c)\\n2009 Cambridge UP\\n20.5\\nReferences and further reading\\n459\\nat least one thread of work has reached the opposite conclusion (Ribeiro-\\nNeto and Barbosa 1998, Badue et al. 2001).\\nSornil (2001) argues for a par-\\ntitioning scheme that is a hybrid between term and document partitioning.\\nBarroso et al. (2003) describe the distribution methods used at Google. The\\nﬁrst implementation of a connectivity server was described by Bharat et al.\\n(1998). The scheme discussed in this chapter, currently believed to be the\\nbest published scheme (achieving as few as 3 bits per link for encoding), is\\ndescribed in a series of papers by Boldi and Vigna (2004a;b).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n', 'Online edition (c)\\n2009 Cambridge UP\\nDRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\\n461\\n21\\nLink analysis\\nThe analysis of hyperlinks and the graph structure of the Web has been in-\\nstrumental in the development of web search. In this chapter we focus on the\\nuse of hyperlinks for ranking web search results. Such link analysis is one\\nof many factors considered by web search engines in computing a compos-\\nite score for a web page on any given query. We begin by reviewing some\\nbasics of the Web as a graph in Section 21.1, then proceed to the technical\\ndevelopment of the elements of link analysis for ranking.\\nLink analysis for web search has intellectual antecedents in the ﬁeld of cita-\\ntion analysis, aspects of which overlap with an area known as bibliometrics.\\nThese disciplines seek to quantify the inﬂuence of scholarly articles by ana-\\nlyzing the pattern of citations amongst them. Much as citations represent the\\nconferral of authority from a scholarly article to others, link analysis on the\\nWeb treats hyperlinks from a web page to another as a conferral of authority.\\nClearly, not every citation or hyperlink implies such authority conferral; for\\nthis reason, simply measuring the quality of a web page by the number of\\nin-links (citations from other pages) is not robust enough. For instance, one\\nmay contrive to set up multiple web pages pointing to a target web page,\\nwith the intent of artiﬁcially boosting the latter’s tally of in-links. This phe-\\nnomenon is referred to as link spam. Nevertheless, the phenomenon of ci-\\ntation is prevalent and dependable enough that it is feasible for web search\\nengines to derive useful signals for ranking from more sophisticated link\\nanalysis. Link analysis also proves to be a useful indicator of what page(s)\\nto crawl next while crawling the web; this is done by using link analysis to\\nguide the priority assignment in the front queues of Chapter 20.\\nSection 21.1 develops the basic ideas underlying the use of the web graph\\nin link analysis. Sections 21.2 and 21.3 then develop two distinct methods for\\nlink analysis, PageRank and HITS.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n462\\n21\\nLink analysis\\n21.1\\nThe Web as a graph\\nRecall the notion of the web graph from Section 19.2.1 and particularly Fig-\\nure 19.2. Our study of link analysis builds on two intuitions:\\n1. The anchor text pointing to page B is a good description of page B.\\n2. The hyperlink from A to B represents an endorsement of page B, by the\\ncreator of page A. This is not always the case; for instance, many links\\namongst pages within a single website stem from the user of a common\\ntemplate. For instance, most corporate websites have a pointer from ev-\\nery page to a page containing a copyright notice – this is clearly not an\\nendorsement. Accordingly, implementations of link analysis algorithms\\nwill typical discount such “internal” links.\\n21.1.1\\nAnchor text and the web graph\\nThe following fragment of HTML code from a web page shows a hyperlink\\npointing to the home page of the Journal of the ACM:\\n<a href=\"http://www.acm.org/jacm/\">Journal of the ACM.</a>\\nIn this case, the link points to the page http://www.acm.org/jacm/ and\\nthe anchor text is Journal of the ACM. Clearly, in this example the anchor is de-\\nscriptive of the target page. But then the target page (B = http://www.acm.org/jacm/)\\nitself contains the same description as well as considerable additional infor-\\nmation on the journal. So what use is the anchor text?\\nThe Web is full of instances where the page B does not provide an accu-\\nrate description of itself. In many cases this is a matter of how the publish-\\ners of page B choose to present themselves; this is especially common with\\ncorporate web pages, where a web presence is a marketing statement. For\\nexample, at the time of the writing of this book the home page of the IBM\\ncorporation (http://www.ibm.com)did not contain the term computer any-\\nwhere in its HTML code, despite the fact that IBM is widely viewed as the\\nworld’s largest computer maker. Similarly, the HTML code for the home\\npage of Yahoo! (http://www.yahoo.com) does not at this time contain the\\nword portal.\\nThus, there is often a gap between the terms in a web page, and how web\\nusers would describe that web page. Consequently, web searchers need not\\nuse the terms in a page to query for it. In addition, many web pages are rich\\nin graphics and images, and/or embed their text in these images; in such\\ncases, the HTML parsing performed when crawling will not extract text that\\nis useful for indexing these pages. The “standard IR” approach to this would\\nbe to use the methods outlined in Chapter 9 and Section 12.4. The insight\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.1\\nThe Web as a graph\\n463\\nbehind anchor text is that such methods can be supplanted by anchor text,\\nthereby tapping the power of the community of web page authors.\\nThe fact that the anchors of many hyperlinks pointing to http://www.ibm.com\\ninclude the word computer can be exploited by web search engines. For in-\\nstance, the anchor text terms can be included as terms under which to index\\nthe target web page. Thus, the postings for the term computer would include\\nthe document http://www.ibm.com and that for the term portal would in-\\nclude the document http://www.yahoo.com, using a special indicator to\\nshow that these terms occur as anchor (rather than in-page) text. As with\\nin-page terms, anchor text terms are generally weighted based on frequency,\\nwith a penalty for terms that occur very often (the most common terms in an-\\nchor text across the Web are Click and here, using methods very similar to idf).\\nThe actual weighting of terms is determined by machine-learned scoring, as\\nin Section 15.4.1; current web search engines appear to assign a substantial\\nweighting to anchor text terms.\\nThe use of anchor text has some interesting side-effects. Searching for big\\nblue on most web search engines returns the home page of the IBM corpora-\\ntion as the top hit; this is consistent with the popular nickname that many\\npeople use to refer to IBM. On the other hand, there have been (and con-\\ntinue to be) many instances where derogatory anchor text such as evil empire\\nleads to somewhat unexpected results on querying for these terms on web\\nsearch engines. This phenomenon has been exploited in orchestrated cam-\\npaigns against speciﬁc sites. Such orchestrated anchor text may be a form\\nof spamming, since a website can create misleading anchor text pointing to\\nitself, to boost its ranking on selected query terms. Detecting and combating\\nsuch systematic abuse of anchor text is another form of spam detection that\\nweb search engines perform.\\nThe window of text surrounding anchor text (sometimes referred to as ex-\\ntended anchor text) is often usable in the same manner as anchor text itself;\\nconsider for instance the fragment of web text there is good discussion\\nof vedic scripture <a>here</a>. This has been considered in a num-\\nber of settings and the useful width of this window has been studied; see\\nSection 21.4 for references.\\n?\\nExercise 21.1\\nIs it always possible to follow directed edges (hyperlinks) in the web graph from any\\nnode (web page) to any other? Why or why not?\\nExercise 21.2\\nFind an instance of misleading anchor-text on the Web.\\nExercise 21.3\\nGiven the collection of anchor-text phrases for a web page x, suggest a heuristic for\\nchoosing one term or phrase from this collection that is most descriptive of x.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n464\\n21\\nLink analysis\\n\\x12\\x11\\n\\x13\\x10\\nA\\n\\x12\\x11\\n\\x13\\x10\\nC\\n\\x12\\x11\\n\\x13\\x10\\nB\\n\\x12\\x11\\n\\x13\\x10\\nD\\n-\\n\\x00\\x00\\x12\\n@\\n@\\nR\\n◮Figure 21.1\\nThe random surfer at node A proceeds with probability 1/3 to each\\nof B, C and D.\\nExercise 21.4\\nDoes your heuristic in the previous exercise take into account a single domain D\\nrepeating anchor text for x from multiple pages in D?\\n21.2\\nPageRank\\nWe now focus on scoring and ranking measures derived from the link struc-\\nture alone. Our ﬁrst technique for link analysis assigns to every node in\\nthe web graph a numerical score between 0 and 1, known as its PageRank.\\nPAGERANK\\nThe PageRank of a node will depend on the link structure of the web graph.\\nGiven a query, a web search engine computes a composite score for each\\nweb page that combines hundreds of features such as cosine similarity (Sec-\\ntion 6.3) and term proximity (Section 7.2.2), together with the PageRank score.\\nThis composite score, developed using the methods of Section 15.4.1, is used\\nto provide a ranked list of results for the query.\\nConsider a random surfer who begins at a web page (a node of the web\\ngraph) and executes a random walk on the Web as follows. At each time\\nstep, the surfer proceeds from his current page A to a randomly chosen web\\npage that A hyperlinks to. Figure 21.1 shows the surfer at a node A, out of\\nwhich there are three hyperlinks to nodes B, C and D; the surfer proceeds at\\nthe next time step to one of these three nodes, with equal probabilities 1/3.\\nAs the surfer proceeds in this random walk from node to node, he visits\\nsome nodes more often than others; intuitively, these are nodes with many\\nlinks coming in from other frequently visited nodes. The idea behind Page-\\nRank is that pages visited more often in this walk are more important.\\nWhat if the current location of the surfer, the node A, has no out-links?\\nTo address this we introduce an additional operation for our random surfer:\\nthe teleport operation. In the teleport operation the surfer jumps from a node\\nTELEPORT\\nto any other node in the web graph. This could happen because he types\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n465\\nan address into the URL bar of his browser. The destination of a teleport\\noperation is modeled as being chosen uniformly at random from all web\\npages. In other words, if N is the total number of nodes in the web graph1,\\nthe teleport operation takes the surfer to each node with probability 1/N.\\nThe surfer would also teleport to his present position with probability 1/N.\\nIn assigning a PageRank score to each node of the web graph, we use the\\nteleport operation in two ways: (1) When at a node with no out-links, the\\nsurfer invokes the teleport operation. (2) At any node that has outgoing links,\\nthe surfer invokes the teleport operation with probability 0 < α < 1 and the\\nstandard random walk (follow an out-link chosen uniformly at random as in\\nFigure 21.1) with probability 1 −α, where α is a ﬁxed parameter chosen in\\nadvance. Typically, α might be 0.1.\\nIn Section 21.2.1, we will use the theory of Markov chains to argue that\\nwhen the surfer follows this combined process (random walk plus teleport)\\nhe visits each node v of the web graph a ﬁxed fraction of the time π(v) that\\ndepends on (1) the structure of the web graph and (2) the value of α. We call\\nthis value π(v) the PageRank of v and will show how to compute this value\\nin Section 21.2.2.\\n21.2.1\\nMarkov chains\\nA Markov chain is a discrete-time stochastic process: a process that occurs in\\na series of time-steps in each of which a random choice is made. A Markov\\nchain consists of N states. Each web page will correspond to a state in the\\nMarkov chain we will formulate.\\nA Markov chain is characterized by an N × N transition probability matrix P\\neach of whose entries is in the interval [0, 1]; the entries in each row of P add\\nup to 1. The Markov chain can be in one of the N states at any given time-\\nstep; then, the entry Pij tells us the probability that the state at the next time-\\nstep is j, conditioned on the current state being i. Each entry Pij is known as a\\ntransition probability and depends only on the current state i; this is known\\nas the Markov property. Thus, by the Markov property,\\n∀i, j, Pij ∈[0, 1]\\nand\\n∀i,\\nN\\n∑\\nj=1\\nPij = 1.\\n(21.1)\\nA matrix with non-negative entries that satisﬁes Equation (21.1) is known\\nas a stochastic matrix. A key property of a stochastic matrix is that it has a\\nSTOCHASTIC MATRIX\\nprincipal left eigenvector corresponding to its largest eigenvalue, which is 1.\\nPRINCIPAL LEFT\\nEIGENVECTOR\\n1. This is consistent with our usage of N for the number of documents in the collection.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n466\\n21\\nLink analysis\\n\\x16\\x15\\n\\x17\\x14\\nA\\n\\x16\\x15\\n\\x17\\x14\\nB\\n\\x16\\x15\\n\\x17\\x14\\nC\\n-\\n1\\n-\\n0.5\\n\\x1b\\n0.5\\n\\x1b\\n1\\n◮Figure 21.2\\nA simple Markov chain with three states; the numbers on the links\\nindicate the transition probabilities.\\nIn a Markov chain, the probability distribution of next states for a Markov\\nchain depends only on the current state, and not on how the Markov chain\\narrived at the current state. Figure 21.2 shows a simple Markov chain with\\nthree states. From the middle state A, we proceed with (equal) probabilities\\nof 0.5 to either B or C. From either B or C, we proceed with probability 1 to\\nA. The transition probability matrix of this Markov chain is then\\n\\uf8eb\\n\\uf8ed\\n0\\n0.5\\n0.5\\n1\\n0\\n0\\n1\\n0\\n0\\n\\uf8f6\\n\\uf8f8\\nA Markov chain’s probability distribution over its states may be viewed as\\na probability vector: a vector all of whose entries are in the interval [0, 1], and\\nPROBABILITY VECTOR\\nthe entries add up to 1. An N-dimensional probability vector each of whose\\ncomponents corresponds to one of the N states of a Markov chain can be\\nviewed as a probability distribution over its states. For our simple Markov\\nchain of Figure 21.2, the probability vector would have 3 components that\\nsum to 1.\\nWe can view a random surfer on the web graph as a Markov chain, with\\none state for each web page, and each transition probability representing the\\nprobability of moving from one web page to another. The teleport operation\\ncontributes to these transition probabilities. The adjacency matrix A of the\\nweb graph is deﬁned as follows: if there is a hyperlink from page i to page\\nj, then Aij = 1, otherwise Aij = 0. We can readily derive the transition\\nprobability matrix P for our Markov chain from the N × N matrix A:\\n1. If a row of A has no 1’s, then replace each element by 1/N. For all other\\nrows proceed as follows.\\n2. Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row\\nwith three 1’s, then each of them is replaced by 1/3.\\n3. Multiply the resulting matrix by 1 −α.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n467\\n4. Add α/N to every entry of the resulting matrix, to obtain P.\\nWe can depict the probability distribution of the surfer’s position at any\\ntime by a probability vector ⃗x. At t = 0 the surfer may begin at a state whose\\ncorresponding entry in ⃗x is 1 while all others are zero. By deﬁnition, the\\nsurfer’s distribution at t = 1 is given by the probability vector ⃗xP; at t = 2\\nby (⃗xP)P = ⃗xP2, and so on. We will detail this process in Section 21.2.2. We\\ncan thus compute the surfer’s distribution over the states at any time, given\\nonly the initial distribution and the transition probability matrix P.\\nIf a Markov chain is allowed to run for many time steps, each state is vis-\\nited at a (different) frequency that depends on the structure of the Markov\\nchain. In our running analogy, the surfer visits certain web pages (say, pop-\\nular news home pages) more often than other pages. We now make this in-\\ntuition precise, establishing conditions under which such the visit frequency\\nconverges to ﬁxed, steady-state quantity. Following this, we set the Page-\\nRank of each node v to this steady-state visit frequency and show how it can\\nbe computed.\\nDeﬁnition:\\nA Markov chain is said to be ergodic if there exists a positive\\nERGODIC MARKOV\\nCHAIN\\ninteger T0 such that for all pairs of states i, j in the Markov chain, if it is\\nstarted at time 0 in state i then for all t > T0, the probability of being in state\\nj at time t is greater than 0.\\nFor a Markov chain to be ergodic, two technical conditions are required\\nof its states and the non-zero transition probabilities; these conditions are\\nknown as irreducibility and aperiodicity. Informally, the ﬁrst ensures that there\\nis a sequence of transitions of non-zero probability from any state to any\\nother, while the latter ensures that the states are not partitioned into sets\\nsuch that all state transitions occur cyclically from one set to another.\\nTheorem 21.1. For any ergodic Markov chain, there is a unique steady-state prob-\\nSTEADY-STATE\\nability vector ⃗π that is the principal left eigenvector of P, such that if η(i, t) is the\\nnumber of visits to state i in t steps, then\\nlim\\nt→∞\\nη(i, t)\\nt\\n= π(i),\\nwhere π(i) > 0 is the steady-state probability for state i.\\nIt follows from Theorem 21.1 that the random walk with teleporting re-\\nsults in a unique distribution of steady-state probabilities over the states of\\nthe induced Markov chain. This steady-state probability for a state is the\\nPageRank of the corresponding web page.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n468\\n21\\nLink analysis\\n21.2.2\\nThe PageRank computation\\nHow do we compute PageRank values? Recall the deﬁnition of a left eigen-\\nvector from Equation 18.2; the left eigenvectors of the transition probability\\nmatrix P are N-vectors ⃗π such that\\n⃗π P = λ⃗π.\\n(21.2)\\nThe N entries in the principal eigenvector ⃗π are the steady-state proba-\\nbilities of the random walk with teleporting, and thus the PageRank values\\nfor the corresponding web pages. We may interpret Equation (21.2) as fol-\\nlows: if ⃗π is the probability distribution of the surfer across the web pages,\\nhe remains in the steady-state distribution ⃗π. Given that ⃗π is the steady-state\\ndistribution, we have that πP = 1π, so 1 is an eigenvalue of P. Thus if we\\nwere to compute the principal left eigenvector of the matrix P — the one with\\neigenvalue 1 — we would have computed the PageRank values.\\nThere are many algorithms available for computing left eigenvectors; the\\nreferences at the end of Chapter 18 and the present chapter are a guide to\\nthese. We give here a rather elementary method, sometimes known as power\\niteration. If ⃗x is the initial distribution over the states, then the distribution at\\ntime t is ⃗xPt. As t grows large, we would expect that the distribution ⃗xPt2\\nis very similar to the distribution ⃗xPt+1, since for large t we would expect\\nthe Markov chain to attain its steady state. By Theorem 21.1 this is indepen-\\ndent of the initial distribution ⃗x. The power iteration method simulates the\\nsurfer’s walk: begin at a state and run the walk for a large number of steps\\nt, keeping track of the visit frequencies for each of the states. After a large\\nnumber of steps t, these frequencies “settle down” so that the variation in the\\ncomputed frequencies is below some predetermined threshold. We declare\\nthese tabulated frequencies to be the PageRank values.\\nWe consider the web graph in Exercise 21.6 with α = 0.5. The transition\\nprobability matrix of the surfer’s walk with teleportation is then\\nP =\\n\\uf8eb\\n\\uf8ed\\n1/6\\n2/3\\n1/6\\n5/12\\n1/6\\n5/12\\n1/6\\n2/3\\n1/6\\n\\uf8f6\\n\\uf8f8.\\n(21.3)\\nImagine that the surfer starts in state 1, corresponding to the initial proba-\\nbility distribution vector ⃗x0 = (1 0 0). Then, after one step the distribution\\nis\\n⃗x0P =\\n\\x001/6\\n2/3\\n1/6\\n\\x01 = ⃗x1.\\n(21.4)\\n2. Note that Pt represents P raised to the tth power, not the transpose of P which is denoted PT.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n469\\n⃗x0\\n1\\n0\\n0\\n⃗x1\\n1/6\\n2/3\\n1/6\\n⃗x2\\n1/3\\n1/3\\n1/3\\n⃗x3\\n1/4\\n1/2\\n1/4\\n⃗x4\\n7/24\\n5/12\\n7/24\\n. . .\\n· · ·\\n· · ·\\n· · ·\\n⃗x\\n5/18\\n4/9\\n5/18\\n◮Figure 21.3\\nThe sequence of probability vectors.\\nAfter two steps it is\\n⃗x1P =\\n\\x00 1/6\\n2/3\\n1/6 \\x01\\n\\uf8eb\\n\\uf8ed\\n1/6\\n2/3\\n1/6\\n5/12\\n1/6\\n5/12\\n1/6\\n2/3\\n1/6\\n\\uf8f6\\n\\uf8f8=\\n\\x00 1/3\\n1/3\\n1/3 \\x01 = ⃗x2.\\n(21.5)\\nContinuing in this fashion gives a sequence of probability vectors as shown\\nin Figure 21.3.\\nContinuing for several steps, we see that the distribution converges to the\\nsteady state of ⃗x = (5/18\\n4/9\\n5/18). In this simple example, we may\\ndirectly calculate this steady-state probability distribution by observing the\\nsymmetry of the Markov chain: states 1 and 3 are symmetric, as evident from\\nthe fact that the ﬁrst and third rows of the transition probability matrix in\\nEquation (21.3) are identical. Postulating, then, that they both have the same\\nsteady-state probability and denoting this probability by p, we know that the\\nsteady-state distribution is of the form ⃗π = (p 1 −2p p). Now, using the\\nidentity ⃗π = ⃗πP, we solve a simple linear equation to obtain p = 5/18 and\\nconsequently, ⃗π = (5/18 4/9 5/18).\\nThe PageRank values of pages (and the implicit ordering amongst them)\\nare independent of any query a user might pose; PageRank is thus a query-\\nindependent measure of the static quality of each web page (recall such static\\nquality measures from Section 7.1.4). On the other hand, the relative order-\\ning of pages should, intuitively, depend on the query being served. For this\\nreason, search engines use static quality measures such as PageRank as just\\none of many factors in scoring a web page on a query. Indeed, the relative\\ncontribution of PageRank to the overall score may again be determined by\\nmachine-learned scoring as in Section 15.4.1.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n470\\n21\\nLink analysis\\nd0\\nd2\\nd1\\nd5\\nd3\\nd6\\nd4\\ncar\\nbenz\\nford\\ngm\\nhonda\\njaguar\\njag\\ncat\\nleopard\\ntiger\\njaguar\\nlion\\ncheetah\\nspeed\\n◮Figure 21.4\\nA small web graph. Arcs are annotated with the word that occurs in\\nthe anchor text of the corresponding link.\\n\\x0f\\nExample 21.1:\\nConsider the graph in Figure 21.4. For a teleportation rate of 0.14\\nits (stochastic) transition probability matrix is:\\n0.02\\n0.02\\n0.88\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.02\\n0.31\\n0.02\\n0.31\\n0.31\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.88\\n0.02\\n0.02\\n0.02\\n0.02\\n0.02\\n0.45\\n0.45\\n0.02\\n0.02\\n0.02\\n0.31\\n0.31\\n0.02\\n0.31\\nThe PageRank vector of this matrix is:\\n⃗x = (0.05\\n0.04\\n0.11\\n0.25\\n0.21\\n0.04\\n0.31)\\n(21.6)\\nObserve that in Figure 21.4, q2, q3, q4 and q6 are the nodes with at least two in-links.\\nOf these, q2 has the lowest PageRank since the random walk tends to drift out of the\\ntop part of the graph – the walker can only return there through teleportation.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n471\\n21.2.3\\nTopic-speciﬁc PageRank\\nThus far we have discussed the PageRank computation with a teleport op-\\neration in which the surfer jumps to a random web page chosen uniformly\\nat random. We now consider teleporting to a random web page chosen non-\\nuniformly. In doing so, we are able to derive PageRank values tailored to\\nparticular interests. For instance, a sports aﬁcionado might wish that pages\\non sports be ranked higher than non-sports pages. Suppose that web pages\\non sports are “near” one another in the web graph. Then, a random surfer\\nwho frequently ﬁnds himself on random sports pages is likely (in the course\\nof the random walk) to spend most of his time at sports pages, so that the\\nsteady-state distribution of sports pages is boosted.\\nSuppose our random surfer, endowed with a teleport operation as before,\\nteleports to a random web page on the topic of sports instead of teleporting to a\\nuniformly chosen random web page. We will not focus on how we collect all\\nweb pages on the topic of sports; in fact, we only need a non-zero subset S of\\nsports-related web pages, so that the teleport operation is feasible. This may\\nbe obtained, for instance, from a manually built directory of sports pages\\nsuch as the open directory project (http://www.dmoz.org/) or that of Yahoo.\\nProvided the set S of sports-related pages is non-empty, it follows that\\nthere is a non-empty set of web pages Y ⊇S over which the random walk\\nhas a steady-state distribution; let us denote this sports PageRank distribution\\nby ⃗πs. For web pages not in Y, we set the PageRank values to zero. We call\\n⃗πs the topic-speciﬁc PageRank for sports.\\nTOPIC-SPECIFIC\\nPAGERANK\\nWe do not demand that teleporting takes the random surfer to a uniformly\\nchosen sports page; the distribution over teleporting targets S could in fact\\nbe arbitrary.\\nIn like manner we can envision topic-speciﬁc PageRank distributions for\\neach of several topics such as science, religion, politics and so on. Each of\\nthese distributions assigns to each web page a PageRank value in the interval\\n[0, 1). For a user interested in only a single topic from among these topics,\\nwe may invoke the corresponding PageRank distribution when scoring and\\nranking search results. This gives us the potential of considering settings in\\nwhich the search engine knows what topic a user is interested in. This may\\nhappen because users either explicitly register their interests, or because the\\nsystem learns by observing each user’s behavior over time.\\nBut what if a user is known to have a mixture of interests from multiple\\ntopics? For instance, a user may have an interest mixture (or proﬁle) that is\\n60% sports and 40% politics; can we compute a personalized PageRank for this\\nPERSONALIZED\\nPAGERANK\\nuser? At ﬁrst glance, this appears daunting: how could we possibly compute\\na different PageRank distribution for each user proﬁle (with, potentially, in-\\nﬁnitely many possible proﬁles)? We can in fact address this provided we\\nassume that an individual’s interests can be well-approximated as a linear\\n', 'Online edition (c)\\n2009 Cambridge UP\\n472\\n21\\nLink analysis\\n◮Figure 21.5\\nTopic-speciﬁc PageRank. In this example we consider a user whose\\ninterests are 60% sports and 40% politics. If the teleportation probability is 10%, this\\nuser is modeled as teleporting 6% to sports pages and 4% to politics pages.\\ncombination of a small number of topic page distributions. A user with this\\nmixture of interests could teleport as follows: determine ﬁrst whether to tele-\\nport to the set S of known sports pages, or to the set of known politics pages.\\nThis choice is made at random, choosing sports pages 60% of the time and\\npolitics pages 40% of the time. Once we choose that a particular teleport step\\nis to (say) a random sports page, we choose a web page in S uniformly at\\nrandom to teleport to. This in turn leads to an ergodic Markov chain with a\\nsteady-state distribution that is personalized to this user’s preferences over\\ntopics (see Exercise 21.16).\\nWhile this idea has intuitive appeal, its implementation appears cumber-\\nsome: it seems to demand that for each user, we compute a transition prob-\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.2\\nPageRank\\n473\\nability matrix and compute its steady-state distribution. We are rescued by\\nthe fact that the evolution of the probability distribution over the states of\\na Markov chain can be viewed as a linear system. In Exercise 21.16 we will\\nshow that it is not necessary to compute a PageRank vector for every distinct\\ncombination of user interests over topics; the personalized PageRank vector\\nfor any user can be expressed as a linear combination of the underlying topic-\\nspeciﬁc PageRanks. For instance, the personalized PageRank vector for the\\nuser whose interests are 60% sports and 40% politics can be computed as\\n0.6⃗πs + 0.4⃗πp,\\n(21.7)\\nwhere ⃗πs and ⃗πp are the topic-speciﬁc PageRank vectors for sports and for\\npolitics, respectively.\\n?\\nExercise 21.5\\nWrite down the transition probability matrix for the example in Figure 21.2.\\nExercise 21.6\\nConsider a web graph with three nodes 1, 2 and 3. The links are as follows: 1 →\\n2, 3 →2, 2 →1, 2 →3. Write down the transition probability matrices for the surfer’s\\nwalk with teleporting, for the following three values of the teleport probability: (a)\\nα = 0; (b) α = 0.5 and (c) α = 1.\\nExercise 21.7\\nA user of a browser can, in addition to clicking a hyperlink on the page x he is cur-\\nrently browsing, use the back button to go back to the page from which he arrived at\\nx. Can such a user of back buttons be modeled as a Markov chain? How would we\\nmodel repeated invocations of the back button?\\nExercise 21.8\\nConsider a Markov chain with three states A, B and C, and transition probabilities as\\nfollows. From state A, the next state is B with probability 1. From B, the next state is\\neither A with probability pA, or state C with probability 1 −pA. From C the next state\\nis A with probability 1. For what values of pA ∈[0, 1] is this Markov chain ergodic?\\nExercise 21.9\\nShow that for any directed graph, the Markov chain induced by a random walk with\\nthe teleport operation is ergodic.\\nExercise 21.10\\nShow that the PageRank of every page is at least α/N. What does this imply about\\nthe difference in PageRank values (over the various pages) as α becomes close to 1?\\nExercise 21.11\\nFor the data in Example 21.1, write a small routine or use a scientiﬁc calculator to\\ncompute the PageRank values stated in Equation (21.6).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n474\\n21\\nLink analysis\\nExercise 21.12\\nSuppose that the web graph is stored on disk as an adjacency list, in such a way that\\nyou may only query for the out-neighbors of pages in the order in which they are\\nstored. You cannot load the graph in main memory but you may do multiple reads\\nover the full graph. Write the algorithm for computing the PageRank in this setting.\\nExercise 21.13\\nRecall the sets S and Y introduced near the beginning of Section 21.2.3. How does the\\nset Y relate to S?\\nExercise 21.14\\nIs the set Y always the set of all web pages? Why or why not?\\nExercise 21.15\\n[⋆⋆⋆]\\nIs the sports PageRank of any page in S at least as large as its PageRank?\\nExercise 21.16\\n[⋆⋆⋆]\\nConsider a setting where we have two topic-speciﬁc PageRank values for each web\\npage: a sports PageRank ⃗πs, and a politics PageRank ⃗πp. Let α be the (common)\\nteleportation probability used in computing both sets of topic-speciﬁc PageRanks.\\nFor q ∈[0, 1], consider a user whose interest proﬁle is divided between a fraction q in\\nsports and a fraction 1 −q in politics. Show that the user’s personalized PageRank is\\nthe steady-state distribution of a random walk in which – on a teleport step – the walk\\nteleports to a sports page with probability q and to a politics page with probability\\n1 −q.\\nExercise 21.17\\nShow that the Markov chain corresponding to the walk in Exercise 21.16 is ergodic\\nand hence the user’s personalized PageRank can be obtained by computing the steady-\\nstate distribution of this Markov chain.\\nExercise 21.18\\nShow that in the steady-state distribution of Exercise 21.17, the steady-state probabil-\\nity for any web page i equals qπs(i) + (1 −q)πp(i).\\n21.3\\nHubs and Authorities\\nWe now develop a scheme in which, given a query, every web page is as-\\nsigned two scores. One is called its hub score and the other its authority score.\\nHUB SCORE\\nAUTHORITY SCORE\\nFor any query, we compute two ranked lists of results rather than one. The\\nranking of one list is induced by the hub scores and that of the other by the\\nauthority scores.\\nThis approach stems from a particular insight into the creation of web\\npages, that there are two primary kinds of web pages useful as results for\\nbroad-topic searches. By a broad topic search we mean an informational query\\nsuch as \"I wish to learn about leukemia\". There are authoritative sources of\\ninformation on the topic; in this case, the National Cancer Institute’s page on\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n475\\nleukemia would be such a page. We will call such pages authorities; in the\\ncomputation we are about to describe, they are the pages that will emerge\\nwith high authority scores.\\nOn the other hand, there are many pages on the Web that are hand-compiled\\nlists of links to authoritative web pages on a speciﬁc topic. These hub pages\\nare not in themselves authoritative sources of topic-speciﬁc information, but\\nrather compilations that someone with an interest in the topic has spent time\\nputting together. The approach we will take, then, is to use these hub pages\\nto discover the authority pages. In the computation we now develop, these\\nhub pages are the pages that will emerge with high hub scores.\\nA good hub page is one that points to many good authorities; a good au-\\nthority page is one that is pointed to by many good hub pages. We thus\\nappear to have a circular deﬁnition of hubs and authorities; we will turn this\\ninto an iterative computation. Suppose that we have a subset of the web con-\\ntaining good hub and authority pages, together with the hyperlinks amongst\\nthem. We will iteratively compute a hub score and an authority score for ev-\\nery web page in this subset, deferring the discussion of how we pick this\\nsubset until Section 21.3.1.\\nFor a web page v in our subset of the web, we use h(v) to denote its hub\\nscore and a(v) its authority score. Initially, we set h(v) = a(v) = 1 for all\\nnodes v. We also denote by v 7→y the existence of a hyperlink from v to\\ny. The core of the iterative algorithm is a pair of updates to the hub and au-\\nthority scores of all pages given by Equation 21.8, which capture the intuitive\\nnotions that good hubs point to good authorities and that good authorities\\nare pointed to by good hubs.\\nh(v)\\n←\\n∑\\nv7→y\\na(y)\\n(21.8)\\na(v)\\n←\\n∑\\ny7→v\\nh(y).\\nThus, the ﬁrst line of Equation (21.8) sets the hub score of page v to the sum\\nof the authority scores of the pages it links to. In other words, if v links to\\npages with high authority scores, its hub score increases. The second line\\nplays the reverse role; if page v is linked to by good hubs, its authority score\\nincreases.\\nWhat happens as we perform these updates iteratively, recomputing hub\\nscores, then new authority scores based on the recomputed hub scores, and\\nso on? Let us recast the equations Equation (21.8) into matrix-vector form.\\nLet⃗h and⃗a denote the vectors of all hub and all authority scores respectively,\\nfor the pages in our subset of the web graph. Let A denote the adjacency\\nmatrix of the subset of the web graph that we are dealing with: A is a square\\nmatrix with one row and one column for each page in the subset. The entry\\n', 'Online edition (c)\\n2009 Cambridge UP\\n476\\n21\\nLink analysis\\nAij is 1 if there is a hyperlink from page i to page j, and 0 otherwise. Then,\\nwe may write Equation (21.8)\\n⃗h\\n←\\nA⃗a\\n(21.9)\\n⃗a\\n←\\nAT⃗h,\\nwhere AT denotes the transpose of the matrix A. Now the right hand side of\\neach line of Equation (21.9) is a vector that is the left hand side of the other\\nline of Equation (21.9). Substituting these into one another, we may rewrite\\nEquation (21.9) as\\n⃗h\\n←\\nAAT⃗h\\n(21.10)\\n⃗a\\n←\\nATA⃗a.\\nNow, Equation (21.10) bears an uncanny resemblance to a pair of eigenvector\\nequations (Section 18.1); indeed, if we replace the ←symbols by = symbols\\nand introduce the (unknown) eigenvalue, the ﬁrst line of Equation (21.10)\\nbecomes the equation for the eigenvectors of AAT, while the second becomes\\nthe equation for the eigenvectors of ATA:\\n⃗h\\n=\\n(1/λh)AAT⃗h\\n⃗a\\n=\\n(1/λa)ATA⃗a.\\n(21.11)\\nHere we have used λh to denote the eigenvalue of AAT and λa to denote the\\neigenvalue of ATA.\\nThis leads to some key consequences:\\n1. The iterative updates in Equation (21.8) (or equivalently, Equation (21.9)),\\nif scaled by the appropriate eigenvalues, are equivalent to the power iter-\\nation method for computing the eigenvectors of AAT and ATA. Provided\\nthat the principal eigenvalue of AAT is unique, the iteratively computed\\nentries of⃗h and⃗a settle into unique steady-state values determined by the\\nentries of A and hence the link structure of the graph.\\n2. In computing these eigenvector entries, we are not restricted to using the\\npower iteration method; indeed, we could use any fast method for com-\\nputing the principal eigenvector of a stochastic matrix.\\nThe resulting computation thus takes the following form:\\n1. Assemble the target subset of web pages, form the graph induced by their\\nhyperlinks and compute AAT and ATA.\\n2. Compute the principal eigenvectors of AAT and ATA to form the vector\\nof hub scores⃗h and authority scores⃗a.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n477\\n3. Output the top-scoring hubs and the top-scoring authorities.\\nThis method of link analysis is known as HITS, which is an acronym for\\nHITS\\nHyperlink-Induced Topic Search.\\n\\x0f\\nExample 21.2:\\nAssuming the query jaguar and double-weighting of links whose\\nanchors contain the query word, the matrix A for Figure 21.4 is as follows:\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n2\\n1\\n0\\n1\\nThe hub and authority vectors are:\\n⃗h = (0.03\\n0.04\\n0.33\\n0.18\\n0.04\\n0.04\\n0.35)\\n⃗a = (0.10\\n0.01\\n0.12\\n0.47\\n0.16\\n0.01\\n0.13)\\nHere, q3 is the main authority – two hubs (q2 and q6) are pointing to it via highly\\nweighted jaguar links.\\nSince the iterative updates captured the intuition of good hubs and good\\nauthorities, the high-scoring pages we output would give us good hubs and\\nauthorities from the target subset of web pages. In Section 21.3.1 we describe\\nthe remaining detail: how do we gather a target subset of web pages around\\na topic such as leukemia?\\n21.3.1\\nChoosing the subset of the Web\\nIn assembling a subset of web pages around a topic such as leukemia, we must\\ncope with the fact that good authority pages may not contain the speciﬁc\\nquery term leukemia. This is especially true, as we noted in Section 21.1.1,\\nwhen an authority page uses its web presence to project a certain market-\\ning image. For instance, many pages on the IBM website are authoritative\\nsources of information on computer hardware, even though these pages may\\nnot contain the term computer or hardware. However, a hub compiling com-\\nputer hardware resources is likely to use these terms and also link to the\\nrelevant pages on the IBM website.\\nBuilding on these observations, the following procedure has been sug-\\ngested for compiling the subset of the Web for which to compute hub and\\nauthority scores.\\n', 'Online edition (c)\\n2009 Cambridge UP\\n478\\n21\\nLink analysis\\n1. Given a query (say leukemia), use a text index to get all pages containing\\nleukemia. Call this the root set of pages.\\n2. Build the base set of pages, to include the root set as well as any page that\\neither links to a page in the root set, or is linked to by a page in the root\\nset.\\nWe then use the base set for computing hub and authority scores. The base\\nset is constructed in this manner for three reasons:\\n1. A good authority page may not contain the query text (such as computer\\nhardware).\\n2. If the text query manages to capture a good hub page vh in the root set,\\nthen the inclusion of all pages linked to by any page in the root set will\\ncapture all the good authorities linked to by vh in the base set.\\n3. Conversely, if the text query manages to capture a good authority page\\nva in the root set, then the inclusion of pages which point to va will bring\\nother good hubs into the base set. In other words, the “expansion” of\\nthe root set into the base set enriches the common pool of good hubs and\\nauthorities.\\nRunning HITS across a variety of queries reveals some interesting insights\\nabout link analysis. Frequently, the documents that emerge as top hubs and\\nauthorities include languages other than the language of the query. These\\npages were presumably drawn into the base set, following the assembly of\\nthe root set. Thus, some elements of cross-language retrieval (where a query\\nin one language retrieves documents in another) are evident here; interest-\\ningly, this cross-language effect resulted purely from link analysis, with no\\nlinguistic translation taking place.\\nWe conclude this section with some notes on implementing this algorithm.\\nThe root set consists of all pages matching the text query; in fact, implemen-\\ntations (see the references in Section 21.4) suggest that it sufﬁces to use 200 or\\nso web pages for the root set, rather than all pages matching the text query.\\nAny algorithm for computing eigenvectors may be used for computing the\\nhub/authority score vector. In fact, we need not compute the exact values\\nof these scores; it sufﬁces to know the relative values of the scores so that\\nwe may identify the top hubs and authorities. To this end, it is possible that\\na small number of iterations of the power iteration method yields the rela-\\ntive ordering of the top hubs and authorities. Experiments have suggested\\nthat in practice, about ﬁve iterations of Equation (21.8) yield fairly good re-\\nsults. Moreover, since the link structure of the web graph is fairly sparse\\n(the average web page links to about ten others), we do not perform these as\\nmatrix-vector products but rather as additive updates as in Equation (21.8).\\n', 'Online edition (c)\\n2009 Cambridge UP\\n21.3\\nHubs and Authorities\\n479\\n◮Figure 21.6\\nA sample run of HITS on the query japan elementary schools.\\nFigure 21.6 shows the results of running HITS on the query japan elemen-\\ntary schools. The ﬁgure shows the top hubs and authorities; each row lists the\\ntitle tag from the corresponding HTML page. Because the resulting string\\nis not necessarily in Latin characters, the resulting print is (in many cases)\\na string of gibberish. Each of these corresponds to a web page that does\\nnot use Latin characters, in this case very likely pages in Japanese. There\\nalso appear to be pages in other non-English languages, which seems sur-\\nprising given that the query string is in English. In fact, this result is em-\\nblematic of the functioning of HITS – following the assembly of the root set,\\nthe (English) query string is ignored. The base set is likely to contain pages\\nin other languages, for instance if an English-language hub page links to\\nthe Japanese-language home pages of Japanese elementary schools. Because\\nthe subsequent computation of the top hubs and authorities is entirely link-\\nbased, some of these non-English pages will appear among the top hubs and\\nauthorities.\\n?\\nExercise 21.19\\nIf all the hub and authority scores are initialized to 1, what is the hub/authority score\\nof a node after one iteration?\\n', 'Online edition (c)\\n2009 Cambridge UP\\n480\\n21\\nLink analysis\\nExercise 21.20\\nHow would you interpret the entries of the matrices AAT and ATA? What is the\\nconnection to the co-occurrence matrix CCT in Chapter 18?\\nExercise 21.21\\nWhat are the principal eigenvalues of AAT and ATA?\\nd1\\nd2\\nd3\\n◮Figure 21.7\\nWeb graph for Exercise 21.22.\\nExercise 21.22\\nFor the web graph in Figure 21.7, compute PageRank, hub and authority scores for\\neach of the three pages. Also give the relative ordering of the 3 nodes for each of these\\nscores, indicating any ties.\\nPageRank: Assume that at each step of the PageRank random walk, we teleport to a\\nrandom page with probability 0.1, with a uniform distribution over which particular\\npage we teleport to.\\nHubs/Authorities: Normalize the hub (authority) scores so that the maximum hub\\n(authority) score is 1.\\nHint 1: Using symmetries to simplify and solving with linear equations might be\\neasier than using iterative methods.\\nHint 2: Provide the relative ordering (indicating any ties) of the three nodes for each\\nof the three scoring measures.\\n21.4\\nReferences and further reading\\nGarﬁeld (1955) is seminal in the science of citation analysis. This was built\\non by Pinski and Narin (1976) to develop a journal inﬂuence weight, whose\\ndeﬁnition is remarkably similar to that of the PageRank measure.\\nThe use of anchor text as an aid to searching and ranking stems from the\\nwork of McBryan (1994). Extended anchor-text was implicit in his work, with\\nsystematic experiments reported in Chakrabarti et al. (1998).\\nKemeny and Snell (1976) is a classic text on Markov chains. The PageRank\\nmeasure was developed in Brin and Page (1998) and in Page et al. (1998).\\n']\n",
      "range(len(page_limits) - 1): range(0, 20)  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  raw\n",
       "0   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "1   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "2   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "3   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "4   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "5   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "6   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "7   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "8   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "9   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "10  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "11  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "12  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "13  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "14  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "15  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "16  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "17  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "18  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "19  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...\n",
       "20  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ..."
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = corpus[37:517]\n",
    "\n",
    "print(f\"Number of pages in corpus: {corpus}\")\n",
    "\n",
    "# corpus\n",
    "\n",
    "\n",
    "# Convert page limits to integers\n",
    "page_limits = list(map(int, pages))\n",
    "\n",
    "\n",
    "# Prepare the groups\n",
    "groups = []\n",
    "\n",
    "print(f\"range(len(page_limits) - 1): {range(len(page_limits) - 1)}  \")\n",
    "for i in range(len(page_limits) - 1):\n",
    "    start = page_limits[i] - 1  # -1 because Python uses 0-based indexing\n",
    "    end = page_limits[i+1] - 1  # up to but not including this index\n",
    "    groups.append(corpus[start:end])\n",
    "\n",
    "# Optionally, add the last group (from last page limit to end)\n",
    "groups.append(corpus[page_limits[-1] - 1:])\n",
    "\n",
    "# groups\n",
    "\n",
    "# Join each group into a single string\n",
    "group_texts = [' '.join(group) for group in groups]\n",
    "\n",
    "corpus_df = pd.DataFrame(group_texts, columns=['raw'])\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8f3d5",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef14409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>removed_trash_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>11Boolean retrievalThe meaning of the term inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>192The term vocabulary and postingslistsRecall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>493Dictionaries and tolerantretrievalIn Chapte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>674Index constructionIn this chapter, we look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>855Index compressionChapter 1 introduced the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1096Scoring, term weighting and thevector spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1357Computing scores in a completesearch syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1518Evaluation in informationretrievalWe have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1779Relevance feedback and queryexpansionIn mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>19510XML retrievalInformation retrieval system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>21911Probabilistic informationretrievalDuring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>23712Language models for informationretrievalA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>25313Text classiﬁcation and NaiveBayesThus far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>28914Vector space classiﬁcationThe document re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>31915Support vector machines andmachine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>34916Flat clusteringClustering algorithms grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>37717Hierarchical clusteringFlat clustering is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>40318Matrix decompositions and latentsemantic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>42119Web search basicsIn this and the followin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>44320Web crawling and indexes20.1OverviewWeb c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>46121Link analysisThe analysis of hyperlinks a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  raw  \\\n",
       "0   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "1   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "2   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "3   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "4   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "5   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "6   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "7   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "8   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "9   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "10  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "11  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "12  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "13  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "14  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "15  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "16  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "17  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "18  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "19  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "20  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "\n",
       "                                   removed_trash_text  \n",
       "0   11Boolean retrievalThe meaning of the term inf...  \n",
       "1   192The term vocabulary and postingslistsRecall...  \n",
       "2   493Dictionaries and tolerantretrievalIn Chapte...  \n",
       "3   674Index constructionIn this chapter, we look ...  \n",
       "4   855Index compressionChapter 1 introduced the d...  \n",
       "5   1096Scoring, term weighting and thevector spac...  \n",
       "6   1357Computing scores in a completesearch syste...  \n",
       "7   1518Evaluation in informationretrievalWe have ...  \n",
       "8   1779Relevance feedback and queryexpansionIn mo...  \n",
       "9   19510XML retrievalInformation retrieval system...  \n",
       "10  21911Probabilistic informationretrievalDuring ...  \n",
       "11  23712Language models for informationretrievalA...  \n",
       "12  25313Text classiﬁcation and NaiveBayesThus far...  \n",
       "13  28914Vector space classiﬁcationThe document re...  \n",
       "14  31915Support vector machines andmachine learni...  \n",
       "15  34916Flat clusteringClustering algorithms grou...  \n",
       "16  37717Hierarchical clusteringFlat clustering is...  \n",
       "17  40318Matrix decompositions and latentsemantic ...  \n",
       "18  42119Web search basicsIn this and the followin...  \n",
       "19  44320Web crawling and indexes20.1OverviewWeb c...  \n",
       "20  46121Link analysisThe analysis of hyperlinks a...  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df[\"removed_trash_text\"] = corpus_df[\"raw\"].str.replace(\"Online edition (c)\\n2009 Cambridge UP\", \"\")\n",
    "corpus_df[\"removed_trash_text\"] = corpus_df[\"removed_trash_text\"].str.replace(\"DRAFT! © April 1, 2009 Cambridge University Press. Feedback welcome.\", \"\")\n",
    "corpus_df[\"removed_trash_text\"] = corpus_df[\"removed_trash_text\"].str.replace(\"\\n\", \"\")\n",
    "\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_doc(doc):\n",
    "    words = word_tokenize(doc.lower())\n",
    "    words_filtered = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6de55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>removed_trash_text</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>11Boolean retrievalThe meaning of the term inf...</td>\n",
       "      <td>retrievalthe meaning term information retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>192The term vocabulary and postingslistsRecall...</td>\n",
       "      <td>term vocabulary postingslistsrecall major step...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>493Dictionaries and tolerantretrievalIn Chapte...</td>\n",
       "      <td>tolerantretrievalin chapters developed ideas u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>674Index constructionIn this chapter, we look ...</td>\n",
       "      <td>constructionin chapter look construct inverted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>855Index compressionChapter 1 introduced the d...</td>\n",
       "      <td>compressionchapter introduced dictionary inver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1096Scoring, term weighting and thevector spac...</td>\n",
       "      <td>term weighting thevector space modelthus far d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1357Computing scores in a completesearch syste...</td>\n",
       "      <td>scores completesearch systemchapter developed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1518Evaluation in informationretrievalWe have ...</td>\n",
       "      <td>informationretrievalwe seen preceding chapters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1779Relevance feedback and queryexpansionIn mo...</td>\n",
       "      <td>feedback queryexpansionin collections concept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>19510XML retrievalInformation retrieval system...</td>\n",
       "      <td>retrievalinformation retrieval systems often c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>21911Probabilistic informationretrievalDuring ...</td>\n",
       "      <td>informationretrievalduring discussion relevanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>23712Language models for informationretrievalA...</td>\n",
       "      <td>models informationretrievala common suggestion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>25313Text classiﬁcation and NaiveBayesThus far...</td>\n",
       "      <td>classiﬁcation naivebayesthus far book mainly d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>28914Vector space classiﬁcationThe document re...</td>\n",
       "      <td>space classiﬁcationthe document representation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>31915Support vector machines andmachine learni...</td>\n",
       "      <td>vector machines andmachine learning documentsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>34916Flat clusteringClustering algorithms grou...</td>\n",
       "      <td>clusteringclustering algorithms group set docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>37717Hierarchical clusteringFlat clustering is...</td>\n",
       "      <td>clusteringflat clustering efﬁcient conceptuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>40318Matrix decompositions and latentsemantic ...</td>\n",
       "      <td>decompositions latentsemantic indexingon page ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>42119Web search basicsIn this and the followin...</td>\n",
       "      <td>search basicsin following two chapters conside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>44320Web crawling and indexes20.1OverviewWeb c...</td>\n",
       "      <td>crawling crawling process gather pages web ino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>46121Link analysisThe analysis of hyperlinks a...</td>\n",
       "      <td>analysisthe analysis hyperlinks graph structur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  raw  \\\n",
       "0   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "1   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "2   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "3   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "4   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "5   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "6   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "7   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "8   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "9   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "10  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "11  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "12  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "13  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "14  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "15  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "16  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "17  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "18  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "19  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "20  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "\n",
       "                                   removed_trash_text  \\\n",
       "0   11Boolean retrievalThe meaning of the term inf...   \n",
       "1   192The term vocabulary and postingslistsRecall...   \n",
       "2   493Dictionaries and tolerantretrievalIn Chapte...   \n",
       "3   674Index constructionIn this chapter, we look ...   \n",
       "4   855Index compressionChapter 1 introduced the d...   \n",
       "5   1096Scoring, term weighting and thevector spac...   \n",
       "6   1357Computing scores in a completesearch syste...   \n",
       "7   1518Evaluation in informationretrievalWe have ...   \n",
       "8   1779Relevance feedback and queryexpansionIn mo...   \n",
       "9   19510XML retrievalInformation retrieval system...   \n",
       "10  21911Probabilistic informationretrievalDuring ...   \n",
       "11  23712Language models for informationretrievalA...   \n",
       "12  25313Text classiﬁcation and NaiveBayesThus far...   \n",
       "13  28914Vector space classiﬁcationThe document re...   \n",
       "14  31915Support vector machines andmachine learni...   \n",
       "15  34916Flat clusteringClustering algorithms grou...   \n",
       "16  37717Hierarchical clusteringFlat clustering is...   \n",
       "17  40318Matrix decompositions and latentsemantic ...   \n",
       "18  42119Web search basicsIn this and the followin...   \n",
       "19  44320Web crawling and indexes20.1OverviewWeb c...   \n",
       "20  46121Link analysisThe analysis of hyperlinks a...   \n",
       "\n",
       "                                            processed  \n",
       "0   retrievalthe meaning term information retrieva...  \n",
       "1   term vocabulary postingslistsrecall major step...  \n",
       "2   tolerantretrievalin chapters developed ideas u...  \n",
       "3   constructionin chapter look construct inverted...  \n",
       "4   compressionchapter introduced dictionary inver...  \n",
       "5   term weighting thevector space modelthus far d...  \n",
       "6   scores completesearch systemchapter developed ...  \n",
       "7   informationretrievalwe seen preceding chapters...  \n",
       "8   feedback queryexpansionin collections concept ...  \n",
       "9   retrievalinformation retrieval systems often c...  \n",
       "10  informationretrievalduring discussion relevanc...  \n",
       "11  models informationretrievala common suggestion...  \n",
       "12  classiﬁcation naivebayesthus far book mainly d...  \n",
       "13  space classiﬁcationthe document representation...  \n",
       "14  vector machines andmachine learning documentsi...  \n",
       "15  clusteringclustering algorithms group set docu...  \n",
       "16  clusteringflat clustering efﬁcient conceptuall...  \n",
       "17  decompositions latentsemantic indexingon page ...  \n",
       "18  search basicsin following two chapters conside...  \n",
       "19  crawling crawling process gather pages web ino...  \n",
       "20  analysisthe analysis hyperlinks graph structur...  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df['processed'] = corpus_df['removed_trash_text'].apply(preprocess_doc)\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce264ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_processed = corpus_df[\"processed\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a520a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(corpus_processed, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b46a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.87567191e-02,  2.41185762e-02, -1.26637202e-02,\n",
       "         2.68820338e-02, -3.52203362e-02, -3.04296166e-02,\n",
       "         8.07284936e-02, -3.67522240e-02,  5.70592098e-02,\n",
       "        -4.87763174e-02,  8.84693787e-02, -1.43028304e-01,\n",
       "        -1.66783202e-02,  3.72372009e-02, -4.10054438e-02,\n",
       "        -2.66659278e-02, -1.10918740e-02,  5.79014197e-02,\n",
       "        -8.12566057e-02,  1.63673796e-02,  3.73923369e-02,\n",
       "         3.11759841e-02,  2.71429378e-03,  1.45310730e-01,\n",
       "        -1.84971455e-03,  9.13950130e-02,  7.86481649e-02,\n",
       "        -4.18036617e-03, -1.83898397e-02, -8.43591914e-02,\n",
       "        -8.66919570e-03,  4.24611056e-03,  1.13893584e-04,\n",
       "        -1.06601072e-02, -1.32846022e-02, -1.62804015e-02,\n",
       "        -5.02117164e-02,  7.26689324e-02,  1.46853272e-02,\n",
       "         3.61582376e-02, -2.73848940e-02, -1.94133576e-02,\n",
       "         3.82205546e-02,  5.39974794e-02, -5.49451709e-02,\n",
       "         2.42900606e-02, -4.03086245e-02,  6.58643916e-02,\n",
       "         3.14030834e-02, -3.72660048e-02,  3.10178753e-02,\n",
       "        -3.51429731e-02, -1.05257705e-01, -2.01734481e-03,\n",
       "        -1.34505080e-02,  1.89945120e-02, -2.10032724e-02,\n",
       "         1.92738753e-02,  2.81592086e-02, -4.03993949e-02,\n",
       "         1.08525597e-01, -4.12448449e-03, -4.39141244e-02,\n",
       "         3.39268073e-02,  1.45145291e-02, -5.64960483e-03,\n",
       "        -1.91203784e-02, -1.00591578e-01, -6.63025603e-02,\n",
       "         4.71916348e-02,  1.49298850e-02,  2.79660616e-02,\n",
       "        -5.97132817e-02, -4.82793003e-02,  1.01287328e-02,\n",
       "         6.45350590e-02, -3.41867544e-02, -1.31908609e-02,\n",
       "        -5.95343439e-03, -5.86202648e-03,  6.47827759e-02,\n",
       "         2.92763580e-02, -4.89029512e-02,  1.88645478e-02,\n",
       "        -2.43598334e-02,  5.11191562e-02, -1.61548273e-03,\n",
       "         1.02353736e-03,  1.61597282e-02, -4.78135347e-02,\n",
       "         3.62890065e-02,  5.34354448e-02, -2.13688128e-02,\n",
       "         4.81430860e-03, -2.39312649e-02, -3.36343946e-04,\n",
       "         4.20232080e-02,  2.44734995e-02,  4.20085341e-03,\n",
       "         8.94292891e-02,  4.12327833e-02,  8.03179853e-03,\n",
       "        -1.57547042e-01,  7.04181716e-02, -4.26296629e-02,\n",
       "        -2.84730885e-02,  5.48562258e-02, -4.78531569e-02,\n",
       "        -1.00112697e-02,  5.36483340e-02, -2.86972970e-02,\n",
       "        -9.06843245e-02, -2.42954437e-02, -8.55281297e-03,\n",
       "        -8.88677314e-03, -1.59663409e-02,  1.20493196e-01,\n",
       "         3.53665054e-02,  2.14560162e-02, -7.02957734e-02,\n",
       "         1.05069634e-02,  3.82170677e-02,  2.18169931e-02,\n",
       "         6.74610734e-02,  3.14315259e-02,  2.72979978e-02,\n",
       "         2.45176349e-02,  1.79343704e-33, -5.41325985e-03,\n",
       "        -1.92337073e-02,  2.01175567e-02,  3.42972986e-02,\n",
       "        -4.20637056e-02,  3.07937935e-02, -4.79330420e-02,\n",
       "        -1.71517655e-02,  4.71557081e-02, -6.23967238e-02,\n",
       "        -1.45326108e-01,  1.49050310e-01, -2.96714846e-02,\n",
       "        -6.81496486e-02, -1.63791273e-02,  8.42572898e-02,\n",
       "         1.14287570e-01,  6.33164048e-02,  4.35067788e-02,\n",
       "         1.40977642e-02, -2.27710903e-02,  6.37212470e-02,\n",
       "         2.31331657e-03, -5.38045876e-02, -8.21899846e-02,\n",
       "        -2.01444719e-02, -4.62653190e-02,  2.39264350e-02,\n",
       "        -5.17287478e-02,  2.22970843e-02,  4.48228754e-02,\n",
       "        -1.11093940e-02, -5.75710125e-02, -1.58821084e-02,\n",
       "         5.16026467e-02,  8.01465102e-03, -1.32352030e-02,\n",
       "         1.73491184e-02, -1.81301625e-03,  2.84832180e-03,\n",
       "        -7.07352683e-02, -2.87657138e-02,  5.71003323e-03,\n",
       "        -3.94382440e-02,  1.24817297e-01,  8.05204883e-02,\n",
       "         5.15928231e-02,  4.97422777e-02, -6.20657392e-02,\n",
       "        -2.70697884e-02,  1.16303097e-02,  6.99072853e-02,\n",
       "        -3.78859304e-02, -8.72421414e-02, -5.39895333e-03,\n",
       "         4.75684628e-02,  1.04820924e-02, -1.22839855e-02,\n",
       "         2.12867856e-02,  2.54826769e-02, -1.00734858e-02,\n",
       "         9.08247903e-02, -2.35520750e-02, -1.79006369e-03,\n",
       "        -2.69653238e-02, -3.12486812e-02, -9.78128016e-02,\n",
       "        -1.68599598e-02, -5.92620820e-02, -2.84863752e-03,\n",
       "        -2.05262974e-02, -3.43521051e-02,  6.91501051e-02,\n",
       "         1.17092662e-01, -2.98897969e-03, -1.13439551e-02,\n",
       "        -1.34071643e-02, -4.59416769e-02, -4.32100333e-02,\n",
       "         2.31655966e-02, -8.83172527e-02, -1.42235402e-02,\n",
       "        -1.38667063e-03, -4.00659554e-02, -1.64681133e-02,\n",
       "         4.55325376e-03, -1.71486363e-02, -3.66153866e-02,\n",
       "         4.64856885e-02,  8.62663835e-02, -2.33586058e-02,\n",
       "        -2.85301507e-02, -4.66686562e-02, -3.85036170e-02,\n",
       "         3.76141220e-02, -3.78067709e-33, -2.96711400e-02,\n",
       "        -3.75706255e-02, -8.23118091e-02, -1.48255695e-02,\n",
       "         3.15529318e-03, -5.41175075e-04, -1.01194866e-01,\n",
       "         6.37355074e-02, -4.30666953e-02,  1.94489062e-02,\n",
       "         1.50870346e-03, -4.81325984e-02, -1.51513554e-02,\n",
       "        -2.59850863e-02, -4.94149141e-03,  4.88621071e-02,\n",
       "         1.28348112e-01,  4.91156802e-02, -8.92054662e-02,\n",
       "         6.50384650e-02, -4.06550616e-02,  2.85461694e-02,\n",
       "        -1.18545242e-01, -5.62590472e-02,  3.14853787e-02,\n",
       "        -1.84506550e-03,  9.91264954e-02, -1.08933961e-02,\n",
       "        -7.68876448e-02, -3.88397388e-02, -7.31934011e-02,\n",
       "        -4.14890610e-02,  5.93857504e-02, -8.00456926e-02,\n",
       "        -9.23148841e-02,  4.70640250e-02, -1.14838369e-02,\n",
       "        -1.15177080e-01, -6.73124269e-02, -2.52830833e-02,\n",
       "        -5.02416231e-02, -5.38499211e-04,  2.93561835e-02,\n",
       "         6.19805790e-03,  5.77042699e-02,  3.36491168e-02,\n",
       "         3.78429107e-02, -5.69523126e-03,  3.17549193e-03,\n",
       "         4.63163443e-02,  5.41117191e-02,  1.47130024e-02,\n",
       "         3.74547904e-03,  5.18725663e-02,  1.10670812e-01,\n",
       "         5.19534647e-02, -4.38872166e-02,  2.96031088e-02,\n",
       "         6.81013912e-02, -3.87633126e-03,  6.02900423e-02,\n",
       "        -3.87580544e-02, -5.97478300e-02,  3.55829448e-02,\n",
       "         8.71696547e-02,  1.22623704e-02, -5.85493520e-02,\n",
       "         6.89313859e-02, -2.14141905e-02,  3.62306833e-02,\n",
       "         4.07001190e-02, -2.56318338e-02, -1.19635865e-01,\n",
       "        -2.72132736e-02, -2.32818834e-02,  2.38757171e-02,\n",
       "        -2.67542456e-03, -8.61001387e-02, -1.21201957e-02,\n",
       "         3.07775084e-02, -4.03593332e-02, -4.45713885e-02,\n",
       "         8.49770233e-02, -2.44913138e-02,  1.13218175e-02,\n",
       "        -3.22301388e-02,  7.76752234e-02, -6.61763847e-02,\n",
       "        -5.07380515e-02,  7.12364987e-02,  5.11825085e-02,\n",
       "         2.50838343e-02, -5.06015569e-02,  1.99329434e-03,\n",
       "         7.67117664e-02, -1.91154861e-08,  8.54665600e-03,\n",
       "         8.24978389e-03,  1.23745752e-02, -6.59370348e-02,\n",
       "        -3.50533053e-02, -8.20436031e-02,  9.84841511e-02,\n",
       "         6.69152066e-02,  7.59311439e-03,  5.16222827e-02,\n",
       "        -3.27855572e-02, -1.49033135e-02,  5.84698915e-02,\n",
       "         1.14395484e-01,  2.88187638e-02,  4.81154136e-02,\n",
       "         2.59284098e-02,  9.03562922e-03, -1.06139155e-02,\n",
       "        -2.43785214e-02, -5.75069487e-02,  4.64149145e-03,\n",
       "        -1.74943153e-02, -7.57040605e-02, -3.98720577e-02,\n",
       "        -5.75265400e-02,  5.02799489e-02,  2.28119772e-02,\n",
       "         5.18135913e-02, -5.73426373e-02,  8.87186173e-03,\n",
       "        -1.17492089e-02,  2.67788973e-02, -7.55111054e-02,\n",
       "        -4.46570031e-02, -3.83665110e-03, -3.47872898e-02,\n",
       "         3.55394483e-02, -2.22259574e-02,  3.32746357e-02,\n",
       "        -7.62924477e-02,  3.11509194e-03, -4.22083810e-02,\n",
       "         4.57158610e-02,  7.58155659e-02, -8.92384071e-03,\n",
       "         5.63977547e-02, -5.64993965e-03, -1.03717610e-01,\n",
       "        -7.93824643e-02, -1.31086288e-02,  2.03683451e-02,\n",
       "         5.62928915e-02,  1.66662019e-02,  5.51673844e-02,\n",
       "        -2.22347286e-02,  1.07861254e-02,  2.84123570e-02,\n",
       "        -1.38393432e-01,  6.67039678e-02, -1.97026436e-03,\n",
       "        -4.43484634e-03,  1.06428601e-02, -3.87994088e-02]], dtype=float32)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"distancia euclidiana\"\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "index.is_trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb10e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the embeddings to the df\n",
    "corpus_df['embeddings'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabc8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw</th>\n",
       "      <th>removed_trash_text</th>\n",
       "      <th>processed</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>11Boolean retrievalThe meaning of the term inf...</td>\n",
       "      <td>retrievalthe meaning term information retrieva...</td>\n",
       "      <td>[0.003079628339037299, -0.009766640141606331, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>192The term vocabulary and postingslistsRecall...</td>\n",
       "      <td>term vocabulary postingslistsrecall major step...</td>\n",
       "      <td>[-0.025380702689290047, -0.06649737060070038, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>493Dictionaries and tolerantretrievalIn Chapte...</td>\n",
       "      <td>tolerantretrievalin chapters developed ideas u...</td>\n",
       "      <td>[0.039726272225379944, -0.036738041788339615, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>674Index constructionIn this chapter, we look ...</td>\n",
       "      <td>constructionin chapter look construct inverted...</td>\n",
       "      <td>[0.0004133421170990914, -0.02456202358007431, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>855Index compressionChapter 1 introduced the d...</td>\n",
       "      <td>compressionchapter introduced dictionary inver...</td>\n",
       "      <td>[-0.01911129243671894, 0.01577311009168625, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1096Scoring, term weighting and thevector spac...</td>\n",
       "      <td>term weighting thevector space modelthus far d...</td>\n",
       "      <td>[0.025751788169145584, -0.0295448899269104, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1357Computing scores in a completesearch syste...</td>\n",
       "      <td>scores completesearch systemchapter developed ...</td>\n",
       "      <td>[0.0235187616199255, -0.0302105862647295, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1518Evaluation in informationretrievalWe have ...</td>\n",
       "      <td>informationretrievalwe seen preceding chapters...</td>\n",
       "      <td>[-0.014001075178384781, -0.005290368106216192,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>1779Relevance feedback and queryexpansionIn mo...</td>\n",
       "      <td>feedback queryexpansionin collections concept ...</td>\n",
       "      <td>[0.016852376982569695, -0.047979291528463364, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>19510XML retrievalInformation retrieval system...</td>\n",
       "      <td>retrievalinformation retrieval systems often c...</td>\n",
       "      <td>[0.006679057143628597, 0.015021566301584244, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>21911Probabilistic informationretrievalDuring ...</td>\n",
       "      <td>informationretrievalduring discussion relevanc...</td>\n",
       "      <td>[-0.017108982428908348, -0.04385941103100777, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>23712Language models for informationretrievalA...</td>\n",
       "      <td>models informationretrievala common suggestion...</td>\n",
       "      <td>[-0.03793055936694145, -0.09445610642433167, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>25313Text classiﬁcation and NaiveBayesThus far...</td>\n",
       "      <td>classiﬁcation naivebayesthus far book mainly d...</td>\n",
       "      <td>[-0.009836948476731777, -0.011432654224336147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>28914Vector space classiﬁcationThe document re...</td>\n",
       "      <td>space classiﬁcationthe document representation...</td>\n",
       "      <td>[-0.023289866745471954, -0.06312847882509232, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>31915Support vector machines andmachine learni...</td>\n",
       "      <td>vector machines andmachine learning documentsi...</td>\n",
       "      <td>[0.013784971088171005, -0.07632390409708023, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>34916Flat clusteringClustering algorithms grou...</td>\n",
       "      <td>clusteringclustering algorithms group set docu...</td>\n",
       "      <td>[0.00901379156857729, -0.059271011501550674, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>37717Hierarchical clusteringFlat clustering is...</td>\n",
       "      <td>clusteringflat clustering efﬁcient conceptuall...</td>\n",
       "      <td>[0.04926469922065735, -0.071595698595047, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>40318Matrix decompositions and latentsemantic ...</td>\n",
       "      <td>decompositions latentsemantic indexingon page ...</td>\n",
       "      <td>[0.003460126230493188, -0.03378783538937569, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>42119Web search basicsIn this and the followin...</td>\n",
       "      <td>search basicsin following two chapters conside...</td>\n",
       "      <td>[-0.033859312534332275, -0.00845685787498951, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>44320Web crawling and indexes20.1OverviewWeb c...</td>\n",
       "      <td>crawling crawling process gather pages web ino...</td>\n",
       "      <td>[-0.07536111772060394, -0.055601801723241806, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...</td>\n",
       "      <td>46121Link analysisThe analysis of hyperlinks a...</td>\n",
       "      <td>analysisthe analysis hyperlinks graph structur...</td>\n",
       "      <td>[-0.018830349668860435, -0.03205515444278717, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  raw  \\\n",
       "0   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "1   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "2   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "3   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "4   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "5   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "6   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "7   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "8   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "9   Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "10  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "11  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "12  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "13  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "14  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "15  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "16  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "17  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "18  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "19  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "20  Online edition (c)\\n2009 Cambridge UP\\nDRAFT! ...   \n",
       "\n",
       "                                   removed_trash_text  \\\n",
       "0   11Boolean retrievalThe meaning of the term inf...   \n",
       "1   192The term vocabulary and postingslistsRecall...   \n",
       "2   493Dictionaries and tolerantretrievalIn Chapte...   \n",
       "3   674Index constructionIn this chapter, we look ...   \n",
       "4   855Index compressionChapter 1 introduced the d...   \n",
       "5   1096Scoring, term weighting and thevector spac...   \n",
       "6   1357Computing scores in a completesearch syste...   \n",
       "7   1518Evaluation in informationretrievalWe have ...   \n",
       "8   1779Relevance feedback and queryexpansionIn mo...   \n",
       "9   19510XML retrievalInformation retrieval system...   \n",
       "10  21911Probabilistic informationretrievalDuring ...   \n",
       "11  23712Language models for informationretrievalA...   \n",
       "12  25313Text classiﬁcation and NaiveBayesThus far...   \n",
       "13  28914Vector space classiﬁcationThe document re...   \n",
       "14  31915Support vector machines andmachine learni...   \n",
       "15  34916Flat clusteringClustering algorithms grou...   \n",
       "16  37717Hierarchical clusteringFlat clustering is...   \n",
       "17  40318Matrix decompositions and latentsemantic ...   \n",
       "18  42119Web search basicsIn this and the followin...   \n",
       "19  44320Web crawling and indexes20.1OverviewWeb c...   \n",
       "20  46121Link analysisThe analysis of hyperlinks a...   \n",
       "\n",
       "                                            processed  \\\n",
       "0   retrievalthe meaning term information retrieva...   \n",
       "1   term vocabulary postingslistsrecall major step...   \n",
       "2   tolerantretrievalin chapters developed ideas u...   \n",
       "3   constructionin chapter look construct inverted...   \n",
       "4   compressionchapter introduced dictionary inver...   \n",
       "5   term weighting thevector space modelthus far d...   \n",
       "6   scores completesearch systemchapter developed ...   \n",
       "7   informationretrievalwe seen preceding chapters...   \n",
       "8   feedback queryexpansionin collections concept ...   \n",
       "9   retrievalinformation retrieval systems often c...   \n",
       "10  informationretrievalduring discussion relevanc...   \n",
       "11  models informationretrievala common suggestion...   \n",
       "12  classiﬁcation naivebayesthus far book mainly d...   \n",
       "13  space classiﬁcationthe document representation...   \n",
       "14  vector machines andmachine learning documentsi...   \n",
       "15  clusteringclustering algorithms group set docu...   \n",
       "16  clusteringflat clustering efﬁcient conceptuall...   \n",
       "17  decompositions latentsemantic indexingon page ...   \n",
       "18  search basicsin following two chapters conside...   \n",
       "19  crawling crawling process gather pages web ino...   \n",
       "20  analysisthe analysis hyperlinks graph structur...   \n",
       "\n",
       "                                           embeddings  \n",
       "0   [0.003079628339037299, -0.009766640141606331, ...  \n",
       "1   [-0.025380702689290047, -0.06649737060070038, ...  \n",
       "2   [0.039726272225379944, -0.036738041788339615, ...  \n",
       "3   [0.0004133421170990914, -0.02456202358007431, ...  \n",
       "4   [-0.01911129243671894, 0.01577311009168625, -0...  \n",
       "5   [0.025751788169145584, -0.0295448899269104, -0...  \n",
       "6   [0.0235187616199255, -0.0302105862647295, -0.0...  \n",
       "7   [-0.014001075178384781, -0.005290368106216192,...  \n",
       "8   [0.016852376982569695, -0.047979291528463364, ...  \n",
       "9   [0.006679057143628597, 0.015021566301584244, -...  \n",
       "10  [-0.017108982428908348, -0.04385941103100777, ...  \n",
       "11  [-0.03793055936694145, -0.09445610642433167, -...  \n",
       "12  [-0.009836948476731777, -0.011432654224336147,...  \n",
       "13  [-0.023289866745471954, -0.06312847882509232, ...  \n",
       "14  [0.013784971088171005, -0.07632390409708023, 0...  \n",
       "15  [0.00901379156857729, -0.059271011501550674, -...  \n",
       "16  [0.04926469922065735, -0.071595698595047, -0.0...  \n",
       "17  [0.003460126230493188, -0.03378783538937569, -...  \n",
       "18  [-0.033859312534332275, -0.00845685787498951, ...  \n",
       "19  [-0.07536111772060394, -0.055601801723241806, ...  \n",
       "20  [-0.018830349668860435, -0.03205515444278717, ...  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d78acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "distances, indices = index.search(query_embedding, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426091f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  8, 19, 13,  0]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c9ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['40318Matrix decompositions and latentsemantic indexingOn page 123 we introduced the notion of a term-document matrix: an M × Nmatrix C, each of whose rows represents a term and each of whose columnsrepresents a document in the collection. Even for a collection of modest size,the term-document matrix C is likely to have several tens of thousands ofrows and columns. In Section 18.1.1 we ﬁrst develop a class of operationsfrom linear algebra, known as matrix decomposition. In Section 18.2 we use aspecial form of matrix decomposition to construct a low-rank approximationto the term-document matrix. In Section 18.3 we examine the applicationof such low-rank approximations to indexing and retrieving documents, atechnique referred to as latent semantic indexing. While latent semantic in-dexing has not been established as a signiﬁcant force in scoring and rankingfor information retrieval, it remains an intriguing approach to clustering in anumber of domains including for collections of text documents (Section 16.6,page 372). Understanding its full potential remains an area of active research.Readers who do not require a refresher on linear algebra may skip Sec-tion 18.1, although Example 18.1 is especially recommended as it highlightsa property of eigenvalues that we exploit later in the chapter.18.1Linear algebra reviewWe brieﬂy review some necessary background in linear algebra. Let C bean M × N matrix with real-valued entries; for a term-document matrix, allentries are in fact non-negative. The rank of a matrix is the number of linearlyRANKindependent rows (or columns) in it; thus, rank(C) ≤min{M, N}. A squarer × r matrix all of whose off-diagonal entries are zero is called a diagonalmatrix; its rank is equal to the number of non-zero diagonal entries. If allr diagonal entries of such a diagonal matrix are 1, it is called the identitymatrix of dimension r and represented by Ir.For a square M × M matrix C and a vector ⃗x that is not all zeros, the values 40418Matrix decompositions and latent semantic indexingof λ satisfyingC⃗x = λ⃗x(18.1)are called the eigenvalues of C . The N-vector ⃗x satisfying Equation (18.1)EIGENVALUEfor an eigenvalue λ is the corresponding right eigenvector. The eigenvectorcorresponding to the eigenvalue of largest magnitude is called the principaleigenvector. In a similar fashion, the left eigenvectors of C are the M-vectors ysuch that⃗yT C = λ⃗yT.(18.2)The number of non-zero eigenvalues of C is at most rank(C).The eigenvalues of a matrix are found by solving the characteristic equation,which is obtained by rewriting Equation (18.1) in the form (C −λIM)⃗x = 0.The eigenvalues of C are then the solutions of |(C −λIM)| = 0, where |S|denotes the determinant of a square matrix S. The equation |(C −λIM)| = 0is an Mth order polynomial equation in λ and can have at most M roots,which are the eigenvalues of C. These eigenvalues can in general be complex,even if all entries of C are real.We now examine some further properties of eigenvalues and eigenvectors,to set up the central idea of singular value decompositions in Section 18.2 be-low. First, we look at the relationship between matrix-vector multiplicationand eigenvalues.\\x0fExample 18.1:Consider the matrixS =\\uf8eb\\uf8ed30000200001\\uf8f6\\uf8f8.Clearly the matrix has rank 3, and has 3 non-zero eigenvalues λ1 = 30, λ2 = 20 andλ3 = 1, with the three corresponding eigenvectors⃗x1 =\\uf8eb\\uf8ed100\\uf8f6\\uf8f8, ⃗x2 =\\uf8eb\\uf8ed010\\uf8f6\\uf8f8and ⃗x3 =\\uf8eb\\uf8ed001\\uf8f6\\uf8f8.For each of the eigenvectors, multiplication by S acts as if we were multiplying theeigenvector by a multiple of the identity matrix; the multiple is different for eacheigenvector. Now, consider an arbitrary vector, such as ⃗v =\\uf8eb\\uf8ed246\\uf8f6\\uf8f8. We can alwaysexpress⃗v as a linear combination of the three eigenvectors of S; in the current examplewe have⃗v =\\uf8eb\\uf8ed246\\uf8f6\\uf8f8= 2⃗x1 + 4⃗x2 + 6⃗x3. 18.1Linear algebra review405Suppose we multiply⃗v by S:S⃗v=S(2⃗x1 + 4⃗x2 + 6⃗x3)=2S⃗x1 + 4S⃗x2 + 6S⃗x3=2λ1⃗x1 + 4λ2⃗x2 + 6λ3⃗x3=60⃗x1 + 80⃗x2 + 6⃗x3.(18.3)Example 18.1 shows that even though ⃗v is an arbitrary vector, the effect ofmultiplication by S is determined by the eigenvalues and eigenvectors of S.Furthermore, it is intuitively apparent from Equation (18.3) that the productS⃗v is relatively unaffected by terms arising from the small eigenvalues of S;in our example, since λ3 = 1, the contribution of the third term on the righthand side of Equation (18.3) is small. In fact, if we were to completely ignorethe contribution in Equation (18.3) from the third eigenvector correspondingto λ3 = 1, then the product S⃗v would be computed to be\\uf8eb\\uf8ed60800\\uf8f6\\uf8f8rather thanthe correct product which is\\uf8eb\\uf8ed60806\\uf8f6\\uf8f8; these two vectors are relatively closeto each other by any of various metrics one could apply (such as the lengthof their vector difference).This suggests that the effect of small eigenvalues (and their eigenvectors)on a matrix-vector product is small. We will carry forward this intuitionwhen studying matrix decompositions and low-rank approximations in Sec-tion 18.2. Before doing so, we examine the eigenvectors and eigenvalues ofspecial forms of matrices that will be of particular interest to us.For a symmetric matrix S, the eigenvectors corresponding to distinct eigen-values are orthogonal. Further, if S is both real and symmetric, the eigenvaluesare all real.\\x0fExample 18.2:Consider the real, symmetric matrixS =\\x12 2112\\x13.(18.4)From the characteristic equation |S −λI| = 0, we have the quadratic (2 −λ)2 −1 =0, whose solutions yield the eigenvalues 3 and 1. The corresponding eigenvectors\\x121−1\\x13and\\x12 11\\x13are orthogonal. 40618Matrix decompositions and latent semantic indexing18.1.1Matrix decompositionsIn this section we examine ways in which a square matrix can be factoredinto the product of matrices derived from its eigenvectors; we refer to thisprocess as matrix decomposition. Matrix decompositions similar to the onesMATRIXDECOMPOSITIONin this section will form the basis of our principal text-analysis techniquein Section 18.3, where we will look at decompositions of non-square term-document matrices. The square decompositions in this section are simplerand can be treated with sufﬁcient mathematical rigor to help the reader un-derstand how such decompositions work. The detailed mathematical deriva-tion of the more complex decompositions in Section 18.2 are beyond thescope of this book.We begin by giving two theorems on the decomposition of a square ma-trix into the product of three matrices of a special form. The ﬁrst of these,Theorem 18.1, gives the basic factorization of a square real-valued matrixinto three factors. The second, Theorem 18.2, applies to square symmetricmatrices and is the basis of the singular value decomposition described inTheorem 18.3.Theorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valuedM × M matrix with M linearly independent eigenvectors. Then there exists aneigen decompositionEIGEN DECOMPOSITIONS = UΛU−1,(18.5)where the columns of U are the eigenvectors of S and Λ is a diagonal matrix whosediagonal entries are the eigenvalues of S in decreasing order\\uf8eb\\uf8ec\\uf8ec\\uf8edλ1λ2· · ·λM\\uf8f6\\uf8f7\\uf8f7\\uf8f8, λi ≥λi+1.(18.6)If the eigenvalues are distinct, then this decomposition is unique.To understand how Theorem 18.1 works, we note that U has the eigenvec-tors of S as columnsU = (⃗u1 ⃗u2 · · · ⃗uM) .(18.7)Then we haveSU=S (⃗u1 ⃗u2 · · · ⃗uM)=(λ1⃗u1 λ2⃗u2 · · · λM ⃗uM)=(⃗u1 ⃗u2 · · · ⃗uM)\\uf8eb\\uf8ec\\uf8ec\\uf8edλ1λ2· · ·λM\\uf8f6\\uf8f7\\uf8f7\\uf8f8. 18.2Term-document matrices and singular value decompositions407Thus, we have SU = UΛ, or S = UΛU−1.We next state a closely related decomposition of a symmetric square matrixinto the product of matrices derived from its eigenvectors. This will pave theway for the development of our main tool for text analysis, the singular valuedecomposition (Section 18.2).Theorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-metric real-valued M × M matrix with M linearly independent eigenvectors. Thenthere exists a symmetric diagonal decompositionSYMMETRIC DIAGONALDECOMPOSITIONS = QΛQT,(18.8)where the columns of Q are the orthogonal and normalized (unit length, real) eigen-vectors of S, and Λ is the diagonal matrix whose entries are the eigenvalues of S.Further, all entries of Q are real and we have Q−1 = QT.We will build on this symmetric diagonal decomposition to build low-rankapproximations to term-document matrices.?Exercise 18.1What is the rank of the 3 × 3 diagonal matrix below?\\uf8eb\\uf8ed110011121\\uf8f6\\uf8f8Exercise 18.2Show that λ = 2 is an eigenvalue ofC =\\x12 6−240\\x13.Find the corresponding eigenvector.Exercise 18.3Compute the unique eigen decomposition of the 2 × 2 matrix in (18.4).18.2Term-document matrices and singular value decompositionsThe decompositions we have been studying thus far apply to square matri-ces. However, the matrix we are interested in is the M × N term-documentmatrix C where (barring a rare coincidence) M ̸= N; furthermore, C is veryunlikely to be symmetric. To this end we ﬁrst describe an extension of thesymmetric diagonal decomposition known as the singular value decomposi-SINGULAR VALUEDECOMPOSITIONtion. We then show in Section 18.3 how this can be used to construct an ap-proximate version of C. It is beyond the scope of this book to develop a full 40818Matrix decompositions and latent semantic indexingtreatment of the mathematics underlying singular value decompositions; fol-lowing the statement of Theorem 18.3 we relate the singular value decompo-sition to the symmetric diagonal decompositions from Section 18.1.1. GivenSYMMETRIC DIAGONALDECOMPOSITIONC, let U be the M × M matrix whose columns are the orthogonal eigenvec-tors of CCT, and V be the N × N matrix whose columns are the orthogonaleigenvectors of CTC. Denote by CT the transpose of a matrix C.Theorem 18.3. Let r be the rank of the M × N matrix C. Then, there is a singular-value decomposition (SVD for short) of C of the formSVDC = UΣVT,(18.9)where1. The eigenvalues λ1, . . . , λr of CCT are the same as the eigenvalues of CTC;2. For 1 ≤i ≤r, let σi = √λi, with λi ≥λi+1. Then the M × N matrix Σ iscomposed by setting Σii = σi for 1 ≤i ≤r, and zero otherwise.The values σi are referred to as the singular values of C. It is instructive toexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this ratherthan derive the general proof of Theorem 18.3, which is beyond the scope ofthis book.By multiplying Equation (18.9) by its transposed version, we haveCCT = UΣVT VΣUT = UΣ2UT.(18.10)Note now that in Equation (18.10), the left-hand side is a square symmetricmatrix real-valued matrix, and the right-hand side represents its symmetricdiagonal decomposition as in Theorem 18.2. What does the left-hand sideCCT represent? It is a square matrix with a row and a column correspond-ing to each of the M terms. The entry (i, j) in the matrix is a measure of theoverlap between the ith and jth terms, based on their co-occurrence in docu-ments. The precise mathematical meaning depends on the manner in whichC is constructed based on term weighting. Consider the case where C is theterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then theentry (i, j) in CCT is the number of documents in which both term i and termj occur.When writing down the numerical values of the SVD, it is conventionalto represent Σ as an r × r matrix with the singular values on the diagonals,since all its entries outside this sub-matrix are zeros. Accordingly, it is con-ventional to omit the rightmost M −r columns of U corresponding to theseomitted rows of Σ; likewise the rightmost N −r columns of V are omittedsince they correspond in VT to the rows that will be multiplied by the N −rcolumns of zeros in Σ. This written form of the SVD is sometimes known 18.2Term-document matrices and singular value decompositions409rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrC=UΣVTrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr◮Figure 18.1Illustration of the singular-value decomposition. In this schematicillustration of (18.9), we see two cases illustrated. In the top half of the ﬁgure, wehave a matrix C for which M > N. The lower half illustrates the case M < N.as the reduced SVD or truncated SVD and we will encounter it again in Ex-REDUCED SVDTRUNCATED SVDercise 18.9. Henceforth, our numerical examples and exercises will use thisreduced form.\\x0fExample 18.3:We now illustrate the singular-value decomposition of a 4 × 2 ma-trix of rank 2; the singular values are Σ11 = 2.236 and Σ22 = 1.C =\\uf8eb\\uf8ec\\uf8ec\\uf8ed1−10110−11\\uf8f6\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\uf8ec\\uf8ec\\uf8ed−0.6320.0000.316−0.707−0.316−0.7070.6320.000\\uf8f6\\uf8f7\\uf8f7\\uf8f8\\x122.2360.0000.0001.000\\x13 \\x12 −0.7070.707−0.707−0.707\\x13.(18.11)As with the matrix decompositions deﬁned in Section 18.1.1, the singu-lar value decomposition of a matrix can be computed by a variety of algo-rithms, many of which have been publicly available software implementa-tions; pointers to these are given in Section 18.5.?Exercise 18.4LetC =\\uf8eb\\uf8ed110110\\uf8f6\\uf8f8(18.12)be the term-document incidence matrix for a collection. Compute the co-occurrencematrix CCT. What is the interpretation of the diagonal entries of CCT when C is aterm-document incidence matrix? 41018Matrix decompositions and latent semantic indexingExercise 18.5Verify that the SVD of the matrix in Equation (18.12) isU =\\uf8eb\\uf8ed−0.8160.000−0.408−0.707−0.4080.707\\uf8f6\\uf8f8, Σ =\\x12 1.7320.0000.0001.000\\x13and VT =\\x12 −0.707−0.7070.707−0.707\\x13,(18.13)by verifying all of the properties in the statement of Theorem 18.3.Exercise 18.6Suppose that C is a binary term-document incidence matrix. What do the entries ofCTC represent?Exercise 18.7LetC =\\uf8eb\\uf8ed021030210\\uf8f6\\uf8f8(18.14)be a term-document matrix whose entries are term frequencies; thus term 1 occurs 2times in document 2 and once in document 3. Compute CCT; observe that its entriesare largest where two terms have their most frequent occurrences together in the samedocument.18.3Low-rank approximationsWe next state a matrix approximation problem that at ﬁrst seems to havelittle to do with information retrieval. We describe a solution to this matrixproblem using singular-value decompositions, then develop its applicationto information retrieval.Given an M × N matrix C and a positive integer k, we wish to ﬁnd anM × N matrix Ck of rank at most k, so as to minimize the Frobenius norm ofFROBENIUS NORMthe matrix difference X = C −Ck, deﬁned to be∥X∥F =vuutM∑i=1N∑j=1X2ij.(18.15)Thus, the Frobenius norm of X measures the discrepancy between Ck and C;our goal is to ﬁnd a matrix Ck that minimizes this discrepancy, while con-straining Ck to have rank at most k. If r is the rank of C, clearly Cr = Cand the Frobenius norm of the discrepancy is zero in this case. When k is farsmaller than r, we refer to Ck as a low-rank approximation.LOW-RANKAPPROXIMATIONThe singular value decomposition can be used to solve the low-rank ma-trix approximation problem. We then derive from it an application to ap-proximating term-document matrices. We invoke the following three-stepprocedure to this end: 18.3Low-rank approximations411Ck=UΣkVTrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr◮Figure 18.2Illustration of low rank approximation using the singular-value de-composition. The dashed boxes indicate the matrix entries affected by “zeroing out”the smallest singular values.1. Given C, construct its SVD in the form shown in (18.9); thus, C = UΣVT.2. Derive from Σ the matrix Σk formed by replacing by zeros the r −k small-est singular values on the diagonal of Σ.3. Compute and output Ck = UΣkVT as the rank-k approximation to C.The rank of Ck is at most k: this follows from the fact that Σk has at mostk non-zero values. Next, we recall the intuition of Example 18.1: the effectof small eigenvalues on matrix products is small. Thus, it seems plausiblethat replacing these small eigenvalues by zero will not substantially alter theproduct, leaving it “close” to C. The following theorem due to Eckart andYoung tells us that, in fact, this procedure yields the matrix of rank k withthe lowest possible Frobenius error.Theorem 18.4.minZ| rank(Z)=k∥C −Z∥F = ∥C −Ck∥F = σk+1.(18.16)Recalling that the singular values are in decreasing order σ1 ≥σ2 ≥· · ·,we learn from Theorem 18.4 that Ck is the best rank-k approximation to C,incurring an error (measured by the Frobenius norm of C −Ck) equal to σk+1.Thus the larger k is, the smaller this error (and in particular, for k = r, theerror is zero since Σr = Σ; provided r < M, N, then σr+1 = 0 and thusCr = C).To derive further insight into why the process of truncating the smallestr −k singular values in Σ helps generate a rank-k approximation of low error,we examine the form of Ck:Ck=UΣkVT(18.17) 41218Matrix decompositions and latent semantic indexing=U\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edσ100000· · ·00000σk00000000000· · ·\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8VT(18.18)=k∑i=1σi⃗ui⃗vTi ,(18.19)where ⃗ui and ⃗vi are the ith columns of U and V, respectively. Thus, ⃗ui⃗vTi isa rank-1 matrix, so that we have just expressed Ck as the sum of k rank-1matrices each weighted by a singular value. As i increases, the contributionof the rank-1 matrix ⃗ui⃗vTi is weighted by a sequence of shrinking singularvalues σi.?Exercise 18.8Compute a rank 1 approximation C1 to the matrix C in Example 18.12, using the SVDas in Exercise 18.13. What is the Frobenius norm of the error of this approximation?Exercise 18.9Consider now the computation in Exercise 18.8.Following the schematic in Fig-ure 18.2, notice that for a rank 1 approximation we have σ1 being a scalar. Denoteby U1 the ﬁrst column of U and by V1 the ﬁrst column of V. Show that the rank-1approximation to C can then be written as U1σ1VT1 = σ1U1VT1 .Exercise 18.10Exercise 18.9 can be generalized to rank k approximations: we let U′k and V′k denotethe “reduced” matrices formed by retaining only the ﬁrst k columns of U and V,respectively. Thus U′k is an M × k matrix while V′Tk is a k × N matrix. Then, we haveCk = U′kΣ′kV′Tk ,(18.20)where Σ′k is the square k × k submatrix of Σk with the singular values σ1, . . . , σk onthe diagonal. The primary advantage of using (18.20) is to eliminate a lot of redun-dant columns of zeros in U and V, thereby explicitly eliminating multiplication bycolumns that do not affect the low-rank approximation; this version of the SVD issometimes known as the reduced SVD or truncated SVD and is a computationallysimpler representation from which to compute the low rank approximation.For the matrix C in Example 18.3, write down both Σ2 and Σ′2.18.4Latent semantic indexingWe now discuss the approximation of a term-document matrix C by one oflower rank using the SVD. The low-rank approximation to C yields a newrepresentation for each document in the collection.We will cast queries 18.4Latent semantic indexing413into this low-rank representation as well, enabling us to compute query-document similarity scores in this low-rank representation. This process isknown as latent semantic indexing (generally abbreviated LSI).LATENT SEMANTICINDEXINGBut ﬁrst, we motivate such an approximation. Recall the vector space rep-resentation of documents and queries introduced in Section 6.3 (page 120).This vector space representation enjoys a number of advantages includingthe uniform treatment of queries and documents as vectors, the inducedscore computation based on cosine similarity, the ability to weight differ-ent terms differently, and its extension beyond document retrieval to suchapplications as clustering and classiﬁcation. The vector space representa-tion suffers, however, from its inability to cope with two classic problemsarising in natural languages: synonymy and polysemy. Synonymy refers to acase where two different words (say car and automobile) have the same mean-ing. Because the vector space representation fails to capture the relationshipbetween synonymous terms such as car and automobile – according each aseparate dimension in the vector space. Consequently the computed simi-larity⃗q · ⃗d between a query⃗q (say, car) and a document ⃗d containing both carand automobile underestimates the true similarity that a user would perceive.Polysemy on the other hand refers to the case where a term such as chargehas multiple meanings, so that the computed similarity ⃗q · ⃗d overestimatesthe similarity that a user would perceive. Could we use the co-occurrencesof terms (whether, for instance, charge occurs in a document containing steedversus in a document containing electron) to capture the latent semantic as-sociations of terms and alleviate these problems?Even for a collection of modest size, the term-document matrix C is likelyto have several tens of thousand of rows and columns, and a rank in thetens of thousands as well. In latent semantic indexing (sometimes referredto as latent semantic analysis (LSA)), we use the SVD to construct a low-rankLSAapproximation Ck to the term-document matrix, for a value of k that is farsmaller than the original rank of C. In the experimental work cited laterin this section, k is generally chosen to be in the low hundreds. We thusmap each row/column (respectively corresponding to a term/document) toa k-dimensional space; this space is deﬁned by the k principal eigenvectors(corresponding to the largest eigenvalues) of CCT and CTC. Note that thematrix Ck is itself still an M × N matrix, irrespective of k.Next, we use the new k-dimensional LSI representation as we did the orig-inal representation – to compute similarities between vectors. A query vector⃗q is mapped into its representation in the LSI space by the transformation⃗qk = Σ−1k UTk ⃗q.(18.21)Now, we may use cosine similarities as in Section 6.3.1 (page 120) to com-pute the similarity between a query and a document, between two docu- 41418Matrix decompositions and latent semantic indexingments, or between two terms. Note especially that Equation (18.21) does notin any way depend on ⃗q being a query; it is simply a vector in the space ofterms. This means that if we have an LSI representation of a collection ofdocuments, a new document not in the collection can be “folded in” to thisrepresentation using Equation (18.21). This allows us to incrementally adddocuments to an LSI representation. Of course, such incremental additionfails to capture the co-occurrences of the newly added documents (and evenignores any new terms they contain). As such, the quality of the LSI rep-resentation will degrade as more documents are added and will eventuallyrequire a recomputation of the LSI representation.The ﬁdelity of the approximation of Ck to C leads us to hope that the rel-ative values of cosine similarities are preserved: if a query is close to a doc-ument in the original space, it remains relatively close in the k-dimensionalspace. But this in itself is not sufﬁciently interesting, especially given thatthe sparse query vector ⃗q turns into a dense query vector ⃗qk in the low-dimensional space. This has a signiﬁcant computational cost, when com-pared with the cost of processing⃗q in its native form.\\x0fExample 18.4:Consider the term-document matrix C =d1d2d3d4d5d6ship101000boat010000ocean110000voyage100110trip000101Its singular value decomposition is the product of three matrices as below. First wehave U which in this example is:12345ship−0.44−0.300.570.580.25boat−0.13−0.33−0.590.000.73ocean−0.48−0.51−0.370.00−0.61voyage−0.700.350.15−0.580.16trip−0.260.65−0.410.58−0.09When applying the SVD to a term-document matrix, U is known as the SVD termmatrix. The singular values are Σ =2.160.000.000.000.000.001.590.000.000.000.000.001.280.000.000.000.000.001.000.000.000.000.000.000.39Finally we have VT, which in the context of a term-document matrix is known asthe SVD document matrix: 18.4Latent semantic indexing415d1d2d3d4d5d61−0.75−0.28−0.20−0.45−0.33−0.122−0.29−0.53−0.190.630.220.4130.28−0.750.45−0.200.12−0.3340.000.000.580.00−0.580.585−0.530.290.630.190.41−0.22By “zeroing out” all but the two largest singular values of Σ, we obtain Σ2 =2.160.000.000.000.000.001.590.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00From this, we compute C2 =d1d2d3d4d5d61−1.62−0.60−0.44−0.97−0.70−0.262−0.46−0.84−0.301.000.350.6530.000.000.000.000.000.0040.000.000.000.000.000.0050.000.000.000.000.000.00Notice that the low-rank approximation, unlike the original matrix C, can havenegative entries.Examination of C2 and Σ2 in Example 18.4 shows that the last 3 rows ofeach of these matrices are populated entirely by zeros. This suggests thatthe SVD product UΣVT in Equation (18.18) can be carried out with only tworows in the representations of Σ2 and VT; we may then replace these matricesby their truncated versions Σ′2 and (V′)T. For instance, the truncated SVDdocument matrix (V′)T in this example is:d1d2d3d4d5d61−1.62−0.60−0.44−0.97−0.70−0.262−0.46−0.84−0.301.000.350.65Figure 18.3 illustrates the documents in (V′)T in two dimensions. Notealso that C2 is dense relative to C.We may in general view the low-rank approximation of C by Ck as a con-strained optimization problem: subject to the constraint that Ck have rank atmost k, we seek a representation of the terms and documents comprising Cwith low Frobenius norm for the error C −Ck. When forced to squeeze theterms/documents down to a k-dimensional space, the SVD should bring to-gether terms with similar co-occurrences. This intuition suggests, then, thatnot only should retrieval quality not suffer too much from the dimensionreduction, but in fact may improve. 41618Matrix decompositions and latent semantic indexing−0.5−1.0−1.50.51.0−0.5−1.0dim 2dim 1×d1×d2× d3×d4×d5× d6◮Figure 18.3The documents of Example 18.4 reduced to two dimensions in (V′)T.Dumais (1993) and Dumais (1995) conducted experiments with LSI onTREC documents and tasks, using the commonly-used Lanczos algorithmto compute the SVD. At the time of their work in the early 1990’s, the LSIcomputation on tens of thousands of documents took approximately a dayon one machine. On these experiments, they achieved precision at or abovethat of the median TREC participant. On about 20% of TREC topics theirsystem was the top scorer, and reportedly slightly better on average thanstandard vector spaces for LSI at about 350 dimensions. Here are some con-clusions on LSI ﬁrst suggested by their work, and subsequently veriﬁed bymany other experiments.• The computational cost of the SVD is signiﬁcant; at the time of this writ-ing, we know of no successful experiment with over one million docu-ments. This has been the biggest obstacle to the widespread adoption toLSI. One approach to this obstacle is to build the LSI representation on arandomly sampled subset of the documents in the collection, followingwhich the remaining documents are “folded in” as detailed with Equa-tion (18.21). 18.5References and further reading417• As we reduce k, recall tends to increase, as expected.• Most surprisingly, a value of k in the low hundreds can actually increaseprecision on some query benchmarks. This appears to suggest that for asuitable value of k, LSI addresses some of the challenges of synonymy.• LSI works best in applications where there is little overlap between queriesand documents.The experiments also documented some modes where LSI failed to matchthe effectiveness of more traditional indexes and score computations. Mostnotably (and perhaps obviously), LSI shares two basic drawbacks of vectorspace retrieval: there is no good way of expressing negations (ﬁnd docu-ments that contain german but not shepherd), and no way of enforcing Booleanconditions.LSI can be viewed as soft clustering by interpreting each dimension of theSOFT CLUSTERINGreduced space as a cluster and the value that a document has on that dimen-sion as its fractional membership in that cluster.18.5References and further readingStrang (1986) provides an excellent introductory overview of matrix decom-positions including the singular value decomposition. Theorem 18.4 is dueto Eckart and Young (1936). The connection between information retrievaland low-rank approximations of the term-document matrix was introducedin Deerwester et al. (1990), with a subsequent survey of results in Berryet al. (1995). Dumais (1993) and Dumais (1995) describe experiments onTREC benchmarks giving evidence that at least on some benchmarks, LSIcan produce better precision and recall than standard vector-space retrieval.http://www.cs.utk.edu/˜berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.htmloffer comprehensive pointers to the literature and software of LSI. Schützeand Silverstein (1997) evaluate LSI and truncated representations of cen-troids for efﬁcient K-means clustering (Section 16.4). Bast and Majumdar(2005) detail the role of the reduced dimension k in LSI and how differentpairs of terms get coalesced together at differing values of k. Applications ofLSI to cross-language information retrieval (where documents in two or moreCROSS-LANGUAGEINFORMATIONRETRIEVALdifferent languages are indexed, and a query posed in one language is ex-pected to retrieve documents in other languages) are developed in Berry andYoung (1995) and Littman et al. (1998). LSI (referred to as LSA in more gen-eral settings) has been applied to host of other problems in computer scienceranging from memory modeling to computer vision.Hofmann (1999a;b) provides an initial probabilistic extension of the basiclatent semantic indexing technique. A more satisfactory formal basis for a 41818Matrix decompositions and latent semantic indexingDocIDDocument text1hello2open house3mi casa4hola Profesor5hola y bienvenido6hello and welcome◮Figure 18.4Documents for Exercise 18.11.SpanishEnglishmimycasahouseholahelloprofesorprofessoryandbienvenidowelcome◮Figure 18.5Glossary for Exercise 18.11.probabilistic latent variable model for dimensionality reduction is the LatentDirichlet Allocation (LDA) model (Blei et al. 2003), which is generative andassigns probabilities to documents outside of the training set. This model isextended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft(2006) present the ﬁrst large scale evaluation of LDA, ﬁnding it to signiﬁ-cantly outperform the query likelihood model of Section 12.2 (page 242), butto not perform quite as well as the relevance model mentioned in Section 12.4(page 250) – but the latter does additional per-query processing unlike LDA.Teh et al. (2006) generalize further by presenting Hierarchical Dirichlet Pro-cesses, a probabilistic model which allows a group (for us, a document) tobe drawn from an inﬁnite mixture of latent topics, while still allowing thesetopics to be shared across documents.?Exercise 18.11Assume you have a set of documents each of which is in either English or in Spanish.The collection is given in Figure 18.4.Figure 18.5 gives a glossary relating the Spanish and English words above for yourown information. This glossary is NOT available to the retrieval system:1. Construct the appropriate term-document matrix C to use for a collection con-sisting of these documents. For simplicity, use raw term frequencies rather thannormalized tf-idf weights. Make sure to clearly label the dimensions of your ma-trix. 18.5References and further reading4192. Write down the matrices U2, Σ′2 and V2 and from these derive the rank 2 approxi-mation C2.3. State succinctly what the (i, j) entry in the matrix CTC represents.4. State succinctly what the (i, j) entry in the matrix CT2 C2 represents, and why itdiffers from that in CTC. ', '1779Relevance feedback and queryexpansionIn most collections, the same concept may be referred to using differentwords. This issue, known as synonymy, has an impact on the recall of mostSYNONYMYinformation retrieval systems. For example, you would want a search foraircraft to match plane (but only for references to an airplane, not a woodwork-ing plane), and for a search on thermodynamics to match references to heat inappropriate discussions. Users often attempt to address this problem them-selves by manually reﬁning a query, as was discussed in Section 1.4; in thischapter we discuss ways in which a system can help with query reﬁnement,either fully automatically or with the user in the loop.The methods for tackling this problem split into two major classes: globalmethods and local methods. Global methods are techniques for expandingor reformulating query terms independent of the query and results returnedfrom it, so that changes in the query wording will cause the new query tomatch other semantically similar terms. Global methods include:• Query expansion/reformulation with a thesaurus or WordNet (Section 9.2.2)• Query expansion via automatic thesaurus generation (Section 9.2.3)• Techniques like spelling correction (discussed in Chapter 3)Local methods adjust a query relative to the documents that initially appearto match the query. The basic methods here are:• Relevance feedback (Section 9.1)• Pseudo relevance feedback, also known as Blind relevance feedback (Sec-tion 9.1.6)• (Global) indirect relevance feedback (Section 9.1.7)In this chapter, we will mention all of these approaches, but we will concen-trate on relevance feedback, which is one of the most used and most success-ful approaches. 1789Relevance feedback and query expansion9.1Relevance feedback and pseudo relevance feedbackThe idea of relevance feedback (RF) is to involve the user in the retrieval processRELEVANCE FEEDBACKso as to improve the ﬁnal result set. In particular, the user gives feedback onthe relevance of documents in an initial set of results. The basic procedure is:• The user issues a (short, simple) query.• The system returns an initial set of retrieval results.• The user marks some returned documents as relevant or nonrelevant.• The system computes a better representation of the information need basedon the user feedback.• The system displays a revised set of retrieval results.Relevance feedback can go through one or more iterations of this sort. Theprocess exploits the idea that it may be difﬁcult to formulate a good querywhen you don’t know the collection well, but it is easy to judge particulardocuments, and so it makes sense to engage in iterative query reﬁnementof this sort. In such a scenario, relevance feedback can also be effective intracking a user’s evolving information need: seeing some documents maylead users to reﬁne their understanding of the information they are seeking.Image search provides a good example of relevance feedback. Not only isit easy to see the results at work, but this is a domain where a user can easilyhave difﬁculty formulating what they want in words, but can easily indicaterelevant or nonrelevant images. After the user enters an initial query for bikeon the demonstration system at:http://nayana.ece.ucsb.edu/imsearch/imsearch.htmlthe initial results (in this case, images) are returned. In Figure 9.1 (a), theuser has selected some of them as relevant. These will be used to reﬁne thequery, while other displayed results have no effect on the reformulation. Fig-ure 9.1 (b) then shows the new top-ranked results calculated after this roundof relevance feedback.Figure 9.2 shows a textual IR example where the user wishes to ﬁnd outabout new applications of space satellites.9.1.1The Rocchio algorithm for relevance feedbackThe Rocchio Algorithm is the classic algorithm for implementing relevancefeedback. It models a way of incorporating relevance feedback informationinto the vector space model of Section 6.3. 9.1Relevance feedback and pseudo relevance feedback179(a)(b)◮Figure 9.1Relevance feedback searching over images. (a) The user views theinitial query results for a query of bike, selects the ﬁrst, third and fourth result inthe top row and the fourth result in the bottom row as relevant, and submits thisfeedback. (b) The users sees the revised result set. Precision is greatly improved.From http://nayana.ece.ucsb.edu/imsearch/imsearch.html (Newsam et al. 2001). 1809Relevance feedback and query expansion(a)Query: New space satellite applications(b)+1. 0.539, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer+2. 0.533, 07/09/91, NASA Scratches Environment Gear From Satel-lite Plan3. 0.528, 04/04/90, Science Panel Backs NASA Satellite Plan, ButUrges Launches of Smaller Probes4. 0.526, 09/09/91, A NASA Satellite Project Accomplishes Incredi-ble Feat: Staying Within Budget5. 0.525, 07/24/90, Scientist Who Exposed Global Warming Pro-poses Satellites for Climate Research6. 0.524, 08/22/90, Report Provides Support for the Critics Of UsingBig Satellites to Study Climate7.0.516, 04/13/87, Arianespace Receives Satellite Launch PactFrom Telesat Canada+8. 0.509, 12/02/87, Telecommunications Tale of Two Companies(c)2.074 new15.106 space30.816 satellite5.660 application5.991 nasa5.196 eos4.196 launch3.972 aster3.516 instrument3.446 arianespace3.004 bundespost2.806 ss2.790 rocket2.053 scientist2.003 broadcast1.172 earth0.836 oil0.646 measure(d)*1. 0.513, 07/09/91, NASA Scratches Environment Gear From Satel-lite Plan*2. 0.500, 08/13/91, NASA Hasn’t Scrapped Imaging Spectrometer3. 0.493, 08/07/89, When the Pentagon Launches a Secret Satellite,Space Sleuths Do Some Spy Work of Their Own4. 0.493, 07/31/89, NASA Uses ‘Warm’ Superconductors For FastCircuit*5. 0.492, 12/02/87, Telecommunications Tale of Two Companies6. 0.491, 07/09/91, Soviets May Adapt Parts of SS-20 Missile ForCommercial Use7. 0.490, 07/12/88, Gaping Gap: Pentagon Lags in Race To Matchthe Soviets In Rocket Launchers8. 0.490, 06/14/90, Rescue of Satellite By Space Agency To Cost $90Million◮Figure 9.2Example of relevance feedback on a text collection. (a) The initial query(a). (b) The user marks some relevant documents (shown with a plus sign). (c) Thequery is then expanded by 18 terms with weights as shown. (d) The revised topresults are then shown. A * marks the documents which were judged relevant in therelevance feedback phase. 9.1Relevance feedback and pseudo relevance feedback181◮Figure 9.3The Rocchio optimal query for separating relevant and nonrelevantdocuments.The underlying theory.We want to ﬁnd a query vector, denoted as ⃗q, thatmaximizes similarity with relevant documents while minimizing similaritywith nonrelevant documents. If Cr is the set of relevant documents and Cnris the set of nonrelevant documents, then we wish to ﬁnd:1⃗qopt = arg max⃗q[sim(⃗q, Cr) −sim(⃗q, Cnr)],(9.1)where sim is deﬁned as in Equation 6.10. Under cosine similarity, the optimalquery vector⃗qopt for separating the relevant and nonrelevant documents is:⃗qopt =1|Cr| ∑⃗dj∈Cr⃗dj −1|Cnr| ∑⃗dj∈Cnr⃗dj(9.2)That is, the optimal query is the vector difference between the centroids of therelevant and nonrelevant documents; see Figure 9.3. However, this observa-tion is not terribly useful, precisely because the full set of relevant documentsis not known: it is what we want to ﬁnd.The Rocchio (1971) algorithm.This was the relevance feedback mecha-ROCCHIO ALGORITHM1. In the equation, arg maxx f (x) returns a value of x which maximizes the value of the functionf (x). Similarly, arg minx f (x) returns a value of x which minimizes the value of the functionf (x). 1829Relevance feedback and query expansion◮Figure 9.4An application of Rocchio’s algorithm. Some documents have beenlabeled as relevant and nonrelevant and the initial query vector is moved in responseto this feedback.nism introduced in and popularized by Salton’s SMART system around 1970.In a real IR query context, we have a user query and partial knowledge ofknown relevant and nonrelevant documents. The algorithm proposes usingthe modiﬁed query ⃗qm:⃗qm = α⃗q0 + β 1|Dr| ∑⃗dj∈Dr⃗dj −γ1|Dnr| ∑⃗dj∈Dnr⃗dj(9.3)where q0 is the original query vector, Dr and Dnr are the set of known rel-evant and nonrelevant documents respectively, and α, β, and γ are weightsattached to each term. These control the balance between trusting the judgeddocument set versus the query: if we have a lot of judged documents, wewould like a higher β and γ. Starting from q0, the new query moves yousome distance toward the centroid of the relevant documents and some dis-tance away from the centroid of the nonrelevant documents. This new querycan be used for retrieval in the standard vector space model (see Section 6.3).We can easily leave the positive quadrant of the vector space by subtractingoff a nonrelevant document’s vector. In the Rocchio algorithm, negative termweights are ignored. That is, the term weight is set to 0. Figure 9.4 shows theeffect of applying relevance feedback.Relevance feedback can improve both recall and precision. But, in prac-tice, it has been shown to be most useful for increasing recall in situations 9.1Relevance feedback and pseudo relevance feedback183where recall is important. This is partly because the technique expands thequery, but it is also partly an effect of the use case: when they want highrecall, users can be expected to take time to review results and to iterate onthe search. Positive feedback also turns out to be much more valuable thannegative feedback, and so most IR systems set γ < β. Reasonable valuesmight be α = 1, β = 0.75, and γ = 0.15. In fact, many systems, such asthe image search system in Figure 9.1, allow only positive feedback, whichis equivalent to setting γ = 0. Another alternative is to use only the markednonrelevant document which received the highest ranking from the IR sys-tem as negative feedback (here, |Dnr| = 1 in Equation (9.3)). While many ofthe experimental results comparing various relevance feedback variants arerather inconclusive, some studies have suggested that this variant, called IdeIDE DEC-HIdec-hi is the most effective or at least the most consistent performer.$9.1.2Probabilistic relevance feedbackRather than reweighting the query in a vector space, if a user has told ussome relevant and nonrelevant documents, then we can proceed to build aclassiﬁer. One way of doing this is with a Naive Bayes probabilistic model.If R is a Boolean indicator variable expressing the relevance of a document,then we can estimate P(xt = 1|R), the probability of a term t appearing in adocument, depending on whether it is relevant or not, as:ˆP(xt = 1|R = 1)=|VRt|/|VR|(9.4)ˆP(xt = 1|R = 0)=(d ft −|VRt|)/(N −|VR|)where N is the total number of documents, d ft is the number that containt, VR is the set of known relevant documents, and VRt is the subset of thisset containing t. Even though the set of known relevant documents is a per-haps small subset of the true set of relevant documents, if we assume thatthe set of relevant documents is a small subset of the set of all documentsthen the estimates given above will be reasonable. This gives a basis foranother way of changing the query term weights. We will discuss such prob-abilistic approaches more in Chapters 11 and 13, and in particular outlinethe application to relevance feedback in Section 11.3.4 (page 228). For themoment, observe that using just Equation (9.4) as a basis for term-weightingis likely insufﬁcient. The equations use only collection statistics and infor-mation about the term distribution within the documents judged relevant.They preserve no memory of the original query.9.1.3When does relevance feedback work?The success of relevance feedback depends on certain assumptions. Firstly,the user has to have sufﬁcient knowledge to be able to make an initial query 1849Relevance feedback and query expansionwhich is at least somewhere close to the documents they desire. This isneeded anyhow for successful information retrieval in the basic case, butit is important to see the kinds of problems that relevance feedback cannotsolve alone. Cases where relevance feedback alone is not sufﬁcient include:• Misspellings. If the user spells a term in a different way to the way itis spelled in any document in the collection, then relevance feedback isunlikely to be effective. This can be addressed by the spelling correctiontechniques of Chapter 3.• Cross-language information retrieval. Documents in another languageare not nearby in a vector space based on term distribution. Rather, docu-ments in the same language cluster more closely together.• Mismatch of searcher’s vocabulary versus collection vocabulary. If theuser searches for laptop but all the documents use the term notebook com-puter, then the query will fail, and relevance feedback is again most likelyineffective.Secondly, the relevance feedback approach requires relevant documents tobe similar to each other. That is, they should cluster. Ideally, the term dis-tribution in all relevant documents will be similar to that in the documentsmarked by the users, while the term distribution in all nonrelevant docu-ments will be different from those in relevant documents. Things will workwell if all relevant documents are tightly clustered around a single proto-type, or, at least, if there are different prototypes, if the relevant documentshave signiﬁcant vocabulary overlap, while similarities between relevant andnonrelevant documents are small. Implicitly, the Rocchio relevance feedbackmodel treats relevant documents as a single cluster, which it models via thecentroid of the cluster. This approach does not work as well if the relevantdocuments are a multimodal class, that is, they consist of several clusters ofdocuments within the vector space. This can happen with:• Subsets of the documents using different vocabulary, such as Burma vs.Myanmar• A query for which the answer set is inherently disjunctive, such as Popstars who once worked at Burger King.• Instances of a general concept, which often appear as a disjunction ofmore speciﬁc concepts, for example, felines.Good editorial content in the collection can often provide a solution to thisproblem. For example, an article on the attitudes of different groups to thesituation in Burma could introduce the terminology used by different parties,thus linking the document clusters. 9.1Relevance feedback and pseudo relevance feedback185Relevance feedback is not necessarily popular with users. Users are oftenreluctant to provide explicit feedback, or in general do not wish to prolongthe search interaction. Furthermore, it is often harder to understand why aparticular document was retrieved after relevance feedback is applied.Relevance feedback can also have practical problems. The long queriesthat are generated by straightforward application of relevance feedback tech-niques are inefﬁcient for a typical IR system. This results in a high computingcost for the retrieval and potentially long response times for the user. A par-tial solution to this is to only reweight certain prominent terms in the relevantdocuments, such as perhaps the top 20 terms by term frequency. Some ex-perimental results have also suggested that using a limited number of termslike this may give better results (Harman 1992) though other work has sug-gested that using more terms is better in terms of retrieved document quality(Buckley et al. 1994b).9.1.4Relevance feedback on the webSome web search engines offer a similar/related pages feature: the user in-dicates a document in the results set as exemplary from the standpoint ofmeeting his information need and requests more documents like it. This canbe viewed as a particular simple form of relevance feedback. However, ingeneral relevance feedback has been little used in web search. One exceptionwas the Excite web search engine, which initially provided full relevancefeedback. However, the feature was in time dropped, due to lack of use. Onthe web, few people use advanced search interfaces and most would like tocomplete their search in a single interaction. But the lack of uptake also prob-ably reﬂects two other factors: relevance feedback is hard to explain to theaverage user, and relevance feedback is mainly a recall enhancing strategy,and web search users are only rarely concerned with getting sufﬁcient recall.Spink et al. (2000) present results from the use of relevance feedback inthe Excite search engine. Only about 4% of user query sessions used therelevance feedback option, and these were usually exploiting the “More likethis” link next to each result. About 70% of users only looked at the ﬁrstpage of results and did not pursue things any further. For people who usedrelevance feedback, results were improved about two thirds of the time.An important more recent thread of work is the use of clickstream data(what links a user clicks on) to provide indirect relevance feedback. Useof this data is studied in detail in (Joachims 2002b, Joachims et al. 2005).The very successful use of web link structure (see Chapter 21) can also beviewed as implicit feedback, but provided by page authors rather than read-ers (though in practice most authors are also readers). 1869Relevance feedback and query expansion?Exercise 9.1In Rocchio’s algorithm, what weight setting for α/β/γ does a “Find pages like thisone” search correspond to?Exercise 9.2[⋆]Give three reasons why relevance feedback has been little used in web search.9.1.5Evaluation of relevance feedback strategiesInteractive relevance feedback can give very substantial gains in retrievalperformance. Empirically, one round of relevance feedback is often veryuseful. Two rounds is sometimes marginally more useful. Successful use ofrelevance feedback requires enough judged documents, otherwise the pro-cess is unstable in that it may drift away from the user’s information need.Accordingly, having at least ﬁve judged documents is recommended.There is some subtlety to evaluating the effectiveness of relevance feed-back in a sound and enlightening way. The obvious ﬁrst strategy is to startwith an initial query q0 and to compute a precision-recall graph. Followingone round of feedback from the user, we compute the modiﬁed query qmand again compute a precision-recall graph. Here, in both rounds we assessperformance over all documents in the collection, which makes comparisonsstraightforward. If we do this, we ﬁnd spectacular gains from relevance feed-back: gains on the order of 50% in mean average precision. But unfortunatelyit is cheating. The gains are partly due to the fact that known relevant doc-uments (judged by the user) are now ranked higher. Fairness demands thatwe should only evaluate with respect to documents not seen by the user.A second idea is to use documents in the residual collection (the set of doc-uments minus those assessed relevant) for the second round of evaluation.This seems like a more realistic evaluation. Unfortunately, the measured per-formance can then often be lower than for the original query. This is partic-ularly the case if there are few relevant documents, and so a fair proportionof them have been judged by the user in the ﬁrst round. The relative per-formance of variant relevance feedback methods can be validly compared,but it is difﬁcult to validly compare performance with and without relevancefeedback because the collection size and the number of relevant documentschanges from before the feedback to after it.Thus neither of these methods is fully satisfactory. A third method is tohave two collections, one which is used for the initial query and relevancejudgments, and the second that is then used for comparative evaluation. Theperformance of both q0 and qm can be validly compared on the second col-lection.Perhaps the best evaluation of the utility of relevance feedback is to do userstudies of its effectiveness, in particular by doing a time-based comparison: 9.1Relevance feedback and pseudo relevance feedback187Precision at k = 50Term weightingno RFpseudo RFlnc.ltc64.2%72.7%Lnu.ltu74.2%87.0%◮Figure 9.5Results showing pseudo relevance feedback greatly improving perfor-mance. These results are taken from the Cornell SMART system at TREC 4 (Buckleyet al. 1995), and also contrast the use of two different length normalization schemes(L vs. l); cf. Figure 6.15 (page 128). Pseudo relevance feedback consisted of adding 20terms to each query.how fast does a user ﬁnd relevant documents with relevance feedback vs.another strategy (such as query reformulation), or alternatively, how manyrelevant documents does a user ﬁnd in a certain amount of time. Such no-tions of user utility are fairest and closest to real system usage.9.1.6Pseudo relevance feedbackPseudo relevance feedback, also known as blind relevance feedback, provides aPSEUDO RELEVANCEFEEDBACKBLIND RELEVANCEFEEDBACKmethod for automatic local analysis. It automates the manual part of rele-vance feedback, so that the user gets improved retrieval performance with-out an extended interaction. The method is to do normal retrieval to ﬁnd aninitial set of most relevant documents, to then assume that the top k rankeddocuments are relevant, and ﬁnally to do relevance feedback as before underthis assumption.This automatic technique mostly works. Evidence suggests that it tendsto work better than global analysis (Section 9.2). It has been found to im-prove performance in the TREC ad hoc task. See for example the results inFigure 9.5. But it is not without the dangers of an automatic process. Forexample, if the query is about copper mines and the top several documentsare all about mines in Chile, then there may be query drift in the direction ofdocuments on Chile.9.1.7Indirect relevance feedbackWe can also use indirect sources of evidence rather than explicit feedback onrelevance as the basis for relevance feedback. This is often called implicit (rel-IMPLICIT RELEVANCEFEEDBACKevance) feedback. Implicit feedback is less reliable than explicit feedback, but ismore useful than pseudo relevance feedback, which contains no evidence ofuser judgments. Moreover, while users are often reluctant to provide explicitfeedback, it is easy to collect implicit feedback in large quantities for a highvolume system, such as a web search engine. 1889Relevance feedback and query expansionOn the web, DirectHit introduced the idea of ranking more highly docu-ments that users chose to look at more often. In other words, clicks on linkswere assumed to indicate that the page was likely relevant to the query. Thisapproach makes various assumptions, such as that the document summariesdisplayed in results lists (on whose basis users choose which documents toclick on) are indicative of the relevance of these documents. In the originalDirectHit search engine, the data about the click rates on pages was gatheredglobally, rather than being user or query speciﬁc. This is one form of the gen-eral area of clickstream mining. Today, a closely related approach is used inCLICKSTREAM MININGranking the advertisements that match a web search query (Chapter 19).9.1.8SummaryRelevance feedback has been shown to be very effective at improving rele-vance of results. Its successful use requires queries for which the set of rele-vant documents is medium to large. Full relevance feedback is often onerousfor the user, and its implementation is not very efﬁcient in most IR systems.In many cases, other types of interactive retrieval may improve relevance byabout as much with less work.Beyond the core ad hoc retrieval scenario, other uses of relevance feedbackinclude:• Following a changing information need (e.g., names of car models of in-terest change over time)• Maintaining an information ﬁlter (e.g., for a news feed). Such ﬁlters arediscussed further in Chapter 13.• Active learning (deciding which examples it is most useful to know theclass of to reduce annotation costs).?Exercise 9.3Under what conditions would the modiﬁed query qm in Equation 9.3 be the same asthe original query q0? In all other cases, is qm closer than q0 to the centroid of therelevant documents?Exercise 9.4Why is positive feedback likely to be more useful than negative feedback to an IRsystem? Why might only using one nonrelevant document be more effective thanusing several?Exercise 9.5Suppose that a user’s initial query is cheap CDs cheap DVDs extremely cheap CDs. Theuser examines two documents, d1 and d2. She judges d1, with the content CDs cheapsoftware cheap CDs relevant and d2 with content cheap thrills DVDs nonrelevant. As-sume that we are using direct term frequency (with no scaling and no document 9.2Global methods for query reformulation189frequency). There is no need to length-normalize vectors. Using Rocchio relevancefeedback as in Equation (9.3) what would the revised query vector be after relevancefeedback? Assume α = 1, β = 0.75, γ = 0.25.Exercise 9.6[⋆]Omar has implemented a relevance feedback web search system, where he is goingto do relevance feedback based only on words in the title text returned for a page (forefﬁciency). The user is going to rank 3 results. The ﬁrst user, Jinxing, queries for:banana slugand the top three titles returned are:banana slug Ariolimax columbianusSanta Cruz mountains banana slugSanta Cruz Campus MascotJinxing judges the ﬁrst two documents relevant, and the third nonrelevant. Assumethat Omar’s search engine uses term frequency but no length normalization nor IDF.Assume that he is using the Rocchio relevance feedback mechanism, with α = β =γ = 1. Show the ﬁnal revised query that would be run. (Please list the vector elementsin alphabetical order.)9.2Global methods for query reformulationIn this section we more brieﬂy discuss three global methods for expanding aquery: by simply aiding the user in doing so, by using a manual thesaurus,and through building a thesaurus automatically.9.2.1Vocabulary tools for query reformulationVarious user supports in the search process can help the user see how theirsearches are or are not working. This includes information about words thatwere omitted from the query because they were on stop lists, what wordswere stemmed to, the number of hits on each term or phrase, and whetherwords were dynamically turned into phrases. The IR system might also sug-gest search terms by means of a thesaurus or a controlled vocabulary. A usercan also be allowed to browse lists of the terms that are in the inverted index,and thus ﬁnd good terms that appear in the collection.9.2.2Query expansionIn relevance feedback, users give additional input on documents (by mark-ing documents in the results set as relevant or not), and this input is usedto reweight the terms in the query for documents. In query expansion on theQUERY EXPANSIONother hand, users give additional input on query words or phrases, possiblysuggesting additional query terms. Some search engines (especially on the 1909Relevance feedback and query expansionYahoo!MyYahoo!MailWelcome,Guest[SignIn]HelpSearchpalmWebImagesVideoLocalShoppingmoreOptionsAlsotry:SPONSORRESULTSpalmtrees,palmsprings,palmcentro,palmtreo,More...PalmbAT&Tatt.com/wirelesslGomobileeffortlesslywiththePALMTreofromAT&T(Cingular).PalmHandheldsPalm.comlOrganizer,Planner,WiFi,MusicBluetooth,Games,Photos&Video.Palm,Inc.MakerofhandheldPDAdevicesthatallowmobileuserstomanageschedules,contacts,andotherpersonalandbusinessinformation.www.palm.comlCachedPalm,Inc.bTreoandCentrosmartphones,handhelds,andaccessoriesPalm,Inc.,innovatorofeasyltolusemobileproductsincludingPalm®Treo_andCentro_smartphones,Palmhandhelds,services,andaccessories.www.palm.com/uslCachedSPONSORRESULTSHandheldsatDellStayConnectedwithHandheldPCs&PDAs.ShopatDell™OfficialSite.www.Dell.comBuyPalmCentroCasesUltimateselectionofcasesandaccessoriesforbusinessdevices.www.Cases.comFreePlamTreoGetAFreePalmTreo700WPhone.ParticipateToday.EvaluationNation.com/treo1ª10ofabout534,000,000forpalm(Aboutthispage)ª0.11sec.◮Figure 9.6An example of query expansion in the interface of the Yahoo! websearch engine in 2006. The expanded query suggestions appear just below the “SearchResults” bar.web) suggest related queries in response to a query; the users then opt to useone of these alternative query suggestions. Figure 9.6 shows an example ofquery suggestion options being presented in the Yahoo! web search engine.The central question in this form of query expansion is how to generate al-ternative or expanded queries for the user. The most common form of queryexpansion is global analysis, using some form of thesaurus. For each termt in a query, the query can be automatically expanded with synonyms andrelated words of t from the thesaurus. Use of a thesaurus can be combinedwith ideas of term weighting: for instance, one might weight added termsless than original query terms.Methods for building a thesaurus for query expansion include:• Use of a controlled vocabulary that is maintained by human editors. Here,there is a canonical term for each concept. The subject headings of tra-ditional library subject indexes, such as the Library of Congress SubjectHeadings, or the Dewey Decimal system are examples of a controlledvocabulary. Use of a controlled vocabulary is quite common for well-resourced domains. A well-known example is the Uniﬁed Medical Lan-guage System (UMLS) used with MedLine for querying the biomedicalresearch literature. For example, in Figure 9.7, neoplasms was added to a 9.2Global methods for query reformulation191• User query: cancer• PubMed query: (“neoplasms”[TIAB] NOT Medline[SB]) OR “neoplasms”[MeSHTerms] OR cancer[Text Word]• User query: skin itch• PubMed query: (“skin”[MeSH Terms] OR (“integumentary system”[TIAB] NOTMedline[SB]) OR “integumentary system”[MeSH Terms] OR skin[Text Word]) AND((“pruritus”[TIAB] NOT Medline[SB]) OR “pruritus”[MeSH Terms] OR itch[TextWord])◮Figure 9.7Examples of query expansion via the PubMed thesaurus. When a userissues a query on the PubMed interface to Medline at http://www.ncbi.nlm.nih.gov/entrez/,their query is mapped on to the Medline vocabulary as shown.search for cancer. This Medline query expansion also contrasts with theYahoo! example. The Yahoo! interface is a case of interactive query expan-sion, whereas PubMed does automatic query expansion. Unless the userchooses to examine the submitted query, they may not even realize thatquery expansion has occurred.• A manual thesaurus. Here, human editors have built up sets of synony-mous names for concepts, without designating a canonical term.TheUMLS metathesaurus is one example of a thesaurus. Statistics Canadamaintains a thesaurus of preferred terms, synonyms, broader terms, andnarrower terms for matters on which the government collects statistics,such as goods and services. This thesaurus is also bilingual English andFrench.• An automatically derived thesaurus. Here, word co-occurrence statisticsover a collection of documents in a domain are used to automatically in-duce a thesaurus; see Section 9.2.3.• Query reformulations based on query log mining. Here, we exploit themanual query reformulations of other users to make suggestions to a newuser. This requires a huge query volume, and is thus particularly appro-priate to web search.Thesaurus-based query expansion has the advantage of not requiring anyuser input. Use of query expansion generally increases recall and is widelyused in many science and engineering ﬁelds. As well as such global analysistechniques, it is also possible to do query expansion by local analysis, forinstance, by analyzing the documents in the result set. User input is now 1929Relevance feedback and query expansionWordNearest neighborsabsolutelyabsurd, whatsoever, totally, exactly, nothingbottomeddip, copper, drops, topped, slide, trimmedcaptivatingshimmer, stunningly, superbly, plucky, wittydoghousedog, porch, crawling, beside, downstairsmakeuprepellent, lotion, glossy, sunscreen, skin, gelmediatingreconciliation, negotiate, case, conciliationkeepinghoping, bring, wiping, could, some, wouldlithographsdrawings, Picasso, Dali, sculptures, Gauguinpathogenstoxins, bacteria, organisms, bacterial, parasitesensesgrasp, psyche, truly, clumsy, naive, innate◮Figure 9.8An example of an automatically generated thesaurus. This exampleis based on the work in Schütze (1998), which employs latent semantic indexing (seeChapter 18).usually required, but a distinction remains as to whether the user is givingfeedback on documents or on query terms.9.2.3Automatic thesaurus generationAs an alternative to the cost of a manual thesaurus, we could attempt togenerate a thesaurus automatically by analyzing a collection of documents.There are two main approaches. One is simply to exploit word cooccurrence.We say that words co-occurring in a document or paragraph are likely to bein some sense similar or related in meaning, and simply count text statisticsto ﬁnd the most similar words. The other approach is to use a shallow gram-matical analysis of the text and to exploit grammatical relations or grammat-ical dependencies. For example, we say that entities that are grown, cooked,eaten, and digested, are more likely to be food items. Simply using wordcooccurrence is more robust (it cannot be misled by parser errors), but usinggrammatical relations is more accurate.The simplest way to compute a co-occurrence thesaurus is based on term-term similarities. We begin with a term-document matrix A, where each cellAt,d is a weighted count wt,d for term t and document d, with weighting soA has length-normalized rows. If we then calculate C = AAT, then Cu,v isa similarity score between terms u and v, with a larger number being better.Figure 9.8 shows an example of a thesaurus derived in basically this manner,but with an extra step of dimensionality reduction via Latent Semantic In-dexing, which we discuss in Chapter 18. While some of the thesaurus termsare good or at least suggestive, others are marginal or bad. The quality of theassociations is typically a problem. Term ambiguity easily introduces irrel- 9.3References and further reading193evant statistically correlated terms. For example, a query for Apple computermay expand to Apple red fruit computer. In general these thesauri suffer fromboth false positives and false negatives. Moreover, since the terms in the au-tomatic thesaurus are highly correlated in documents anyway (and often thecollection used to derive the thesaurus is the same as the one being indexed),this form of query expansion may not retrieve many additional documents.Query expansion is often effective in increasing recall. However, there isa high cost to manually producing a thesaurus and then updating it for sci-entiﬁc and terminological developments within a ﬁeld. In general a domain-speciﬁc thesaurus is required: general thesauri and dictionaries give far toolittle coverage of the rich domain-particular vocabularies of most scientiﬁcﬁelds. However, query expansion may also signiﬁcantly decrease precision,particularly when the query contains ambiguous terms. For example, if theuser searches for interest rate, expanding the query to interest rate fascinate eval-uate is unlikely to be useful. Overall, query expansion is less successful thanrelevance feedback, though it may be as good as pseudo relevance feedback.It does, however, have the advantage of being much more understandable tothe system user.?Exercise 9.7If A is simply a Boolean cooccurrence matrix, then what do you get as the entries inC?9.3References and further readingWork in information retrieval quickly confronted the problem of variant ex-pression which meant that the words in a query might not appear in a doc-ument, despite it being relevant to the query. An early experiment about1960 cited by Swanson (1988) found that only 11 out of 23 documents prop-erly indexed under the subject toxicity had any use of a word containing thestem toxi. There is also the issue of translation, of users knowing what termsa document will use. Blair and Maron (1985) conclude that “it is impossiblydifﬁcult for users to predict the exact words, word combinations, and phrasesthat are used by all (or most) relevant documents and only (or primarily) bythose documents”.The main initial papers on relevance feedback using vector space modelsall appear in Salton (1971b), including the presentation of the Rocchio al-gorithm (Rocchio 1971) and the Ide dec-hi variant along with evaluation ofseveral variants (Ide 1971). Another variant is to regard all documents inthe collection apart from those judged relevant as nonrelevant, rather thanonly ones that are explicitly judged nonrelevant. However, Schütze et al.(1995) and Singhal et al. (1997) show that better results are obtained for rout-ing by using only documents close to the query of interest rather than all 1949Relevance feedback and query expansiondocuments. Other later work includes Salton and Buckley (1990), Riezleret al. (2007) (a statistical NLP approach to RF) and the recent survey paperRuthven and Lalmas (2003).The effectiveness of interactive relevance feedback systems is discussed in(Salton 1989, Harman 1992, Buckley et al. 1994b). Koenemann and Belkin(1996) do user studies of the effectiveness of relevance feedback.Traditionally Roget’s thesaurus has been the best known English languagethesaurus (Roget 1946). In recent computational work, people almost alwaysuse WordNet (Fellbaum 1998), not only because it is free, but also because ofits rich link structure. It is available at: http://wordnet.princeton.edu.Qiu and Frei (1993) and Schütze (1998) discuss automatic thesaurus gener-ation. Xu and Croft (1996) explore using both local and global query expan-sion.', '44320Web crawling and indexes20.1OverviewWeb crawling is the process by which we gather pages from the Web, inorder to index them and support a search engine. The objective of crawlingis to quickly and efﬁciently gather as many useful web pages as possible,together with the link structure that interconnects them. In Chapter 19 westudied the complexities of the Web stemming from its creation by millions ofuncoordinated individuals. In this chapter we study the resulting difﬁcultiesfor crawling the Web. The focus of this chapter is the component shown inFigure 19.7 as web crawler; it is sometimes referred to as a spider.WEB CRAWLERSPIDERThe goal of this chapter is not to describe how to build the crawler fora full-scale commercial web search engine. We focus instead on a range ofissues that are generic to crawling from the student project scale to substan-tial research projects. We begin (Section 20.1.1) by listing desiderata for webcrawlers, and then discuss in Section 20.2 how each of these issues is ad-dressed. The remainder of this chapter describes the architecture and someimplementation details for a distributed web crawler that satisﬁes these fea-tures. Section 20.3 discusses distributing indexes across many machines fora web-scale implementation.20.1.1Features a crawler must provideWe list the desiderata for web crawlers in two categories: features that webcrawlers must provide, followed by features they should provide.Robustness: The Web contains servers that create spider traps, which are gen-erators of web pages that mislead crawlers into getting stuck fetching aninﬁnite number of pages in a particular domain. Crawlers must be de-signed to be resilient to such traps. Not all such traps are malicious; someare the inadvertent side-effect of faulty website development. 44420Web crawling and indexesPoliteness: Web servers have both implicit and explicit policies regulatingthe rate at which a crawler can visit them. These politeness policies mustbe respected.20.1.2Features a crawler should provideDistributed: The crawler should have the ability to execute in a distributedfashion across multiple machines.Scalable: The crawler architecture should permit scaling up the crawl rateby adding extra machines and bandwidth.Performance and efﬁciency: The crawl system should make efﬁcient use ofvarious system resources including processor, storage and network band-width.Quality: Given that a signiﬁcant fraction of all web pages are of poor util-ity for serving user query needs, the crawler should be biased towardsfetching “useful” pages ﬁrst.Freshness: In many applications, the crawler should operate in continuousmode: it should obtain fresh copies of previously fetched pages. A searchengine crawler, for instance, can thus ensure that the search engine’s indexcontains a fairly current representation of each indexed web page. Forsuch continuous crawling, a crawler should be able to crawl a page witha frequency that approximates the rate of change of that page.Extensible: Crawlers should be designed to be extensible in many ways –to cope with new data formats, new fetch protocols, and so on. This de-mands that the crawler architecture be modular.20.2CrawlingThe basic operation of any hypertext crawler (whether for the Web, an in-tranet or other hypertext document collection) is as follows. The crawlerbegins with one or more URLs that constitute a seed set. It picks a URL fromthis seed set, then fetches the web page at that URL. The fetched page is thenparsed, to extract both the text and the links from the page (each of whichpoints to another URL). The extracted text is fed to a text indexer (describedin Chapters 4 and 5). The extracted links (URLs) are then added to a URLfrontier, which at all times consists of URLs whose corresponding pages haveyet to be fetched by the crawler. Initially, the URL frontier contains the seedset; as pages are fetched, the corresponding URLs are deleted from the URLfrontier. The entire process may be viewed as traversing the web graph (see 20.2Crawling445Chapter 19). In continuous crawling, the URL of a fetched page is addedback to the frontier for fetching again in the future.This seemingly simple recursive traversal of the web graph is complicatedby the many demands on a practical web crawling system: the crawler has tobe distributed, scalable, efﬁcient, polite, robust and extensible while fetchingpages of high quality. We examine the effects of each of these issues. Ourtreatment follows the design of the Mercator crawler that has formed the ba-MERCATORsis of a number of research and commercial crawlers. As a reference point,fetching a billion pages (a small fraction of the static Web at present) in amonth-long crawl requires fetching several hundred pages each second. Wewill see how to use a multi-threaded design to address several bottlenecks inthe overall crawler system in order to attain this fetch rate.Before proceeding to this detailed description, we reiterate for readers whomay attempt to build crawlers of some basic properties any non-professionalcrawler should satisfy:1. Only one connection should be open to any given host at a time.2. A waiting time of a few seconds should occur between successive requeststo a host.3. Politeness restrictions detailed in Section 20.2.1 should be obeyed.20.2.1Crawler architectureThe simple scheme outlined above for crawling demands several modulesthat ﬁt together as shown in Figure 20.1.1. The URL frontier, containing URLs yet to be fetched in the current crawl(in the case of continuous crawling, a URL may have been fetched previ-ously but is back in the frontier for re-fetching). We describe this furtherin Section 20.2.3.2. A DNS resolution module that determines the web server from which tofetch the page speciﬁed by a URL. We describe this further in Section 20.2.2.3. A fetch module that uses the http protocol to retrieve the web page at aURL.4. A parsing module that extracts the text and set of links from a fetched webpage.5. A duplicate elimination module that determines whether an extractedlink is already in the URL frontier or has recently been fetched. 44620Web crawling and indexeswwwFetchDNSParseURL FrontierContentSeen?\\x13\\x12\\x10\\x11\\x12\\x11DocFP’s\\x13\\x12\\x10\\x11\\x12\\x11robotstemplates\\x13\\x12\\x10\\x11\\x12\\x11URLsetURLFilterDupURLElim-\\x1b-6\\x1b-?6---\\x1b6?6?6?◮Figure 20.1The basic crawler architecture.Crawling is performed by anywhere from one to potentially hundreds ofthreads, each of which loops through the logical cycle in Figure 20.1. Thesethreads may be run in a single process, or be partitioned amongst multipleprocesses running at different nodes of a distributed system. We begin byassuming that the URL frontier is in place and non-empty and defer our de-scription of the implementation of the URL frontier to Section 20.2.3. Wefollow the progress of a single URL through the cycle of being fetched, pass-ing through various checks and ﬁlters, then ﬁnally (for continuous crawling)being returned to the URL frontier.A crawler thread begins by taking a URL from the frontier and fetchingthe web page at that URL, generally using the http protocol. The fetchedpage is then written into a temporary store, where a number of operationsare performed on it. Next, the page is parsed and the text as well as thelinks in it are extracted. The text (with any tag information – e.g., terms inboldface) is passed on to the indexer. Link information including anchor textis also passed on to the indexer for use in ranking in ways that are describedin Chapter 21. In addition, each extracted link goes through a series of teststo determine whether the link should be added to the URL frontier.First, the thread tests whether a web page with the same content has al-ready been seen at another URL. The simplest implementation for this woulduse a simple ﬁngerprint such as a checksum (placed in a store labeled \"DocFP’s\" in Figure 20.1). A more sophisticated test would use shingles instead 20.2Crawling447of ﬁngerprints, as described in Chapter 19.Next, a URL ﬁlter is used to determine whether the extracted URL shouldbe excluded from the frontier based on one of several tests. For instance, thecrawl may seek to exclude certain domains (say, all .com URLs) – in this casethe test would simply ﬁlter out the URL if it were from the .com domain.A similar test could be inclusive rather than exclusive. Many hosts on theWeb place certain portions of their websites off-limits to crawling, under astandard known as the Robots Exclusion Protocol. This is done by placing aROBOTS EXCLUSIONPROTOCOLﬁle with the name robots.txt at the root of the URL hierarchy at the site. Hereis an example robots.txt ﬁle that speciﬁes that no robot should visit any URLwhose position in the ﬁle hierarchy starts with /yoursite/temp/, except for therobot called “searchengine”.User-agent: *Disallow: /yoursite/temp/User-agent: searchengineDisallow:The robots.txt ﬁle must be fetched from a website in order to test whetherthe URL under consideration passes the robot restrictions, and can there-fore be added to the URL frontier. Rather than fetch it afresh for testing oneach URL to be added to the frontier, a cache can be used to obtain a re-cently fetched copy of the ﬁle for the host. This is especially important sincemany of the links extracted from a page fall within the host from which thepage was fetched and therefore can be tested against the host’s robots.txtﬁle. Thus, by performing the ﬁltering during the link extraction process, wewould have especially high locality in the stream of hosts that we need to testfor robots.txt ﬁles, leading to high cache hit rates. Unfortunately, this runsafoul of webmasters’ politeness expectations. A URL (particularly one refer-ring to a low-quality or rarely changing document) may be in the frontier fordays or even weeks. If we were to perform the robots ﬁltering before addingsuch a URL to the frontier, its robots.txt ﬁle could have changed by the timethe URL is dequeued from the frontier and fetched. We must consequentlyperform robots-ﬁltering immediately before attempting to fetch a web page.As it turns out, maintaining a cache of robots.txt ﬁles is still highly effective;there is sufﬁcient locality even in the stream of URLs dequeued from the URLfrontier.Next, a URL should be normalized in the following sense: often the HTMLURL NORMALIZATIONencoding of a link from a web page p indicates the target of that link relativeto the page p. Thus, there is a relative link encoded thus in the HTML of thepage en.wikipedia.org/wiki/Main_Page: 44820Web crawling and indexes<a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:Generaldisclaimer\">Disclaimers</a>points to the URL http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer.Finally, the URL is checked for duplicate elimination: if the URL is alreadyin the frontier or (in the case of a non-continuous crawl) already crawled,we do not add it to the frontier. When the URL is added to the frontier, it isassigned a priority based on which it is eventually removed from the frontierfor fetching. The details of this priority queuing are in Section 20.2.3.Certain housekeeping tasks are typically performed by a dedicated thread.This thread is generally quiescent except that it wakes up once every fewseconds to log crawl progress statistics (URLs crawled, frontier size, etc.),decide whether to terminate the crawl, or (once every few hours of crawling)checkpoint the crawl. In checkpointing, a snapshot of the crawler’s state (say,the URL frontier) is committed to disk. In the event of a catastrophic crawlerfailure, the crawl is restarted from the most recent checkpoint.Distributing the crawlerWe have mentioned that the threads in a crawler could run under differentprocesses, each at a different node of a distributed crawling system. Suchdistribution is essential for scaling; it can also be of use in a geographicallydistributed crawler system where each node crawls hosts “near” it. Parti-tioning the hosts being crawled amongst the crawler nodes can be done bya hash function, or by some more speciﬁcally tailored policy. For instance,we may locate a crawler node in Europe to focus on European domains, al-though this is not dependable for several reasons – the routes that packetstake through the internet do not always reﬂect geographic proximity, and inany case the domain of a host does not always reﬂect its physical location.How do the various nodes of a distributed crawler communicate and shareURLs? The idea is to replicate the ﬂow of Figure 20.1 at each node, with oneessential difference: following the URL ﬁlter, we use a host splitter to dispatcheach surviving URL to the crawler node responsible for the URL; thus the setof hosts being crawled is partitioned among the nodes. This modiﬁed ﬂow isshown in Figure 20.2. The output of the host splitter goes into the DuplicateURL Eliminator block of each other node in the distributed system.The “Content Seen?” module in the distributed architecture of Figure 20.2is, however, complicated by several factors:1. Unlike the URL frontier and the duplicate elimination module, documentﬁngerprints/shingles cannot be partitioned based on host name. There isnothing preventing the same (or highly similar) content from appearingon different web servers. Consequently, the set of ﬁngerprints/shinglesmust be partitioned across the nodes based on some property of the ﬁn- 20.2Crawling449wwwFetchDNSParseURL FrontierContentSeen?\\x13\\x12\\x10\\x11\\x12\\x11DocFP’s\\x13\\x12\\x10\\x11\\x12\\x11URLsetURLFilterHostsplitterToothernodesFromothernodesDupURLElim-\\x1b-6\\x1b-?6----\\x1b6?6?666---◮Figure 20.2Distributing the basic crawl architecture.gerprint/shingle (say by taking the ﬁngerprint modulo the number ofnodes). The result of this locality-mismatch is that most “Content Seen?”tests result in a remote procedure call (although it is possible to batchlookup requests).2. There is very little locality in the stream of document ﬁngerprints/shingles.Thus, caching popular ﬁngerprints does not help (since there are no pop-ular ﬁngerprints).3. Documents change over time and so, in the context of continuous crawl-ing, we must be able to delete their outdated ﬁngerprints/shingles fromthe content-seen set(s). In order to do so, it is necessary to save the ﬁnger-print/shingle of the document in the URL frontier, along with the URLitself.20.2.2DNS resolutionEach web server (and indeed any host connected to the internet) has a uniqueIP address: a sequence of four bytes generally represented as four integersIP ADDRESSseparated by dots; for instance 207.142.131.248is the numerical IP address as-sociated with the host www.wikipedia.org. Given a URL such as www.wikipedia.orgin textual form, translating it to an IP address (in this case, 207.142.131.248)is 45020Web crawling and indexesa process known as DNS resolution or DNS lookup; here DNS stands for Do-DNS RESOLUTIONmain Name Service. During DNS resolution, the program that wishes to per-form this translation (in our case, a component of the web crawler) contacts aDNS server that returns the translated IP address. (In practice the entire trans-DNS SERVERlation may not occur at a single DNS server; rather, the DNS server contactedinitially may recursively call upon other DNS servers to complete the transla-tion.) For a more complex URL such as en.wikipedia.org/wiki/Domain_Name_System,the crawler component responsible for DNS resolution extracts the host name– in this case en.wikipedia.org – and looks up the IP address for the hosten.wikipedia.org.DNS resolution is a well-known bottleneck in web crawling. Due to thedistributed nature of the Domain Name Service, DNS resolution may entailmultiple requests and round-trips across the internet, requiring seconds andsometimes even longer. Right away, this puts in jeopardy our goal of fetchingseveral hundred documents a second. A standard remedy is to introducecaching: URLs for which we have recently performed DNS lookups are likelyto be found in the DNS cache, avoiding the need to go to the DNS serverson the internet. However, obeying politeness constraints (see Section 20.2.3)limits the of cache hit rate.There is another important difﬁculty in DNS resolution; the lookup imple-mentations in standard libraries (likely to be used by anyone developing acrawler) are generally synchronous. This means that once a request is madeto the Domain Name Service, other crawler threads at that node are blockeduntil the ﬁrst request is completed. To circumvent this, most web crawlersimplement their own DNS resolver as a component of the crawler. Threadi executing the resolver code sends a message to the DNS server and thenperforms a timed wait: it resumes either when being signaled by anotherthread or when a set time quantum expires. A single, separate DNS threadlistens on the standard DNS port (port 53) for incoming response packetsfrom the name service. Upon receiving a response, it signals the appropriatecrawler thread (in this case, i) and hands it the response packet if i has notyet resumed because its time quantum has expired. A crawler thread that re-sumes because its wait time quantum has expired retries for a ﬁxed numberof attempts, sending out a new message to the DNS server and performinga timed wait each time; the designers of Mercator recommend of the orderof ﬁve attempts. The time quantum of the wait increases exponentially witheach of these attempts; Mercator started with one second and ended withroughly 90 seconds, in consideration of the fact that there are host namesthat take tens of seconds to resolve. 20.2Crawling45120.2.3The URL frontierThe URL frontier at a node is given a URL by its crawl process (or by thehost splitter of another crawl process). It maintains the URLs in the frontierand regurgitates them in some order whenever a crawler thread seeks a URL.Two important considerations govern the order in which URLs are returnedby the frontier. First, high-quality pages that change frequently should beprioritized for frequent crawling. Thus, the priority of a page should be afunction of both its change rate and its quality (using some reasonable qualityestimate). The combination is necessary because a large number of spampages change completely on every fetch.The second consideration is politeness: we must avoid repeated fetch re-quests to a host within a short time span. The likelihood of this is exacerbatedbecause of a form of locality of reference: many URLs link to other URLs atthe same host. As a result, a URL frontier implemented as a simple priorityqueue might result in a burst of fetch requests to a host. This might occureven if we were to constrain the crawler so that at most one thread couldfetch from any single host at any time. A common heuristic is to insert agap between successive fetch requests to a host that is an order of magnitudelarger than the time taken for the most recent fetch from that host.Figure 20.3 shows a polite and prioritizing implementation of a URL fron-tier. Its goals are to ensure that (i) only one connection is open at a time to anyhost; (ii) a waiting time of a few seconds occurs between successive requeststo a host and (iii) high-priority pages are crawled preferentially.The two major sub-modules are a set of F front queues in the upper por-tion of the ﬁgure, and a set of B back queues in the lower part; all of these areFIFO queues. The front queues implement the prioritization, while the backqueues implement politeness. In the ﬂow of a URL added to the frontier asit makes its way through the front and back queues, a prioritizer ﬁrst assignsto the URL an integer priority i between 1 and F based on its fetch history(taking into account the rate at which the web page at this URL has changedbetween previous crawls). For instance, a document that has exhibited fre-quent change would be assigned a higher priority. Other heuristics could beapplication-dependent and explicit – for instance, URLs from news servicesmay always be assigned the highest priority. Now that it has been assignedpriority i, the URL is now appended to the ith of the front queues.Each of the B back queues maintains the following invariants: (i) it is non-empty while the crawl is in progress and (ii) it only contains URLs from asingle host1. An auxiliary table T (Figure 20.4) is used to maintain the map-ping from hosts to back queues. Whenever a back-queue is empty and isbeing re-ﬁlled from a front-queue, table T must be updated accordingly.1. The number of hosts is assumed to far exceed B. 45220Web crawling and indexesBack queueselector-\\x1bBiased front queue selectorBack queue routerPrioritizerrrrrB back queuesSingle host on eachrrrrrF front queues12F12B?XXXXXXXXXXXXzXXXXXXXXXXXXz\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x18\\x189XXXXXXXXXXXXXXz\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10)\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10)PPPPPPPPPPPq?HHHHHHHHHHHjHHHHHHHHHHHj\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x08\\x19@@@\\x00\\x00\\x00 Heap◮Figure 20.3The URL frontier. URLs extracted from already crawled pages ﬂow inat the top of the ﬁgure. A crawl thread requesting a URL extracts it from the bottom ofthe ﬁgure. En route, a URL ﬂows through one of several front queues that manage itspriority for crawling, followed by one of several back queues that manage the crawler’spoliteness. 20.2Crawling453HostBack queuestanford.edu23microsoft.com47acm.org12◮Figure 20.4Example of an auxiliary hosts-to-back queues table.In addition, we maintain a heap with one entry for each back queue, theentry being the earliest time te at which the host corresponding to that queuecan be contacted again.A crawler thread requesting a URL from the frontier extracts the root ofthis heap and (if necessary) waits until the corresponding time entry te. Itthen takes the URL u at the head of the back queue j corresponding to theextracted heap root, and proceeds to fetch the URL u. After fetching u, thecalling thread checks whether j is empty. If so, it picks a front queue andextracts from its head a URL v. The choice of front queue is biased (usuallyby a random process) towards queues of higher priority, ensuring that URLsof high priority ﬂow more quickly into the back queues. We examine v tocheck whether there is already a back queue holding URLs from its host.If so, v is added to that queue and we reach back to the front queues toﬁnd another candidate URL for insertion into the now-empty queue j. Thisprocess continues until j is non-empty again. In any case, the thread insertsa heap entry for j with a new earliest time te based on the properties of theURL in j that was last fetched (such as when its host was last contacted aswell as the time taken for the last fetch), then continues with its processing.For instance, the new entry te could be the current time plus ten times thelast fetch time.The number of front queues, together with the policy of assigning priori-ties and picking queues, determines the priority properties we wish to buildinto the system. The number of back queues governs the extent to which wecan keep all crawl threads busy while respecting politeness. The designersof Mercator recommend a rough rule of three times as many back queues ascrawler threads.On a Web-scale crawl, the URL frontier may grow to the point where itdemands more memory at a node than is available. The solution is to letmost of the URL frontier reside on disk. A portion of each queue is kept inmemory, with more brought in from disk as it is drained in memory.?Exercise 20.1Why is it better to partition hosts (rather than individual URLs) between the nodes ofa distributed crawl system?Exercise 20.2Why should the host splitter precede the Duplicate URL Eliminator? 45420Web crawling and indexesExercise 20.3[⋆⋆⋆]In the preceding discussion we encountered two recommended “hard constants” –the increment on te being ten times the last fetch time, and the number of backqueues being three times the number of crawl threads. How are these two constantsrelated?20.3Distributing indexesIn Section 4.4 we described distributed indexing. We now consider the distri-bution of the index across a large computer cluster2 that supports querying.Two obvious alternative index implementations suggest themselves: parti-TERM PARTITIONINGtioning by terms, also known as global index organization, and partitioning byDOCUMENTPARTITIONINGdocuments, also know as local index organization. In the former, the diction-ary of index terms is partitioned into subsets, each subset residing at a node.Along with the terms at a node, we keep the postings for those terms. Aquery is routed to the nodes corresponding to its query terms. In principle,this allows greater concurrency since a stream of queries with different queryterms would hit different sets of machines.In practice, partitioning indexes by vocabulary terms turns out to be non-trivial. Multi-word queries require the sending of long postings lists betweensets of nodes for merging, and the cost of this can outweigh the greater con-currency. Load balancing the partition is governed not by an a priori analysisof relative term frequencies, but rather by the distribution of query termsand their co-occurrences, which can drift with time or exhibit sudden bursts.Achieving good partitions is a function of the co-occurrences of query termsand entails the clustering of terms to optimize objectives that are not easy toquantify. Finally, this strategy makes implementation of dynamic indexingmore difﬁcult.A more common implementation is to partition by documents: each nodecontains the index for a subset of all documents. Each query is distributed toall nodes, with the results from various nodes being merged before presenta-tion to the user. This strategy trades more local disk seeks for less inter-nodecommunication. One difﬁculty in this approach is that global statistics usedin scoring – such as idf – must be computed across the entire document col-lection even though the index at any single node only contains a subset ofthe documents. These are computed by distributed “background” processesthat periodically refresh the node indexes with fresh global statistics.How do we decide the partition of documents to nodes? Based on our de-velopment of the crawler architecture in Section 20.2.1, one simple approachwould be to assign all pages from a host to a single node. This partitioning2. Please note the different usage of “clusters” elsewhere in this book, in the sense of Chapters16 and 17. 20.4Connectivity servers455could follow the partitioning of hosts to crawler nodes. A danger of suchpartitioning is that on many queries, a preponderance of the results wouldcome from documents at a small number of hosts (and hence a small numberof index nodes).A hash of each URL into the space of index nodes results in a more uni-form distribution of query-time computation across nodes. At query time,the query is broadcast to each of the nodes, with the top k results from eachnode being merged to ﬁnd the top k documents for the query. A commonimplementation heuristic is to partition the document collection into indexesof documents that are more likely to score highly on most queries (using,for instance, techniques in Chapter 21) and low-scoring indexes with the re-maining documents. We only search the low-scoring indexes when there aretoo few matches in the high-scoring indexes, as described in Section 7.2.1.20.4Connectivity serversFor reasons to become clearer in Chapter 21, web search engines require aconnectivity server that supports fast connectivity queries on the web graph.CONNECTIVITY SERVERCONNECTIVITYQUERIESTypical connectivity queries are which URLs link to a given URL? and whichURLs does a given URL link to? To this end, we wish to store mappings inmemory from URL to out-links, and from URL to in-links. Applications in-clude crawl control, web graph analysis, sophisticated crawl optimizationand link analysis (to be covered in Chapter 21).Suppose that the Web had four billion pages, each with ten links to otherpages. In the simplest form, we would require 32 bits or 4 bytes to specifyeach end (source and destination) of each link, requiring a total of4 × 109 × 10 × 8 = 3.2 × 1011bytes of memory. Some basic properties of the web graph can be exploited touse well under 10% of this memory requirement. At ﬁrst sight, we appear tohave a data compression problem – which is amenable to a variety of stan-dard solutions. However, our goal is not to simply compress the web graphto ﬁt into memory; we must do so in a way that efﬁciently supports connec-tivity queries; this challenge is reminiscent of index compression (Chapter 5).We assume that each web page is represented by a unique integer; thespeciﬁc scheme used to assign these integers is described below. We buildan adjacency table that resembles an inverted index: it has a row for each webpage, with the rows ordered by the corresponding integers. The row for anypage p contains a sorted list of integers, each corresponding to a web pagethat links to p. This table permits us to respond to queries of the form whichpages link to p? In similar fashion we build a table whose entries are the pageslinked to by p. 45620Web crawling and indexes1: www.stanford.edu/alchemy2: www.stanford.edu/biology3: www.stanford.edu/biology/plant4: www.stanford.edu/biology/plant/copyright5: www.stanford.edu/biology/plant/people6: www.stanford.edu/chemistry◮Figure 20.5A lexicographically ordered set of URLs.This table representation cuts the space taken by the naive representation(in which we explicitly represent each link by its two end points, each a 32-bitinteger) by 50%. Our description below will focus on the table for the linksfrom each page; it should be clear that the techniques apply just as well tothe table of links to each page. To further reduce the storage for the table, weexploit several ideas:1. Similarity between lists: Many rows of the table have many entries incommon.Thus, if we explicitly represent a prototype row for severalsimilar rows, the remainder can be succinctly expressed in terms of theprototypical row.2. Locality: many links from a page go to “nearby” pages – pages on thesame host, for instance. This suggests that in encoding the destination ofa link, we can often use small integers and thereby save space.3. We use gap encodings in sorted lists: rather than store the destination ofeach link, we store the offset from the previous entry in the row.We now develop each of these techniques.In a lexicographic ordering of all URLs, we treat each URL as an alphanu-meric string and sort these strings. Figure 20.5 shows a segment of this sortedorder. For a true lexicographic sort of web pages, the domain name part ofthe URL should be inverted, so that www.stanford.edu becomes edu.stanford.www,but this is not necessary here since we are mainly concerned with links localto a single host.To each URL, we assign its position in this ordering as the unique identi-fying integer. Figure 20.6 shows an example of such a numbering and theresulting table. In this example sequence, www.stanford.edu/biologyis assigned the integer 2 since it is second in the sequence.We next exploit a property that stems from the way most websites arestructured to get similarity and locality. Most websites have a template witha set of links from each page in the site to a ﬁxed set of pages on the site (such 20.4Connectivity servers4571: 1, 2, 4, 8, 16, 32, 642: 1, 4, 9, 16, 25, 36, 49, 643: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 1444: 1, 4, 8, 16, 25, 36, 49, 64◮Figure 20.6A four-row segment of the table of links.as its copyright notice, terms of use, and so on). In this case, the rows cor-responding to pages in a website will have many table entries in common.Moreover, under the lexicographic ordering of URLs, it is very likely that thepages from a website appear as contiguous rows in the table.We adopt the following strategy: we walk down the table, encoding eachtable row in terms of the seven preceding rows. In the example of Figure 20.6,we could encode the fourth row as “the same as the row at offset 2 (mean-ing, two rows earlier in the table), with 9 replaced by 8”. This requires thespeciﬁcation of the offset, the integer(s) dropped (in this case 9) and the in-teger(s) added (in this case 8). The use of only the seven preceding rows hastwo advantages: (i) the offset can be expressed with only 3 bits; this choiceis optimized empirically (the reason for seven and not eight preceding rowsis the subject of Exercise 20.4) and (ii) ﬁxing the maximum offset to a smallvalue like seven avoids having to perform an expensive search among manycandidate prototypes in terms of which to express the current row.What if none of the preceding seven rows is a good prototype for express-ing the current row? This would happen, for instance, at each boundarybetween different websites as we walk down the rows of the table. In thiscase we simply express the row as starting from the empty set and “addingin” each integer in that row. By using gap encodings to store the gaps (ratherthan the actual integers) in each row, and encoding these gaps tightly basedon the distribution of their values, we obtain further space reduction. In ex-periments mentioned in Section 20.5, the series of techniques outlined hereappears to use as few as 3 bits per link, on average – a dramatic reductionfrom the 64 required in the naive representation.While these ideas give us a representation of sizable web graphs that com-fortably ﬁt in memory, we still need to support connectivity queries. Whatis entailed in retrieving from this representation the set of links from a page?First, we need an index lookup from (a hash of) the URL to its row numberin the table. Next, we need to reconstruct these entries, which may be en-coded in terms of entries in other rows. This entails following the offsets toreconstruct these other rows – a process that in principle could lead throughmany levels of indirection. In practice however, this does not happen veryoften. A heuristic for controlling this can be introduced into the construc- 45820Web crawling and indexestion of the table: when examining the preceding seven rows as candidatesfrom which to model the current row, we demand a threshold of similaritybetween the current row and the candidate prototype. This threshold mustbe chosen with care. If the threshold is set too high, we seldom use proto-types and express many rows afresh. If the threshold is too low, most rowsget expressed in terms of prototypes, so that at query time the reconstructionof a row leads to many levels of indirection through preceding prototypes.?Exercise 20.4We noted that expressing a row in terms of one of seven preceding rows allowed usto use no more than three bits to specify which of the preceding rows we are usingas prototype. Why seven and not eight preceding rows? (Hint: consider the case whennone of the preceding seven rows is a good prototype.)Exercise 20.5We noted that for the scheme in Section 20.4, decoding the links incident on a URLcould result in many levels of indirection. Construct an example in which the numberof levels of indirection grows linearly with the number of URLs.20.5References and further readingThe ﬁrst web crawler appears to be Matthew Gray’s Wanderer, written in thespring of 1993. The Mercator crawler is due to Najork and Heydon (Najorkand Heydon 2001; 2002); the treatment in this chapter follows their work.Other classic early descriptions of web crawling include Burner (1997), Brinand Page (1998), Cho et al. (1998) and the creators of the Webbase systemat Stanford (Hirai et al. 2000). Cho and Garcia-Molina (2002) give a taxon-omy and comparative study of different modes of communication betweenthe nodes of a distributed crawler. The Robots Exclusion Protocol standardis described at http://www.robotstxt.org/wc/exclusion.html. Boldi et al. (2002) andShkapenyuk and Suel (2002) provide more recent details of implementinglarge-scale distributed web crawlers.Our discussion of DNS resolution (Section 20.2.2) uses the current conven-tion for internet addresses, known as IPv4 (for Internet Protocol version 4) –each IP address is a sequence of four bytes. In the future, the convention foraddresses (collectively known as the internet address space) is likely to use anew standard known as IPv6 (http://www.ipv6.org/).Tomasic and Garcia-Molina (1993) and Jeong and Omiecinski (1995) arekey early papers evaluating term partitioning versus document partitioningfor distributed indexes. Document partitioning is found to be superior, atleast when the distribution of terms is skewed, as it typically is in practice.This result has generally been conﬁrmed in more recent work (MacFarlaneet al. 2000). But the outcome depends on the details of the distributed system; 20.5References and further reading459at least one thread of work has reached the opposite conclusion (Ribeiro-Neto and Barbosa 1998, Badue et al. 2001).Sornil (2001) argues for a par-titioning scheme that is a hybrid between term and document partitioning.Barroso et al. (2003) describe the distribution methods used at Google. Theﬁrst implementation of a connectivity server was described by Bharat et al.(1998). The scheme discussed in this chapter, currently believed to be thebest published scheme (achieving as few as 3 bits per link for encoding), isdescribed in a series of papers by Boldi and Vigna (2004a;b). ', '28914Vector space classiﬁcationThe document representation in Naive Bayes is a sequence of terms or a bi-nary vector ⟨e1, . . . , e|V|⟩∈{0, 1}|V|. In this chapter we adopt a differentrepresentation for text classiﬁcation, the vector space model, developed inChapter 6. It represents each document as a vector with one real-valued com-ponent, usually a tf-idf weight, for each term. Thus, the document space X,the domain of the classiﬁcation function γ, is R|V|. This chapter introduces anumber of classiﬁcation methods that operate on real-valued vectors.The basic hypothesis in using the vector space model for classiﬁcation isthe contiguity hypothesis.CONTIGUITYHYPOTHESISContiguity hypothesis. Documents in the same class form a contigu-ous region and regions of different classes do not overlap.There are many classiﬁcation tasks, in particular the type of text classiﬁcationthat we encountered in Chapter 13, where classes can be distinguished byword patterns. For example, documents in the class China tend to have highvalues on dimensions like Chinese, Beijing, and Mao whereas documents in theclass UK tend to have high values for London, British and Queen. Documentsof the two classes therefore form distinct contiguous regions as shown inFigure 14.1 and we can draw boundaries that separate them and classify newdocuments. How exactly this is done is the topic of this chapter.Whether or not a set of documents is mapped into a contiguous region de-pends on the particular choices we make for the document representation:type of weighting, stop list etc. To see that the document representation iscrucial, consider the two classes written by a group vs. written by a single per-son. Frequent occurrence of the ﬁrst person pronoun I is evidence for thesingle-person class. But that information is likely deleted from the documentrepresentation if we use a stop list. If the document representation chosenis unfavorable, the contiguity hypothesis will not hold and successful vectorspace classiﬁcation is not possible.The same considerations that led us to prefer weighted representations, inparticular length-normalized tf-idf representations, in Chapters 6 and 7 also 29014Vector space classiﬁcationxxxx⋄⋄⋄⋄⋄⋄ChinaKenyaUK⋆◮Figure 14.1Vector space classiﬁcation into three classes.apply here. For example, a term with 5 occurrences in a document should geta higher weight than a term with one occurrence, but a weight 5 times largerwould give too much emphasis to the term. Unweighted and unnormalizedcounts should not be used in vector space classiﬁcation.We introduce two vector space classiﬁcation methods in this chapter, Roc-chio and kNN. Rocchio classiﬁcation (Section 14.2) divides the vector spaceinto regions centered on centroids or prototypes, one for each class, computedPROTOTYPEas the center of mass of all documents in the class. Rocchio classiﬁcation issimple and efﬁcient, but inaccurate if classes are not approximately sphereswith similar radii.kNN or k nearest neighbor classiﬁcation (Section 14.3) assigns the majorityclass of the k nearest neighbors to a test document. kNN requires no explicittraining and can use the unprocessed training set directly in classiﬁcation.It is less efﬁcient than other classiﬁcation methods in classifying documents.If the training set is large, then kNN can handle non-spherical and othercomplex classes better than Rocchio.A large number of text classiﬁers can be viewed as linear classiﬁers – clas-siﬁers that classify based on a simple linear combination of the features (Sec-tion 14.4). Such classiﬁers partition the space of features into regions sepa-rated by linear decision hyperplanes, in a manner to be detailed below. Becauseof the bias-variance tradeoff (Section 14.6) more complex nonlinear models 14.1Document representations and measures of relatedness in vector spaces291dtruedprojectedx1x2 x3 x4x5x′1x′2 x′3 x′4x′5x′1x′2x′3x′4x′5◮Figure 14.2Projections of small areas of the unit sphere preserve distances. Left:A projection of the 2D semicircle to 1D. For the points x1, x2, x3, x4, x5 at x coordinates−0.9, −0.2, 0, 0.2, 0.9 the distance |x2x3| ≈0.201 only differs by 0.5% from |x′2x′3| =0.2; but |x1x3|/|x′1x′3| = dtrue/dprojected ≈1.06/0.9 ≈1.18 is an example of a largedistortion (18%) when projecting a large area. Right: The corresponding projection ofthe 3D hemisphere to 2D.are not systematically better than linear models. Nonlinear models havemore parameters to ﬁt on a limited amount of training data and are morelikely to make mistakes for small and noisy data sets.When applying two-class classiﬁers to problems with more than two classes,there are one-of tasks – a document must be assigned to exactly one of severalmutually exclusive classes – and any-of tasks – a document can be assigned toany number of classes as we will explain in Section 14.5. Two-class classiﬁerssolve any-of problems and can be combined to solve one-of problems.14.1Document representations and measures of relatedness in vec-tor spacesAs in Chapter 6, we represent documents as vectors in R|V| in this chapter.To illustrate properties of document vectors in vector classiﬁcation, we willrender these vectors as points in a plane as in the example in Figure 14.1.In reality, document vectors are length-normalized unit vectors that pointto the surface of a hypersphere. We can view the 2D planes in our ﬁguresas projections onto a plane of the surface of a (hyper-)sphere as shown inFigure 14.2. Distances on the surface of the sphere and on the projectionplane are approximately the same as long as we restrict ourselves to smallareas of the surface and choose an appropriate projection (Exercise 14.1). 29214Vector space classiﬁcationDecisions of many vector space classiﬁers are based on a notion of dis-tance, e.g., when computing the nearest neighbors in kNN classiﬁcation.We will use Euclidean distance in this chapter as the underlying distancemeasure. We observed earlier (Exercise 6.18, page 131) that there is a directcorrespondence between cosine similarity and Euclidean distance for length-normalized vectors. In vector space classiﬁcation, it rarely matters whetherthe relatedness of two documents is expressed in terms of similarity or dis-tance.However, in addition to documents, centroids or averages of vectors alsoplay an important role in vector space classiﬁcation. Centroids are not length-normalized. For unnormalized vectors, dot product, cosine similarity andEuclidean distance all have different behavior in general (Exercise 14.6). Wewill be mostly concerned with small local regions when computing the sim-ilarity between a document and a centroid, and the smaller the region themore similar the behavior of the three measures is.?Exercise 14.1For small areas, distances on the surface of the hypersphere are approximated wellby distances on its projection (Figure 14.2) because α ≈sin α for small angles. Forwhat size angle is the distortion α/ sin(α) (i) 1.01, (ii) 1.05 and (iii) 1.1?14.2Rocchio classiﬁcationFigure 14.1 shows three classes, China, UK and Kenya, in a two-dimensional(2D) space. Documents are shown as circles, diamonds and X’s. The bound-aries in the ﬁgure, which we call decision boundaries, are chosen to separateDECISION BOUNDARYthe three classes, but are otherwise arbitrary. To classify a new document,depicted as a star in the ﬁgure, we determine the region it occurs in and as-sign it the class of that region – China in this case. Our task in vector spaceclassiﬁcation is to devise algorithms that compute good boundaries where“good” means high classiﬁcation accuracy on data unseen during training.Perhaps the best-known way of computing good class boundaries is Roc-ROCCHIOCLASSIFICATIONchio classiﬁcation, which uses centroids to deﬁne the boundaries. The centroidCENTROIDof a class c is computed as the vector average or center of mass of its mem-bers:⃗µ(c) =1|Dc| ∑d∈Dc⃗v(d)(14.1)where Dc is the set of documents in D whose class is c: Dc = {d : ⟨d, c⟩∈D}.We denote the normalized vector of d by ⃗v(d) (Equation (6.11), page 122).Three example centroids are shown as solid circles in Figure 14.3.The boundary between two classes in Rocchio classiﬁcation is the set ofpoints with equal distance from the two centroids. For example, |a1| = |a2|, 14.2Rocchio classiﬁcation293xxxx⋄⋄⋄⋄⋄⋄ChinaKenyaUK⋆a1a2b1b2c1c2◮Figure 14.3Rocchio classiﬁcation.|b1| = |b2|, and |c1| = |c2| in the ﬁgure. This set of points is always a line.The generalization of a line in M-dimensional space is a hyperplane, whichwe deﬁne as the set of points ⃗x that satisfy:⃗wT⃗x = b(14.2)where ⃗w is the M-dimensional normal vector1 of the hyperplane and b is aNORMAL VECTORconstant. This deﬁnition of hyperplanes includes lines (any line in 2D canbe deﬁned by w1x1 + w2x2 = b) and 2-dimensional planes (any plane in 3Dcan be deﬁned by w1x1 + w2x2 + w3x3 = b). A line divides a plane in two,a plane divides 3-dimensional space in two, and hyperplanes divide higher-dimensional spaces in two.Thus, the boundaries of class regions in Rocchio classiﬁcation are hyper-planes. The classiﬁcation rule in Rocchio is to classify a point in accordancewith the region it falls into. Equivalently, we determine the centroid⃗µ(c) thatthe point is closest to and then assign it to c. As an example, consider the starin Figure 14.3. It is located in the China region of the space and Rocchiotherefore assigns it to China. We show the Rocchio algorithm in pseudocodein Figure 14.4.1. Recall from basic linear algebra that ⃗v · ⃗w = ⃗vT⃗w, i.e., the dot product of ⃗v and ⃗w equals theproduct by matrix multiplication of the transpose of ⃗v and ⃗w. 29414Vector space classiﬁcationterm weightsvectorChineseJapanTokyoMacaoBeijingShanghai⃗d100001.00⃗d2000001.0⃗d30001.000⃗d400.710.71000⃗d500.710.71000⃗µc0000.330.330.33⃗µc00.710.71000◮Table 14.1Vectors and class centroids for the data in Table 13.1.\\x0fExample 14.1:Table 14.1 shows the tf-idf vector representations of the ﬁve docu-ments in Table 13.1 (page 261), using the formula (1+ log10 tft,d) log10(4/dft) if tft,d >0 (Equation (6.14), page 127). The two class centroids are µc = 1/3 · (⃗d1 + ⃗d2 + ⃗d3)and µc = 1/1 · (⃗d4).The distances of the test document from the centroids are|µc −⃗d5| ≈1.15 and |µc −⃗d5| = 0.0. Thus, Rocchio assigns d5 to c.The separating hyperplane in this case has the following parameters:⃗w≈(0 −0.71 −0.71 1/3 1/3 1/3)Tb=−1/3See Exercise 14.15 for how to compute ⃗w and b. We can easily verify that this hy-perplane separates the documents as desired: ⃗wT⃗d1 ≈0 · 0 + −0.71 · 0 + −0.71 · 0 +1/3 · 0 + 1/3 · 1.0 + 1/3 · 0 = 1/3 > b (and, similarly, ⃗wT⃗di > b for i = 2 and i = 3)and ⃗wT⃗d4 = −1 < b. Thus, documents in c are above the hyperplane (⃗wT⃗d > b) anddocuments in c are below the hyperplane (⃗wT⃗d < b).The assignment criterion in Figure 14.4 is Euclidean distance (APPLYROC-CHIO, line 1). An alternative is cosine similarity:Assign d to class c = arg maxc′cos(⃗µ(c′),⃗v(d))As discussed in Section 14.1, the two assignment criteria will sometimesmake different classiﬁcation decisions. We present the Euclidean distancevariant of Rocchio classiﬁcation here because it emphasizes Rocchio’s closecorrespondence to K-means clustering (Section 16.4, page 360).Rocchio classiﬁcation is a form of Rocchio relevance feedback (Section 9.1.1,page 178). The average of the relevant documents, corresponding to the mostimportant component of the Rocchio vector in relevance feedback (Equa-tion (9.3), page 182), is the centroid of the “class” of relevant documents.We omit the query component of the Rocchio formula in Rocchio classiﬁca-tion since there is no query in text classiﬁcation. Rocchio classiﬁcation can be 14.2Rocchio classiﬁcation295TRAINROCCHIO(C, D)1for each cj ∈C2do Dj ←{d : ⟨d, cj⟩∈D}3⃗µj ←1|Dj| ∑d∈Dj ⃗v(d)4return {⃗µ1, . . . ,⃗µJ}APPLYROCCHIO({⃗µ1, . . . ,⃗µJ}, d)1return arg minj |⃗µj −⃗v(d)|◮Figure 14.4Rocchio classiﬁcation: Training and testing.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbXXABo◮Figure 14.5The multimodal class “a” consists of two different clusters (smallupper circles centered on X’s).Rocchio classiﬁcation will misclassify “o” as “a”because it is closer to the centroid A of the “a” class than to the centroid B of the “b”class.applied to J > 2 classes whereas Rocchio relevance feedback is designed todistinguish only two classes, relevant and nonrelevant.In addition to respecting contiguity, the classes in Rocchio classiﬁcationmust be approximate spheres with similar radii. In Figure 14.3, the solidsquare just below the boundary between UK and Kenya is a better ﬁt for theclass UK since UK is more scattered than Kenya. But Rocchio assigns it toKenya because it ignores details of the distribution of points in a class andonly uses distance from the centroid for classiﬁcation.The assumption of sphericity also does not hold in Figure 14.5. We can- 29614Vector space classiﬁcationmodetime complexitytrainingΘ(|D|Lave + |C||V|)testingΘ(La + |C|Ma) = Θ(|C|Ma)◮Table 14.2Training and test times for Rocchio classiﬁcation.Lave is the averagenumber of tokens per document. La and Ma are the numbers of tokens and types,respectively, in the test document. Computing Euclidean distance between the classcentroids and a document is Θ(|C|Ma).not represent the “a” class well with a single prototype because it has twoclusters. Rocchio often misclassiﬁes this type of multimodal class. A text clas-MULTIMODAL CLASSsiﬁcation example for multimodality is a country like Burma, which changedits name to Myanmar in 1989. The two clusters before and after the namechange need not be close to each other in space. We also encountered theproblem of multimodality in relevance feedback (Section 9.1.2, page 184).Two-class classiﬁcation is another case where classes are rarely distributedlike spheres with similar radii. Most two-class classiﬁers distinguish betweena class like China that occupies a small region of the space and its widelyscattered complement. Assuming equal radii will result in a large numberof false positives. Most two-class classiﬁcation problems therefore require amodiﬁed decision rule of the form:Assign d to class c iff |⃗µ(c) −⃗v(d)| < |⃗µ(c) −⃗v(d)| −bfor a positive constant b. As in Rocchio relevance feedback, the centroid ofthe negative documents is often not used at all, so that the decision criterionsimpliﬁes to |⃗µ(c) −⃗v(d)| < b′ for a positive constant b′.Table 14.2 gives the time complexity of Rocchio classiﬁcation.2 Adding alldocuments to their respective (unnormalized) centroid is Θ(|D|Lave) (as op-posed to Θ(|D||V|)) since we need only consider non-zero entries. Dividingeach vector sum by the size of its class to compute the centroid is Θ(|V|).Overall, training time is linear in the size of the collection (cf. Exercise 13.1).Thus, Rocchio classiﬁcation and Naive Bayes have the same linear trainingtime complexity.In the next section, we will introduce another vector space classiﬁcationmethod, kNN, that deals better with classes that have non-spherical, discon-nected or other irregular shapes.?Exercise 14.2[⋆]Show that Rocchio classiﬁcation can assign a label to a document that is different fromits training set label.2. We write Θ(|D|Lave) for Θ(T) and assume that the length of test documents is bounded aswe did on page 262. 14.3k nearest neighbor297xxxxxxxxxxx⋄⋄⋄⋄⋄⋄⋄⋄⋄⋄⋄⋆◮Figure 14.6Voronoi tessellation and decision boundaries (double lines) in 1NNclassiﬁcation. The three classes are: X, circle and diamond.14.3k nearest neighborUnlike Rocchio, k nearest neighbor or kNN classiﬁcation determines the deci-k NEAREST NEIGHBORCLASSIFICATIONsion boundary locally. For 1NN we assign each document to the class of itsclosest neighbor. For kNN we assign each document to the majority class ofits k closest neighbors where k is a parameter. The rationale of kNN classiﬁ-cation is that, based on the contiguity hypothesis, we expect a test documentd to have the same label as the training documents located in the local regionsurrounding d.Decision boundaries in 1NN are concatenated segments of the Voronoi tes-VORONOITESSELLATIONsellation as shown in Figure 14.6. The Voronoi tessellation of a set of objectsdecomposes space into Voronoi cells, where each object’s cell consists of allpoints that are closer to the object than to other objects. In our case, the ob-jects are documents. The Voronoi tessellation then partitions the plane into|D| convex polygons, each containing its corresponding document (and noother) as shown in Figure 14.6, where a convex polygon is a convex region in2-dimensional space bounded by lines.For general k ∈N in kNN, consider the region in the space for which theset of k nearest neighbors is the same. This again is a convex polygon and thespace is partitioned into convex polygons, within each of which the set of k 29814Vector space classiﬁcationTRAIN-KNN(C, D)1D′ ←PREPROCESS(D)2k ←SELECT-K(C, D′)3return D′, kAPPLY-KNN(C, D′, k, d)1Sk ←COMPUTENEARESTNEIGHBORS(D′, k, d)2for each cj ∈C3do pj ←|Sk ∩cj|/k4return arg maxj pj◮Figure 14.7kNN training (with preprocessing) and testing. pj is an estimate forP(cj|Sk) = P(cj|d). cj denotes the set of all documents in the class cj.nearest neighbors is invariant (Exercise 14.11).31NN is not very robust. The classiﬁcation decision of each test documentrelies on the class of a single training document, which may be incorrectlylabeled or atypical. kNN for k > 1 is more robust. It assigns documents tothe majority class of their k closest neighbors, with ties broken randomly.There is a probabilistic version of this kNN classiﬁcation algorithm. Wecan estimate the probability of membership in class c as the proportion of thek nearest neighbors in c. Figure 14.6 gives an example for k = 3. Probabil-ity estimates for class membership of the star are ˆP(circle class|star) = 1/3,ˆP(X class|star) = 2/3, and ˆP(diamond class|star) = 0. The 3nn estimate( ˆP1(circle class|star) = 1/3) and the 1nn estimate ( ˆP1(circle class|star) = 1)differ with 3nn preferring the X class and 1nn preferring the circle class .The parameter k in kNN is often chosen based on experience or knowledgeabout the classiﬁcation problem at hand. It is desirable for k to be odd tomake ties less likely. k = 3 and k = 5 are common choices, but much largervalues between 50 and 100 are also used. An alternative way of setting theparameter is to select the k that gives best results on a held-out portion of thetraining set.We can also weight the “votes” of the k nearest neighbors by their cosine3. The generalization of a polygon to higher dimensions is a polytope. A polytope is a regionin M-dimensional space bounded by (M −1)-dimensional hyperplanes. In M dimensions, thedecision boundaries for kNN consist of segments of (M −1)-dimensional hyperplanes that formthe Voronoi tessellation into convex polytopes for the training set of documents. The decisioncriterion of assigning a document to the majority class of its k nearest neighbors applies equallyto M = 2 (tessellation into polygons) and M > 2 (tessellation into polytopes). 14.3k nearest neighbor299kNN with preprocessing of training settrainingΘ(|D|Lave)testingΘ(La + |D|MaveMa) = Θ(|D|MaveMa)kNN without preprocessing of training settrainingΘ(1)testingΘ(La + |D|LaveMa) = Θ(|D|LaveMa)◮Table 14.3Training and test times for kNN classiﬁcation. Mave is the average sizeof the vocabulary of documents in the collection.similarity. In this scheme, a class’s score is computed as:score(c, d) =∑d′∈Sk(d)Ic(d′) cos(⃗v(d′),⃗v(d))where Sk(d) is the set of d’s k nearest neighbors and Ic(d′) = 1 iff d′ is in classc and 0 otherwise. We then assign the document to the class with the highestscore. Weighting by similarities is often more accurate than simple voting.For example, if two classes have the same number of neighbors in the top k,the class with the more similar neighbors wins.Figure 14.7 summarizes the kNN algorithm.\\x0fExample 14.2:The distances of the test document from the four training docu-ments in Table 14.1 are |⃗d1 −⃗d5| = |⃗d2 −⃗d5| = |⃗d3 −⃗d5| ≈1.41 and |⃗d4 −⃗d5| = 0.0.d5’s nearest neighbor is therefore d4 and 1NN assigns d5 to d4’s class, c.$14.3.1Time complexity and optimality of kNNTable 14.3 gives the time complexity of kNN. kNN has properties that arequite different from most other classiﬁcation algorithms. Training a kNNclassiﬁer simply consists of determining k and preprocessing documents. Infact, if we preselect a value for k and do not preprocess, then kNN requiresno training at all. In practice, we have to perform preprocessing steps liketokenization. It makes more sense to preprocess training documents onceas part of the training phase rather than repeatedly every time we classify anew test document.Test time is Θ(|D|MaveMa) for kNN. It is linear in the size of the trainingset as we need to compute the distance of each training document from thetest document. Test time is independent of the number of classes J. kNNtherefore has a potential advantage for problems with large J.In kNN classiﬁcation, we do not perform any estimation of parameters aswe do in Rocchio classiﬁcation (centroids) or in Naive Bayes (priors and con-ditional probabilities). kNN simply memorizes all examples in the training 30014Vector space classiﬁcationset and then compares the test document to them. For this reason, kNN isalso called memory-based learning or instance-based learning. It is usually desir-MEMORY-BASEDLEARNINGable to have as much training data as possible in machine learning. But inkNN large training sets come with a severe efﬁciency penalty in classiﬁca-tion.Can kNN testing be made more efﬁcient than Θ(|D|MaveMa) or, ignoringthe length of documents, more efﬁcient than Θ(|D|)? There are fast kNNalgorithms for small dimensionality M (Exercise 14.12). There are also ap-proximations for large M that give error bounds for speciﬁc efﬁciency gains(see Section 14.7). These approximations have not been extensively testedfor text classiﬁcation applications, so it is not clear whether they can achievemuch better efﬁciency than Θ(|D|) without a signiﬁcant loss of accuracy.The reader may have noticed the similarity between the problem of ﬁndingnearest neighbors of a test document and ad hoc retrieval, where we searchfor the documents with the highest similarity to the query (Section 6.3.2,page 123). In fact, the two problems are both k nearest neighbor problemsand only differ in the relative density of (the vector of) the test documentin kNN (10s or 100s of non-zero entries) versus the sparseness of (the vec-tor of) the query in ad hoc retrieval (usually fewer than 10 non-zero entries).We introduced the inverted index for efﬁcient ad hoc retrieval in Section 1.1(page 6). Is the inverted index also the solution for efﬁcient kNN?An inverted index restricts a search to those documents that have at leastone term in common with the query. Thus in the context of kNN, the in-verted index will be efﬁcient if the test document has no term overlap with alarge number of training documents. Whether this is the case depends on theclassiﬁcation problem. If documents are long and no stop list is used, thenless time will be saved. But with short documents and a large stop list, aninverted index may well cut the average test time by a factor of 10 or more.The search time in an inverted index is a function of the length of the post-ings lists of the terms in the query. Postings lists grow sublinearly with thelength of the collection since the vocabulary increases according to Heaps’law – if the probability of occurrence of some terms increases, then the prob-ability of occurrence of others must decrease. However, most new terms areinfrequent. We therefore take the complexity of inverted index search to beΘ(T) (as discussed in Section 2.4.2, page 41) and, assuming average docu-ment length does not change over time, Θ(T) = Θ(|D|).As we will see in the next chapter, kNN’s effectiveness is close to that of themost accurate learning methods in text classiﬁcation (Table 15.2, page 334). Ameasure of the quality of a learning method is its Bayes error rate, the averageBAYES ERROR RATEerror rate of classiﬁers learned by it for a particular problem. kNN is notoptimal for problems with a non-zero Bayes error rate – that is, for problemswhere even the best possible classiﬁer has a non-zero classiﬁcation error. Theerror of 1NN is asymptotically (as the training set increases) bounded by 14.4Linear versus nonlinear classiﬁers301◮Figure 14.8There are an inﬁnite number of hyperplanes that separate two linearlyseparable classes.twice the Bayes error rate. That is, if the optimal classiﬁer has an error rateof x, then 1NN has an asymptotic error rate of less than 2x. This is due to theeffect of noise – we already saw one example of noise in the form of noisyfeatures in Section 13.5 (page 271), but noise can also take other forms as wewill discuss in the next section. Noise affects two components of kNN: thetest document and the closest training document. The two sources of noiseare additive, so the overall error of 1NN is twice the optimal error rate. Forproblems with Bayes error rate 0, the error rate of 1NN will approach 0 asthe size of the training set increases.?Exercise 14.3Explain why kNN handles multimodal classes better than Rocchio.14.4Linear versus nonlinear classiﬁersIn this section, we show that the two learning methods Naive Bayes andRocchio are instances of linear classiﬁers, the perhaps most important groupof text classiﬁers, and contrast them with nonlinear classiﬁers. To simplifythe discussion, we will only consider two-class classiﬁers in this section anddeﬁne a linear classiﬁer as a two-class classiﬁer that decides class membershipLINEAR CLASSIFIERby comparing a linear combination of the features to a threshold.In two dimensions, a linear classiﬁer is a line. Five examples are shownin Figure 14.8. These lines have the functional form w1x1 + w2x2 = b. Theclassiﬁcation rule of a linear classiﬁer is to assign a document to c if w1x1 +w2x2 > b and to c if w1x1 + w2x2 ≤b. Here, (x1, x2)T is the two-dimensionalvector representation of the document and (w1, w2)T is the parameter vector 30214Vector space classiﬁcationAPPLYLINEARCLASSIFIER(⃗w, b,⃗x)1score ←∑Mi=1 wixi2if score > b3then return 14else return 0◮Figure 14.9Linear classiﬁcation algorithm.that deﬁnes (together with b) the decision boundary. An alternative geomet-ric interpretation of a linear classiﬁer is provided in Figure 15.7 (page 343).We can generalize this 2D linear classiﬁer to higher dimensions by deﬁninga hyperplane as we did in Equation (14.2), repeated here as Equation (14.3):⃗wT⃗x = b(14.3)The assignment criterion then is: assign to c if ⃗wT⃗x > b and to c if ⃗wT⃗x ≤b.We call a hyperplane that we use as a linear classiﬁer a decision hyperplane.DECISION HYPERPLANEThe corresponding algorithm for linear classiﬁcation in M dimensions isshown in Figure 14.9. Linear classiﬁcation at ﬁrst seems trivial given thesimplicity of this algorithm. However, the difﬁculty is in training the lin-ear classiﬁer, that is, in determining the parameters ⃗w and b based on thetraining set. In general, some learning methods compute much better param-eters than others where our criterion for evaluating the quality of a learningmethod is the effectiveness of the learned linear classiﬁer on new data.We now show that Rocchio and Naive Bayes are linear classiﬁers. To seethis for Rocchio, observe that a vector ⃗x is on the decision boundary if it hasequal distance to the two class centroids:|⃗µ(c1) −⃗x| = |⃗µ(c2) −⃗x|(14.4)Some basic arithmetic shows that this corresponds to a linear classiﬁer withnormal vector ⃗w = ⃗µ(c1) −⃗µ(c2) and b = 0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2) (Exer-cise 14.15).We can derive the linearity of Naive Bayes from its decision rule, whichchooses the category c with the largest ˆP(c|d) (Figure 13.2, page 260) where:ˆP(c|d) ∝ˆP(c) ∏1≤k≤ndˆP(tk|c)and nd is the number of tokens in the document that are part of the vocabu-lary. Denoting the complement category as ¯c, we obtain for the log odds:logˆP(c|d)ˆP(¯c|d) = logˆP(c)ˆP(¯c) + ∑1≤k≤ndlogˆP(tk|c)ˆP(tk|¯c)(14.5) 14.4Linear versus nonlinear classiﬁers303tiwid1id2itiwid1id2iprime0.7001dlrs-0.7111rate0.6710world-0.3510interest0.6300sees-0.3300rates0.6000year-0.2500discount0.4610group-0.2400bundesbank0.4300dlr-0.2400◮Table 14.4A linear classiﬁer. The dimensions ti and parameters wi of a linearclassiﬁer for the class interest (as in interest rate) in Reuters-21578. The threshold isb = 0. Terms like dlr and world have negative weights because they are indicators forthe competing class currency.We choose class c if the odds are greater than 1 or, equivalently, if the logodds are greater than 0. It is easy to see that Equation (14.5) is an instanceof Equation (14.3) for wi = log[ ˆP(ti|c)/ ˆP(ti|¯c)], xi = number of occurrencesof ti in d, and b = −log[ ˆP(c)/ ˆP(¯c)]. Here, the index i, 1 ≤i ≤M, refersto terms of the vocabulary (not to positions in d as k does; cf. Section 13.4.1,page 270) and ⃗x and ⃗w are M-dimensional vectors. So in log space, NaiveBayes is a linear classiﬁer.\\x0fExample 14.3:Table 14.4 deﬁnes a linear classiﬁer for the category interest inReuters-21578 (see Section 13.6, page 279). We assign document ⃗d1 “rate discountdlrs world” to interest since ⃗wT⃗d1 = 0.67 · 1 + 0.46 · 1 + (−0.71) · 1 + (−0.35) · 1 =0.07 > 0 = b. We assign ⃗d2 “prime dlrs” to the complement class (not in interest) since⃗wT⃗d2 = −0.01 ≤b. For simplicity, we assume a simple binary vector representationin this example: 1 for occurring terms, 0 for non-occurring terms.Figure 14.10 is a graphical example of a linear problem, which we deﬁne tomean that the underlying distributions P(d|c) and P(d|c) of the two classesare separated by a line. We call this separating line the class boundary. It isCLASS BOUNDARYthe “true” boundary of the two classes and we distinguish it from the deci-sion boundary that the learning method computes to approximate the classboundary.As is typical in text classiﬁcation, there are some noise documents in Fig-NOISE DOCUMENTure 14.10 (marked with arrows) that do not ﬁt well into the overall distri-bution of the classes. In Section 13.5 (page 271), we deﬁned a noise featureas a misleading feature that, when included in the document representation,on average increases the classiﬁcation error. Analogously, a noise documentis a document that, when included in the training set, misleads the learn-ing method and increases classiﬁcation error.Intuitively, the underlyingdistribution partitions the representation space into areas with mostly ho- 30414Vector space classiﬁcation◮Figure 14.10A linear problem with noise. In this hypothetical web page classiﬁ-cation scenario, Chinese-only web pages are solid circles and mixed Chinese-Englishweb pages are squares. The two classes are separated by a linear class boundary(dashed line, short dashes), except for three noise documents (marked with arrows).mogeneous class assignments. A document that does not conform with thedominant class in its area is a noise document.Noise documents are one reason why training a linear classiﬁer is hard. Ifwe pay too much attention to noise documents when choosing the decisionhyperplane of the classiﬁer, then it will be inaccurate on new data. Morefundamentally, it is usually difﬁcult to determine which documents are noisedocuments and therefore potentially misleading.If there exists a hyperplane that perfectly separates the two classes, thenwe call the two classes linearly separable. In fact, if linear separability holds,LINEAR SEPARABILITYthen there is an inﬁnite number of linear separators (Exercise 14.4) as illus-trated by Figure 14.8, where the number of possible separating hyperplanesis inﬁnite.Figure 14.8 illustrates another challenge in training a linear classiﬁer. If weare dealing with a linearly separable problem, then we need a criterion forselecting among all decision hyperplanes that perfectly separate the trainingdata. In general, some of these hyperplanes will do well on new data, some 14.4Linear versus nonlinear classiﬁers3050.00.20.40.60.81.00.00.20.40.60.81.0◮Figure 14.11A nonlinear problem.will not.An example of a nonlinear classiﬁer is kNN. The nonlinearity of kNN isNONLINEARCLASSIFIERintuitively clear when looking at examples like Figure 14.6. The decisionboundaries of kNN (the double lines in Figure 14.6) are locally linear seg-ments, but in general have a complex shape that is not equivalent to a line in2D or a hyperplane in higher dimensions.Figure 14.11 is another example of a nonlinear problem: there is no goodlinear separator between the distributions P(d|c) and P(d|c) because of thecircular “enclave” in the upper left part of the graph. Linear classiﬁers mis-classify the enclave, whereas a nonlinear classiﬁer like kNN will be highlyaccurate for this type of problem if the training set is large enough.If a problem is nonlinear and its class boundaries cannot be approximatedwell with linear hyperplanes, then nonlinear classiﬁers are often more accu-rate than linear classiﬁers. If a problem is linear, it is best to use a simplerlinear classiﬁer.?Exercise 14.4Prove that the number of linear separators of two classes is either inﬁnite or zero. 30614Vector space classiﬁcation14.5Classiﬁcation with more than two classesWe can extend two-class linear classiﬁers to J > 2 classes. The method to usedepends on whether the classes are mutually exclusive or not.Classiﬁcation for classes that are not mutually exclusive is called any-of,ANY-OFCLASSIFICATIONmultilabel, or multivalue classiﬁcation. In this case, a document can belong toseveral classes simultaneously, or to a single class, or to none of the classes.A decision on one class leaves all options open for the others. It is some-times said that the classes are independent of each other, but this is misleadingsince the classes are rarely statistically independent in the sense deﬁned onpage 275. In terms of the formal deﬁnition of the classiﬁcation problem inEquation (13.1) (page 256), we learn J different classiﬁers γj in any-of classi-ﬁcation, each returning either cj or cj: γj(d) ∈{cj, cj}.Solving an any-of classiﬁcation task with linear classiﬁers is straightfor-ward:1. Build a classiﬁer for each class, where the training set consists of the setof documents in the class (positive labels) and its complement (negativelabels).2. Given the test document, apply each classiﬁer separately. The decision ofone classiﬁer has no inﬂuence on the decisions of the other classiﬁers.The second type of classiﬁcation with more than two classes is one-of clas-ONE-OFCLASSIFICATIONsiﬁcation. Here, the classes are mutually exclusive. Each document mustbelong to exactly one of the classes. One-of classiﬁcation is also called multi-nomial, polytomous4, multiclass, or single-label classiﬁcation. Formally, there is asingle classiﬁcation function γ in one-of classiﬁcation whose range is C, i.e.,γ(d) ∈{c1, . . . , cJ}. kNN is a (nonlinear) one-of classiﬁer.True one-of problems are less common in text classiﬁcation than any-ofproblems. With classes like UK, China, poultry, or coffee, a document can berelevant to many topics simultaneously – as when the prime minister of theUK visits China to talk about the coffee and poultry trade.Nevertheless, we will often make a one-of assumption, as we did in Fig-ure 14.1, even if classes are not really mutually exclusive. For the classiﬁca-tion problem of identifying the language of a document, the one-of assump-tion is a good approximation as most text is written in only one language.In such cases, imposing a one-of constraint can increase the classiﬁer’s ef-fectiveness because errors that are due to the fact that the any-of classiﬁersassigned a document to either no class or more than one class are eliminated.J hyperplanes do not divide R|V| into J distinct regions as illustrated inFigure 14.12. Thus, we must use a combination method when using two-class linear classiﬁers for one-of classiﬁcation. The simplest method is to4. A synonym of polytomous is polychotomous. 14.5Classiﬁcation with more than two classes307?◮Figure 14.12J hyperplanes do not divide space into J disjoint regions.rank classes and then select the top-ranked class. Geometrically, the rankingcan be with respect to the distances from the J linear separators. Documentsclose to a class’s separator are more likely to be misclassiﬁed, so the greaterthe distance from the separator, the more plausible it is that a positive clas-siﬁcation decision is correct. Alternatively, we can use a direct measure ofconﬁdence to rank classes, e.g., probability of class membership. We canstate this algorithm for one-of classiﬁcation with linear classiﬁers as follows:1. Build a classiﬁer for each class, where the training set consists of the setof documents in the class (positive labels) and its complement (negativelabels).2. Given the test document, apply each classiﬁer separately.3. Assign the document to the class with• the maximum score,• the maximum conﬁdence value,• or the maximum probability.An important tool for analyzing the performance of a classiﬁer for J > 2classes is the confusion matrix. The confusion matrix shows for each pair ofCONFUSION MATRIXclasses ⟨c1, c2⟩, how many documents from c1 were incorrectly assigned to c2.In Table 14.5, the classiﬁer manages to distinguish the three ﬁnancial classesmoney-fx, trade, and interest from the three agricultural classes wheat, corn,and grain, but makes many errors within these two groups. The confusionmatrix can help pinpoint opportunities for improving the accuracy of the 30814Vector space classiﬁcationassigned classmoney-fxtradeinterestwheatcorngraintrue classmoney-fx95010000trade1190010interest1300000wheat0013437corn10213265grain00214510◮Table 14.5A confusion matrix for Reuters-21578. For example, 14 documentsfrom grain were incorrectly assigned to wheat. Adapted from Picca et al. (2006).system. For example, to address the second largest error in Table 14.5 (14 inthe row grain), one could attempt to introduce features that distinguish wheatdocuments from grain documents.?Exercise 14.5Create a training set of 300 documents, 100 each from three different languages (e.g.,English, French, Spanish). Create a test set by the same procedure, but also add 100documents from a fourth language. Train (i) a one-of classiﬁer (ii) an any-of classi-ﬁer on this training set and evaluate it on the test set. (iii) Are there any interestingdifferences in how the two classiﬁers behave on this task?$14.6The bias-variance tradeoffNonlinear classiﬁers are more powerful than linear classiﬁers.For someproblems, there exists a nonlinear classiﬁer with zero classiﬁcation error, butno such linear classiﬁer. Does that mean that we should always use nonlinearclassiﬁers for optimal effectiveness in statistical text classiﬁcation?To answer this question, we introduce the bias-variance tradeoff in this sec-tion, one of the most important concepts in machine learning. The tradeoffhelps explain why there is no universally optimal learning method. Selectingan appropriate learning method is therefore an unavoidable part of solvinga text classiﬁcation problem.Throughout this section, we use linear and nonlinear classiﬁers as proto-typical examples of “less powerful” and “more powerful” learning, respec-tively. This is a simpliﬁcation for a number of reasons. First, many nonlinearmodels subsume linear models as a special case. For instance, a nonlinearlearning method like kNN will in some cases produce a linear classiﬁer. Sec-ond, there are nonlinear models that are less complex than linear models.For instance, a quadratic polynomial with two parameters is less powerfulthan a 10,000-dimensional linear classiﬁer. Third, the complexity of learn-ing is not really a property of the classiﬁer because there are many aspects 14.6The bias-variance tradeoff309of learning (such as feature selection, cf. (Section 13.5, page 271), regulariza-tion, and constraints such as margin maximization in Chapter 15) that makea learning method either more powerful or less powerful without affectingthe type of classiﬁer that is the ﬁnal result of learning – regardless of whetherthat classiﬁer is linear or nonlinear. We refer the reader to the publicationslisted in Section 14.7 for a treatment of the bias-variance tradeoff that takesinto account these complexities. In this section, linear and nonlinear classi-ﬁers will simply serve as proxies for weaker and stronger learning methodsin text classiﬁcation.We ﬁrst need to state our objective in text classiﬁcation more precisely. InSection 13.1 (page 256), we said that we want to minimize classiﬁcation er-ror on the test set. The implicit assumption was that training documentsand test documents are generated according to the same underlying distri-bution. We will denote this distribution P(⟨d, c⟩) where d is the documentand c its label or class. Figures 13.4 and 13.5 were examples of generativemodels that decompose P(⟨d, c⟩) into the product of P(c) and P(d|c). Fig-ures 14.10 and 14.11 depict generative models for ⟨d, c⟩with d ∈R2 andc ∈{square, solid circle}.In this section, instead of using the number of correctly classiﬁed test doc-uments (or, equivalently, the error rate on test documents) as evaluationmeasure, we adopt an evaluation measure that addresses the inherent un-certainty of labeling. In many text classiﬁcation problems, a given documentrepresentation can arise from documents belonging to different classes. Thisis because documents from different classes can be mapped to the same doc-ument representation. For example, the one-sentence documents China suesFrance and France sues China are mapped to the same document representa-tion d′ = {China, France, sues} in a bag of words model. But only the latterdocument is relevant to the class c′ = legal actions brought by France (whichmight be deﬁned, for example, as a standing query by an international tradelawyer).To simplify the calculations in this section, we do not count the numberof errors on the test set when evaluating a classiﬁer, but instead look at howwell the classiﬁer estimates the conditional probability P(c|d) of a documentbeing in a class. In the above example, we might have P(c′|d′) = 0.5.Our goal in text classiﬁcation then is to ﬁnd a classiﬁer γ such that, aver-aged over documents d, γ(d) is as close as possible to the true probabilityP(c|d). We measure this using mean squared error:MSE(γ) = Ed[γ(d) −P(c|d)]2(14.6)where Ed is the expectation with respect to P(d). The mean squared errorterm gives partial credit for decisions by γ that are close if not completelyright. 31014Vector space classiﬁcationE[x −α]2=Ex2 −2Exα + α2(14.8)=(Ex)2 −2Exα + α2+Ex2 −2(Ex)2 + (Ex)2=[Ex −α]2+Ex2 −E2x(Ex) + E(Ex)2=[Ex −α]2 + E[x −Ex]2EDEd[ΓD(d) −P(c|d)]2=EdED[ΓD(d) −P(c|d)]2(14.9)=Ed[ [EDΓD(d) −P(c|d)]2+ED[ΓD(d) −EDΓD(d)]2 ]◮Figure 14.13Arithmetic transformations for the bias-variance decomposition.For the derivation of Equation (14.9), we set α = P(c|d) and x = ΓD(d) in Equa-tion (14.8).We deﬁne a classiﬁer γ to be optimal for a distribution P(⟨d, c⟩) if it mini-OPTIMAL CLASSIFIERmizes MSE(γ).Minimizing MSE is a desideratum for classiﬁers. We also need a criterionfor learning methods. Recall that we deﬁned a learning method Γ as a functionthat takes a labeled training set D as input and returns a classiﬁer γ.For learning methods, we adopt as our goal to ﬁnd a Γ that, averaged overtraining sets, learns classiﬁers γ with minimal MSE. We can formalize this asminimizing learning error:LEARNING ERRORlearning-error(Γ) = ED[MSE(Γ(D))](14.7)where ED is the expectation over labeled training sets. To keep things simple,we can assume that training sets have a ﬁxed size – the distribution P(⟨d, c⟩)then deﬁnes a distribution P(D) over training sets.We can use learning error as a criterion for selecting a learning method instatistical text classiﬁcation. A learning method Γ is optimal for a distributionOPTIMAL LEARNINGMETHODP(D) if it minimizes the learning error.Writing ΓD for Γ(D) for better readability, we can transform Equation (14.7)as follows:learning-error(Γ)=ED[MSE(ΓD)]=EDEd[ΓD(d) −P(c|d)]2(14.10)=Ed[bias(Γ, d) + variance(Γ, d)](14.11) 14.6The bias-variance tradeoff311bias(Γ, d)=[P(c|d) −EDΓD(d)]2(14.12)variance(Γ, d)=ED[ΓD(d) −EDΓD(d)]2(14.13)where the equivalence between Equations (14.10) and (14.11) is shown inEquation (14.9) in Figure 14.13. Note that d and D are independent of eachother. In general, for a random document d and a random training set D, Ddoes not contain a labeled instance of d.Bias is the squared difference between P(c|d), the true conditional prob-BIASability of d being in c, and ΓD(d), the prediction of the learned classiﬁer,averaged over training sets. Bias is large if the learning method producesclassiﬁers that are consistently wrong. Bias is small if (i) the classiﬁers areconsistently right or (ii) different training sets cause errors on different docu-ments or (iii) different training sets cause positive and negative errors on thesame documents, but that average out to close to 0. If one of these three con-ditions holds, then EDΓD(d), the expectation over all training sets, is close toP(c|d).Linear methods like Rocchio and Naive Bayes have a high bias for non-linear problems because they can only model one type of class boundary, alinear hyperplane. If the generative model P(⟨d, c⟩) has a complex nonlinearclass boundary, the bias term in Equation (14.11) will be high because a largenumber of points will be consistently misclassiﬁed. For example, the circularenclave in Figure 14.11 does not ﬁt a linear model and will be misclassiﬁedconsistently by linear classiﬁers.We can think of bias as resulting from our domain knowledge (or lackthereof) that we build into the classiﬁer. If we know that the true boundarybetween the two classes is linear, then a learning method that produces linearclassiﬁers is more likely to succeed than a nonlinear method. But if the trueclass boundary is not linear and we incorrectly bias the classiﬁer to be linear,then classiﬁcation accuracy will be low on average.Nonlinear methods like kNN have low bias. We can see in Figure 14.6 thatthe decision boundaries of kNN are variable – depending on the distribu-tion of documents in the training set, learned decision boundaries can varygreatly. As a result, each document has a chance of being classiﬁed correctlyfor some training sets. The average prediction EDΓD(d) is therefore closer toP(c|d) and bias is smaller than for a linear learning method.Variance is the variation of the prediction of learned classiﬁers: the aver-VARIANCEage squared difference between ΓD(d) and its average EDΓD(d). Variance islarge if different training sets D give rise to very different classiﬁers ΓD. It issmall if the training set has a minor effect on the classiﬁcation decisions ΓDmakes, be they correct or incorrect. Variance measures how inconsistent thedecisions are, not whether they are correct or incorrect.Linear learning methods have low variance because most randomly drawntraining sets produce similar decision hyperplanes. The decision lines pro- 31214Vector space classiﬁcationduced by linear learning methods in Figures 14.10 and 14.11 will deviateslightly from the main class boundaries, depending on the training set, butthe class assignment for the vast majority of documents (with the exceptionof those close to the main boundary) will not be affected. The circular enclavein Figure 14.11 will be consistently misclassiﬁed.Nonlinear methods like kNN have high variance. It is apparent from Fig-ure 14.6 that kNN can model very complex boundaries between two classes.It is therefore sensitive to noise documents of the sort depicted in Figure 14.10.As a result the variance term in Equation (14.11) is large for kNN: Test doc-uments are sometimes misclassiﬁed – if they happen to be close to a noisedocument in the training set – and sometimes correctly classiﬁed – if thereare no noise documents in the training set near them. This results in highvariation from training set to training set.High-variance learning methods are prone to overﬁtting the training data.OVERFITTINGThe goal in classiﬁcation is to ﬁt the training data to the extent that we cap-ture true properties of the underlying distribution P(⟨d, c⟩). In overﬁtting,the learning method also learns from noise. Overﬁtting increases MSE andfrequently is a problem for high-variance learning methods.We can also think of variance as the model complexity or, equivalently, mem-MEMORY CAPACITYory capacity of the learning method – how detailed a characterization of thetraining set it can remember and then apply to new data. This capacity corre-sponds to the number of independent parameters available to ﬁt the trainingset. Each kNN neighborhood Sk makes an independent classiﬁcation deci-sion. The parameter in this case is the estimate ˆP(c|Sk) from Figure 14.7.Thus, kNN’s capacity is only limited by the size of the training set. It canmemorize arbitrarily large training sets. In contrast, the number of parame-ters of Rocchio is ﬁxed – J parameters per dimension, one for each centroid– and independent of the size of the training set. The Rocchio classiﬁer (inform of the centroids deﬁning it) cannot “remember” ﬁne-grained details ofthe distribution of the documents in the training set.According to Equation (14.7), our goal in selecting a learning method is tominimize learning error. The fundamental insight captured by Equation (14.11),which we can succinctly state as: learning-error = bias + variance, is that thelearning error has two components, bias and variance, which in general can-not be minimized simultaneously. When comparing two learning methodsΓ1 and Γ2, in most cases the comparison comes down to one method havinghigher bias and lower variance and the other lower bias and higher variance.The decision for one learning method vs. another is then not simply a mat-ter of selecting the one that reliably produces good classiﬁers across trainingsets (small variance) or the one that can learn classiﬁcation problems withvery difﬁcult decision boundaries (small bias). Instead, we have to weighthe respective merits of bias and variance in our application and choose ac-cordingly. This tradeoff is called the bias-variance tradeoff.BIAS-VARIANCETRADEOFF 14.6The bias-variance tradeoff313Figure 14.10 provides an illustration, which is somewhat contrived, butwill be useful as an example for the tradeoff. Some Chinese text containsEnglish words written in the Roman alphabet like CPU, ONLINE, and GPS.Consider the task of distinguishing Chinese-only web pages from mixedChinese-English web pages. A search engine might offer Chinese users with-out knowledge of English (but who understand loanwords like CPU) the op-tion of ﬁltering out mixed pages. We use two features for this classiﬁcationtask: number of Roman alphabet characters and number of Chinese char-acters on the web page. As stated earlier, the distribution P(⟨d, c⟩) of thegenerative model generates most mixed (respectively, Chinese) documentsabove (respectively, below) the short-dashed line, but there are a few noisedocuments.In Figure 14.10, we see three classiﬁers:• One-feature classiﬁer. Shown as a dotted horizontal line. This classiﬁeruses only one feature, the number of Roman alphabet characters. Assum-ing a learning method that minimizes the number of misclassiﬁcationsin the training set, the position of the horizontal decision boundary isnot greatly affected by differences in the training set (e.g., noise docu-ments). So a learning method producing this type of classiﬁer has lowvariance. But its bias is high since it will consistently misclassify squaresin the lower left corner and “solid circle” documents with more than 50Roman characters.• Linear classiﬁer. Shown as a dashed line with long dashes. Learning lin-ear classiﬁers has less bias since only noise documents and possibly a fewdocuments close to the boundary between the two classes are misclassi-ﬁed. The variance is higher than for the one-feature classiﬁers, but stillsmall: The dashed line with long dashes deviates only slightly from thetrue boundary between the two classes, and so will almost all linear de-cision boundaries learned from training sets. Thus, very few documents(documents close to the class boundary) will be inconsistently classiﬁed.• “Fit-training-set-perfectly” classiﬁer. Shown as a solid line. Here, thelearning method constructs a decision boundary that perfectly separatesthe classes in the training set. This method has the lowest bias becausethere is no document that is consistently misclassiﬁed – the classiﬁerssometimes even get noise documents in the test set right. But the varianceof this learning method is high. Because noise documents can move thedecision boundary arbitrarily, test documents close to noise documentsin the training set will be misclassiﬁed – something that a linear learningmethod is unlikely to do.It is perhaps surprising that so many of the best-known text classiﬁcationalgorithms are linear. Some of these methods, in particular linear SVMs, reg- 31414Vector space classiﬁcationularized logistic regression and regularized linear regression, are among themost effective known methods. The bias-variance tradeoff provides insightinto their success. Typical classes in text classiﬁcation are complex and seemunlikely to be modeled well linearly. However, this intuition is misleadingfor the high-dimensional spaces that we typically encounter in text appli-cations. With increased dimensionality, the likelihood of linear separabilityincreases rapidly (Exercise 14.17). Thus, linear models in high-dimensionalspaces are quite powerful despite their linearity. Even more powerful nonlin-ear learning methods can model decision boundaries that are more complexthan a hyperplane, but they are also more sensitive to noise in the trainingdata. Nonlinear learning methods sometimes perform better if the trainingset is large, but by no means in all cases.14.7References and further readingAs discussed in Chapter 9, Rocchio relevance feedback is due to Rocchio(1971). Joachims (1997) presents a probabilistic analysis of the method. Roc-chio classiﬁcation was widely used as a classiﬁcation method in TREC in the1990s (Buckley et al. 1994a;b, Voorhees and Harman 2005). Initially, it wasused as a form of routing. Routing merely ranks documents according to rel-ROUTINGevance to a class without assigning them. Early work on ﬁltering, a true clas-FILTERINGsiﬁcation approach that makes an assignment decision on each document,was published by Ittner et al. (1995) and Schapire et al. (1998). The deﬁnitionof routing we use here should not be confused with another sense. Routingcan also refer to the electronic distribution of documents to subscribers, theso-called push model of document distribution. In a pull model, each transferPUSH MODELPULL MODELof a document to the user is initiated by the user – for example, by meansof search or by selecting it from a list of documents on a news aggregationwebsite.Some authors restrict the name Roccchio classiﬁcation to two-class problemsand use the terms cluster-based (Iwayama and Tokunaga 1995) and centroid-CENTROID-BASEDCLASSIFICATIONbased classiﬁcation (Han and Karypis 2000, Tan and Cheng 2007) for Rocchioclassiﬁcation with J > 2.A more detailed treatment of kNN can be found in (Hastie et al. 2001), in-cluding methods for tuning the parameter k. An example of an approximatefast kNN algorithm is locality-based hashing (Andoni et al. 2006). Klein-berg (1997) presents an approximate Θ((M log2 M)(M + log N)) kNN algo-rithm (where M is the dimensionality of the space and N the number of datapoints), but at the cost of exponential storage requirements: Θ((N log M)2M).Indyk (2004) surveys nearest neighbor methods in high-dimensional spaces.Early work on kNN in text classiﬁcation was motivated by the availabilityof massively parallel hardware architectures (Creecy et al. 1992). Yang (1994) 14.8Exercises315uses an inverted index to speed up kNN classiﬁcation. The optimality resultfor 1NN (twice the Bayes error rate asymptotically) is due to Cover and Hart(1967).The effectiveness of Rocchio classiﬁcation and kNN is highly dependenton careful parameter tuning (in particular, the parameters b′ for Rocchio onpage 296 and k for kNN), feature engineering (Section 15.3, page 334) andfeature selection (Section 13.5, page 271). Buckley and Salton (1995), Schapireet al. (1998), Yang and Kisiel (2003) and Moschitti (2003) address these issuesfor Rocchio and Yang (2001) and Ault and Yang (2002) for kNN. Zavrel et al.(2000) compare feature selection methods for kNN.The bias-variance tradeoff was introduced by Geman et al. (1992). Thederivation in Section 14.6 is for MSE(γ), but the tradeoff applies to manyloss functions (cf. Friedman (1997), Domingos (2000)). Schütze et al. (1995)and Lewis et al. (1996) discuss linear classiﬁers for text and Hastie et al. (2001)linear classiﬁers in general. Readers interested in the algorithms mentioned,but not described in this chapter may wish to consult Bishop (2006) for neu-ral networks, Hastie et al. (2001) for linear and logistic regression, and Min-sky and Papert (1988) for the perceptron algorithm. Anagnostopoulos et al.(2006) show that an inverted index can be used for highly efﬁcient documentclassiﬁcation with any linear classiﬁer, provided that the classiﬁer is still ef-fective when trained on a modest number of features via feature selection.We have only presented the simplest method for combining two-class clas-siﬁers into a one-of classiﬁer. Another important method is the use of error-correcting codes, where a vector of decisions of different two-class classiﬁersis constructed for each document. A test document’s decision vector is then“corrected” based on the distribution of decision vectors in the training set,a procedure that incorporates information from all two-class classiﬁers andtheir correlations into the ﬁnal classiﬁcation decision (Dietterich and Bakiri1995). Ghamrawi and McCallum (2005) also exploit dependencies betweenclasses in any-of classiﬁcation. Allwein et al. (2000) propose a general frame-work for combining two-class classiﬁers.14.8Exercises?Exercise 14.6In Figure 14.14, which of the three vectors⃗a,⃗b, and⃗c is (i) most similar to ⃗x accordingto dot product similarity, (ii) most similar to ⃗x according to cosine similarity, (iii)closest to ⃗x according to Euclidean distance?Exercise 14.7Download Reuters-21578 and train and test Rocchio and kNN classiﬁers for the classesacquisitions, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat. Use theModApte split. You may want to use one of a number of software packages that im- 31614Vector space classiﬁcation0 1 2 3 4 5 6 7 8012345678axbc◮Figure 14.14Example for differences between Euclidean distance, dot productsimilarity and cosine similarity. The vectors are ⃗a = (0.5 1.5)T, ⃗x = (2 2)T, ⃗b =(4 4)T, and⃗c = (8 6)T.plement Rocchio classiﬁcation and kNN classiﬁcation, for example, the Bow toolkit(McCallum 1996).Exercise 14.8Download 20 Newgroups (page 154) and train and test Rocchio and kNN classiﬁersfor its 20 classes.Exercise 14.9Show that the decision boundaries in Rocchio classiﬁcation are, as in kNN, given bythe Voronoi tessellation.Exercise 14.10[⋆]Computing the distance between a dense centroid and a sparse vector is Θ(M) fora naive implementation that iterates over all M dimensions. Based on the equality∑(xi −µi)2 = 1.0 + ∑µ2i −2 ∑xiµi and assuming that ∑µ2i has been precomputed,write down an algorithm that is Θ(Ma) instead, where Ma is the number of distinctterms in the test document.Exercise 14.11[⋆⋆⋆]Prove that the region of the plane consisting of all points with the same k nearestneighbors is a convex polygon.Exercise 14.12Design an algorithm that performs an efﬁcient 1NN search in 1 dimension (whereefﬁciency is with respect to the number of documents N). What is the time complexityof the algorithm?Exercise 14.13[⋆⋆⋆]Design an algorithm that performs an efﬁcient 1NN search in 2 dimensions with atmost polynomial (in N) preprocessing time. 14.8Exercises317bb◮Figure 14.15A simple non-separable set of points.Exercise 14.14[⋆⋆⋆]Can one design an exact efﬁcient algorithm for 1NN for very large M along the ideasyou used to solve the last exercise?Exercise 14.15Show that Equation (14.4) deﬁnes a hyperplane with ⃗w = ⃗µ(c1) −⃗µ(c2) and b =0.5 ∗(|⃗µ(c1)|2 −|⃗µ(c2)|2).Exercise 14.16We can easily construct non-separable data sets in high dimensions by embeddinga non-separable set like the one shown in Figure 14.15. Consider embedding Fig-ure 14.15 in 3D and then perturbing the 4 points slightly (i.e., moving them a smalldistance in a random direction). Why would you expect the resulting conﬁgurationto be linearly separable? How likely is then a non-separable set of m ≪M points inM-dimensional space?Exercise 14.17Assuming two classes, show that the percentage of non-separable assignments of thevertices of a hypercube decreases with dimensionality M for M > 1. For example,for M = 1 the proportion of non-separable assignments is 0, for M = 2, it is 2/16.One of the two non-separable cases for M = 2 is shown in Figure 14.15, the other isits mirror image. Solve the exercise either analytically or by simulation.Exercise 14.18Although we point out the similarities of Naive Bayes with linear vector space classi-ﬁers, it does not make sense to represent count vectors (the document representationsin NB) in a continuous vector space. There is however a formalization of NB that isanalogous to Rocchio. Show that NB assigns a document to the class (represented asa parameter vector) whose Kullback-Leibler (KL) divergence (Section 12.4, page 251)to the document (represented as a count vector as in Section 13.4.1 (page 270), nor-malized to sum to 1) is smallest. ', '11Boolean retrievalThe meaning of the term information retrieval can be very broad. Just gettinga credit card out of your wallet so that you can type in the card numberis a form of information retrieval. However, as an academic ﬁeld of study,information retrieval might be deﬁned thus:INFORMATIONRETRIEVALInformation retrieval (IR) is ﬁnding material (usually documents) ofan unstructured nature (usually text) that satisﬁes an information needfrom within large collections (usually stored on computers).As deﬁned in this way, information retrieval used to be an activity that onlya few people engaged in: reference librarians, paralegals, and similar pro-fessional searchers. Now the world has changed, and hundreds of millionsof people engage in information retrieval every day when they use a websearch engine or search their email.1 Information retrieval is fast becomingthe dominant form of information access, overtaking traditional database-style searching (the sort that is going on when a clerk says to you: “I’m sorry,I can only look up your order if you can give me your Order ID”).IR can also cover other kinds of data and information problems beyondthat speciﬁed in the core deﬁnition above. The term “unstructured data”refers to data which does not have clear, semantically overt, easy-for-a-computerstructure. It is the opposite of structured data, the canonical example ofwhich is a relational database, of the sort companies usually use to main-tain product inventories and personnel records. In reality, almost no dataare truly “unstructured”. This is deﬁnitely true of all text data if you countthe latent linguistic structure of human languages. But even accepting thatthe intended notion of structure is overt structure, most text has structure,such as headings and paragraphs and footnotes, which is commonly repre-sented in documents by explicit markup (such as the coding underlying web1. In modern parlance, the word “search” has tended to replace “(information) retrieval”; theterm “search” is quite ambiguous, but in context we use the two synonymously. 21Boolean retrievalpages). IR is also used to facilitate “semistructured” search such as ﬁnding adocument where the title contains Java and the body contains threading.The ﬁeld of information retrieval also covers supporting users in browsingor ﬁltering document collections or further processing a set of retrieved doc-uments. Given a set of documents, clustering is the task of coming up with agood grouping of the documents based on their contents. It is similar to ar-ranging books on a bookshelf according to their topic. Given a set of topics,standing information needs, or other categories (such as suitability of textsfor different age groups), classiﬁcation is the task of deciding which class(es),if any, each of a set of documents belongs to. It is often approached by ﬁrstmanually classifying some documents and then hoping to be able to classifynew documents automatically.Information retrieval systems can also be distinguished by the scale atwhich they operate, and it is useful to distinguish three prominent scales.In web search, the system has to provide search over billions of documentsstored on millions of computers. Distinctive issues are needing to gatherdocuments for indexing, being able to build systems that work efﬁcientlyat this enormous scale, and handling particular aspects of the web, such asthe exploitation of hypertext and not being fooled by site providers manip-ulating page content in an attempt to boost their search engine rankings,given the commercial importance of the web. We focus on all these issuesin Chapters 19–21. At the other extreme is personal information retrieval. Inthe last few years, consumer operating systems have integrated informationretrieval (such as Apple’s Mac OS X Spotlight or Windows Vista’s InstantSearch). Email programs usually not only provide search but also text clas-siﬁcation: they at least provide a spam (junk mail) ﬁlter, and commonly alsoprovide either manual or automatic means for classifying mail so that it canbe placed directly into particular folders. Distinctive issues here include han-dling the broad range of document types on a typical personal computer,and making the search system maintenance free and sufﬁciently lightweightin terms of startup, processing, and disk space usage that it can run on onemachine without annoying its owner. In between is the space of enterprise,institutional, and domain-speciﬁc search, where retrieval might be provided forcollections such as a corporation’s internal documents, a database of patents,or research articles on biochemistry. In this case, the documents will typi-cally be stored on centralized ﬁle systems and one or a handful of dedicatedmachines will provide search over the collection. This book contains tech-niques of value over this whole spectrum, but our coverage of some aspectsof parallel and distributed search in web-scale search systems is compara-tively light owing to the relatively small published literature on the detailsof such systems. However, outside of a handful of web search companies, asoftware developer is most likely to encounter the personal search and en-terprise scenarios. 1.1An example information retrieval problem3In this chapter we begin with a very simple example of an informationretrieval problem, and introduce the idea of a term-document matrix (Sec-tion 1.1) and the central inverted index data structure (Section 1.2). We willthen examine the Boolean retrieval model and how Boolean queries are pro-cessed (Sections 1.3 and 1.4).1.1An example information retrieval problemA fat book which many people own is Shakespeare’s Collected Works. Sup-pose you wanted to determine which plays of Shakespeare contain the wordsBrutus AND Caesar AND NOT Calpurnia. One way to do that is to start at thebeginning and to read through all the text, noting for each play whetherit contains Brutus and Caesar and excluding it from consideration if it con-tains Calpurnia. The simplest form of document retrieval is for a computerto do this sort of linear scan through documents. This process is commonlyreferred to as grepping through text, after the Unix command grep, whichGREPperforms this process. Grepping through text can be a very effective process,especially given the speed of modern computers, and often allows usefulpossibilities for wildcard pattern matching through the use of regular expres-sions. With modern computers, for simple querying of modest collections(the size of Shakespeare’s Collected Works is a bit under one million wordsof text in total), you really need nothing more.But for many purposes, you do need more:1. To process large document collections quickly. The amount of online datahas grown at least as quickly as the speed of computers, and we wouldnow like to be able to search collections that total in the order of billionsto trillions of words.2. To allow more ﬂexible matching operations. For example, it is impracticalto perform the query Romans NEAR countrymen with grep, where NEARmight be deﬁned as “within 5 words” or “within the same sentence”.3. To allow ranked retrieval: in many cases you want the best answer to aninformation need among many documents that contain certain words.The way to avoid linearly scanning the texts for each query is to index theINDEXdocuments in advance. Let us stick with Shakespeare’s Collected Works,and use it to introduce the basics of the Boolean retrieval model. Supposewe record for each document – here a play of Shakespeare’s – whether itcontains each word out of all the words Shakespeare used (Shakespeare usedabout 32,000 different words). The result is a binary term-document incidenceINCIDENCE MATRIXmatrix, as in Figure 1.1. Terms are the indexed units (further discussed inTERMSection 2.2); they are usually words, and for the moment you can think of 41Boolean retrievalAntonyJuliusTheHamletOthelloMacbeth...andCaesarTempestCleopatraAntony110001Brutus110100Caesar110111Calpurnia010000Cleopatra100000mercy101111worser101110...◮Figure 1.1A term-document incidence matrix. Matrix element (t, d) is 1 if theplay in column d contains the word in row t, and is 0 otherwise.them as words, but the information retrieval literature normally speaks ofterms because some of them, such as perhaps I-9 or Hong Kong are not usuallythought of as words. Now, depending on whether we look at the matrix rowsor columns, we can have a vector for each term, which shows the documentsit appears in, or a vector for each document, showing the terms that occur init.2To answer the query Brutus AND Caesar AND NOT Calpurnia, we take thevectors for Brutus, Caesar and Calpurnia, complement the last, and then do abitwise AND:110100 AND 110111 AND 101111 = 100100The answers for this query are thus Antony and Cleopatra and Hamlet (Fig-ure 1.2).The Boolean retrieval model is a model for information retrieval in which weBOOLEAN RETRIEVALMODELcan pose any query which is in the form of a Boolean expression of terms,that is, in which terms are combined with the operators AND, OR, and NOT.The model views each document as just a set of words.Let us now consider a more realistic scenario, simultaneously using theopportunity to introduce some terminology and notation. Suppose we haveN = 1 million documents. By documents we mean whatever units we haveDOCUMENTdecided to build a retrieval system over. They might be individual memosor chapters of a book (see Section 2.1.2 (page 20) for further discussion). Wewill refer to the group of documents over which we perform retrieval as the(document) collection. It is sometimes also referred to as a corpus (a body ofCOLLECTIONCORPUStexts). Suppose each document is about 1000 words long (2–3 book pages). If2. Formally, we take the transpose of the matrix to be able to get the terms as column vectors. 1.1An example information retrieval problem5Antony and Cleopatra, Act III, Scene iiAgrippa [Aside to Domitius Enobarbus]:Why, Enobarbus,When Antony found Julius Caesar dead,He cried almost to roaring; and he weptWhen at Philippi he found Brutus slain.Hamlet, Act III, Scene iiLord Polonius:I did enact Julius Caesar: I was killed i’ theCapitol; Brutus killed me.◮Figure 1.2Results from Shakespeare for the query Brutus AND Caesar AND NOTCalpurnia.we assume an average of 6 bytes per word including spaces and punctuation,then this is a document collection about 6 GB in size. Typically, there mightbe about M = 500,000 distinct terms in these documents. There is nothingspecial about the numbers we have chosen, and they might vary by an orderof magnitude or more, but they give us some idea of the dimensions of thekinds of problems we need to handle. We will discuss and model these sizeassumptions in Section 5.1 (page 86).Our goal is to develop a system to address the ad hoc retrieval task. This isAD HOC RETRIEVALthe most standard IR task. In it, a system aims to provide documents fromwithin the collection that are relevant to an arbitrary user information need,communicated to the system by means of a one-off, user-initiated query. Aninformation need is the topic about which the user desires to know more, andINFORMATION NEEDis differentiated from a query, which is what the user conveys to the com-QUERYputer in an attempt to communicate the information need. A document isrelevant if it is one that the user perceives as containing information of valueRELEVANCEwith respect to their personal information need. Our example above wasrather artiﬁcial in that the information need was deﬁned in terms of par-ticular words, whereas usually a user is interested in a topic like “pipelineleaks” and would like to ﬁnd relevant documents regardless of whether theyprecisely use those words or express the concept with other words such aspipeline rupture. To assess the effectiveness of an IR system (i.e., the quality ofEFFECTIVENESSits search results), a user will usually want to know two key statistics aboutthe system’s returned results for a query:Precision: What fraction of the returned results are relevant to the informa-PRECISIONtion need?Recall: What fraction of the relevant documents in the collection were re-RECALLturned by the system? 61Boolean retrievalDetailed discussion of relevance and evaluation measures including preci-sion and recall is found in Chapter 8.We now cannot build a term-document matrix in a naive way. A 500K ×1M matrix has half-a-trillion 0’s and 1’s – too many to ﬁt in a computer’smemory. But the crucial observation is that the matrix is extremely sparse,that is, it has few non-zero entries. Because each document is 1000 wordslong, the matrix has no more than one billion 1’s, so a minimum of 99.8% ofthe cells are zero. A much better representation is to record only the thingsthat do occur, that is, the 1 positions.This idea is central to the ﬁrst major concept in information retrieval, theinverted index. The name is actually redundant: an index always maps backINVERTED INDEXfrom terms to the parts of a document where they occur. Nevertheless, in-verted index, or sometimes inverted ﬁle, has become the standard term in infor-mation retrieval.3 The basic idea of an inverted index is shown in Figure 1.3.We keep a dictionary of terms (sometimes also referred to as a vocabulary orDICTIONARYVOCABULARYlexicon; in this book, we use dictionary for the data structure and vocabularyLEXICONfor the set of terms). Then for each term, we have a list that records whichdocuments the term occurs in. Each item in the list – which records that aterm appeared in a document (and, later, often, the positions in the docu-ment) – is conventionally called a posting.4 The list is then called a postingsPOSTINGPOSTINGS LISTlist (or inverted list), and all the postings lists taken together are referred to asthe postings. The dictionary in Figure 1.3 has been sorted alphabetically andPOSTINGSeach postings list is sorted by document ID. We will see why this is useful inSection 1.3, below, but later we will also consider alternatives to doing this(Section 7.1.5).1.2A ﬁrst take at building an inverted indexTo gain the speed beneﬁts of indexing at retrieval time, we have to build theindex in advance. The major steps in this are:1. Collect the documents to be indexed:Friends, Romans, countrymen.So let it be with Caesar ...2. Tokenize the text, turning each document into a list of tokens:FriendsRomanscountrymenSo ...3. Some information retrieval researchers prefer the term inverted ﬁle, but expressions like in-dex construction and index compression are much more common than inverted ﬁle construction andinverted ﬁle compression. For consistency, we use (inverted) index throughout this book.4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherentlyassociated with a term, via the postings list it is placed on; sometimes we will also talk of a(term, docID) pair as a posting. 1.2A ﬁrst take at building an inverted index7Brutus−→124113145173174Caesar−→124561657132...Calpurnia−→23154101...|{z}|{z}DictionaryPostings◮Figure 1.3The two parts of an inverted index. The dictionary is commonly keptin memory, with pointers to each postings list, which is stored on disk.3. Do linguistic preprocessing, producing a list of normalized tokens, whichare the indexing terms: friendromancountrymanso ...4. Index the documents that each term occurs in by creating an inverted in-dex, consisting of a dictionary and postings.We will deﬁne and discuss the earlier stages of processing, that is, steps 1–3,in Section 2.2 (page 22). Until then you can think of tokens and normalizedtokens as also loosely equivalent to words. Here, we assume that the ﬁrst3 steps have already been done, and we examine building a basic invertedindex by sort-based indexing.Within a document collection, we assume that each document has a uniqueserial number, known as the document identiﬁer (docID). During index con-DOCIDstruction, we can simply assign successive integers to each new documentwhen it is ﬁrst encountered. The input to indexing is a list of normalizedtokens for each document, which we can equally think of as a list of pairs ofterm and docID, as in Figure 1.4. The core indexing step is sorting this listSORTINGso that the terms are alphabetical, giving us the representation in the middlecolumn of Figure 1.4. Multiple occurrences of the same term from the samedocument are then merged.5 Instances of the same term are then grouped,and the result is split into a dictionary and postings, as shown in the rightcolumn of Figure 1.4. Since a term generally occurs in a number of docu-ments, this data organization already reduces the storage requirements ofthe index. The dictionary also records some statistics, such as the number ofdocuments which contain each term (the document frequency, which is hereDOCUMENTFREQUENCYalso the length of each postings list). This information is not vital for a ba-sic Boolean search engine, but it allows us to improve the efﬁciency of the5. Unix users can note that these steps are similar to use of the sort and then uniq commands. 81Boolean retrievalDoc 1Doc 2I did enact Julius Caesar: I was killedi’ the Capitol; Brutus killed me.So let it be with Caesar. The noble Brutushath told you Caesar was ambitious:termdocIDI1did1enact1julius1caesar1I1was1killed1i’1the1capitol1brutus1killed1me1so2let2it2be2with2caesar2the2noble2brutus2hath2told2you2caesar2was2ambitious2=⇒termdocIDambitious2be2brutus1brutus2capitol1caesar1caesar2caesar2did1enact1hath1I1I1i’1it2julius1killed1killed1let2me1noble2so2the1the2told2you2was1was2with2=⇒termdoc. freq.→postings listsambitious1→2be1→2brutus2→1 →2capitol1→1caesar2→1 →2did1→1enact1→1hath1→2I1→1i’1→1it1→2julius1→1killed1→1let1→2me1→1noble1→2so1→2the2→1 →2told1→2you1→2was2→1 →2with1→2◮Figure 1.4Building an index by sorting and grouping. The sequence of termsin each document, tagged by their documentID (left) is sorted alphabetically (mid-dle). Instances of the same term are then grouped by word and then by documentID.The terms and documentIDs are then separated out (right). The dictionary storesthe terms, and has a pointer to the postings list for each term. It commonly alsostores other summary information such as, here, the document frequency of eachterm. We use this information for improving query time efﬁciency and, later, forweighting in ranked retrieval models. Each postings list stores the list of documentsin which a term occurs, and may store other information such as the term frequency(the frequency of each term in each document) or the position(s) of the term in eachdocument. 1.2A ﬁrst take at building an inverted index9search engine at query time, and it is a statistic later used in many ranked re-trieval models. The postings are secondarily sorted by docID. This providesthe basis for efﬁcient query processing. This inverted index structure is es-sentially without rivals as the most efﬁcient structure for supporting ad hoctext search.In the resulting index, we pay for storage of both the dictionary and thepostings lists. The latter are much larger, but the dictionary is commonlykept in memory, while postings lists are normally kept on disk, so the sizeof each is important, and in Chapter 5 we will examine how each can beoptimized for storage and access efﬁciency. What data structure should beused for a postings list? A ﬁxed length array would be wasteful as somewords occur in many documents, and others in very few. For an in-memorypostings list, two good alternatives are singly linked lists or variable lengtharrays. Singly linked lists allow cheap insertion of documents into postingslists (following updates, such as when recrawling the web for updated doc-uments), and naturally extend to more advanced indexing strategies such asskip lists (Section 2.3), which require additional pointers. Variable length ar-rays win in space requirements by avoiding the overhead for pointers and intime requirements because their use of contiguous memory increases speedon modern processors with memory caches. Extra pointers can in practice beencoded into the lists as offsets. If updates are relatively infrequent, variablelength arrays will be more compact and faster to traverse. We can also use ahybrid scheme with a linked list of ﬁxed length arrays for each term. Whenpostings lists are stored on disk, they are stored (perhaps compressed) as acontiguous run of postings without explicit pointers (as in Figure 1.3), so asto minimize the size of the postings list and the number of disk seeks to reada postings list into memory.?Exercise 1.1[⋆]Draw the inverted index that would be built for the following document collection.(See Figure 1.3 for an example.)Doc 1new home sales top forecastsDoc 2home sales rise in julyDoc 3increase in home sales in julyDoc 4july new home sales riseExercise 1.2[⋆]Consider these documents:Doc 1breakthrough drug for schizophreniaDoc 2new schizophrenia drugDoc 3new approach for treatment of schizophreniaDoc 4new hopes for schizophrenia patientsa. Draw the term-document incidence matrix for this document collection. 101Boolean retrievalBrutus−→1 →2 →4 →11 →31 →45 →173 →174Calpurnia−→2 →31 →54 →101Intersection=⇒2 →31◮Figure 1.5Intersecting the postings lists for Brutus and Calpurnia from Figure 1.3.b. Draw the inverted index representation for this collection, as in Figure 1.3 (page 7).Exercise 1.3[⋆]For the document collection shown in Exercise 1.2, what are the returned results forthese queries:a. schizophrenia AND drugb. for AND NOT(drug OR approach)1.3Processing Boolean queriesHow do we process a query using an inverted index and the basic Booleanretrieval model? Consider processing the simple conjunctive query:SIMPLE CONJUNCTIVEQUERIES(1.1)Brutus AND Calpurniaover the inverted index partially shown in Figure 1.3 (page 7). We:1. Locate Brutus in the Dictionary2. Retrieve its postings3. Locate Calpurnia in the Dictionary4. Retrieve its postings5. Intersect the two postings lists, as shown in Figure 1.5.The intersection operation is the crucial one: we need to efﬁciently intersectPOSTINGS LISTINTERSECTIONpostings lists so as to be able to quickly ﬁnd documents that contain bothterms. (This operation is sometimes referred to as merging postings lists:POSTINGS MERGEthis slightly counterintuitive name reﬂects using the term merge algorithm fora general family of algorithms that combine multiple sorted lists by inter-leaved advancing of pointers through each; here we are merging the listswith a logical AND operation.)There is a simple and effective method of intersecting postings lists usingthe merge algorithm (see Figure 1.6): we maintain pointers into both lists 1.3Processing Boolean queries11INTERSECT(p1, p2)1answer ←⟨⟩2while p1 ̸= NIL and p2 ̸= NIL3do if docID(p1) = docID(p2)4then ADD(answer, docID(p1))5p1 ←next(p1)6p2 ←next(p2)7else if docID(p1) < docID(p2)8then p1 ←next(p1)9else p2 ←next(p2)10return answer◮Figure 1.6Algorithm for the intersection of two postings lists p1 and p2.and walk through the two postings lists simultaneously, in time linear inthe total number of postings entries. At each step, we compare the docIDpointed to by both pointers. If they are the same, we put that docID in theresults list, and advance both pointers. Otherwise we advance the pointerpointing to the smaller docID. If the lengths of the postings lists are x andy, the intersection takes O(x + y) operations. Formally, the complexity ofquerying is Θ(N), where N is the number of documents in the collection.6Our indexing methods gain us just a constant, not a difference in Θ timecomplexity compared to a linear scan, but in practice the constant is huge.To use this algorithm, it is crucial that postings be sorted by a single globalordering. Using a numeric sort by docID is one simple way to achieve this.We can extend the intersection operation to process more complicated querieslike:(1.2)(Brutus OR Caesar) AND NOT CalpurniaQuery optimization is the process of selecting how to organize the work of an-QUERY OPTIMIZATIONswering a query so that the least total amount of work needs to be done bythe system. A major element of this for Boolean queries is the order in whichpostings lists are accessed. What is the best order for query processing? Con-sider a query that is an AND of t terms, for instance:(1.3)Brutus AND Caesar AND CalpurniaFor each of the t terms, we need to get its postings, then AND them together.The standard heuristic is to process terms in order of increasing document6. The notation Θ(·) is used to express an asymptotically tight bound on the complexity ofan algorithm. Informally, this is often written as O(·), but this notation really expresses anasymptotic upper bound, which need not be tight (Cormen et al. 1990). 121Boolean retrievalINTERSECT(⟨t1, . . . , tn⟩)1terms ←SORTBYINCREASINGFREQUENCY(⟨t1, . . . , tn⟩)2result ←postings( first(terms))3terms ←rest(terms)4while terms ̸= NIL and result ̸= NIL5do result ←INTERSECT(result, postings( first(terms)))6terms ←rest(terms)7return result◮Figure 1.7Algorithm for conjunctive queries that returns the set of documentscontaining each term in the input list of terms.frequency: if we start by intersecting the two smallest postings lists, then allintermediate results must be no bigger than the smallest postings list, and weare therefore likely to do the least amount of total work. So, for the postingslists in Figure 1.3 (page 7), we execute the above query as:(1.4)(Calpurnia AND Brutus) AND CaesarThis is a ﬁrst justiﬁcation for keeping the frequency of terms in the dictionary:it allows us to make this ordering decision based on in-memory data beforeaccessing any postings list.Consider now the optimization of more general queries, such as:(1.5)(madding OR crowd) AND (ignoble OR strife) AND (killed OR slain)As before, we will get the frequencies for all terms, and we can then (con-servatively) estimate the size of each OR by the sum of the frequencies of itsdisjuncts. We can then process the query in increasing order of the size ofeach disjunctive term.For arbitrary Boolean queries, we have to evaluate and temporarily storethe answers for intermediate expressions in a complex expression. However,in many circumstances, either because of the nature of the query language,or just because this is the most common type of query that users submit, aquery is purely conjunctive. In this case, rather than viewing merging post-ings lists as a function with two inputs and a distinct output, it is more ef-ﬁcient to intersect each retrieved postings list with the current intermediateresult in memory, where we initialize the intermediate result by loading thepostings list of the least frequent term. This algorithm is shown in Figure 1.7.The intersection operation is then asymmetric: the intermediate results listis in memory while the list it is being intersected with is being read fromdisk. Moreover the intermediate results list is always at least as short as theother list, and in many cases it is orders of magnitude shorter. The postings 1.3Processing Boolean queries13intersection can still be done by the algorithm in Figure 1.6, but when thedifference between the list lengths is very large, opportunities to use alter-native techniques open up. The intersection can be calculated in place bydestructively modifying or marking invalid items in the intermediate resultslist. Or the intersection can be done as a sequence of binary searches in thelong postings lists for each posting in the intermediate results list. Anotherpossibility is to store the long postings list as a hashtable, so that membershipof an intermediate result item can be calculated in constant rather than linearor log time. However, such alternative techniques are difﬁcult to combinewith postings list compression of the sort discussed in Chapter 5. Moreover,standard postings list intersection operations remain necessary when bothterms of a query are very common.?Exercise 1.4[⋆]For the queries below, can we still run through the intersection in time O(x + y),where x and y are the lengths of the postings lists for Brutus and Caesar? If not, whatcan we achieve?a. Brutus AND NOT Caesarb. Brutus OR NOT CaesarExercise 1.5[⋆]Extend the postings merge algorithm to arbitrary Boolean query formulas. What isits time complexity? For instance, consider:c. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)Can we always merge in linear time? Linear in what? Can we do better than this?Exercise 1.6[⋆⋆]We can use distributive laws for AND and OR to rewrite queries.a. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form usingthe distributive laws.b. Would the resulting query be more or less efﬁciently evaluated than the originalform of this query?c. Is this result true in general or does it depend on the words and the contents ofthe document collection?Exercise 1.7[⋆]Recommend a query processing order ford. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)given the following postings list sizes: 141Boolean retrievalTermPostings sizeeyes213312kaleidoscope87009marmalade107913skies271658tangerine46653trees316812Exercise 1.8[⋆]If the query is:e. friends AND romans AND (NOT countrymen)how could we use the frequency of countrymen in evaluating the best query evaluationorder? In particular, propose a way of handling negation in determining the order ofquery processing.Exercise 1.9[⋆⋆]For a conjunctive query, is processing postings lists in order of size guaranteed to beoptimal? Explain why it is, or give an example where it isn’t.Exercise 1.10[⋆⋆]Write out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an x OR yquery.Exercise 1.11[⋆⋆]How should the Boolean query x AND NOT y be handled? Why is naive evaluationof this query normally very expensive? Write out a postings merge algorithm thatevaluates this query efﬁciently.1.4The extended Boolean model versus ranked retrievalThe Boolean retrieval model contrasts with ranked retrieval models such as theRANKED RETRIEVALMODELvector space model (Section 6.3), in which users largely use free text queries,FREE TEXT QUERIESthat is, just typing one or more words rather than using a precise languagewith operators for building up query expressions, and the system decideswhich documents best satisfy the query. Despite decades of academic re-search on the advantages of ranked retrieval, systems implementing the Boo-lean retrieval model were the main or only search option provided by largecommercial information providers for three decades until the early 1990s (ap-proximately the date of arrival of the World Wide Web). However, thesesystems did not have just the basic Boolean operations (AND, OR, and NOT)which we have presented so far. A strict Boolean expression over terms withan unordered results set is too limited for many of the information needsthat people have, and these systems implemented extended Boolean retrievalmodels by incorporating additional operators such as term proximity oper-ators. A proximity operator is a way of specifying that two terms in a queryPROXIMITY OPERATOR 1.4The extended Boolean model versus ranked retrieval15must occur close to each other in a document, where closeness may be mea-sured by limiting the allowed number of intervening words or by referenceto a structural unit such as a sentence or paragraph.\\x0fExample 1.1: Commercial Boolean searching: Westlaw.Westlaw (http://www.westlaw.com/)is the largest commercial legal search service (in terms of the number of paying sub-scribers), with over half a million subscribers performing millions of searches a dayover tens of terabytes of text data. The service was started in 1975. In 2005, Booleansearch (called “Terms and Connectors” by Westlaw) was still the default, and usedby a large percentage of users, although ranked free text querying (called “NaturalLanguage” by Westlaw) was added in 1992. Here are some example Boolean querieson Westlaw:Information need: Information on the legal theories involved in preventing thedisclosure of trade secrets by employees formerly employed by a competingcompany. Query: \"trade secret\" /s disclos! /s prevent /s employe!Information need: Requirements for disabled people to be able to access a work-place.Query: disab! /p access! /s work-site work-place (employment /3 place)Information need: Cases about a host’s responsibility for drunk guests.Query: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guestNote the long, precise queries and the use of proximity operators, both uncommonin web search. Submitted queries average about ten words in length. Unlike websearch conventions, a space between words represents disjunction (the tightest bind-ing operator), & is AND and /s, /p, and /k ask for matches in the same sentence,same paragraph or within k words respectively. Double quotes give a phrase search(consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trail-ing wildcard query (see Section 3.2, page 51); thus liab! matches all words startingwith liab. Additionally work-site matches any of worksite, work-site or work site; seeSection 2.2.1 (page 22). Typical expert queries are usually carefully deﬁned and incre-mentally developed until they obtain what look to be good results to the user.Many users, particularly professionals, prefer Boolean query models.Booleanqueries are precise: a document either matches the query or it does not. This of-fers the user greater control and transparency over what is retrieved. And some do-mains, such as legal materials, allow an effective means of document ranking within aBoolean model: Westlaw returns documents in reverse chronological order, which isin practice quite effective. In 2007, the majority of law librarians still seem to rec-ommend terms and connectors for high recall searches, and the majority of legalusers think they are getting greater control by using them. However, this does notmean that Boolean queries are more effective for professional searchers. Indeed, ex-perimenting on a Westlaw subcollection, Turtle (1994) found that free text queriesproduced better results than Boolean queries prepared by Westlaw’s own referencelibrarians for the majority of the information needs in his experiments. A generalproblem with Boolean search is that using AND operators tends to produce high pre-cision but low recall searches, while using OR operators gives low precision but highrecall searches, and it is difﬁcult or impossible to ﬁnd a satisfactory middle ground.In this chapter, we have looked at the structure and construction of a basic 161Boolean retrievalinverted index, comprising a dictionary and postings lists. We introducedthe Boolean retrieval model, and examined how to do efﬁcient retrieval vialinear time merges and simple query optimization. In Chapters 2–7 we willconsider in detail richer query models and the sort of augmented index struc-tures that are needed to handle them efﬁciently. Here we just mention a fewof the main additional things we would like to be able to do:1. We would like to better determine the set of terms in the dictionary andto provide retrieval that is tolerant to spelling mistakes and inconsistentchoice of words.2. It is often useful to search for compounds or phrases that denote a conceptsuch as “operating system”. As the Westlaw examples show, we might alsowish to do proximity queries such as Gates NEAR Microsoft. To answersuch queries, the index has to be augmented to capture the proximities ofterms in documents.3. A Boolean model only records term presence or absence, but often wewould like to accumulate evidence, giving more weight to documents thathave a term several times as opposed to ones that contain it only once. Tobe able to do this we need term frequency information (the number of timesTERM FREQUENCYa term occurs in a document) in postings lists.4. Boolean queries just retrieve a set of matching documents, but commonlywe wish to have an effective method to order (or “rank”) the returnedresults. This requires having a mechanism for determining a documentscore which encapsulates how good a match a document is for a query.With these additional ideas, we will have seen most of the basic technol-ogy that supports ad hoc searching over unstructured information. Ad hocsearching over documents has recently conquered the world, powering notonly web search engines but the kind of unstructured search that lies behindthe large eCommerce websites. Although the main web search engines differby emphasizing free text querying, most of the basic issues and technologiesof indexing and querying remain the same, as we will see in later chapters.Moreover, over time, web search engines have added at least partial imple-mentations of some of the most popular operators from extended Booleanmodels: phrase search is especially popular and most have a very partialimplementation of Boolean operators. Nevertheless, while these options areliked by expert searchers, they are little used by most people and are not themain focus in work on trying to improve web search engine performance.?Exercise 1.12[⋆]Write a query using Westlaw syntax which would ﬁnd any of the words professor,teacher, or lecturer in the same sentence as a form of the verb explain. 1.5References and further reading17Exercise 1.13[⋆]Try using the Boolean search features on a couple of major web search engines. Forinstance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglarAND burglar, and (iii) burglar OR burglar. Look at the estimated number of results andtop hits. Do they make sense in terms of Boolean logic? Often they haven’t for majorsearch engines. Can you make sense of what is going on? What about if you trydifferent words? For example, query for (i) knight, (ii) conquer, and then (iii) knight ORconquer. What bound should the number of results from the ﬁrst two queries placeon the third query? Is this bound observed?1.5References and further readingThe practical pursuit of computerized information retrieval began in the late1940s (Cleverdon 1991, Liddy 2005). A great increase in the production ofscientiﬁc literature, much in the form of less formal technical reports ratherthan traditional journal articles, coupled with the availability of computers,led to interest in automatic document retrieval. However, in those days, doc-ument retrieval was always based on author, title, and keywords; full-textsearch came much later.The article of Bush (1945) provided lasting inspiration for the new ﬁeld:“Consider a future device for individual use, which is a sort of mech-anized private ﬁle and library. It needs a name, and, to coin one atrandom, ‘memex’ will do. A memex is a device in which an individualstores all his books, records, and communications, and which is mech-anized so that it may be consulted with exceeding speed and ﬂexibility.It is an enlarged intimate supplement to his memory.”The term Information Retrieval was coined by Calvin Mooers in 1948/1950(Mooers 1950).In 1958, much newspaper attention was paid to demonstrations at a con-ference (see Taube and Wooster 1958) of IBM “auto-indexing” machines, basedprimarily on the work of H. P. Luhn. Commercial interest quickly gravitatedtowards Boolean retrieval systems, but the early years saw a heady debateover various disparate technologies for retrieval systems. For example Moo-ers (1961) dissented:“It is a common fallacy, underwritten at this date by the investment ofseveral million dollars in a variety of retrieval hardware, that the al-gebra of George Boole (1847) is the appropriate formalism for retrievalsystem design. This view is as widely and uncritically accepted as it iswrong.”The observation of AND vs. OR giving you opposite extremes in a precision/recall tradeoff, but not the middle ground comes from (Lee and Fox 1988). 181Boolean retrievalThe book (Witten et al. 1999) is the standard reference for an in-depth com-parison of the space and time efﬁciency of the inverted index versus otherpossible data structures; a more succinct and up-to-date presentation ap-pears in Zobel and Moffat (2006). We further discuss several approaches inChapter 5.Friedl (2006) covers the practical usage of regular expressions for searching.REGULAR EXPRESSIONSThe underlying computer science appears in (Hopcroft et al. 2000).']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'40318Matrix decompositions and latentsemantic indexingOn page 123 we introduced the notion of a term-document matrix: an M × Nmatrix C, each of whose rows represents a term and each of whose columnsrepresents a document in the collection. Even for a collection of modest size,the term-document matrix C is likely to have several tens of thousands ofrows and columns. In Section 18.1.1 we ﬁrst develop a class of operationsfrom linear algebra, known as matrix decomposition. In Section 18.2 we use aspecial form of matrix decomposition to construct a low-rank approximationto the term-document matrix. In Section 18.3 we examine the applicationof such low-rank approximations to indexing and retrieving documents, atechnique referred to as latent semantic indexing. While latent semantic in-dexing has not been established as a signiﬁcant force in scoring and rankingfor information retrieval, it remains an intriguing approach to clustering in anumber of domains including for collections of text documents (Section 16.6,page 372). Understanding its full potential remains an area of active research.Readers who do not require a refresher on linear algebra may skip Sec-tion 18.1, although Example 18.1 is especially recommended as it highlightsa property of eigenvalues that we exploit later in the chapter.18.1Linear algebra reviewWe brieﬂy review some necessary background in linear algebra. Let C bean M × N matrix with real-valued entries; for a term-document matrix, allentries are in fact non-negative. The rank of a matrix is the number of linearlyRANKindependent rows (or columns) in it; thus, rank(C) ≤min{M, N}. A squarer × r matrix all of whose off-diagonal entries are zero is called a diagonalmatrix; its rank is equal to the number of non-zero diagonal entries. If allr diagonal entries of such a diagonal matrix are 1, it is called the identitymatrix of dimension r and represented by Ir.For a square M × M matrix C and a vector ⃗x that is not all zeros, the values 40418Matrix decompositions and latent semantic indexingof λ satisfyingC⃗x = λ⃗x(18.1)are called the eigenvalues of C . The N-vector ⃗x satisfying Equation (18.1)EIGENVALUEfor an eigenvalue λ is the corresponding right eigenvector. The eigenvectorcorresponding to the eigenvalue of largest magnitude is called the principaleigenvector. In a similar fashion, the left eigenvectors of C are the M-vectors ysuch that⃗yT C = λ⃗yT.(18.2)The number of non-zero eigenvalues of C is at most rank(C).The eigenvalues of a matrix are found by solving the characteristic equation,which is obtained by rewriting Equation (18.1) in the form (C −λIM)⃗x = 0.The eigenvalues of C are then the solutions of |(C −λIM)| = 0, where |S|denotes the determinant of a square matrix S. The equation |(C −λIM)| = 0is an Mth order polynomial equation in λ and can have at most M roots,which are the eigenvalues of C. These eigenvalues can in general be complex,even if all entries of C are real.We now examine some further properties of eigenvalues and eigenvectors,to set up the central idea of singular value decompositions in Section 18.2 be-low. First, we look at the relationship between matrix-vector multiplicationand eigenvalues.\\x0fExample 18.1:Consider the matrixS =\\uf8eb\\uf8ed30000200001\\uf8f6\\uf8f8.Clearly the matrix has rank 3, and has 3 non-zero eigenvalues λ1 = 30, λ2 = 20 andλ3 = 1, with the three corresponding eigenvectors⃗x1 =\\uf8eb\\uf8ed100\\uf8f6\\uf8f8, ⃗x2 =\\uf8eb\\uf8ed010\\uf8f6\\uf8f8and ⃗x3 =\\uf8eb\\uf8ed001\\uf8f6\\uf8f8.For each of the eigenvectors, multiplication by S acts as if we were multiplying theeigenvector by a multiple of the identity matrix; the multiple is different for eacheigenvector. Now, consider an arbitrary vector, such as ⃗v =\\uf8eb\\uf8ed246\\uf8f6\\uf8f8. We can alwaysexpress⃗v as a linear combination of the three eigenvectors of S; in the current examplewe have⃗v =\\uf8eb\\uf8ed246\\uf8f6\\uf8f8= 2⃗x1 + 4⃗x2 + 6⃗x3. 18.1Linear algebra review405Suppose we multiply⃗v by S:S⃗v=S(2⃗x1 + 4⃗x2 + 6⃗x3)=2S⃗x1 + 4S⃗x2 + 6S⃗x3=2λ1⃗x1 + 4λ2⃗x2 + 6λ3⃗x3=60⃗x1 + 80⃗x2 + 6⃗x3.(18.3)Example 18.1 shows that even though ⃗v is an arbitrary vector, the effect ofmultiplication by S is determined by the eigenvalues and eigenvectors of S.Furthermore, it is intuitively apparent from Equation (18.3) that the productS⃗v is relatively unaffected by terms arising from the small eigenvalues of S;in our example, since λ3 = 1, the contribution of the third term on the righthand side of Equation (18.3) is small. In fact, if we were to completely ignorethe contribution in Equation (18.3) from the third eigenvector correspondingto λ3 = 1, then the product S⃗v would be computed to be\\uf8eb\\uf8ed60800\\uf8f6\\uf8f8rather thanthe correct product which is\\uf8eb\\uf8ed60806\\uf8f6\\uf8f8; these two vectors are relatively closeto each other by any of various metrics one could apply (such as the lengthof their vector difference).This suggests that the effect of small eigenvalues (and their eigenvectors)on a matrix-vector product is small. We will carry forward this intuitionwhen studying matrix decompositions and low-rank approximations in Sec-tion 18.2. Before doing so, we examine the eigenvectors and eigenvalues ofspecial forms of matrices that will be of particular interest to us.For a symmetric matrix S, the eigenvectors corresponding to distinct eigen-values are orthogonal. Further, if S is both real and symmetric, the eigenvaluesare all real.\\x0fExample 18.2:Consider the real, symmetric matrixS =\\x12 2112\\x13.(18.4)From the characteristic equation |S −λI| = 0, we have the quadratic (2 −λ)2 −1 =0, whose solutions yield the eigenvalues 3 and 1. The corresponding eigenvectors\\x121−1\\x13and\\x12 11\\x13are orthogonal. 40618Matrix decompositions and latent semantic indexing18.1.1Matrix decompositionsIn this section we examine ways in which a square matrix can be factoredinto the product of matrices derived from its eigenvectors; we refer to thisprocess as matrix decomposition. Matrix decompositions similar to the onesMATRIXDECOMPOSITIONin this section will form the basis of our principal text-analysis techniquein Section 18.3, where we will look at decompositions of non-square term-document matrices. The square decompositions in this section are simplerand can be treated with sufﬁcient mathematical rigor to help the reader un-derstand how such decompositions work. The detailed mathematical deriva-tion of the more complex decompositions in Section 18.2 are beyond thescope of this book.We begin by giving two theorems on the decomposition of a square ma-trix into the product of three matrices of a special form. The ﬁrst of these,Theorem 18.1, gives the basic factorization of a square real-valued matrixinto three factors. The second, Theorem 18.2, applies to square symmetricmatrices and is the basis of the singular value decomposition described inTheorem 18.3.Theorem 18.1. (Matrix diagonalization theorem) Let S be a square real-valuedM × M matrix with M linearly independent eigenvectors. Then there exists aneigen decompositionEIGEN DECOMPOSITIONS = UΛU−1,(18.5)where the columns of U are the eigenvectors of S and Λ is a diagonal matrix whosediagonal entries are the eigenvalues of S in decreasing order\\uf8eb\\uf8ec\\uf8ec\\uf8edλ1λ2· · ·λM\\uf8f6\\uf8f7\\uf8f7\\uf8f8, λi ≥λi+1.(18.6)If the eigenvalues are distinct, then this decomposition is unique.To understand how Theorem 18.1 works, we note that U has the eigenvec-tors of S as columnsU = (⃗u1 ⃗u2 · · · ⃗uM) .(18.7)Then we haveSU=S (⃗u1 ⃗u2 · · · ⃗uM)=(λ1⃗u1 λ2⃗u2 · · · λM ⃗uM)=(⃗u1 ⃗u2 · · · ⃗uM)\\uf8eb\\uf8ec\\uf8ec\\uf8edλ1λ2· · ·λM\\uf8f6\\uf8f7\\uf8f7\\uf8f8. 18.2Term-document matrices and singular value decompositions407Thus, we have SU = UΛ, or S = UΛU−1.We next state a closely related decomposition of a symmetric square matrixinto the product of matrices derived from its eigenvectors. This will pave theway for the development of our main tool for text analysis, the singular valuedecomposition (Section 18.2).Theorem 18.2. (Symmetric diagonalization theorem) Let S be a square, sym-metric real-valued M × M matrix with M linearly independent eigenvectors. Thenthere exists a symmetric diagonal decompositionSYMMETRIC DIAGONALDECOMPOSITIONS = QΛQT,(18.8)where the columns of Q are the orthogonal and normalized (unit length, real) eigen-vectors of S, and Λ is the diagonal matrix whose entries are the eigenvalues of S.Further, all entries of Q are real and we have Q−1 = QT.We will build on this symmetric diagonal decomposition to build low-rankapproximations to term-document matrices.?Exercise 18.1What is the rank of the 3 × 3 diagonal matrix below?\\uf8eb\\uf8ed110011121\\uf8f6\\uf8f8Exercise 18.2Show that λ = 2 is an eigenvalue ofC =\\x12 6−240\\x13.Find the corresponding eigenvector.Exercise 18.3Compute the unique eigen decomposition of the 2 × 2 matrix in (18.4).18.2Term-document matrices and singular value decompositionsThe decompositions we have been studying thus far apply to square matri-ces. However, the matrix we are interested in is the M × N term-documentmatrix C where (barring a rare coincidence) M ̸= N; furthermore, C is veryunlikely to be symmetric. To this end we ﬁrst describe an extension of thesymmetric diagonal decomposition known as the singular value decomposi-SINGULAR VALUEDECOMPOSITIONtion. We then show in Section 18.3 how this can be used to construct an ap-proximate version of C. It is beyond the scope of this book to develop a full 40818Matrix decompositions and latent semantic indexingtreatment of the mathematics underlying singular value decompositions; fol-lowing the statement of Theorem 18.3 we relate the singular value decompo-sition to the symmetric diagonal decompositions from Section 18.1.1. GivenSYMMETRIC DIAGONALDECOMPOSITIONC, let U be the M × M matrix whose columns are the orthogonal eigenvec-tors of CCT, and V be the N × N matrix whose columns are the orthogonaleigenvectors of CTC. Denote by CT the transpose of a matrix C.Theorem 18.3. Let r be the rank of the M × N matrix C. Then, there is a singular-value decomposition (SVD for short) of C of the formSVDC = UΣVT,(18.9)where1. The eigenvalues λ1, . . . , λr of CCT are the same as the eigenvalues of CTC;2. For 1 ≤i ≤r, let σi = √λi, with λi ≥λi+1. Then the M × N matrix Σ iscomposed by setting Σii = σi for 1 ≤i ≤r, and zero otherwise.The values σi are referred to as the singular values of C. It is instructive toexamine the relationship of Theorem 18.3 to Theorem 18.2; we do this ratherthan derive the general proof of Theorem 18.3, which is beyond the scope ofthis book.By multiplying Equation (18.9) by its transposed version, we haveCCT = UΣVT VΣUT = UΣ2UT.(18.10)Note now that in Equation (18.10), the left-hand side is a square symmetricmatrix real-valued matrix, and the right-hand side represents its symmetricdiagonal decomposition as in Theorem 18.2. What does the left-hand sideCCT represent? It is a square matrix with a row and a column correspond-ing to each of the M terms. The entry (i, j) in the matrix is a measure of theoverlap between the ith and jth terms, based on their co-occurrence in docu-ments. The precise mathematical meaning depends on the manner in whichC is constructed based on term weighting. Consider the case where C is theterm-document incidence matrix of page 3, illustrated in Figure 1.1. Then theentry (i, j) in CCT is the number of documents in which both term i and termj occur.When writing down the numerical values of the SVD, it is conventionalto represent Σ as an r × r matrix with the singular values on the diagonals,since all its entries outside this sub-matrix are zeros. Accordingly, it is con-ventional to omit the rightmost M −r columns of U corresponding to theseomitted rows of Σ; likewise the rightmost N −r columns of V are omittedsince they correspond in VT to the rows that will be multiplied by the N −rcolumns of zeros in Σ. This written form of the SVD is sometimes known 18.2Term-document matrices and singular value decompositions409rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrC=UΣVTrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr◮Figure 18.1Illustration of the singular-value decomposition. In this schematicillustration of (18.9), we see two cases illustrated. In the top half of the ﬁgure, wehave a matrix C for which M > N. The lower half illustrates the case M < N.as the reduced SVD or truncated SVD and we will encounter it again in Ex-REDUCED SVDTRUNCATED SVDercise 18.9. Henceforth, our numerical examples and exercises will use thisreduced form.\\x0fExample 18.3:We now illustrate the singular-value decomposition of a 4 × 2 ma-trix of rank 2; the singular values are Σ11 = 2.236 and Σ22 = 1.C =\\uf8eb\\uf8ec\\uf8ec\\uf8ed1−10110−11\\uf8f6\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\uf8ec\\uf8ec\\uf8ed−0.6320.0000.316−0.707−0.316−0.7070.6320.000\\uf8f6\\uf8f7\\uf8f7\\uf8f8\\x122.2360.0000.0001.000\\x13 \\x12 −0.7070.707−0.707−0.707\\x13.(18.11)As with the matrix decompositions deﬁned in Section 18.1.1, the singu-lar value decomposition of a matrix can be computed by a variety of algo-rithms, many of which have been publicly available software implementa-tions; pointers to these are given in Section 18.5.?Exercise 18.4LetC =\\uf8eb\\uf8ed110110\\uf8f6\\uf8f8(18.12)be the term-document incidence matrix for a collection. Compute the co-occurrencematrix CCT. What is the interpretation of the diagonal entries of CCT when C is aterm-document incidence matrix? 41018Matrix decompositions and latent semantic indexingExercise 18.5Verify that the SVD of the matrix in Equation (18.12) isU =\\uf8eb\\uf8ed−0.8160.000−0.408−0.707−0.4080.707\\uf8f6\\uf8f8, Σ =\\x12 1.7320.0000.0001.000\\x13and VT =\\x12 −0.707−0.7070.707−0.707\\x13,(18.13)by verifying all of the properties in the statement of Theorem 18.3.Exercise 18.6Suppose that C is a binary term-document incidence matrix. What do the entries ofCTC represent?Exercise 18.7LetC =\\uf8eb\\uf8ed021030210\\uf8f6\\uf8f8(18.14)be a term-document matrix whose entries are term frequencies; thus term 1 occurs 2times in document 2 and once in document 3. Compute CCT; observe that its entriesare largest where two terms have their most frequent occurrences together in the samedocument.18.3Low-rank approximationsWe next state a matrix approximation problem that at ﬁrst seems to havelittle to do with information retrieval. We describe a solution to this matrixproblem using singular-value decompositions, then develop its applicationto information retrieval.Given an M × N matrix C and a positive integer k, we wish to ﬁnd anM × N matrix Ck of rank at most k, so as to minimize the Frobenius norm ofFROBENIUS NORMthe matrix difference X = C −Ck, deﬁned to be∥X∥F =vuutM∑i=1N∑j=1X2ij.(18.15)Thus, the Frobenius norm of X measures the discrepancy between Ck and C;our goal is to ﬁnd a matrix Ck that minimizes this discrepancy, while con-straining Ck to have rank at most k. If r is the rank of C, clearly Cr = Cand the Frobenius norm of the discrepancy is zero in this case. When k is farsmaller than r, we refer to Ck as a low-rank approximation.LOW-RANKAPPROXIMATIONThe singular value decomposition can be used to solve the low-rank ma-trix approximation problem. We then derive from it an application to ap-proximating term-document matrices. We invoke the following three-stepprocedure to this end: 18.3Low-rank approximations411Ck=UΣkVTrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr◮Figure 18.2Illustration of low rank approximation using the singular-value de-composition. The dashed boxes indicate the matrix entries affected by “zeroing out”the smallest singular values.1. Given C, construct its SVD in the form shown in (18.9); thus, C = UΣVT.2. Derive from Σ the matrix Σk formed by replacing by zeros the r −k small-est singular values on the diagonal of Σ.3. Compute and output Ck = UΣkVT as the rank-k approximation to C.The rank of Ck is at most k: this follows from the fact that Σk has at mostk non-zero values. Next, we recall the intuition of Example 18.1: the effectof small eigenvalues on matrix products is small. Thus, it seems plausiblethat replacing these small eigenvalues by zero will not substantially alter theproduct, leaving it “close” to C. The following theorem due to Eckart andYoung tells us that, in fact, this procedure yields the matrix of rank k withthe lowest possible Frobenius error.Theorem 18.4.minZ| rank(Z)=k∥C −Z∥F = ∥C −Ck∥F = σk+1.(18.16)Recalling that the singular values are in decreasing order σ1 ≥σ2 ≥· · ·,we learn from Theorem 18.4 that Ck is the best rank-k approximation to C,incurring an error (measured by the Frobenius norm of C −Ck) equal to σk+1.Thus the larger k is, the smaller this error (and in particular, for k = r, theerror is zero since Σr = Σ; provided r < M, N, then σr+1 = 0 and thusCr = C).To derive further insight into why the process of truncating the smallestr −k singular values in Σ helps generate a rank-k approximation of low error,we examine the form of Ck:Ck=UΣkVT(18.17) 41218Matrix decompositions and latent semantic indexing=U\\uf8eb\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edσ100000· · ·00000σk00000000000· · ·\\uf8f6\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8VT(18.18)=k∑i=1σi⃗ui⃗vTi ,(18.19)where ⃗ui and ⃗vi are the ith columns of U and V, respectively. Thus, ⃗ui⃗vTi isa rank-1 matrix, so that we have just expressed Ck as the sum of k rank-1matrices each weighted by a singular value. As i increases, the contributionof the rank-1 matrix ⃗ui⃗vTi is weighted by a sequence of shrinking singularvalues σi.?Exercise 18.8Compute a rank 1 approximation C1 to the matrix C in Example 18.12, using the SVDas in Exercise 18.13. What is the Frobenius norm of the error of this approximation?Exercise 18.9Consider now the computation in Exercise 18.8.Following the schematic in Fig-ure 18.2, notice that for a rank 1 approximation we have σ1 being a scalar. Denoteby U1 the ﬁrst column of U and by V1 the ﬁrst column of V. Show that the rank-1approximation to C can then be written as U1σ1VT1 = σ1U1VT1 .Exercise 18.10Exercise 18.9 can be generalized to rank k approximations: we let U′k and V′k denotethe “reduced” matrices formed by retaining only the ﬁrst k columns of U and V,respectively. Thus U′k is an M × k matrix while V′Tk is a k × N matrix. Then, we haveCk = U′kΣ′kV′Tk ,(18.20)where Σ′k is the square k × k submatrix of Σk with the singular values σ1, . . . , σk onthe diagonal. The primary advantage of using (18.20) is to eliminate a lot of redun-dant columns of zeros in U and V, thereby explicitly eliminating multiplication bycolumns that do not affect the low-rank approximation; this version of the SVD issometimes known as the reduced SVD or truncated SVD and is a computationallysimpler representation from which to compute the low rank approximation.For the matrix C in Example 18.3, write down both Σ2 and Σ′2.18.4Latent semantic indexingWe now discuss the approximation of a term-document matrix C by one oflower rank using the SVD. The low-rank approximation to C yields a newrepresentation for each document in the collection.We will cast queries 18.4Latent semantic indexing413into this low-rank representation as well, enabling us to compute query-document similarity scores in this low-rank representation. This process isknown as latent semantic indexing (generally abbreviated LSI).LATENT SEMANTICINDEXINGBut ﬁrst, we motivate such an approximation. Recall the vector space rep-resentation of documents and queries introduced in Section 6.3 (page 120).This vector space representation enjoys a number of advantages includingthe uniform treatment of queries and documents as vectors, the inducedscore computation based on cosine similarity, the ability to weight differ-ent terms differently, and its extension beyond document retrieval to suchapplications as clustering and classiﬁcation. The vector space representa-tion suffers, however, from its inability to cope with two classic problemsarising in natural languages: synonymy and polysemy. Synonymy refers to acase where two different words (say car and automobile) have the same mean-ing. Because the vector space representation fails to capture the relationshipbetween synonymous terms such as car and automobile – according each aseparate dimension in the vector space. Consequently the computed simi-larity⃗q · ⃗d between a query⃗q (say, car) and a document ⃗d containing both carand automobile underestimates the true similarity that a user would perceive.Polysemy on the other hand refers to the case where a term such as chargehas multiple meanings, so that the computed similarity ⃗q · ⃗d overestimatesthe similarity that a user would perceive. Could we use the co-occurrencesof terms (whether, for instance, charge occurs in a document containing steedversus in a document containing electron) to capture the latent semantic as-sociations of terms and alleviate these problems?Even for a collection of modest size, the term-document matrix C is likelyto have several tens of thousand of rows and columns, and a rank in thetens of thousands as well. In latent semantic indexing (sometimes referredto as latent semantic analysis (LSA)), we use the SVD to construct a low-rankLSAapproximation Ck to the term-document matrix, for a value of k that is farsmaller than the original rank of C. In the experimental work cited laterin this section, k is generally chosen to be in the low hundreds. We thusmap each row/column (respectively corresponding to a term/document) toa k-dimensional space; this space is deﬁned by the k principal eigenvectors(corresponding to the largest eigenvalues) of CCT and CTC. Note that thematrix Ck is itself still an M × N matrix, irrespective of k.Next, we use the new k-dimensional LSI representation as we did the orig-inal representation – to compute similarities between vectors. A query vector⃗q is mapped into its representation in the LSI space by the transformation⃗qk = Σ−1k UTk ⃗q.(18.21)Now, we may use cosine similarities as in Section 6.3.1 (page 120) to com-pute the similarity between a query and a document, between two docu- 41418Matrix decompositions and latent semantic indexingments, or between two terms. Note especially that Equation (18.21) does notin any way depend on ⃗q being a query; it is simply a vector in the space ofterms. This means that if we have an LSI representation of a collection ofdocuments, a new document not in the collection can be “folded in” to thisrepresentation using Equation (18.21). This allows us to incrementally adddocuments to an LSI representation. Of course, such incremental additionfails to capture the co-occurrences of the newly added documents (and evenignores any new terms they contain). As such, the quality of the LSI rep-resentation will degrade as more documents are added and will eventuallyrequire a recomputation of the LSI representation.The ﬁdelity of the approximation of Ck to C leads us to hope that the rel-ative values of cosine similarities are preserved: if a query is close to a doc-ument in the original space, it remains relatively close in the k-dimensionalspace. But this in itself is not sufﬁciently interesting, especially given thatthe sparse query vector ⃗q turns into a dense query vector ⃗qk in the low-dimensional space. This has a signiﬁcant computational cost, when com-pared with the cost of processing⃗q in its native form.\\x0fExample 18.4:Consider the term-document matrix C =d1d2d3d4d5d6ship101000boat010000ocean110000voyage100110trip000101Its singular value decomposition is the product of three matrices as below. First wehave U which in this example is:12345ship−0.44−0.300.570.580.25boat−0.13−0.33−0.590.000.73ocean−0.48−0.51−0.370.00−0.61voyage−0.700.350.15−0.580.16trip−0.260.65−0.410.58−0.09When applying the SVD to a term-document matrix, U is known as the SVD termmatrix. The singular values are Σ =2.160.000.000.000.000.001.590.000.000.000.000.001.280.000.000.000.000.001.000.000.000.000.000.000.39Finally we have VT, which in the context of a term-document matrix is known asthe SVD document matrix: 18.4Latent semantic indexing415d1d2d3d4d5d61−0.75−0.28−0.20−0.45−0.33−0.122−0.29−0.53−0.190.630.220.4130.28−0.750.45−0.200.12−0.3340.000.000.580.00−0.580.585−0.530.290.630.190.41−0.22By “zeroing out” all but the two largest singular values of Σ, we obtain Σ2 =2.160.000.000.000.000.001.590.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00From this, we compute C2 =d1d2d3d4d5d61−1.62−0.60−0.44−0.97−0.70−0.262−0.46−0.84−0.301.000.350.6530.000.000.000.000.000.0040.000.000.000.000.000.0050.000.000.000.000.000.00Notice that the low-rank approximation, unlike the original matrix C, can havenegative entries.Examination of C2 and Σ2 in Example 18.4 shows that the last 3 rows ofeach of these matrices are populated entirely by zeros. This suggests thatthe SVD product UΣVT in Equation (18.18) can be carried out with only tworows in the representations of Σ2 and VT; we may then replace these matricesby their truncated versions Σ′2 and (V′)T. For instance, the truncated SVDdocument matrix (V′)T in this example is:d1d2d3d4d5d61−1.62−0.60−0.44−0.97−0.70−0.262−0.46−0.84−0.301.000.350.65Figure 18.3 illustrates the documents in (V′)T in two dimensions. Notealso that C2 is dense relative to C.We may in general view the low-rank approximation of C by Ck as a con-strained optimization problem: subject to the constraint that Ck have rank atmost k, we seek a representation of the terms and documents comprising Cwith low Frobenius norm for the error C −Ck. When forced to squeeze theterms/documents down to a k-dimensional space, the SVD should bring to-gether terms with similar co-occurrences. This intuition suggests, then, thatnot only should retrieval quality not suffer too much from the dimensionreduction, but in fact may improve. 41618Matrix decompositions and latent semantic indexing−0.5−1.0−1.50.51.0−0.5−1.0dim 2dim 1×d1×d2× d3×d4×d5× d6◮Figure 18.3The documents of Example 18.4 reduced to two dimensions in (V′)T.Dumais (1993) and Dumais (1995) conducted experiments with LSI onTREC documents and tasks, using the commonly-used Lanczos algorithmto compute the SVD. At the time of their work in the early 1990’s, the LSIcomputation on tens of thousands of documents took approximately a dayon one machine. On these experiments, they achieved precision at or abovethat of the median TREC participant. On about 20% of TREC topics theirsystem was the top scorer, and reportedly slightly better on average thanstandard vector spaces for LSI at about 350 dimensions. Here are some con-clusions on LSI ﬁrst suggested by their work, and subsequently veriﬁed bymany other experiments.• The computational cost of the SVD is signiﬁcant; at the time of this writ-ing, we know of no successful experiment with over one million docu-ments. This has been the biggest obstacle to the widespread adoption toLSI. One approach to this obstacle is to build the LSI representation on arandomly sampled subset of the documents in the collection, followingwhich the remaining documents are “folded in” as detailed with Equa-tion (18.21). 18.5References and further reading417• As we reduce k, recall tends to increase, as expected.• Most surprisingly, a value of k in the low hundreds can actually increaseprecision on some query benchmarks. This appears to suggest that for asuitable value of k, LSI addresses some of the challenges of synonymy.• LSI works best in applications where there is little overlap between queriesand documents.The experiments also documented some modes where LSI failed to matchthe effectiveness of more traditional indexes and score computations. Mostnotably (and perhaps obviously), LSI shares two basic drawbacks of vectorspace retrieval: there is no good way of expressing negations (ﬁnd docu-ments that contain german but not shepherd), and no way of enforcing Booleanconditions.LSI can be viewed as soft clustering by interpreting each dimension of theSOFT CLUSTERINGreduced space as a cluster and the value that a document has on that dimen-sion as its fractional membership in that cluster.18.5References and further readingStrang (1986) provides an excellent introductory overview of matrix decom-positions including the singular value decomposition. Theorem 18.4 is dueto Eckart and Young (1936). The connection between information retrievaland low-rank approximations of the term-document matrix was introducedin Deerwester et al. (1990), with a subsequent survey of results in Berryet al. (1995). Dumais (1993) and Dumais (1995) describe experiments onTREC benchmarks giving evidence that at least on some benchmarks, LSIcan produce better precision and recall than standard vector-space retrieval.http://www.cs.utk.edu/˜berry/lsi++/and http://lsi.argreenhouse.com/lsi/LSIpapers.htmloffer comprehensive pointers to the literature and software of LSI. Schützeand Silverstein (1997) evaluate LSI and truncated representations of cen-troids for efﬁcient K-means clustering (Section 16.4). Bast and Majumdar(2005) detail the role of the reduced dimension k in LSI and how differentpairs of terms get coalesced together at differing values of k. Applications ofLSI to cross-language information retrieval (where documents in two or moreCROSS-LANGUAGEINFORMATIONRETRIEVALdifferent languages are indexed, and a query posed in one language is ex-pected to retrieve documents in other languages) are developed in Berry andYoung (1995) and Littman et al. (1998). LSI (referred to as LSA in more gen-eral settings) has been applied to host of other problems in computer scienceranging from memory modeling to computer vision.Hofmann (1999a;b) provides an initial probabilistic extension of the basiclatent semantic indexing technique. A more satisfactory formal basis for a 41818Matrix decompositions and latent semantic indexingDocIDDocument text1hello2open house3mi casa4hola Profesor5hola y bienvenido6hello and welcome◮Figure 18.4Documents for Exercise 18.11.SpanishEnglishmimycasahouseholahelloprofesorprofessoryandbienvenidowelcome◮Figure 18.5Glossary for Exercise 18.11.probabilistic latent variable model for dimensionality reduction is the LatentDirichlet Allocation (LDA) model (Blei et al. 2003), which is generative andassigns probabilities to documents outside of the training set. This model isextended to a hierarchical clustering by Rosen-Zvi et al. (2004). Wei and Croft(2006) present the ﬁrst large scale evaluation of LDA, ﬁnding it to signiﬁ-cantly outperform the query likelihood model of Section 12.2 (page 242), butto not perform quite as well as the relevance model mentioned in Section 12.4(page 250) – but the latter does additional per-query processing unlike LDA.Teh et al. (2006) generalize further by presenting Hierarchical Dirichlet Pro-cesses, a probabilistic model which allows a group (for us, a document) tobe drawn from an inﬁnite mixture of latent topics, while still allowing thesetopics to be shared across documents.?Exercise 18.11Assume you have a set of documents each of which is in either English or in Spanish.The collection is given in Figure 18.4.Figure 18.5 gives a glossary relating the Spanish and English words above for yourown information. This glossary is NOT available to the retrieval system:1. Construct the appropriate term-document matrix C to use for a collection con-sisting of these documents. For simplicity, use raw term frequencies rather thannormalized tf-idf weights. Make sure to clearly label the dimensions of your ma-trix. 18.5References and further reading4192. Write down the matrices U2, Σ′2 and V2 and from these derive the rank 2 approxi-mation C2.3. State succinctly what the (i, j) entry in the matrix CTC represents.4. State succinctly what the (i, j) entry in the matrix CT2 C2 represents, and why itdiffers from that in CTC. '"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(corpus_df.iloc[indices.flatten()]['removed_trash_text'].tolist())\n",
    "corpus_df.iloc[indices.flatten()]['removed_trash_text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9ec07",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1 in organization org-JPMJiOUIlC0lMChqy2n1Pljh on tokens per min (TPM): Limit 30000, Requested 51236. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[255]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mEres una aplicacion de Retrieval Augmented Generation que siempre responde en español.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mUsa el siguiente contexto para responder a la pregunta del usuario.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33mEl usuario esta preguntando sobre \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:735\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    704\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    734\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/EPN/ir/ir-2025A/.venv/lib/python3.12/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4.1 in organization org-JPMJiOUIlC0lMChqy2n1Pljh on tokens per min (TPM): Limit 30000, Requested 51236. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Eres una aplicacion de Retrieval Augmented Generation que siempre responde en español.\n",
    "Usa el siguiente contexto para responder a la pregunta del usuario.\n",
    "Si la respuesta no se encuentra en el contexto, responde \"No tengo suficiente información para responder a esa pregunta\".\n",
    "\n",
    "Contexto:\n",
    "{corpus_df.iloc[indices.flatten()]['removed_trash_text'].tolist()}\n",
    "\n",
    "Pregunta:\n",
    "El usuario esta preguntando sobre \"{query}\".\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=prompt\n",
    ")\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
